Title,Abstract,Bib Item,Matched Search Term 1,Matched Search Term 2,Matched Search Term 4
" ""Discourse Mode Categorization of {B}engali Social Media Health Text"","," ""The scarcity of annotated data is a major impediment to natural language processing (NLP) research in Bengali, a language that is considered low-resource. In particular, the health and medical domains suffer from a severe paucity of annotated data. Thus, this study aims to introduce BanglaSocialHealth, an annotated social media health corpus that provides sentence-level annotations of four distinct types of expression modes, namely narrative (NAR), informative (INF), suggestive (SUG), and inquiring (INQ) modes in Bengali. We provide details regarding the annotation procedures and report various statistics, such as the median and mean length of words in different sentence modes. Additionally, we apply classical machine learning (CML) classifiers and transformer-based language models to classify sentence modes. We find that most of the statistical properties are similar in different types of sentence modes. To determine the sentence mode, the transformer-based M-BERT model provides slightly better efficacy than the CML classifiers. Our developed corpus and analysis represent a much-needed contribution to Bengali NLP research in medical and health domains and have the potential to facilitate a range of downstream tasks, including question-answering, misinformation detection, and information retrieval."",","{sazzed-2023-discourse,
    title = ""Discourse Mode Categorization of {B}engali Social Media Health Text"",
    author = ""Sazzed, Salim"",
    editor = ""Barnes, Jeremy  and
      De Clercq, Orph{\'e}e  and
      Klinger, Roman"",
    booktitle = ""Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, {\&} Social Media Analysis"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.wassa-1.6"",
    doi = ""10.18653/v1/2023.wassa-1.6"",
    pages = ""52--57"",
    abstract = ""The scarcity of annotated data is a major impediment to natural language processing (NLP) research in Bengali, a language that is considered low-resource. In particular, the health and medical domains suffer from a severe paucity of annotated data. Thus, this study aims to introduce BanglaSocialHealth, an annotated social media health corpus that provides sentence-level annotations of four distinct types of expression modes, namely narrative (NAR), informative (INF), suggestive (SUG), and inquiring (INQ) modes in Bengali. We provide details regarding the annotation procedures and report various statistics, such as the median and mean length of words in different sentence modes. Additionally, we apply classical machine learning (CML) classifiers and transformer-based language models to classify sentence modes. We find that most of the statistical properties are similar in different types of sentence modes. To determine the sentence mode, the transformer-based M-BERT model provides slightly better efficacy than the CML classifiers. Our developed corpus and analysis represent a much-needed contribution to Bengali NLP research in medical and health domains and have the potential to facilitate a range of downstream tasks, including question-answering, misinformation detection, and information retrieval."",
}
@",medical domain,natural language process,natural languag,language process,nlp,information retriev,sentiment,annotat,question-answ
" ""{YNU}-{HPCC} at {S}em{E}val-2023 Task7: Multi-evidence Natural Language Inference for Clinical Trial Data Based a {B}io{BERT} Model"","," ""This paper describes the system for the YNU-HPCC team in subtask 1 of the SemEval-2023 Task 7: Multi-evidence Natural Language Inference for Clinical Trial Data (NLI4CT). This task requires judging the textual entailment relationship between the given CTR and the statement annotated by the expert annotator. This system is based on the fine-tuned Bi-directional Encoder Representation from Transformers for Biomedical Text Mining (BioBERT) model with supervised contrastive learning and back translation. Supervised contrastive learning is to enhance the classification, and back translation is to enhance the training data. Our system achieved relatively good results on the competition{'}s official leaderboard. The code of this paper is available at \url{https://github.com/facanhe/SemEval-2023-Task7}."",","{feng-etal-2023-ynu,
    title = ""{YNU}-{HPCC} at {S}em{E}val-2023 Task7: Multi-evidence Natural Language Inference for Clinical Trial Data Based a {B}io{BERT} Model"",
    author = ""Feng, Chao  and
      Wang, Jin  and
      Zhang, Xuejie"",
    editor = {Ojha, Atul Kr.  and
      Do{\u{g}}ru{\""o}z, A. Seza  and
      Da San Martino, Giovanni  and
      Tayyar Madabushi, Harish  and
      Kumar, Ritesh  and
      Sartori, Elisa},
    booktitle = ""Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.semeval-1.91"",
    doi = ""10.18653/v1/2023.semeval-1.91"",
    pages = ""664--670"",
    abstract = ""This paper describes the system for the YNU-HPCC team in subtask 1 of the SemEval-2023 Task 7: Multi-evidence Natural Language Inference for Clinical Trial Data (NLI4CT). This task requires judging the textual entailment relationship between the given CTR and the statement annotated by the expert annotator. This system is based on the fine-tuned Bi-directional Encoder Representation from Transformers for Biomedical Text Mining (BioBERT) model with supervised contrastive learning and back translation. Supervised contrastive learning is to enhance the classification, and back translation is to enhance the training data. Our system achieved relatively good results on the competition{'}s official leaderboard. The code of this paper is available at \url{https://github.com/facanhe/SemEval-2023-Task7}."",
}
@",medical text,natural languag,translat,infer,semant,annotat,evalu
" ""{NCUEE}-{NLP} at {S}em{E}val-2023 Task 7: Ensemble Biomedical {L}ink{BERT} Transformers in Multi-evidence Natural Language Inference for Clinical Trial Data"","," ""This study describes the model design of the NCUEE-NLP system for the SemEval-2023 NLI4CT task that focuses on multi-evidence natural language inference for clinical trial data. We use the LinkBERT transformer in the biomedical domain (denoted as BioLinkBERT) as our main system architecture. First, a set of sentences in clinical trial reports is extracted as evidence for premise-statement inference. This identified evidence is then used to determine the inference relation (i.e., entailment or contradiction). Finally, a soft voting ensemble mechanism is applied to enhance the system performance. For Subtask 1 on textual entailment, our best submission had an F1-score of 0.7091, ranking sixth among all 30 participating teams. For Subtask 2 on evidence retrieval, our best result obtained an F1-score of 0.7940, ranking ninth of 19 submissions."",","{chen-etal-2023-ncuee-nlp,
    title = ""{NCUEE}-{NLP} at {S}em{E}val-2023 Task 7: Ensemble Biomedical {L}ink{BERT} Transformers in Multi-evidence Natural Language Inference for Clinical Trial Data"",
    author = ""Chen, Chao-Yi  and
      Tien, Kao-Yuan  and
      Cheng, Yuan-Hao  and
      Lee, Lung-Hao"",
    editor = {Ojha, Atul Kr.  and
      Do{\u{g}}ru{\""o}z, A. Seza  and
      Da San Martino, Giovanni  and
      Tayyar Madabushi, Harish  and
      Kumar, Ritesh  and
      Sartori, Elisa},
    booktitle = ""Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.semeval-1.107"",
    doi = ""10.18653/v1/2023.semeval-1.107"",
    pages = ""776--781"",
    abstract = ""This study describes the model design of the NCUEE-NLP system for the SemEval-2023 NLI4CT task that focuses on multi-evidence natural language inference for clinical trial data. We use the LinkBERT transformer in the biomedical domain (denoted as BioLinkBERT) as our main system architecture. First, a set of sentences in clinical trial reports is extracted as evidence for premise-statement inference. This identified evidence is then used to determine the inference relation (i.e., entailment or contradiction). Finally, a soft voting ensemble mechanism is applied to enhance the system performance. For Subtask 1 on textual entailment, our best submission had an F1-score of 0.7091, ranking sixth among all 30 participating teams. For Subtask 2 on evidence retrieval, our best result obtained an F1-score of 0.7940, ranking ninth of 19 submissions."",
}
@",medical domain,natural languag,nlp,infer,semant,evalu
" ""{SSNS}heerin{K}avitha at {S}em{E}val-2023 Task 7: Semantic Rule Based Label Prediction Using {TF}-{IDF} and {BM}25 Techniques"","," ""The advancement in the healthcare sector assures improved diagnosis and supports appropriate decision making in medical domain. The medical domain data can be either radiology images or clinical data. The clinical data plays a major role in the healthcare sector by preventing and treating the health problem based on the evidence learned from the trials. This paper is related to multi-evidence natural language inference for clinical trial data analysis and its solution for the given subtasks (SemEval 2023 Task 7 - NLI4CT). In subtask 1 of NLI4CT, the inference relationship (entailment or contradiction) between the Clinical Trial Reports (CTRs) statement pairs with respect to the Clinical Trial Data (CTD) statement are determined. In subtask 2 of NLI4CT, predicted label (inference relationship) are defined and justified using set of supporting facts extracted from the premises. The objective of this work is to derive the conclusion from premises (CTRs statement pairs) and extracting the supporting premises using proposed Semantic Rule based Clinical Data Analysis (SRCDA) approach. From the results, the proposed model attained an highest F1-score of 0.667 and 0.716 for subtasks 1 and 2 respectively. The novelty of this proposed approach includes, creation of External Knowledge Base (EKB) along with its suitable semantic rules based on the input statements."",","{noor-mohamed-srinivasan-2023-ssnsheerinkavitha,
    title = ""{SSNS}heerin{K}avitha at {S}em{E}val-2023 Task 7: Semantic Rule Based Label Prediction Using {TF}-{IDF} and {BM}25 Techniques"",
    author = ""Noor Mohamed, Sheerin Sitara  and
      Srinivasan, Kavitha"",
    editor = {Ojha, Atul Kr.  and
      Do{\u{g}}ru{\""o}z, A. Seza  and
      Da San Martino, Giovanni  and
      Tayyar Madabushi, Harish  and
      Kumar, Ritesh  and
      Sartori, Elisa},
    booktitle = ""Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.semeval-1.131"",
    doi = ""10.18653/v1/2023.semeval-1.131"",
    pages = ""950--957"",
    abstract = ""The advancement in the healthcare sector assures improved diagnosis and supports appropriate decision making in medical domain. The medical domain data can be either radiology images or clinical data. The clinical data plays a major role in the healthcare sector by preventing and treating the health problem based on the evidence learned from the trials. This paper is related to multi-evidence natural language inference for clinical trial data analysis and its solution for the given subtasks (SemEval 2023 Task 7 - NLI4CT). In subtask 1 of NLI4CT, the inference relationship (entailment or contradiction) between the Clinical Trial Reports (CTRs) statement pairs with respect to the Clinical Trial Data (CTD) statement are determined. In subtask 2 of NLI4CT, predicted label (inference relationship) are defined and justified using set of supporting facts extracted from the premises. The objective of this work is to derive the conclusion from premises (CTRs statement pairs) and extracting the supporting premises using proposed Semantic Rule based Clinical Data Analysis (SRCDA) approach. From the results, the proposed model attained an highest F1-score of 0.667 and 0.716 for subtasks 1 and 2 respectively. The novelty of this proposed approach includes, creation of External Knowledge Base (EKB) along with its suitable semantic rules based on the input statements."",
}
@",medical domain,natural languag,infer,semant,evalu
" ""Using Membership Inference Attacks to Evaluate Privacy-Preserving Language Modeling Fails for Pseudonymizing Data"","," ""Large pre-trained language models dominate the current state-of-the-art for many natural language processing applications, including the field of clinical NLP. Several studies have found that these can be susceptible to privacy attacks that are unacceptable in the clinical domain where personally identifiable information (PII) must not be exposed. However, there is no consensus regarding how to quantify the privacy risks of different models. One prominent suggestion is to quantify these risks using membership inference attacks. In this study, we show that a state-of-the-art membership inference attack on a clinical BERT model fails to detect the privacy benefits from pseudonymizing data. This suggests that such attacks may be inadequate for evaluating token-level privacy preservation of PIIs."",","{vakili-dalianis-2023-using,
    title = ""Using Membership Inference Attacks to Evaluate Privacy-Preserving Language Modeling Fails for Pseudonymizing Data"",
    author = ""Vakili, Thomas  and
      Dalianis, Hercules"",
    editor = {Alum{\""a}e, Tanel  and
      Fishel, Mark},
    booktitle = ""Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)"",
    month = may,
    year = ""2023"",
    address = ""T{\'o}rshavn, Faroe Islands"",
    publisher = ""University of Tartu Library"",
    url = ""https://aclanthology.org/2023.nodalida-1.33"",
    pages = ""318--323"",
    abstract = ""Large pre-trained language models dominate the current state-of-the-art for many natural language processing applications, including the field of clinical NLP. Several studies have found that these can be susceptible to privacy attacks that are unacceptable in the clinical domain where personally identifiable information (PII) must not be exposed. However, there is no consensus regarding how to quantify the privacy risks of different models. One prominent suggestion is to quantify these risks using membership inference attacks. In this study, we show that a state-of-the-art membership inference attack on a clinical BERT model fails to detect the privacy benefits from pseudonymizing data. This suggests that such attacks may be inadequate for evaluating token-level privacy preservation of PIIs."",
}
@",clinical domain,natural language process,natural languag,language process,nlp,infer,evalu
" ""{D}anish Clinical Named Entity Recognition and Relation Extraction"","," ""Electronic health records contain important information regarding the patients{'} medical history but much of this information is stored in unstructured narrative text. This paper presents the first Danish clinical named entity recognition and relation extraction dataset for extraction of six types of clinical events, six types of attributes, and three types of relations. The dataset contains 11,607 paragraphs from Danish electronic health records containing 54,631 clinical events, 41,954 attributes, and 14,604 relations. We detail the methodology of developing the annotation scheme, and train a transformer-based architecture on the developed dataset with macro F1 performance of 60.05{\%}, 44.85{\%}, and 70.64{\%} for clinical events, attributes, and relations, respectively."",","{laursen-etal-2023-danish,
    title = ""{D}anish Clinical Named Entity Recognition and Relation Extraction"",
    author = ""Laursen, Martin  and
      Pedersen, Jannik  and
      Hansen, Rasmus  and
      Savarimuthu, Thiusius Rajeeth  and
      Vinholt, Pernille"",
    editor = {Alum{\""a}e, Tanel  and
      Fishel, Mark},
    booktitle = ""Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)"",
    month = may,
    year = ""2023"",
    address = ""T{\'o}rshavn, Faroe Islands"",
    publisher = ""University of Tartu Library"",
    url = ""https://aclanthology.org/2023.nodalida-1.65"",
    pages = ""655--666"",
    abstract = ""Electronic health records contain important information regarding the patients{'} medical history but much of this information is stored in unstructured narrative text. This paper presents the first Danish clinical named entity recognition and relation extraction dataset for extraction of six types of clinical events, six types of attributes, and three types of relations. The dataset contains 11,607 paragraphs from Danish electronic health records containing 54,631 clinical events, 41,954 attributes, and 14,604 relations. We detail the methodology of developing the annotation scheme, and train a transformer-based architecture on the developed dataset with macro F1 performance of 60.05{\%}, 44.85{\%}, and 70.64{\%} for clinical events, attributes, and relations, respectively."",
}
@",electronic health record,health record,relation extract,entity recognit,annotat
" ""Proceedings of the The Sixth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2023)"","," ""Entity standardization maps noisy mentions from free-form text to standard entities in a knowledge base. The unique challenge of this task relative to other entity-related tasks is the lack of surrounding context and numerous variations in the surface form of the mentions, especially when it comes to generalization across domains where labeled data is scarce. Previous research mostly focuses on developing models either heavily relying on context, or dedicated solely to a specific domain. In contrast, we propose CoSiNES, a generic and adaptable framework with Contrastive Siamese Network for Entity Standardization that effectively adapts a pretrained language model to capture the syntax and semantics of the entities in a new domain. We construct a new dataset in the technology domain, which contains 640 technical stack entities and 6,412 mentions collected from industrial content management systems. We demonstrate that CoSiNES yields higher accuracy and faster runtime than baselines derived from leading methods in this domain. CoSiNES also achieves competitive performance in four standard datasets from the chemistry, medicine, and biomedical domains, demonstrating its cross-domain applicability. Code and data is available at \url{https://github.com/konveyor/tackle-container-advisor/tree/main/entity_standardizer/cosines}"",","{yuan-etal-2023-cosines,
    title = ""{C}o{S}i{NES}: Contrastive {S}iamese Network for Entity Standardization"",
    author = ""Yuan, Jiaqing  and
      Merler, Michele  and
      Choudhury, Mihir  and
      Pavuluri, Raju  and
      Singh, Munindar  and
      Vukovic, Maja"",
    editor = ""Hruschka, Estevam  and
      Mitchell, Tom  and
      Rahman, Sajjadur  and
      Mladeni{\'c}, Dunja  and
      Grobelnik, Marko"",
    booktitle = ""Proceedings of the First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, ON, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.matching-1.9"",
    doi = ""10.18653/v1/2023.matching-1.9"",
    pages = ""109--119"",
    abstract = ""Entity standardization maps noisy mentions from free-form text to standard entities in a knowledge base. The unique challenge of this task relative to other entity-related tasks is the lack of surrounding context and numerous variations in the surface form of the mentions, especially when it comes to generalization across domains where labeled data is scarce. Previous research mostly focuses on developing models either heavily relying on context, or dedicated solely to a specific domain. In contrast, we propose CoSiNES, a generic and adaptable framework with Contrastive Siamese Network for Entity Standardization that effectively adapts a pretrained language model to capture the syntax and semantics of the entities in a new domain. We construct a new dataset in the technology domain, which contains 640 technical stack entities and 6,412 mentions collected from industrial content management systems. We demonstrate that CoSiNES yields higher accuracy and faster runtime than baselines derived from leading methods in this domain. CoSiNES also achieves competitive performance in four standard datasets from the chemistry, medicine, and biomedical domains, demonstrating its cross-domain applicability. Code and data is available at \url{https://github.com/konveyor/tackle-container-advisor/tree/main/entity_standardizer/cosines}"",
}
@proceedings{loresmt-2023-technologies,
    title = ""Proceedings of the The Sixth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2023)"",
    editor = ""Ojha, Atul Kr.  and
      Liu, Chao-hong  and
      Vylomova, Ekaterina  and
      Pirinen, Flammie  and
      Abbott, Jade  and
      Washington, Jonathan  and
      Oco, Nathaniel  and
      Malykh, Valentin  and
      Logacheva, Varvara  and
      Zhao, Xiaobing"",
    month = may,
    year = ""2023"",
    address = ""Dubrovnik, Croatia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.loresmt-1.0"",
}
@",medical domain,translat,semant,challeng
" ""Annotating {P}ub{M}ed Abstracts with {M}e{SH} Headings using Graph Neural Network"","," ""The number of scientific publications in the biomedical domain is continuously increasing with time. An efficient system for indexing these publications is required to make the information accessible according to the user{'}s information needs. Task 10a of the BioASQ challenge aims to classify PubMed articles according to the MeSH ontology so that new publications can be grouped with similar preexisting publications in the field without the assistance of time-consuming and costly annotations by human annotators. In this work, we use Graph Neural Network (GNN) in the link prediction setting to exploit potential graph-structured information present in the dataset which could otherwise be neglected by transformer-based models. Additionally, we provide error analysis and a plausible reason for the substandard performance achieved by GNN."",","{mustafa-etal-2023-annotating,
    title = ""Annotating {P}ub{M}ed Abstracts with {M}e{SH} Headings using Graph Neural Network"",
    author = ""Mustafa, Faizan E  and
      Boutalbi, Rafika  and
      Iurshina, Anastasiia"",
    editor = ""Tafreshi, Shabnam  and
      Akula, Arjun  and
      Sedoc, Jo{\~a}o  and
      Drozd, Aleksandr  and
      Rogers, Anna  and
      Rumshisky, Anna"",
    booktitle = ""The Fourth Workshop on Insights from Negative Results in NLP"",
    month = may,
    year = ""2023"",
    address = ""Dubrovnik, Croatia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.insights-1.9"",
    doi = ""10.18653/v1/2023.insights-1.9"",
    pages = ""75--81"",
    abstract = ""The number of scientific publications in the biomedical domain is continuously increasing with time. An efficient system for indexing these publications is required to make the information accessible according to the user{'}s information needs. Task 10a of the BioASQ challenge aims to classify PubMed articles according to the MeSH ontology so that new publications can be grouped with similar preexisting publications in the field without the assistance of time-consuming and costly annotations by human annotators. In this work, we use Graph Neural Network (GNN) in the link prediction setting to exploit potential graph-structured information present in the dataset which could otherwise be neglected by transformer-based models. Additionally, we provide error analysis and a plausible reason for the substandard performance achieved by GNN."",
}
@",medical domain,nlp,annotat,challeng
" ""{N}ap{SS}: Paragraph-level Medical Text Simplification via Narrative Prompting and Sentence-matching Summarization"","," ""Accessing medical literature is difficult for laypeople as the content is written for specialists and contains medical jargon. Automated text simplification methods offer a potential means to address this issue. In this work, we propose a summarize-then-simplify two-stage strategy, which we call NapSS, identifying the relevant content to simplify while ensuring that the original narrative flow is preserved. In this approach, we first generate reference summaries via sentence matching between the original and the simplified abstracts. These summaries are then used to train an extractive summarizer, learning the most relevant content to be simplified. Then, to ensure the narrative consistency of the simplified text, we synthesize auxiliary narrative prompts combining key phrases derived from the syntactical analyses of the original text. Our model achieves results significantly better than the seq2seq baseline on an English medical corpus, yielding 3{\%} 4{\%} absolute improvements in terms of lexical similarity, and providing a further 1.1{\%} improvement of SARI score when combined with the baseline. We also highlight shortcomings of existing evaluation methods, and introduce new metrics that take into account both lexical and high-level semantic similarity. A human evaluation conducted on a random sample of the test set further establishes the effectiveness of the proposed approach. Codes and models are released here: \url{https://github.com/LuJunru/NapSS}."",","{lu-etal-2023-napss,
    title = ""{N}ap{SS}: Paragraph-level Medical Text Simplification via Narrative Prompting and Sentence-matching Summarization"",
    author = ""Lu, Junru  and
      Li, Jiazheng  and
      Wallace, Byron  and
      He, Yulan  and
      Pergola, Gabriele"",
    editor = ""Vlachos, Andreas  and
      Augenstein, Isabelle"",
    booktitle = ""Findings of the Association for Computational Linguistics: EACL 2023"",
    month = may,
    year = ""2023"",
    address = ""Dubrovnik, Croatia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.findings-eacl.80"",
    doi = ""10.18653/v1/2023.findings-eacl.80"",
    pages = ""1079--1091"",
    abstract = ""Accessing medical literature is difficult for laypeople as the content is written for specialists and contains medical jargon. Automated text simplification methods offer a potential means to address this issue. In this work, we propose a summarize-then-simplify two-stage strategy, which we call NapSS, identifying the relevant content to simplify while ensuring that the original narrative flow is preserved. In this approach, we first generate reference summaries via sentence matching between the original and the simplified abstracts. These summaries are then used to train an extractive summarizer, learning the most relevant content to be simplified. Then, to ensure the narrative consistency of the simplified text, we synthesize auxiliary narrative prompts combining key phrases derived from the syntactical analyses of the original text. Our model achieves results significantly better than the seq2seq baseline on an English medical corpus, yielding 3{\%} 4{\%} absolute improvements in terms of lexical similarity, and providing a further 1.1{\%} improvement of SARI score when combined with the baseline. We also highlight shortcomings of existing evaluation methods, and introduce new metrics that take into account both lexical and high-level semantic similarity. A human evaluation conducted on a random sample of the test set further establishes the effectiveness of the proposed approach. Codes and models are released here: \url{https://github.com/LuJunru/NapSS}."",
}
@",medical text,generat,summar,semant,evalu
" ""An Investigation of Evaluation Methods in Automatic Medical Note Generation"","," ""Recent studies on automatic note generation have shown that doctors can save significant amounts of time when using automatic clinical note generation (Knoll et al., 2022). Summarization models have been used for this task to generate clinical notes as summaries of doctor-patient conversations (Krishna et al., 2021; Cai et al., 2022). However, assessing which model would best serve clinicians in their daily practice is still a challenging task due to the large set of possible correct summaries, and the potential limitations of automatic evaluation metrics. In this paper we study evaluation methods and metrics for the automatic generation of clinical notes from medical conversation. In particular, we propose new task-specific metrics and we compare them to SOTA evaluation metrics in text summarization and generation, including: (i) knowledge-graph embedding-based metrics, (ii) customized model-based metrics with domain-specific weights, (iii) domain-adapted/fine-tuned metrics, and (iv) ensemble metrics. To study the correlation between the automatic metrics and manual judgments, we evaluate automatic notes/summaries by comparing the system and reference facts and computing the factual correctness, and the hallucination and omission rates for critical medical facts. This study relied on seven datasets manually annotated by domain experts. Our experiments show that automatic evaluation metrics can have substantially different behaviors on different types of clinical notes datasets. However, the results highlight one stable subset of metrics as the most correlated with human judgments with a relevant aggregation of different evaluation criteria."",","{ben-abacha-etal-2023-investigation,
    title = ""An Investigation of Evaluation Methods in Automatic Medical Note Generation"",
    author = ""Ben Abacha, Asma  and
      Yim, Wen-wai  and
      Michalopoulos, George  and
      Lin, Thomas"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Findings of the Association for Computational Linguistics: ACL 2023"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.findings-acl.161"",
    doi = ""10.18653/v1/2023.findings-acl.161"",
    pages = ""2575--2588"",
    abstract = ""Recent studies on automatic note generation have shown that doctors can save significant amounts of time when using automatic clinical note generation (Knoll et al., 2022). Summarization models have been used for this task to generate clinical notes as summaries of doctor-patient conversations (Krishna et al., 2021; Cai et al., 2022). However, assessing which model would best serve clinicians in their daily practice is still a challenging task due to the large set of possible correct summaries, and the potential limitations of automatic evaluation metrics. In this paper we study evaluation methods and metrics for the automatic generation of clinical notes from medical conversation. In particular, we propose new task-specific metrics and we compare them to SOTA evaluation metrics in text summarization and generation, including: (i) knowledge-graph embedding-based metrics, (ii) customized model-based metrics with domain-specific weights, (iii) domain-adapted/fine-tuned metrics, and (iv) ensemble metrics. To study the correlation between the automatic metrics and manual judgments, we evaluate automatic notes/summaries by comparing the system and reference facts and computing the factual correctness, and the hallucination and omission rates for critical medical facts. This study relied on seven datasets manually annotated by domain experts. Our experiments show that automatic evaluation metrics can have substantially different behaviors on different types of clinical notes datasets. However, the results highlight one stable subset of metrics as the most correlated with human judgments with a relevant aggregation of different evaluation criteria."",
}
@",clinical not,generat,summar,annotat,challeng,evalu,assess
" ""A Two-Stage Decoder for Efficient {ICD} Coding"","," ""Clinical notes in healthcare facilities are tagged with the International Classification of Diseases (ICD) code; a list of classification codes for medical diagnoses and procedures. ICD coding is a challenging multilabel text classification problem due to noisy clinical document inputs and long-tailed label distribution. Recent automated ICD coding efforts improve performance by encoding medical notes and codes with additional data and knowledge bases. However, most of them do not reflect how human coders generate the code: first, the coders select general code categories and then look for specific subcategories that are relevant to a patient{'}s condition. Inspired by this, we propose a two-stage decoding mechanism to predict ICD codes. Our model uses the hierarchical properties of the codes to split the prediction into two steps: At first, we predict the parent code and then predict the child code based on the previous prediction. Experiments on the public MIMIC-III data set have shown that our model performs well in single-model settings without external data or knowledge."",","{nguyen-etal-2023-two,
    title = ""A Two-Stage Decoder for Efficient {ICD} Coding"",
    author = ""Nguyen, Thanh-Tung  and
      Schlegel, Viktor  and
      Ramesh Kashyap, Abhinav  and
      Winkler, Stefan"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Findings of the Association for Computational Linguistics: ACL 2023"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.findings-acl.285"",
    doi = ""10.18653/v1/2023.findings-acl.285"",
    pages = ""4658--4665"",
    abstract = ""Clinical notes in healthcare facilities are tagged with the International Classification of Diseases (ICD) code; a list of classification codes for medical diagnoses and procedures. ICD coding is a challenging multilabel text classification problem due to noisy clinical document inputs and long-tailed label distribution. Recent automated ICD coding efforts improve performance by encoding medical notes and codes with additional data and knowledge bases. However, most of them do not reflect how human coders generate the code: first, the coders select general code categories and then look for specific subcategories that are relevant to a patient{'}s condition. Inspired by this, we propose a two-stage decoding mechanism to predict ICD codes. Our model uses the hierarchical properties of the codes to split the prediction into two steps: At first, we predict the parent code and then predict the child code based on the previous prediction. Experiments on the public MIMIC-III data set have shown that our model performs well in single-model settings without external data or knowledge."",
}
@",clinical not,generat,challeng
" ""Efficient Document Embeddings via Self-Contrastive Bregman Divergence Learning"","," ""Learning quality document embeddings is a fundamental problem in natural language processing (NLP), information retrieval (IR), recommendation systems, and search engines. Despite recent advances in the development of transformer-based models that produce sentence embeddings with self-contrastive learning, the encoding of long documents (Ks of words) is still challenging with respect to both efficiency and quality considerations. Therefore, we train Longfomer-based document encoders using a state-of-the-art unsupervised contrastive learning method (SimCSE). Further on, we complement the baseline method -siamese neural network- with additional convex neural networks based on functional Bregman divergence aiming to enhance the quality of the output document representations. We show that overall the combination of a self-contrastive siamese network and our proposed neural Bregman network outperforms the baselines in two linear classification settings on three long document topic classification tasks from the legal and biomedical domains."",","{saggau-etal-2023-efficient,
    title = ""Efficient Document Embeddings via Self-Contrastive Bregman Divergence Learning"",
    author = ""Saggau, Daniel  and
      Rezaei, Mina  and
      Bischl, Bernd  and
      Chalkidis, Ilias"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Findings of the Association for Computational Linguistics: ACL 2023"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.findings-acl.771"",
    doi = ""10.18653/v1/2023.findings-acl.771"",
    pages = ""12181--12190"",
    abstract = ""Learning quality document embeddings is a fundamental problem in natural language processing (NLP), information retrieval (IR), recommendation systems, and search engines. Despite recent advances in the development of transformer-based models that produce sentence embeddings with self-contrastive learning, the encoding of long documents (Ks of words) is still challenging with respect to both efficiency and quality considerations. Therefore, we train Longfomer-based document encoders using a state-of-the-art unsupervised contrastive learning method (SimCSE). Further on, we complement the baseline method -siamese neural network- with additional convex neural networks based on functional Bregman divergence aiming to enhance the quality of the output document representations. We show that overall the combination of a self-contrastive siamese network and our proposed neural Bregman network outperforms the baselines in two linear classification settings on three long document topic classification tasks from the legal and biomedical domains."",
}
@",medical domain,natural language process,natural languag,language process,nlp,information retriev,challeng
" ""Focus-aware Response Generation in Inquiry Conversation"","," ""Inquiry conversation is a common form of conversation that aims to complete the investigation (e.g., court hearing, medical consultation and police interrogation) during which a series of focus shifts occurs. While many models have been proposed to generate a smooth response to a given conversation history, neglecting the focus can limit performance in inquiry conversation where the order of the focuses plays there a key role. In this paper, we investigate the problem of response generation in inquiry conversation by taking the focus into consideration. We propose a novel Focus-aware Response Generation (FRG) method by jointly optimizing a multi-level encoder and a set of focal decoders to generate several candidate responses that correspond to different focuses. Additionally, a focus ranking module is proposed to predict the next focus and rank the candidate responses. Experiments on two orthogonal inquiry conversation datasets (judicial, medical domain) demonstrate that our method generates results significantly better in automatic metrics and human evaluation compared to the state-of-the-art approaches."",","{wu-etal-2023-focus,
    title = ""Focus-aware Response Generation in Inquiry Conversation"",
    author = ""Wu, Yiquan  and
      Lu, Weiming  and
      Zhang, Yating  and
      Jatowt, Adam  and
      Feng, Jun  and
      Sun, Changlong  and
      Wu, Fei  and
      Kuang, Kun"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Findings of the Association for Computational Linguistics: ACL 2023"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.findings-acl.797"",
    doi = ""10.18653/v1/2023.findings-acl.797"",
    pages = ""12585--12599"",
    abstract = ""Inquiry conversation is a common form of conversation that aims to complete the investigation (e.g., court hearing, medical consultation and police interrogation) during which a series of focus shifts occurs. While many models have been proposed to generate a smooth response to a given conversation history, neglecting the focus can limit performance in inquiry conversation where the order of the focuses plays there a key role. In this paper, we investigate the problem of response generation in inquiry conversation by taking the focus into consideration. We propose a novel Focus-aware Response Generation (FRG) method by jointly optimizing a multi-level encoder and a set of focal decoders to generate several candidate responses that correspond to different focuses. Additionally, a focus ranking module is proposed to predict the next focus and rank the candidate responses. Experiments on two orthogonal inquiry conversation datasets (judicial, medical domain) demonstrate that our method generates results significantly better in automatic metrics and human evaluation compared to the state-of-the-art approaches."",
}
@",medical domain,generat,evalu
" ""Quality Analysis of Multilingual Neural Machine Translation Systems and Reference Test Translations for the {E}nglish-{R}omanian language pair in the Medical Domain"","," ""Multilingual Neural Machine Translation (MNMT) models allow to translate across multiple languages based on only one system. We study the quality of a domain-adapted MNMT model in the medical domain for English-Romanian with automatic metrics and a human error typology annotation based on the Multidimensional Quality Metrics (MQM). We further expand the MQM typology to include terminology-specific error categories. We compare the out-of-domain MNMT with the in-domain adapted MNMT on a standard test dataset of abstracts from medical publications. The in-domain MNMT model outperforms the out-of-domain MNMT in all measured automatic metrics and produces fewer errors. In addition, we perform the manual annotation over the reference test dataset to study the quality of the reference translations. We identify a high number of omissions, additions, and mistranslations in the reference dataset, and comment on the assumed accuracy of existing datasets. Finally, we compare the correlation between the COMET, BERTScore, and chrF automatic metrics with the MQM annotated translations. COMET shows a better correlation with the MQM scores compared to the other metrics."",","{gaona-etal-2023-quality,
    title = ""Quality Analysis of Multilingual Neural Machine Translation Systems and Reference Test Translations for the {E}nglish-{R}omanian language pair in the Medical Domain"",
    author = ""Gaona, Miguel Angel Rios  and
      Chereji, Raluca-Maria  and
      Secara, Alina  and
      Ciobanu, Dragos"",
    editor = ""Nurminen, Mary  and
      Brenner, Judith  and
      Koponen, Maarit  and
      Latomaa, Sirkku  and
      Mikhailov, Mikhail  and
      Schierl, Frederike  and
      Ranasinghe, Tharindu  and
      Vanmassenhove, Eva  and
      Vidal, Sergi Alvarez  and
      Aranberri, Nora  and
      Nunziatini, Mara  and
      Escart{\'\i}n, Carla Parra  and
      Forcada, Mikel  and
      Popovic, Maja  and
      Scarton, Carolina  and
      Moniz, Helena"",
    booktitle = ""Proceedings of the 24th Annual Conference of the European Association for Machine Translation"",
    month = jun,
    year = ""2023"",
    address = ""Tampere, Finland"",
    publisher = ""European Association for Machine Translation"",
    url = ""https://aclanthology.org/2023.eamt-1.35"",
    pages = ""355--364"",
    abstract = ""Multilingual Neural Machine Translation (MNMT) models allow to translate across multiple languages based on only one system. We study the quality of a domain-adapted MNMT model in the medical domain for English-Romanian with automatic metrics and a human error typology annotation based on the Multidimensional Quality Metrics (MQM). We further expand the MQM typology to include terminology-specific error categories. We compare the out-of-domain MNMT with the in-domain adapted MNMT on a standard test dataset of abstracts from medical publications. The in-domain MNMT model outperforms the out-of-domain MNMT in all measured automatic metrics and produces fewer errors. In addition, we perform the manual annotation over the reference test dataset to study the quality of the reference translations. We identify a high number of omissions, additions, and mistranslations in the reference dataset, and comment on the assumed accuracy of existing datasets. Finally, we compare the correlation between the COMET, BERTScore, and chrF automatic metrics with the MQM annotated translations. COMET shows a better correlation with the MQM scores compared to the other metrics."",
}
@",medical domain,translat,annotat
" ""Development of pre-trained language models for clinical {NLP} in {S}panish"","," ""Clinical natural language processing aims to tackle language and prediction tasks using text from medical practice, such as clinical notes, prescriptions, and discharge summaries. Several approaches have been tried to deal with these tasks. Since 2017, pre-trained language models (PLMs) have achieved state-of-the-art performance in many tasks. However, most works have been developed in English. This PhD research proposal addresses the development of PLMs for clinical NLP in Spanish. To carry out this study, we will build a clinical corpus big enough to implement a functional PLM. We will test several PLM architectures and evaluate them with language and prediction tasks. The novelty of this work lies in the use of only clinical text, while previous clinical PLMs have used a mix of general, biomedical, and clinical text."",","{aracena-dunstan-2023-development,
    title = ""Development of pre-trained language models for clinical {NLP} in {S}panish"",
    author = ""Aracena, Claudio  and
      Dunstan, Jocelyn"",
    editor = ""Bassignana, Elisa  and
      Lindemann, Matthias  and
      Petit, Alban"",
    booktitle = ""Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop"",
    month = may,
    year = ""2023"",
    address = ""Dubrovnik, Croatia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.eacl-srw.5"",
    doi = ""10.18653/v1/2023.eacl-srw.5"",
    pages = ""52--60"",
    abstract = ""Clinical natural language processing aims to tackle language and prediction tasks using text from medical practice, such as clinical notes, prescriptions, and discharge summaries. Several approaches have been tried to deal with these tasks. Since 2017, pre-trained language models (PLMs) have achieved state-of-the-art performance in many tasks. However, most works have been developed in English. This PhD research proposal addresses the development of PLMs for clinical NLP in Spanish. To carry out this study, we will build a clinical corpus big enough to implement a functional PLM. We will test several PLM architectures and evaluate them with language and prediction tasks. The novelty of this work lies in the use of only clinical text, while previous clinical PLMs have used a mix of general, biomedical, and clinical text."",
}
@",discharge summar,clinical not,clinical text,natural language process,natural languag,language process,nlp,summar,evalu
" ""Patient Outcome and Zero-shot Diagnosis Prediction with Hypernetwork-guided Multitask Learning"","," ""Multitask deep learning has been applied to patient outcome prediction from text, taking clinical notes as input and training deep neural networks with a joint loss function of multiple tasks. However, the joint training scheme of multitask learning suffers from inter-task interference, and diagnosis prediction among the multiple tasks has the generalizability issue due to rare diseases or unseen diagnoses. To solve these challenges, we propose a hypernetwork-based approach that generates task-conditioned parameters and coefficients of multitask prediction heads to learn task-specific prediction and balance the multitask learning. We also incorporate semantic task information to improve the generalizability of our task-conditioned multitask model. Experiments on early and discharge notes extracted from the real-world MIMIC database show our method can achieve better performance on multitask patient outcome prediction than strong baselines in most cases. Besides, our method can effectively handle the scenario with limited information and improve zero-shot prediction on unseen diagnosis categories."",","{ji-marttinen-2023-patient,
    title = ""Patient Outcome and Zero-shot Diagnosis Prediction with Hypernetwork-guided Multitask Learning"",
    author = ""Ji, Shaoxiong  and
      Marttinen, Pekka"",
    editor = ""Vlachos, Andreas  and
      Augenstein, Isabelle"",
    booktitle = ""Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics"",
    month = may,
    year = ""2023"",
    address = ""Dubrovnik, Croatia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.eacl-main.43"",
    doi = ""10.18653/v1/2023.eacl-main.43"",
    pages = ""589--598"",
    abstract = ""Multitask deep learning has been applied to patient outcome prediction from text, taking clinical notes as input and training deep neural networks with a joint loss function of multiple tasks. However, the joint training scheme of multitask learning suffers from inter-task interference, and diagnosis prediction among the multiple tasks has the generalizability issue due to rare diseases or unseen diagnoses. To solve these challenges, we propose a hypernetwork-based approach that generates task-conditioned parameters and coefficients of multitask prediction heads to learn task-specific prediction and balance the multitask learning. We also incorporate semantic task information to improve the generalizability of our task-conditioned multitask model. Experiments on early and discharge notes extracted from the real-world MIMIC database show our method can achieve better performance on multitask patient outcome prediction than strong baselines in most cases. Besides, our method can effectively handle the scenario with limited information and improve zero-shot prediction on unseen diagnosis categories."",
}
@",clinical not,generat,semant,challeng
" ""Modelling Temporal Document Sequences for Clinical {ICD} Coding"","," ""Past studies on the ICD coding problem focus on predicting clinical codes primarily based on the discharge summary. This covers only a small fraction of the notes generated during each hospital stay and leaves potential for improving performance by analysing all the available clinical notes. We propose a hierarchical transformer architecture that uses text across the entire sequence of clinical notes in each hospital stay for ICD coding, and incorporates embeddings for text metadata such as their position, time, and type of note. While using all clinical notes increases the quantity of data substantially, superconvergence can be used to reduce training costs. We evaluate the model on the MIMIC-III dataset. Our model exceeds the prior state-of-the-art when using only discharge summaries as input, and achieves further performance improvements when all clinical notes are used as input."",","{ng-etal-2023-modelling,
    title = ""Modelling Temporal Document Sequences for Clinical {ICD} Coding"",
    author = ""Ng, Boon Liang Clarence  and
      Santos, Diogo  and
      Rei, Marek"",
    editor = ""Vlachos, Andreas  and
      Augenstein, Isabelle"",
    booktitle = ""Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics"",
    month = may,
    year = ""2023"",
    address = ""Dubrovnik, Croatia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.eacl-main.120"",
    doi = ""10.18653/v1/2023.eacl-main.120"",
    pages = ""1640--1649"",
    abstract = ""Past studies on the ICD coding problem focus on predicting clinical codes primarily based on the discharge summary. This covers only a small fraction of the notes generated during each hospital stay and leaves potential for improving performance by analysing all the available clinical notes. We propose a hierarchical transformer architecture that uses text across the entire sequence of clinical notes in each hospital stay for ICD coding, and incorporates embeddings for text metadata such as their position, time, and type of note. While using all clinical notes increases the quantity of data substantially, superconvergence can be used to reduce training costs. We evaluate the model on the MIMIC-III dataset. Our model exceeds the prior state-of-the-art when using only discharge summaries as input, and achieves further performance improvements when all clinical notes are used as input."",
}
@",discharge summar,clinical not,generat,summar,evalu
" ""{GLADIS}: A General and Large Acronym Disambiguation Benchmark"","," ""Acronym Disambiguation (AD) is crucial for natural language understanding on various sources, including biomedical reports, scientific papers, and search engine queries. However, existing acronym disambiguationbenchmarks and tools are limited to specific domains, and the size of prior benchmarks is rather small. To accelerate the research on acronym disambiguation, we construct a new benchmark with three components: (1) a much larger acronym dictionary with 1.5M acronyms and 6.4M long forms; (2) a pre-training corpus with 160 million sentences;(3) three datasets that cover thegeneral, scientific, and biomedical domains. We then pre-train a language model, \textit{AcroBERT}, on our constructed corpus for general acronym disambiguation, and show the challenges and values of our new benchmark."",","{chen-etal-2023-gladis,
    title = ""{GLADIS}: A General and Large Acronym Disambiguation Benchmark"",
    author = ""Chen, Lihu  and
      Varoquaux, Gael  and
      Suchanek, Fabian M."",
    editor = ""Vlachos, Andreas  and
      Augenstein, Isabelle"",
    booktitle = ""Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics"",
    month = may,
    year = ""2023"",
    address = ""Dubrovnik, Croatia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.eacl-main.152"",
    doi = ""10.18653/v1/2023.eacl-main.152"",
    pages = ""2073--2088"",
    abstract = ""Acronym Disambiguation (AD) is crucial for natural language understanding on various sources, including biomedical reports, scientific papers, and search engine queries. However, existing acronym disambiguationbenchmarks and tools are limited to specific domains, and the size of prior benchmarks is rather small. To accelerate the research on acronym disambiguation, we construct a new benchmark with three components: (1) a much larger acronym dictionary with 1.5M acronyms and 6.4M long forms; (2) a pre-training corpus with 160 million sentences;(3) three datasets that cover thegeneral, scientific, and biomedical domains. We then pre-train a language model, \textit{AcroBERT}, on our constructed corpus for general acronym disambiguation, and show the challenges and values of our new benchmark."",
}
@",medical domain,natural languag,challeng,benchmark
" ""An Empirical Study of Clinical Note Generation from Doctor-Patient Encounters"","," ""Medical doctors spend on average 52 to 102 minutes per day writing clinical notes from their patient encounters (Hripcsak et al., 2011). Reducing this workload calls for relevant and efficient summarization methods. In this paper, we introduce new resources and empirical investigations for the automatic summarization of doctor-patient conversations in a clinical setting. In particular, we introduce the MTS-Dialog dataset; a new collection of 1,700 doctor-patient dialogues and corresponding clinical notes. We use this new dataset to investigate the feasibility of this task and the relevance of existing language models, data augmentation, and guided summarization techniques. We compare standard evaluation metrics based on n-gram matching, contextual embeddings, and Fact Extraction to assess the accuracy and the factual consistency of the generated summaries. To ground these results, we perform an expert-based evaluation using relevant natural language generation criteria and task-specific criteria such as critical omissions, and study the correlation between the automatic metrics and expert judgments. To the best of our knowledge, this study is the first attempt to introduce an open dataset of doctor-patient conversations and clinical notes, with detailed automated and manual evaluations of clinical note generation."",","{ben-abacha-etal-2023-empirical,
    title = ""An Empirical Study of Clinical Note Generation from Doctor-Patient Encounters"",
    author = ""Ben Abacha, Asma  and
      Yim, Wen-wai  and
      Fan, Yadan  and
      Lin, Thomas"",
    editor = ""Vlachos, Andreas  and
      Augenstein, Isabelle"",
    booktitle = ""Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics"",
    month = may,
    year = ""2023"",
    address = ""Dubrovnik, Croatia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.eacl-main.168"",
    doi = ""10.18653/v1/2023.eacl-main.168"",
    pages = ""2291--2302"",
    abstract = ""Medical doctors spend on average 52 to 102 minutes per day writing clinical notes from their patient encounters (Hripcsak et al., 2011). Reducing this workload calls for relevant and efficient summarization methods. In this paper, we introduce new resources and empirical investigations for the automatic summarization of doctor-patient conversations in a clinical setting. In particular, we introduce the MTS-Dialog dataset; a new collection of 1,700 doctor-patient dialogues and corresponding clinical notes. We use this new dataset to investigate the feasibility of this task and the relevance of existing language models, data augmentation, and guided summarization techniques. We compare standard evaluation metrics based on n-gram matching, contextual embeddings, and Fact Extraction to assess the accuracy and the factual consistency of the generated summaries. To ground these results, we perform an expert-based evaluation using relevant natural language generation criteria and task-specific criteria such as critical omissions, and study the correlation between the automatic metrics and expert judgments. To the best of our knowledge, this study is the first attempt to introduce an open dataset of doctor-patient conversations and clinical notes, with detailed automated and manual evaluations of clinical note generation."",
}
@",clinical not,natural languag,generat,summar,evalu,assess
" ""Can Synthetic Text Help Clinical Named Entity Recognition? A Study of Electronic Health Records in {F}rench"","," ""In sensitive domains, the sharing of corpora is restricted due to confidentiality, copyrights or trade secrets. Automatic text generation can help alleviate these issues by producing synthetic texts that mimic the linguistic properties of real documents while preserving confidentiality. In this study, we assess the usability of synthetic corpus as a substitute training corpus for clinical information extraction. Our goal is to automatically produce a clinical case corpus annotated with clinical entities and to evaluate it for a named entity recognition (NER) task. We use two auto-regressive neural models partially or fully trained on generic French texts and fine-tuned on clinical cases to produce a corpus of synthetic clinical cases. We study variants of the generation process: (i) fine-tuning on annotated vs. plain text (in that case, annotations are obtained a posteriori) and (ii) selection of generated texts based on models parameters and filtering criteria. We then train NER models with the resulting synthetic text and evaluate them on a gold standard clinical corpus. Our experiments suggest that synthetic text is useful for clinical NER."",","{hiebel-etal-2023-synthetic,
    title = ""Can Synthetic Text Help Clinical Named Entity Recognition? A Study of Electronic Health Records in {F}rench"",
    author = ""Hiebel, Nicolas  and
      Ferret, Olivier  and
      Fort, Karen  and
      N{\'e}v{\'e}ol, Aur{\'e}lie"",
    editor = ""Vlachos, Andreas  and
      Augenstein, Isabelle"",
    booktitle = ""Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics"",
    month = may,
    year = ""2023"",
    address = ""Dubrovnik, Croatia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.eacl-main.170"",
    doi = ""10.18653/v1/2023.eacl-main.170"",
    pages = ""2320--2338"",
    abstract = ""In sensitive domains, the sharing of corpora is restricted due to confidentiality, copyrights or trade secrets. Automatic text generation can help alleviate these issues by producing synthetic texts that mimic the linguistic properties of real documents while preserving confidentiality. In this study, we assess the usability of synthetic corpus as a substitute training corpus for clinical information extraction. Our goal is to automatically produce a clinical case corpus annotated with clinical entities and to evaluate it for a named entity recognition (NER) task. We use two auto-regressive neural models partially or fully trained on generic French texts and fine-tuned on clinical cases to produce a corpus of synthetic clinical cases. We study variants of the generation process: (i) fine-tuning on annotated vs. plain text (in that case, annotations are obtained a posteriori) and (ii) selection of generated texts based on models parameters and filtering criteria. We then train NER models with the resulting synthetic text and evaluate them on a gold standard clinical corpus. Our experiments suggest that synthetic text is useful for clinical NER."",
}
@",electronic health record,health record,generat,entity recognit,annotat,evalu,assess
" ""Enriching Biomedical Knowledge for Low-resource Language Through Large-scale Translation"","," ""Biomedical data and benchmarks are highly valuable yet very limited in low-resource languages other than English, such as Vietnamese. In this paper, we use a state-of-the-art translation model in English-Vietnamese to translate and produce both pretrained and supervised data in the biomedical domains. Thanks to such large-scale translation, we introduce ViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20 million translated abstracts from the high-quality public PubMed corpus. ViPubMedT5 demonstrates state-of-the-art results on two different biomedical benchmarks in summarization and acronym disambiguation. Further, we release ViMedNLI - a new NLP task in Vietnamese translated from MedNLI using the recently public En-vi translation model and carefully refined by human experts, with evaluations of existing methods against ViPubmedT5."",","{phan-etal-2023-enriching,
    title = ""Enriching Biomedical Knowledge for Low-resource Language Through Large-scale Translation"",
    author = ""Phan, Long  and
      Dang, Tai  and
      Tran, Hieu  and
      Trinh, Trieu H.  and
      Phan, Vy  and
      Chau, Lam D.  and
      Luong, Minh-Thang"",
    editor = ""Vlachos, Andreas  and
      Augenstein, Isabelle"",
    booktitle = ""Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics"",
    month = may,
    year = ""2023"",
    address = ""Dubrovnik, Croatia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.eacl-main.228"",
    doi = ""10.18653/v1/2023.eacl-main.228"",
    pages = ""3131--3142"",
    abstract = ""Biomedical data and benchmarks are highly valuable yet very limited in low-resource languages other than English, such as Vietnamese. In this paper, we use a state-of-the-art translation model in English-Vietnamese to translate and produce both pretrained and supervised data in the biomedical domains. Thanks to such large-scale translation, we introduce ViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20 million translated abstracts from the high-quality public PubMed corpus. ViPubMedT5 demonstrates state-of-the-art results on two different biomedical benchmarks in summarization and acronym disambiguation. Further, we release ViMedNLI - a new NLP task in Vietnamese translated from MedNLI using the recently public En-vi translation model and carefully refined by human experts, with evaluations of existing methods against ViPubmedT5."",
}
@",medical domain,translat,nlp,summar,benchmark,evalu
" ""{INCOGNITUS}: A Toolbox for Automated Clinical Notes Anonymization"","," ""Automated text anonymization is a classical problem in Natural Language Processing (NLP). The topic has evolved immensely throughout the years, with the first list-search and rule-based solutions evolving to statistical modeling approaches and later to advanced systems that rely on powerful state-of-the-art language models. Even so, these solutions fail to be widely implemented in the most privacy-demanding areas of activity, such as healthcare; none of them is perfect, and most can not guarantee rigorous anonymization. This paper presents INCOGNITUS, a flexible platform for the automated anonymization of clinical notes that offers the possibility of applying different techniques. The available tools include an underexplored yet promising method that guarantees 100{\%} recall by replacing each word with a semantically identical one. In addition, the presented framework incorporates a performance evaluation module to compute a novel metric for information loss assessment in real-time."",","{ribeiro-etal-2023-incognitus,
    title = ""{INCOGNITUS}: A Toolbox for Automated Clinical Notes Anonymization"",
    author = ""Ribeiro, Bruno  and
      Rolla, Vitor  and
      Santos, Ricardo"",
    editor = ""Croce, Danilo  and
      Soldaini, Luca"",
    booktitle = ""Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations"",
    month = may,
    year = ""2023"",
    address = ""Dubrovnik, Croatia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.eacl-demo.22"",
    doi = ""10.18653/v1/2023.eacl-demo.22"",
    pages = ""187--194"",
    abstract = ""Automated text anonymization is a classical problem in Natural Language Processing (NLP). The topic has evolved immensely throughout the years, with the first list-search and rule-based solutions evolving to statistical modeling approaches and later to advanced systems that rely on powerful state-of-the-art language models. Even so, these solutions fail to be widely implemented in the most privacy-demanding areas of activity, such as healthcare; none of them is perfect, and most can not guarantee rigorous anonymization. This paper presents INCOGNITUS, a flexible platform for the automated anonymization of clinical notes that offers the possibility of applying different techniques. The available tools include an underexplored yet promising method that guarantees 100{\%} recall by replacing each word with a semantically identical one. In addition, the presented framework incorporates a performance evaluation module to compute a novel metric for information loss assessment in real-time."",
}
@",clinical not,natural language process,natural languag,language process,nlp,semant,evalu,assess
" ""Entity Coreference and Co-occurrence Aware Argument Mining from Biomedical Literature"","," ""Biomedical argument mining (BAM) aims at automatically identifying the argumentative structure in biomedical texts. However, identifying and classifying argumentative relations (AR) between argumentative components (AC) is challenging since it not only needs to understand the semantics of ACs but also need to capture the interactions between them. We argue that entities can serve as bridges that connect different ACs since entities and their mentions convey significant semantic information in biomedical argumentation. For example, it is common that related AC pairs share a common entity. Capturing such entity information can be beneficial for the Relation Identification (RI) task. In order to incorporate this entity information into BAM, we propose an Entity Coreference and Co-occurrence aware Argument Mining (ECCAM) framework based on an edge-oriented graph model for BAM. We evaluate our model on a benchmark dataset and from the experimental results we find that our method improves upon state-of-the-art methods."",","{liu-etal-2023-entity,
    title = ""Entity Coreference and Co-occurrence Aware Argument Mining from Biomedical Literature"",
    author = ""Liu, Boyang  and
      Schlegel, Viktor  and
      Batista-navarro, Riza  and
      Ananiadou, Sophia"",
    editor = ""Strube, Michael  and
      Braud, Chloe  and
      Hardmeier, Christian  and
      Li, Junyi Jessy  and
      Loaiciga, Sharid  and
      Zeldes, Amir"",
    booktitle = ""Proceedings of the 4th Workshop on Computational Approaches to Discourse (CODI 2023)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.codi-1.6"",
    doi = ""10.18653/v1/2023.codi-1.6"",
    pages = ""54--60"",
    abstract = ""Biomedical argument mining (BAM) aims at automatically identifying the argumentative structure in biomedical texts. However, identifying and classifying argumentative relations (AR) between argumentative components (AC) is challenging since it not only needs to understand the semantics of ACs but also need to capture the interactions between them. We argue that entities can serve as bridges that connect different ACs since entities and their mentions convey significant semantic information in biomedical argumentation. For example, it is common that related AC pairs share a common entity. Capturing such entity information can be beneficial for the Relation Identification (RI) task. In order to incorporate this entity information into BAM, we propose an Entity Coreference and Co-occurrence aware Argument Mining (ECCAM) framework based on an edge-oriented graph model for BAM. We evaluate our model on a benchmark dataset and from the experimental results we find that our method improves upon state-of-the-art methods."",
}
@",medical text,semant,challeng,benchmark,evalu
" ""Medical Visual Textual Entailment for Numerical Understanding of Vision-and-Language Models"","," ""Assessing the capacity of numerical understanding of vision-and-language models over images and texts is crucial for real vision-and-language applications, such as systems for automated medical image analysis. We provide a visual reasoning dataset focusing on numerical understanding in the medical domain. The experiments using our dataset show that current vision-and-language models fail to perform numerical inference in the medical domain. However, the data augmentation with only a small amount of our dataset improves the model performance, while maintaining the performance in the general domain."",","{yanaka-etal-2023-medical,
    title = ""Medical Visual Textual Entailment for Numerical Understanding of Vision-and-Language Models"",
    author = ""Yanaka, Hitomi  and
      Nakamura, Yuta  and
      Chida, Yuki  and
      Kurosawa, Tomoya"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.2"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.2"",
    pages = ""8--18"",
    abstract = ""Assessing the capacity of numerical understanding of vision-and-language models over images and texts is crucial for real vision-and-language applications, such as systems for automated medical image analysis. We provide a visual reasoning dataset focusing on numerical understanding in the medical domain. The experiments using our dataset show that current vision-and-language models fail to perform numerical inference in the medical domain. However, the data augmentation with only a small amount of our dataset improves the model performance, while maintaining the performance in the general domain."",
}
@",medical domain,natural language process,natural languag,language process,nlp,infer,assess
" ""Investigating Massive Multilingual Pre-Trained Machine Translation Models for Clinical Domain via Transfer Learning"","," ""Massively multilingual pre-trained language models (MMPLMs) are developed in recent years demonstrating superpowers and the pre-knowledge they acquire for downstream tasks. This work investigates whether MMPLMs can be applied to clinical domain machine translation (MT) towards entirely unseen languages via transfer learning. We carry out an experimental investigation using Meta-AI{'}s MMPLMs {``}wmt21-dense-24-wide-en-X and X-en (WMT21fb){''} which were pre-trained on 7 language pairs and 14 translation directions including English to Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite direction. We fine-tune these MMPLMs towards English-\textit{Spanish} language pair which \textit{did not exist at all} in their original pre-trained corpora both implicitly and explicitly.We prepare carefully aligned \textit{clinical} domain data for this fine-tuning, which is different from their original mixed domain knowledge.Our experimental result shows that the fine-tuning is very successful using just 250k well-aligned in-domain EN-ES segments for three sub-task translation testings: clinical cases, clinical terms, and ontology concepts. It achieves very close evaluation scores to another MMPLM NLLB from Meta-AI, which included Spanish as a high-resource setting in the pre-training.To the best of our knowledge, this is the first work on using MMPLMs towards \textit{clinical domain transfer-learning NMT} successfully for totally unseen languages during pre-training."",","{han-etal-2023-investigating,
    title = ""Investigating Massive Multilingual Pre-Trained Machine Translation Models for Clinical Domain via Transfer Learning"",
    author = ""Han, Lifeng  and
      Erofeev, Gleb  and
      Sorokina, Irina  and
      Gladkoff, Serge  and
      Nenadic, Goran"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.5"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.5"",
    pages = ""31--40"",
    abstract = ""Massively multilingual pre-trained language models (MMPLMs) are developed in recent years demonstrating superpowers and the pre-knowledge they acquire for downstream tasks. This work investigates whether MMPLMs can be applied to clinical domain machine translation (MT) towards entirely unseen languages via transfer learning. We carry out an experimental investigation using Meta-AI{'}s MMPLMs {``}wmt21-dense-24-wide-en-X and X-en (WMT21fb){''} which were pre-trained on 7 language pairs and 14 translation directions including English to Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite direction. We fine-tune these MMPLMs towards English-\textit{Spanish} language pair which \textit{did not exist at all} in their original pre-trained corpora both implicitly and explicitly.We prepare carefully aligned \textit{clinical} domain data for this fine-tuning, which is different from their original mixed domain knowledge.Our experimental result shows that the fine-tuning is very successful using just 250k well-aligned in-domain EN-ES segments for three sub-task translation testings: clinical cases, clinical terms, and ontology concepts. It achieves very close evaluation scores to another MMPLM NLLB from Meta-AI, which included Spanish as a high-resource setting in the pre-training.To the best of our knowledge, this is the first work on using MMPLMs towards \textit{clinical domain transfer-learning NMT} successfully for totally unseen languages during pre-training."",
}
@",clinical domain,natural language process,natural languag,language process,translat,nlp,evalu
" ""Context-aware Medication Event Extraction from Unstructured Text"","," ""Accurately capturing medication history is crucial in delivering high-quality medical care. The extraction of medication events from unstructured clinical notes, however, is challenging because the information is presented in complex narratives. We address this challenge by leveraging the newly released Contextualized Medication Event Dataset (CMED) as part of our participation in the 2022 National NLP Clinical Challenges (n2c2) shared task. Our study evaluates the performance of various pretrained language models in this task. Further, we find that data augmentation coupled with domain-specific training provides notable improvements. With experiments, we also underscore the importance of careful data preprocessing in medical event detection."",","{salek-faramarzi-etal-2023-context,
    title = ""Context-aware Medication Event Extraction from Unstructured Text"",
    author = ""Salek Faramarzi, Noushin  and
      Patel, Meet  and
      Bandarupally, Sai Harika  and
      Banerjee, Ritwik"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.11"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.11"",
    pages = ""86--95"",
    abstract = ""Accurately capturing medication history is crucial in delivering high-quality medical care. The extraction of medication events from unstructured clinical notes, however, is challenging because the information is presented in complex narratives. We address this challenge by leveraging the newly released Contextualized Medication Event Dataset (CMED) as part of our participation in the 2022 National NLP Clinical Challenges (n2c2) shared task. Our study evaluates the performance of various pretrained language models in this task. Further, we find that data augmentation coupled with domain-specific training provides notable improvements. With experiments, we also underscore the importance of careful data preprocessing in medical event detection."",
}
@",clinical not,natural language process,natural languag,language process,nlp,shared task,challeng,evalu
" ""Knowledge Injection for Disease Names in Logical Inference between {J}apanese Clinical Texts"","," ""In the medical field, there are many clinical texts such as electronic medical records, and research on Japanese natural language processing using these texts has been conducted. One such research involves Recognizing Textual Entailment (RTE) in clinical texts using a semantic analysis and logical inference system, ccg2lambda. However, it is difficult for existing inference systems to correctly determine the entailment relations , if the input sentence contains medical domain specific paraphrases such as disease names. In this study, we propose a method to supplement the equivalence relations of disease names as axioms by identifying candidates for paraphrases that lack in theorem proving. Candidates of paraphrases are identified by using a model for the NER task for disease names and a disease name dictionary. We also construct an inference test set that requires knowledge injection of disease names and evaluate our inference system. Experiments showed that our inference system was able to correctly infer for 106 out of 149 inference test sets."",","{murakami-etal-2023-knowledge,
    title = ""Knowledge Injection for Disease Names in Logical Inference between {J}apanese Clinical Texts"",
    author = ""Murakami, Natsuki  and
      Ishida, Mana  and
      Takahashi, Yuta  and
      Yanaka, Hitomi  and
      Bekki, Daisuke"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.14"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.14"",
    pages = ""108--117"",
    abstract = ""In the medical field, there are many clinical texts such as electronic medical records, and research on Japanese natural language processing using these texts has been conducted. One such research involves Recognizing Textual Entailment (RTE) in clinical texts using a semantic analysis and logical inference system, ccg2lambda. However, it is difficult for existing inference systems to correctly determine the entailment relations , if the input sentence contains medical domain specific paraphrases such as disease names. In this study, we propose a method to supplement the equivalence relations of disease names as axioms by identifying candidates for paraphrases that lack in theorem proving. Candidates of paraphrases are identified by using a model for the NER task for disease names and a disease name dictionary. We also construct an inference test set that requires knowledge injection of disease names and evaluate our inference system. Experiments showed that our inference system was able to correctly infer for 106 out of 149 inference test sets."",
}
@",medical record,medical domain,clinical text,natural language process,natural languag,language process,nlp,infer,semant,evalu
" ""Leveraging Natural Language Processing and Clinical Notes for Dementia Detection"","," ""Early detection and automated classification of dementia has recently gained considerable attention using neuroimaging data and spontaneous speech. In this paper, we validate the possibility of dementia detection with in-hospital clinical notes. We collected 954 patients{'} clinical notes from a local hospital and assign dementia/non-dementia labels to those patients based on clinical assessment and telephone interview. Given the labeled dementia data sets, we fine tune a ClinicalBioBERT based on some filtered clinical notes and conducted experiments on both binary and three class dementia classification. Our experiment results show that the fine tuned ClinicalBioBERT achieved satisfied performance on binary classification but failed on three class dementia classification. Further analysis suggests that more human prior knowledge should be considered."",","{liu-etal-2023-leveraging,
    title = ""Leveraging Natural Language Processing and Clinical Notes for Dementia Detection"",
    author = ""Liu, Ming  and
      Beare, Richard  and
      Collyer, Taya  and
      Andrew, Nadine  and
      Srikanth, Velandai"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.20"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.20"",
    pages = ""150--155"",
    abstract = ""Early detection and automated classification of dementia has recently gained considerable attention using neuroimaging data and spontaneous speech. In this paper, we validate the possibility of dementia detection with in-hospital clinical notes. We collected 954 patients{'} clinical notes from a local hospital and assign dementia/non-dementia labels to those patients based on clinical assessment and telephone interview. Given the labeled dementia data sets, we fine tune a ClinicalBioBERT based on some filtered clinical notes and conducted experiments on both binary and three class dementia classification. Our experiment results show that the fine tuned ClinicalBioBERT achieved satisfied performance on binary classification but failed on three class dementia classification. Further analysis suggests that more human prior knowledge should be considered."",
}
@",clinical not,natural language process,natural languag,language process,nlp,assess
" ""Harnessing the Power of {BERT} in the {T}urkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios"","," ""Recent advancements in natural language processing (NLP) have been driven by large language models (LLMs), thereby revolutionizing the field. Our study investigates the impact of diverse pre-training strategies on the performance of Turkish clinical language models in a multi-label classification task involving radiology reports, with a focus on overcoming language resource limitations. Additionally, for the first time, we evaluated the simultaneous pre-training approach by utilizing limited clinical task data. We developed four models: TurkRadBERT-task v1, TurkRadBERT-task v2, TurkRadBERT-sim v1, and TurkRadBERT-sim v2. Our results revealed superior performance from BERTurk and TurkRadBERT-task v1, both of which leverage a broad general-domain corpus. Although task-adaptive pre-training is capable of identifying domain-specific patterns, it may be prone to overfitting because of the constraints of the task-specific corpus. Our findings highlight the importance of domain-specific vocabulary during pre-training to improve performance. They also affirmed that a combination of general domain knowledge and task-specific fine-tuning is crucial for optimal performance across various categories. This study offers key insights for future research on pre-training techniques in the clinical domain, particularly for low-resource languages."",","{turkmen-etal-2023-harnessing,
    title = ""Harnessing the Power of {BERT} in the {T}urkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios"",
    author = {T{\""u}rkmen, Hazal  and
      Dikenelli, Oguz  and
      Eraslan, Cenk  and
      Calli, Mehmet  and
      Ozbek, Suha},
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.22"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.22"",
    pages = ""161--170"",
    abstract = ""Recent advancements in natural language processing (NLP) have been driven by large language models (LLMs), thereby revolutionizing the field. Our study investigates the impact of diverse pre-training strategies on the performance of Turkish clinical language models in a multi-label classification task involving radiology reports, with a focus on overcoming language resource limitations. Additionally, for the first time, we evaluated the simultaneous pre-training approach by utilizing limited clinical task data. We developed four models: TurkRadBERT-task v1, TurkRadBERT-task v2, TurkRadBERT-sim v1, and TurkRadBERT-sim v2. Our results revealed superior performance from BERTurk and TurkRadBERT-task v1, both of which leverage a broad general-domain corpus. Although task-adaptive pre-training is capable of identifying domain-specific patterns, it may be prone to overfitting because of the constraints of the task-specific corpus. Our findings highlight the importance of domain-specific vocabulary during pre-training to improve performance. They also affirmed that a combination of general domain knowledge and task-specific fine-tuning is crucial for optimal performance across various categories. This study offers key insights for future research on pre-training techniques in the clinical domain, particularly for low-resource languages."",
}
@",clinical domain,natural language process,natural languag,language process,nlp,evalu
" ""Textual Entailment for Temporal Dependency Graph Parsing"","," ""We explore temporal dependency graph (TDG) parsing in the clinical domain. We leverage existing annotations on the THYME dataset to semi-automatically construct a TDG corpus. Then we propose a new natural language inference (NLI) approach to TDG parsing, and evaluate it both on general domain TDGs from wikinews and the newly constructed clinical TDG corpus. We achieve competitive performance on general domain TDGs with a much simpler model than prior work. On the clinical TDGs, our method establishes the first result of TDG parsing on clinical data with 0.79/0.88 micro/macro F1."",","{yao-etal-2023-textual,
    title = ""Textual Entailment for Temporal Dependency Graph Parsing"",
    author = ""Yao, Jiarui  and
      Bethard, Steven  and
      Wright-Bettner, Kristin  and
      Goldner, Eli  and
      Harris, David  and
      Savova, Guergana"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.25"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.25"",
    pages = ""191--199"",
    abstract = ""We explore temporal dependency graph (TDG) parsing in the clinical domain. We leverage existing annotations on the THYME dataset to semi-automatically construct a TDG corpus. Then we propose a new natural language inference (NLI) approach to TDG parsing, and evaluate it both on general domain TDGs from wikinews and the newly constructed clinical TDG corpus. We achieve competitive performance on general domain TDGs with a much simpler model than prior work. On the clinical TDGs, our method establishes the first result of TDG parsing on clinical data with 0.79/0.88 micro/macro F1."",
}
@",clinical domain,natural language process,natural languag,language process,nlp,infer,annotat,evalu
" ""Method for Designing Semantic Annotation of Sepsis Signs in Clinical Text"","," ""Annotated clinical text corpora are essential for machine learning studies that model and predict care processes and disease progression. However, few studies describe the necessary experimental design of the annotation guideline and annotation phases. This makes replication, reuse, and adoption challenging. Using clinical questions about sepsis, we designed a semantic annotation guideline to capture sepsis signs from clinical text. The clinical questions aid guideline design, application, and evaluation. Our method incrementally evaluates each change in the guideline by testing the resulting annotated corpus using clinical questions. Additionally, our method uses inter-annotator agreement to judge the annotator compliance and quality of the guideline. We show that the method, combined with controlled design increments, is simple and allows the development and measurable improvement of a purpose-built semantic annotation guideline. We believe that our approach is useful for incremental design of semantic annotation guidelines in general."",","{yan-etal-2023-method,
    title = ""Method for Designing Semantic Annotation of Sepsis Signs in Clinical Text"",
    author = ""Yan, Melissa  and
      Gustad, Lise  and
      H{\o}vik, Lise  and
      Nytr{\o}, {\O}ystein"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.29"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.29"",
    pages = ""236--246"",
    abstract = ""Annotated clinical text corpora are essential for machine learning studies that model and predict care processes and disease progression. However, few studies describe the necessary experimental design of the annotation guideline and annotation phases. This makes replication, reuse, and adoption challenging. Using clinical questions about sepsis, we designed a semantic annotation guideline to capture sepsis signs from clinical text. The clinical questions aid guideline design, application, and evaluation. Our method incrementally evaluates each change in the guideline by testing the resulting annotated corpus using clinical questions. Additionally, our method uses inter-annotator agreement to judge the annotator compliance and quality of the guideline. We show that the method, combined with controlled design increments, is simple and allows the development and measurable improvement of a purpose-built semantic annotation guideline. We believe that our approach is useful for incremental design of semantic annotation guidelines in general."",
}
@",clinical text,natural language process,natural languag,language process,nlp,semant,annotat,challeng,evalu
" ""Cross-domain {G}erman Medical Named Entity Recognition using a Pre-Trained Language Model and Unified Medical Semantic Types"","," ""Information extraction from clinical text has the potential to facilitate clinical research and personalized clinical care, but annotating large amounts of data for each set of target tasks is prohibitive. We present a German medical Named Entity Recognition (NER) system capable of cross-domain knowledge transferring. The system builds on a pre-trained German language model and a token-level binary classifier, employing semantic types sourced from the Unified Medical Language System (UMLS) as entity labels to identify corresponding entity spans within the input text. To enhance the system{'}s performance and robustness, we pre-train it using a medical literature corpus that incorporates UMLS semantic term annotations. We evaluate the system{'}s effectiveness on two German annotated datasets obtained from different clinics in zero- and few-shot settings. The results show that our approach outperforms task-specific Condition Random Fields (CRF) classifiers in terms of accuracy. Our work contributes to developing robust and transparent German medical NER models that can support the extraction of information from various clinical texts."",","{liang-etal-2023-cross,
    title = ""Cross-domain {G}erman Medical Named Entity Recognition using a Pre-Trained Language Model and Unified Medical Semantic Types"",
    author = ""Liang, Siting  and
      Hartmann, Mareike  and
      Sonntag, Daniel"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.31"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.31"",
    pages = ""259--271"",
    abstract = ""Information extraction from clinical text has the potential to facilitate clinical research and personalized clinical care, but annotating large amounts of data for each set of target tasks is prohibitive. We present a German medical Named Entity Recognition (NER) system capable of cross-domain knowledge transferring. The system builds on a pre-trained German language model and a token-level binary classifier, employing semantic types sourced from the Unified Medical Language System (UMLS) as entity labels to identify corresponding entity spans within the input text. To enhance the system{'}s performance and robustness, we pre-train it using a medical literature corpus that incorporates UMLS semantic term annotations. We evaluate the system{'}s effectiveness on two German annotated datasets obtained from different clinics in zero- and few-shot settings. The results show that our approach outperforms task-specific Condition Random Fields (CRF) classifiers in terms of accuracy. Our work contributes to developing robust and transparent German medical NER models that can support the extraction of information from various clinical texts."",
}
@",clinical text,natural language process,natural languag,language process,nlp,entity recognit,semant,annotat,evalu
" ""Multilingual Clinical {NER}: Translation or Cross-lingual Transfer?"","," ""Natural language tasks like Named Entity Recognition (NER) in the clinical domain on non-English texts can be very time-consuming and expensive due to the lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent this issue thanks to the ability of multilingual large language models to be fine-tuned on a specific task in one language and to provide high accuracy for the same task in another language. However, other methods leveraging translation models can be used to perform NER without annotated data in the target language, by either translating the training set or test set. This paper compares cross-lingual transfer with these two alternative methods, to perform clinical NER in French and in German without any training data in those languages. To this end, we release MedNERF a medical NER test set extracted from French drug prescriptions and annotated with the same guidelines as an English dataset. Through extensive experiments on this dataset and on a German medical dataset (Frei and Kramer, 2021), we show that translation-based methods can achieve similar performance to CLT but require more care in their design. And while they can take advantage of monolingual clinical language models, those do not guarantee better results than large general-purpose multilingual models, whether with cross-lingual transfer or translation."",","{gaschi-etal-2023-multilingual,
    title = ""Multilingual Clinical {NER}: Translation or Cross-lingual Transfer?"",
    author = ""Gaschi, F{\'e}lix  and
      Fontaine, Xavier  and
      Rastin, Parisa  and
      Toussaint, Yannick"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.34"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.34"",
    pages = ""289--311"",
    abstract = ""Natural language tasks like Named Entity Recognition (NER) in the clinical domain on non-English texts can be very time-consuming and expensive due to the lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent this issue thanks to the ability of multilingual large language models to be fine-tuned on a specific task in one language and to provide high accuracy for the same task in another language. However, other methods leveraging translation models can be used to perform NER without annotated data in the target language, by either translating the training set or test set. This paper compares cross-lingual transfer with these two alternative methods, to perform clinical NER in French and in German without any training data in those languages. To this end, we release MedNERF a medical NER test set extracted from French drug prescriptions and annotated with the same guidelines as an English dataset. Through extensive experiments on this dataset and on a German medical dataset (Frei and Kramer, 2021), we show that translation-based methods can achieve similar performance to CLT but require more care in their design. And while they can take advantage of monolingual clinical language models, those do not guarantee better results than large general-purpose multilingual models, whether with cross-lingual transfer or translation."",
}
@",clinical domain,natural language process,natural languag,language process,translat,nlp,entity recognit,annotat
" ""{UMLS}-{KGI}-{BERT}: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition"","," ""Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the biomedical domain, significant progress has been made in adapting this paradigm to NLP tasks that require the integration of domain-specific knowledge as well as statistical modelling of language. In particular, research in this area has focused on the question of how best to construct LMs that take into account not only the patterns of token distribution in medical text, but also the wealth of structured information contained in terminology resources such as the UMLS. This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS.This allows for graph-based learning objectives to be combined with masked-language pre-training. Preliminary results from experiments in the extension of pre-trained LMs as well as training from scratch show that this framework improves downstream performance on multiple biomedical and clinical Named Entity Recognition (NER) tasks. All pre-trained models, data processing pipelines and evaluation scripts will be made publicly available."",","{mannion-etal-2023-umls,
    title = ""{UMLS}-{KGI}-{BERT}: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition"",
    author = ""Mannion, Aidan  and
      Schwab, Didier  and
      Goeuriot, Lorraine"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.35"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.35"",
    pages = ""312--322"",
    abstract = ""Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the biomedical domain, significant progress has been made in adapting this paradigm to NLP tasks that require the integration of domain-specific knowledge as well as statistical modelling of language. In particular, research in this area has focused on the question of how best to construct LMs that take into account not only the patterns of token distribution in medical text, but also the wealth of structured information contained in terminology resources such as the UMLS. This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS.This allows for graph-based learning objectives to be combined with masked-language pre-training. Preliminary results from experiments in the extension of pre-trained LMs as well as training from scratch show that this framework improves downstream performance on multiple biomedical and clinical Named Entity Recognition (NER) tasks. All pre-trained models, data processing pipelines and evaluation scripts will be made publicly available."",
}
@",medical domain,medical text,natural language process,natural languag,language process,nlp,entity recognit,sentiment,evalu
" ""{W}ang{L}ab at {MEDIQA}-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models"","," ""This paper describes our submission to the MEDIQA-Chat 2023 shared task for automatic clinical note generation from doctor-patient conversations. We report results for two approaches: the first fine-tunes a pre-trained language model (PLM) on the shared task data, and the second uses few-shot in-context learning (ICL) with a large language model (LLM). Both achieve high performance as measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and first, respectively, of all submissions to the shared task. Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations."",","{giorgi-etal-2023-wanglab,
    title = ""{W}ang{L}ab at {MEDIQA}-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models"",
    author = ""Giorgi, John  and
      Toma, Augustin  and
      Xie, Ronald  and
      Chen, Sondra  and
      An, Kevin  and
      Zheng, Grace  and
      Wang, Bo"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.36"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.36"",
    pages = ""323--334"",
    abstract = ""This paper describes our submission to the MEDIQA-Chat 2023 shared task for automatic clinical note generation from doctor-patient conversations. We report results for two approaches: the first fine-tunes a pre-trained language model (PLM) on the shared task data, and the second uses few-shot in-context learning (ICL) with a large language model (LLM). Both achieve high performance as measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and first, respectively, of all submissions to the shared task. Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations."",
}
@",clinical not,natural language process,natural languag,language process,nlp,generat,shared task
" ""Intersectionality and Testimonial Injustice in Medical Records"","," ""Detecting testimonial injustice is an essential element of addressing inequities and promoting inclusive healthcare practices, many of which are life-critical. However, using a single demographic factor to detect testimonial injustice does not fully encompass the nuanced identities that contribute to a patient{'}s experience. Further, some injustices may only be evident when examining the nuances that arise through the lens of intersectionality. Ignoring such injustices can result in poor quality of care or life-endangering events. Thus, considering intersectionality could result in more accurate classifications and just decisions. To illustrate this, we use real-world medical data to determine whether medical records exhibit words that could lead to testimonial injustice, employ fairness metrics (e.g. demographic parity, differential intersectional fairness, and subgroup fairness) to assess the severity to which subgroups are experiencing testimonial injustice, and analyze how the intersectionality of demographic features (e.g. gender and race) make a difference in uncovering testimonial injustice. From our analysis we found that with intersectionality we can better see disparities in how subgroups are treated and there are differences in how someone is treated based on the intersection of their demographic attributes. This has not been previously studied in clinical records, nor has it been proven through empirical study."",","{andrews-etal-2023-intersectionality,
    title = ""Intersectionality and Testimonial Injustice in Medical Records"",
    author = ""Andrews, Kenya  and
      Shah, Bhuvni  and
      Cheng, Lu"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.39"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.39"",
    pages = ""358--372"",
    abstract = ""Detecting testimonial injustice is an essential element of addressing inequities and promoting inclusive healthcare practices, many of which are life-critical. However, using a single demographic factor to detect testimonial injustice does not fully encompass the nuanced identities that contribute to a patient{'}s experience. Further, some injustices may only be evident when examining the nuances that arise through the lens of intersectionality. Ignoring such injustices can result in poor quality of care or life-endangering events. Thus, considering intersectionality could result in more accurate classifications and just decisions. To illustrate this, we use real-world medical data to determine whether medical records exhibit words that could lead to testimonial injustice, employ fairness metrics (e.g. demographic parity, differential intersectional fairness, and subgroup fairness) to assess the severity to which subgroups are experiencing testimonial injustice, and analyze how the intersectionality of demographic features (e.g. gender and race) make a difference in uncovering testimonial injustice. From our analysis we found that with intersectionality we can better see disparities in how subgroups are treated and there are differences in how someone is treated based on the intersection of their demographic attributes. This has not been previously studied in clinical records, nor has it been proven through empirical study."",
}
@",medical record,clinical record,natural language process,natural languag,language process,nlp,assess
" ""Interactive Span Recommendation for Biomedical Text"","," ""Motivated by the scarcity of high-quality labeled biomedical text, as well as the success of data programming, we introduce KRISS-Search. By leveraging the Unified Medical Language Systems (UMLS) ontology, KRISS-Search addresses an interactive few-shot span recommendation task that we propose. We first introduce unsupervised KRISS-Search and show that our method outperforms existing methods in identifying spans that are semantically similar to a given span of interest, with {\textgreater}50{\%} AUPRC improvement relative to PubMedBERT. We then introduce supervised KRISS-Search, which leverages human interaction to improve the notion of similarity used by unsupervised KRISS-Search. Through simulated human feedback, we demonstrate an enhanced F1 score of 0.68 in classifying spans as semantically similar or different in the low-label setting, outperforming PubMedBERT by 2 F1 points. Finally, supervised KRISS-Search demonstrates competitive or superior performance compared to PubMedBERT in few-shot biomedical named entity recognition (NER) across five benchmark datasets, with an average improvement of 5.6 F1 points. We envision KRISS-Search increasing the efficiency of programmatic data labeling and also providing broader utility as an interactive biomedical search engine."",","{blankemeier-etal-2023-interactive,
    title = ""Interactive Span Recommendation for Biomedical Text"",
    author = ""Blankemeier, Louis  and
      Zhao, Theodore  and
      Tinn, Robert  and
      Kiblawi, Sid  and
      Gu, Yu  and
      Chaudhari, Akshay  and
      Poon, Hoifung  and
      Zhang, Sheng  and
      Wei, Mu  and
      Preston, J."",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.40"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.40"",
    pages = ""373--384"",
    abstract = ""Motivated by the scarcity of high-quality labeled biomedical text, as well as the success of data programming, we introduce KRISS-Search. By leveraging the Unified Medical Language Systems (UMLS) ontology, KRISS-Search addresses an interactive few-shot span recommendation task that we propose. We first introduce unsupervised KRISS-Search and show that our method outperforms existing methods in identifying spans that are semantically similar to a given span of interest, with {\textgreater}50{\%} AUPRC improvement relative to PubMedBERT. We then introduce supervised KRISS-Search, which leverages human interaction to improve the notion of similarity used by unsupervised KRISS-Search. Through simulated human feedback, we demonstrate an enhanced F1 score of 0.68 in classifying spans as semantically similar or different in the low-label setting, outperforming PubMedBERT by 2 F1 points. Finally, supervised KRISS-Search demonstrates competitive or superior performance compared to PubMedBERT in few-shot biomedical named entity recognition (NER) across five benchmark datasets, with an average improvement of 5.6 F1 points. We envision KRISS-Search increasing the efficiency of programmatic data labeling and also providing broader utility as an interactive biomedical search engine."",
}
@",medical text,natural language process,natural languag,language process,nlp,entity recognit,semant,benchmark
" ""Prompt-based Extraction of Social Determinants of Health Using Few-shot Learning"","," ""Social determinants of health (SDOH) documented in the electronic health record through unstructured text are increasingly being studied to understand how SDOH impacts patient health outcomes. In this work, we utilize the Social History Annotation Corpus (SHAC), a multi-institutional corpus of de-identified social history sections annotated for SDOH, including substance use, employment, and living status information. We explore the automatic extraction of SDOH information with SHAC in both standoff and inline annotation formats using GPT-4 in a one-shot prompting setting. We compare GPT-4 extraction performance with a high-performing supervised approach and perform thorough error analyses. Our prompt-based GPT-4 method achieved an overall 0.652 F1 on the SHAC test set, similar to the 7th best-performing system among all teams in the n2c2 challenge with SHAC."",","{ramachandran-etal-2023-prompt,
    title = ""Prompt-based Extraction of Social Determinants of Health Using Few-shot Learning"",
    author = ""Ramachandran, Giridhar Kaushik  and
      Fu, Yujuan  and
      Han, Bin  and
      Lybarger, Kevin  and
      Dobbins, Nic  and
      Uzuner, Ozlem  and
      Yetisgen, Meliha"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.41"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.41"",
    pages = ""385--393"",
    abstract = ""Social determinants of health (SDOH) documented in the electronic health record through unstructured text are increasingly being studied to understand how SDOH impacts patient health outcomes. In this work, we utilize the Social History Annotation Corpus (SHAC), a multi-institutional corpus of de-identified social history sections annotated for SDOH, including substance use, employment, and living status information. We explore the automatic extraction of SDOH information with SHAC in both standoff and inline annotation formats using GPT-4 in a one-shot prompting setting. We compare GPT-4 extraction performance with a high-performing supervised approach and perform thorough error analyses. Our prompt-based GPT-4 method achieved an overall 0.652 F1 on the SHAC test set, similar to the 7th best-performing system among all teams in the n2c2 challenge with SHAC."",
}
@",electronic health record,health record,natural language process,natural languag,language process,nlp,annotat,challeng
" ""Teddysum at {MEDIQA}-Chat 2023: an analysis of fine-tuning strategy for long dialog summarization"","," ""In this paper, we introduce the design and various attempts for TaskB of MEDIQA-Chat 2023. The goal of TaskB in MEDIQA-Chat 2023 is to generate full clinical note from doctor-patient consultation dialogues. This task has several challenging issues, such as lack of training data, handling long dialogue inputs, and generating semi-structured clinical note which have section heads. To address these issues, we conducted various experiments and analyzed their results. We utilized the DialogLED model pre-trained on long dialogue data to handle long inputs, and we pre-trained on other dialogue datasets to address the lack of training data. We also attempted methods such as using prompts and contrastive learning for handling sections. This paper provides insights into clinical note generation through analyzing experimental methods and results, and it suggests future research directions."",","{jeong-etal-2023-teddysum,
    title = ""Teddysum at {MEDIQA}-Chat 2023: an analysis of fine-tuning strategy for long dialog summarization"",
    author = ""Jeong, Yongbin  and
      Han, Ju-Hyuck  and
      Chae, Kyung Min  and
      Cho, Yousang  and
      Seo, Hyunbin  and
      Lim, KyungTae  and
      Choi, Key-Sun  and
      Hahm, Younggyun"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.42"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.42"",
    pages = ""394--402"",
    abstract = ""In this paper, we introduce the design and various attempts for TaskB of MEDIQA-Chat 2023. The goal of TaskB in MEDIQA-Chat 2023 is to generate full clinical note from doctor-patient consultation dialogues. This task has several challenging issues, such as lack of training data, handling long dialogue inputs, and generating semi-structured clinical note which have section heads. To address these issues, we conducted various experiments and analyzed their results. We utilized the DialogLED model pre-trained on long dialogue data to handle long inputs, and we pre-trained on other dialogue datasets to address the lack of training data. We also attempted methods such as using prompts and contrastive learning for handling sections. This paper provides insights into clinical note generation through analyzing experimental methods and results, and it suggests future research directions."",
}
@",clinical not,natural language process,natural languag,language process,nlp,generat,summar,challeng
" ""Rare Codes Count: Mining Inter-code Relations for Long-tail Clinical Text Classification"","," ""Multi-label clinical text classification, such as automatic ICD coding, has always been a challenging subject in Natural Language Processing, due to its long, domain-specific documents and long-tail distribution over a large label set. Existing methods adopt different model architectures to encode the clinical notes. Whereas without digging out the useful connections between labels, the model presents a huge gap in predicting performances between rare and frequent codes. In this work, we propose a novel method for further mining the helpful relations between different codes via a relation-enhanced code encoder to improve the rare code performance. Starting from the simple code descriptions, the model reaches comparable, even better performances than models with heavy external knowledge. Our proposed method is evaluated on MIMIC-III, a common dataset in the medical domain. It outperforms the previous state-of-art models on both overall metrics and rare code performances. Moreover, the interpretation results further prove the effectiveness of our methods. Our code is publicly available at \url{https://github.com/jiaminchen-1031/Rare-ICD}."",","{chen-etal-2023-rare,
    title = ""Rare Codes Count: Mining Inter-code Relations for Long-tail Clinical Text Classification"",
    author = ""Chen, Jiamin  and
      Li, Xuhong  and
      Xi, Junting  and
      Yu, Lei  and
      Xiong, Haoyi"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.43"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.43"",
    pages = ""403--413"",
    abstract = ""Multi-label clinical text classification, such as automatic ICD coding, has always been a challenging subject in Natural Language Processing, due to its long, domain-specific documents and long-tail distribution over a large label set. Existing methods adopt different model architectures to encode the clinical notes. Whereas without digging out the useful connections between labels, the model presents a huge gap in predicting performances between rare and frequent codes. In this work, we propose a novel method for further mining the helpful relations between different codes via a relation-enhanced code encoder to improve the rare code performance. Starting from the simple code descriptions, the model reaches comparable, even better performances than models with heavy external knowledge. Our proposed method is evaluated on MIMIC-III, a common dataset in the medical domain. It outperforms the previous state-of-art models on both overall metrics and rare code performances. Moreover, the interpretation results further prove the effectiveness of our methods. Our code is publicly available at \url{https://github.com/jiaminchen-1031/Rare-ICD}."",
}
@",medical domain,clinical not,clinical text,natural language process,natural languag,language process,nlp,challeng,evalu
" ""{N}ew{A}ge{H}ealth{W}arriors at {MEDIQA}-Chat 2023 Task A: Summarizing Short Medical Conversation with Transformers"","," ""This paper presents the MEDIQA-Chat 2023 shared task organized at the ACL-Clinical NLP workshop. The shared task is motivated by the need to develop methods to automatically generate clinical notes from doctor-patient conversations. In this paper, we present our submission for \textit{MEDIQA-Chat 2023 Task A: Short Dialogue2Note Summarization}. Manual creation of these clinical notes requires extensive human efforts, thus making it a time-consuming and expensive process. To address this, we propose an ensemble-based method over GPT-3, BART, BERT variants, and Rule-based systems to automatically generate clinical notes from these conversations. The proposed system achieves a score of 0.730 and 0.544 for both the sub-tasks on the test set (ranking 8th on the leaderboard for both tasks) and shows better performance compared to a baseline system using BART variants."",","{mishra-desetty-2023-newagehealthwarriors,
    title = ""{N}ew{A}ge{H}ealth{W}arriors at {MEDIQA}-Chat 2023 Task A: Summarizing Short Medical Conversation with Transformers"",
    author = ""Mishra, Prakhar  and
      Desetty, Ravi Theja"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.44"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.44"",
    pages = ""414--421"",
    abstract = ""This paper presents the MEDIQA-Chat 2023 shared task organized at the ACL-Clinical NLP workshop. The shared task is motivated by the need to develop methods to automatically generate clinical notes from doctor-patient conversations. In this paper, we present our submission for \textit{MEDIQA-Chat 2023 Task A: Short Dialogue2Note Summarization}. Manual creation of these clinical notes requires extensive human efforts, thus making it a time-consuming and expensive process. To address this, we propose an ensemble-based method over GPT-3, BART, BERT variants, and Rule-based systems to automatically generate clinical notes from these conversations. The proposed system achieves a score of 0.730 and 0.544 for both the sub-tasks on the test set (ranking 8th on the leaderboard for both tasks) and shows better performance compared to a baseline system using BART variants."",
}
@",clinical not,natural language process,natural languag,language process,nlp,generat,summar,shared task
" ""A Survey of Evaluation Methods of Generated Medical Textual Reports"","," ""Medical Report Generation (MRG) is a sub-task of Natural Language Generation (NLG) and aims to present information from various sources in textual form and synthesize salient information, with the goal of reducing the time spent by domain experts in writing medical reports and providing support information for decision-making. Given the specificity of the medical domain, the evaluation of automatically generated medical reports is of paramount importance to the validity of these systems. Therefore, in this paper, we focus on the evaluation of automatically generated medical reports from the perspective of automatic and human evaluation. We present evaluation methods for general NLG evaluation and how they have been applied to domain-specific medical tasks. The study shows that MRG evaluation methods are very diverse, and that further work is needed to build shared evaluation methods. The state of the art also emphasizes that such an evaluation must be task specific and include human assessments, requesting the participation of experts in the field."",","{zhou-etal-2023-survey,
    title = ""A Survey of Evaluation Methods of Generated Medical Textual Reports"",
    author = ""Zhou, Yongxin  and
      Ringeval, Fabien  and
      Portet, Fran{\c{c}}ois"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.48"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.48"",
    pages = ""447--459"",
    abstract = ""Medical Report Generation (MRG) is a sub-task of Natural Language Generation (NLG) and aims to present information from various sources in textual form and synthesize salient information, with the goal of reducing the time spent by domain experts in writing medical reports and providing support information for decision-making. Given the specificity of the medical domain, the evaluation of automatically generated medical reports is of paramount importance to the validity of these systems. Therefore, in this paper, we focus on the evaluation of automatically generated medical reports from the perspective of automatic and human evaluation. We present evaluation methods for general NLG evaluation and how they have been applied to domain-specific medical tasks. The study shows that MRG evaluation methods are very diverse, and that further work is needed to build shared evaluation methods. The state of the art also emphasizes that such an evaluation must be task specific and include human assessments, requesting the participation of experts in the field."",
}
@",medical domain,medical text,natural language process,natural languag,language process,nlp,generat,evalu,assess
" ""Overview of the {MEDIQA}-Chat 2023 Shared Tasks on the Summarization {\&} Generation of Doctor-Patient Conversations"","," ""Automatic generation of clinical notes from doctor-patient conversations can play a key role in reducing daily doctors{'} workload and improving their interactions with the patients. MEDIQA-Chat 2023 aims to advance and promote research on effective solutions through shared tasks on the automatic summarization of doctor-patient conversations and on the generation of synthetic dialogues from clinical notes for data augmentation. Seventeen teams participated in the challenge and experimented with a broad range of approaches and models. In this paper, we describe the three MEDIQA-Chat 2023 tasks, the datasets, and the participants{'} results and methods. We hope that these shared tasks will lead to additional research efforts and insights on the automatic generation and evaluation of clinical notes."",","{ben-abacha-etal-2023-overview,
    title = ""Overview of the {MEDIQA}-Chat 2023 Shared Tasks on the Summarization {\&} Generation of Doctor-Patient Conversations"",
    author = ""Ben Abacha, Asma  and
      Yim, Wen-wai  and
      Adams, Griffin  and
      Snider, Neal  and
      Yetisgen, Meliha"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.52"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.52"",
    pages = ""503--513"",
    abstract = ""Automatic generation of clinical notes from doctor-patient conversations can play a key role in reducing daily doctors{'} workload and improving their interactions with the patients. MEDIQA-Chat 2023 aims to advance and promote research on effective solutions through shared tasks on the automatic summarization of doctor-patient conversations and on the generation of synthetic dialogues from clinical notes for data augmentation. Seventeen teams participated in the challenge and experimented with a broad range of approaches and models. In this paper, we describe the three MEDIQA-Chat 2023 tasks, the datasets, and the participants{'} results and methods. We hope that these shared tasks will lead to additional research efforts and insights on the automatic generation and evaluation of clinical notes."",
}
@",clinical not,natural language process,natural languag,language process,nlp,generat,summar,shared task,challeng,evalu
" ""Transfer Learning for Low-Resource Clinical Named Entity Recognition"","," ""We propose a transfer learning method that adapts a high-resource English clinical NER model to low-resource languages and domains using only small amounts of in-domain annotated data. Our approach involves translating in-domain datasets to English, fine-tuning the English model on the translated data, and then transferring it to the target language/domain. Experiments on Spanish, French, and conversational clinical text datasets show accuracy gains over models trained on target data alone. Our method achieves state-of-the-art performance and can enable clinical NLP in more languages and modalities with limited resources."",","{sasikumar-mantri-2023-transfer,
    title = ""Transfer Learning for Low-Resource Clinical Named Entity Recognition"",
    author = ""Sasikumar, Nevasini  and
      Mantri, Krishna Sri Ipsit"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.53"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.53"",
    pages = ""514--518"",
    abstract = ""We propose a transfer learning method that adapts a high-resource English clinical NER model to low-resource languages and domains using only small amounts of in-domain annotated data. Our approach involves translating in-domain datasets to English, fine-tuning the English model on the translated data, and then transferring it to the target language/domain. Experiments on Spanish, French, and conversational clinical text datasets show accuracy gains over models trained on target data alone. Our method achieves state-of-the-art performance and can enable clinical NLP in more languages and modalities with limited resources."",
}
@",clinical text,natural language process,natural languag,language process,translat,nlp,entity recognit,annotat
" ""{C}are4{L}ang at {MEDIQA}-Chat 2023: Fine-tuning Language Models for Classifying and Summarizing Clinical Dialogues"","," ""Summarizing medical conversations is one of the tasks proposed by MEDIQA-Chat to promote research on automatic clinical note generation from doctor-patient conversations. In this paper, we present our submission to this task using fine-tuned language models, including T5, BART and BioGPT models. The fine-tuned models are evaluated using ensemble metrics including ROUGE, BERTScore andBLEURT. Among the fine-tuned models, Flan-T5 achieved the highest aggregated score for dialogue summarization."",","{alqahtani-etal-2023-care4lang,
    title = ""{C}are4{L}ang at {MEDIQA}-Chat 2023: Fine-tuning Language Models for Classifying and Summarizing Clinical Dialogues"",
    author = ""Alqahtani, Amal  and
      Salama, Rana  and
      Diab, Mona  and
      Youssef, Abdou"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.55"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.55"",
    pages = ""524--528"",
    abstract = ""Summarizing medical conversations is one of the tasks proposed by MEDIQA-Chat to promote research on automatic clinical note generation from doctor-patient conversations. In this paper, we present our submission to this task using fine-tuned language models, including T5, BART and BioGPT models. The fine-tuned models are evaluated using ensemble metrics including ROUGE, BERTScore andBLEURT. Among the fine-tuned models, Flan-T5 achieved the highest aggregated score for dialogue summarization."",
}
@",clinical not,natural language process,natural languag,language process,nlp,generat,summar,evalu
" ""{C}alvados at {MEDIQA}-Chat 2023: Improving Clinical Note Generation with Multi-Task Instruction Finetuning"","," ""This paper presents our system for the MEDIQA-Chat 2023 shared task on medical conversation summarization. Our approach involves finetuning a LongT5 model on multiple tasks simultaneously, which we demonstrate improves the model{'}s overall performance while reducing the number of factual errors and hallucinations in the generated summary. Furthermore, we investigated the effect of augmenting the data with in-text annotations from a clinical named entity recognition model, finding that this approach decreased summarization quality. Lastly, we explore using different text generation strategies for medical note generation based on the length of the note. Our findings suggest that the application of our proposed approach can be beneficial for improving the accuracy and effectiveness of medical conversation summarization."",","{milintsevich-agarwal-2023-calvados,
    title = ""{C}alvados at {MEDIQA}-Chat 2023: Improving Clinical Note Generation with Multi-Task Instruction Finetuning"",
    author = ""Milintsevich, Kirill  and
      Agarwal, Navneet"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.56"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.56"",
    pages = ""529--535"",
    abstract = ""This paper presents our system for the MEDIQA-Chat 2023 shared task on medical conversation summarization. Our approach involves finetuning a LongT5 model on multiple tasks simultaneously, which we demonstrate improves the model{'}s overall performance while reducing the number of factual errors and hallucinations in the generated summary. Furthermore, we investigated the effect of augmenting the data with in-text annotations from a clinical named entity recognition model, finding that this approach decreased summarization quality. Lastly, we explore using different text generation strategies for medical note generation based on the length of the note. Our findings suggest that the application of our proposed approach can be beneficial for improving the accuracy and effectiveness of medical conversation summarization."",
}
@",clinical not,natural language process,natural languag,language process,nlp,generat,entity recognit,summar,shared task,annotat
" ""Proceedings of the 2023 CLASP Conference on Learning with Small Data (LSD)"","," ""This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared task, encompassing both subtask A and subtask B. We approach the task as a dialogue summarization problem and implement two distinct pipelines: (a) a fine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b) few-shot in-context learning (ICL) using a large language model, GPT-4. Both methods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1 (deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421, respectively. Additionally, we predict the associated section headers using RoBERTa and SciBERT based classification models. Our team ranked fourth among all teams, while each team is allowed to submit three runs as part of their submission. We also utilize expert annotations to demonstrate that the notes generated through the ICL GPT-4 are better than all other baselines. The code for our submission is available."",","{tang-etal-2023-gersteinlab,
    title = ""{G}erstein{L}ab at {MEDIQA}-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning"",
    author = ""Tang, Xiangru  and
      Tran, Andrew  and
      Tan, Jeffrey  and
      Gerstein, Mark"",
    editor = ""Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clinicalnlp-1.58"",
    doi = ""10.18653/v1/2023.clinicalnlp-1.58"",
    pages = ""546--554"",
    abstract = ""This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared task, encompassing both subtask A and subtask B. We approach the task as a dialogue summarization problem and implement two distinct pipelines: (a) a fine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b) few-shot in-context learning (ICL) using a large language model, GPT-4. Both methods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1 (deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421, respectively. Additionally, we predict the associated section headers using RoBERTa and SciBERT based classification models. Our team ranked fourth among all teams, while each team is allowed to submit three runs as part of their submission. We also utilize expert annotations to demonstrate that the notes generated through the ICL GPT-4 are better than all other baselines. The code for our submission is available."",
}
@proceedings{clasp-2023-2023,
    title = ""Proceedings of the 2023 CLASP Conference on Learning with Small Data (LSD)"",
    editor = ""Breitholtz, Ellen  and
      Lappin, Shalom  and
      Loaiciga, Sharid  and
      Ilinykh, Nikolai  and
      Dobnik, Simon"",
    month = sep,
    year = ""2023"",
    address = ""Gothenburg, Sweden"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clasp-1.0"",
}
@",clinical not,natural language process,natural languag,language process,nlp,generat,summar,shared task,annotat
" ""Conversational {AI}: Dialogue Systems, Conversational Agents, and Chatbots by {M}ichael {M}c{T}ear"","," ""Against the backdrop of the ever-improving Natural Language Inference (NLI) models, recent efforts have focused on the suitability of the current NLI datasets and on the feasibility of the NLI task as it is currently approached. Many of the recent studies have exposed the inherent human disagreements of the inference task and have proposed a shift from categorical labels to human subjective probability assessments, capturing human uncertainty. In this work, we show how neither the current task formulation nor the proposed uncertainty gradient are entirely suitable for solving the NLI challenges. Instead, we propose an ordered sense space annotation, which distinguishes between logical and common-sense inference. One end of the space captures non-sensical inferences, while the other end represents strictly logical scenarios. In the middle of the space, we find a continuum of common-sense, namely, the subjective and graded opinion of a {``}person on the street.{''} To arrive at the proposed annotation scheme, we perform a careful investigation of the SICK corpus and we create a taxonomy of annotation issues and guidelines. We re-annotate the corpus with the proposed annotation scheme, utilizing four symbolic inference systems, and then perform a thorough evaluation of the scheme by fine-tuning and testing commonly used pre-trained language models on the re-annotated SICK within various settings. We also pioneer a crowd annotation of a small portion of the MultiNLI corpus, showcasing that it is possible to adapt our scheme for annotation by non-experts on another NLI corpus. Our work shows the efficiency and benefits of the proposed mechanism and opens the way for a careful NLI task refinement."",","{dobnik-kelleher-2023-role,
    title = ""On the role of resources in the age of large language models"",
    author = ""Dobnik, Simon  and
      Kelleher, John"",
    editor = ""Breitholtz, Ellen  and
      Lappin, Shalom  and
      Loaiciga, Sharid  and
      Ilinykh, Nikolai  and
      Dobnik, Simon"",
    booktitle = ""Proceedings of the 2023 CLASP Conference on Learning with Small Data (LSD)"",
    month = sep,
    year = ""2023"",
    address = ""Gothenburg, Sweden"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.clasp-1.20"",
    pages = ""191--197"",
    abstract = ""We evaluate the role of expert-based domain knowledge and resources in relation to training large language models by referring to our work on training and evaluating neural models, also in under-resourced scenarios which we believe also informs training models for {``}well-resourced{''} languages and domains. We argue that our community needs both large-scale datasets and small but high-quality data based on expert knowledge and that both activities should work hand-in-hand."",
}
@article{varda-marelli-2023-data,
    title = ""Data-driven Cross-lingual Syntax: An Agreement Study with Massively Multilingual Models"",
    author = ""Varda, Andrea Gregor de  and
      Marelli, Marco"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""2"",
    month = jun,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-2.1"",
    doi = ""10.1162/coli_a_00472"",
    pages = ""261--299"",
    abstract = ""Massively multilingual models such as mBERT and XLM-R are increasingly valued in Natural Language Processing research and applications, due to their ability to tackle the uneven distribution of resources available for different languages. The models{'} ability to process multiple languages relying on a shared set of parameters raises the question of whether the grammatical knowledge they extracted during pre-training can be considered as a data-driven cross-lingual grammar. The present work studies the inner workings of mBERT and XLM-R in order to test the cross-lingual consistency of the individual neural units that respond to a precise syntactic phenomenon, that is, number agreement, in five languages (English, German, French, Hebrew, Russian). We found that there is a significant overlap in the latent dimensions that encode agreement across the languages we considered. This overlap is larger (a) for long- vis-{\`a}-vis short-distance agreement and (b) when considering XLM-R as compared to mBERT, and peaks in the intermediate layers of the network. We further show that a small set of syntax-sensitive neurons can capture agreement violations across languages; however, their contribution is not decisive in agreement processing."",
}
@article{pasquini-etal-2023-gradual,
    title = ""Gradual Modifications and Abrupt Replacements: Two Stochastic Lexical Ingredients of Language Evolution"",
    author = ""Pasquini, Michele  and
      Serva, Maurizio  and
      Vergni, Davide"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""2"",
    month = jun,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-2.2"",
    doi = ""10.1162/coli_a_00471"",
    pages = ""301--323"",
    abstract = ""The evolution of the vocabulary of a language is characterized by two different random processes: abrupt lexical replacements, when a complete new word emerges to represent a given concept (which was at the basis of the Swadesh foundation of glottochronology in the 1950s), and gradual lexical modifications that progressively alter words over the centuries, considered here in detail for the first time. The main discriminant between these two processes is their impact on cognacy within a family of languages or dialects, since the former modifies the subsets of cognate terms and the latter does not. The automated cognate detection, which is here performed following a new approach inspired by graph theory, is a key preliminary step that allows us to later measure the effects of the slow modification process. We test our dual approach on the family of Malagasy dialects using a cladistic analysis, which provides strong evidence that lexical replacements and gradual lexical modifications are two random processes that separately drive the evolution of languages."",
}
@article{mendonca-etal-2023-onception,
    title = ""Onception: Active Learning with Expert Advice for Real World Machine Translation"",
    author = ""Mendon{\c{c}}a, V{\^a}nia  and
      Rei, Ricardo  and
      Coheur, Lu{\'\i}sa  and
      Sardinha, Alberto"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""2"",
    month = jun,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-2.3"",
    doi = ""10.1162/coli_a_00473"",
    pages = ""325--372"",
    abstract = ""Active learning can play an important role in low-resource settings (i.e., where annotated data is scarce), by selecting which instances may be more worthy to annotate. Most active learning approaches for Machine Translation assume the existence of a pool of sentences in a source language, and rely on human annotators to provide translations or post-edits, which can still be costly. In this article, we apply active learning to a real-world human-in-the-loop scenario in which we assume that: (1) the source sentences may not be readily available, but instead arrive in a stream; (2) the automatic translations receive feedback in the form of a rating, instead of a correct/edited translation, since the human-in-the-loop might be a user looking for a translation, but not be able to provide one. To tackle the challenge of deciding whether each incoming pair source{--}translations is worthy to query for human feedback, we resort to a number of stream-based active learning query strategies. Moreover, because we do not know in advance which query strategy will be the most adequate for a certain language pair and set of Machine Translation models, we propose to dynamically combine multiple strategies using prediction with expert advice. Our experiments on different language pairs and feedback settings show that using active learning allows us to converge on the best Machine Translation systems with fewer human interactions. Furthermore, combining multiple strategies using prediction with expert advice outperforms several individual active learning strategies with even fewer interactions, particularly in partial feedback settings."",
}
@article{garimella-etal-2023-reflection,
    title = ""Reflection of Demographic Background on Word Usage"",
    author = ""Garimella, Aparna  and
      Banea, Carmen  and
      Mihalcea, Rada"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""2"",
    month = jun,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-2.4"",
    doi = ""10.1162/coli_a_00475"",
    pages = ""373--394"",
    abstract = ""The availability of personal writings in electronic format provides researchers in the fields of linguistics, psychology, and computational linguistics with an unprecedented chance to study, on a large scale, the relationship between language use and the demographic background of writers, allowing us to better understand people across different demographics. In this article, we analyze the relation between language and demographics by developing cross-demographic word models to identify words with usage bias, or words that are used in significantly different ways by speakers of different demographics. Focusing on three demographic categories, namely, location, gender, and industry, we identify words with significant usage differences in each category and investigate various approaches of encoding a word{'}s usage, allowing us to identify language aspects that contribute to the differences. Our word models using topic-based features achieve at least 20{\%} improvement in accuracy over the baseline for all demographic categories, even for scenarios with classification into 15 categories, illustrating the usefulness of topic-based features in identifying word usage differences. Further, we note that for location and industry, topics extracted from immediate context are the best predictors of word usages, hinting at the importance of word meaning and its grammatical function for these demographics, while for gender, topics obtained from longer contexts are better predictors for word usage."",
}
@article{zeng-etal-2023-certified,
    title = ""Certified Robustness to Text Adversarial Attacks by Randomized [{MASK}]"",
    author = ""Zeng, Jiehang  and
      Xu, Jianhan  and
      Zheng, Xiaoqing  and
      Huang, Xuanjing"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""2"",
    month = jun,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-2.5"",
    doi = ""10.1162/coli_a_00476"",
    pages = ""395--427"",
    abstract = ""Very recently, few certified defense methods have been developed to provably guarantee the robustness of a text classifier to adversarial synonym substitutions. However, all the existing certified defense methods assume that the defenders have been informed of how the adversaries generate synonyms, which is not a realistic scenario. In this study, we propose a certifiably robust defense method by randomly masking a certain proportion of the words in an input text, in which the above unrealistic assumption is no longer necessary. The proposed method can defend against not only word substitution-based attacks, but also character-level perturbations. We can certify the classifications of over 50{\%} of texts to be robust to any perturbation of five words on AGNEWS, and two words on SST2 dataset. The experimental results show that our randomized smoothing method significantly outperforms recently proposed defense methods across multiple datasets under different attack algorithms."",
}
@article{reig-alamillo-etal-2023-analysis,
    title = ""The Analysis of Synonymy and Antonymy in Discourse Relations: An Interpretable Modeling Approach"",
    author = ""Reig Alamillo, Asela  and
      Torres Moreno, David  and
      Morales Gonz{\'a}lez, Eliseo  and
      Toledo Acosta, Mauricio  and
      Taroni, Antoine  and
      Hermosillo Valadez, Jorge"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""2"",
    month = jun,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-2.6"",
    doi = ""10.1162/coli_a_00477"",
    pages = ""429--464"",
    abstract = ""The idea that discourse relations are interpreted both by explicit content and by shared knowledge between producer and interpreter is pervasive in discourse and linguistic studies. How much weight should be ascribed in this process to the lexical semantics of the arguments is, however, uncertain. We propose a computational approach to analyze contrast and concession relations in the PDTB corpus. Our work sheds light on the question of how much lexical relations contribute to the signaling of such explicit and implicit relations, as well as on the contribution of different parts of speech to these semantic relations. This study contributes to bridging the gap between corpus and computational linguistics by proposing transparent and explainable computational models of discourse relations based on the synonymy and antonymy of their arguments."",
}
@article{apidianaki-2023-word,
    title = ""From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation"",
    author = ""Apidianaki, Marianna"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""2"",
    month = jun,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-2.7"",
    doi = ""10.1162/coli_a_00474"",
    pages = ""465--523"",
    abstract = ""Vector-based word representation paradigms situate lexical meaning at different levels of abstraction. Distributional and static embedding models generate a single vector per word type, which is an aggregate across the instances of the word in a corpus. Contextual language models, on the contrary, directly capture the meaning of individual word instances. The goal of this survey is to provide an overview of word meaning representation methods, and of the strategies that have been proposed for improving the quality of the generated vectors. These often involve injecting external knowledge about lexical semantic relationships, or refining the vectors to describe different senses. The survey also covers recent approaches for obtaining word type-level representations from token-level ones, and for combining static and contextualized representations. Special focus is given to probing and interpretation studies aimed at discovering the lexical semantic knowledge that is encoded in contextualized representations. The challenges posed by this exploration have motivated the interest towards static embedding derivation from contextualized embeddings, and for methods aimed at improving the similarity estimates that can be drawn from the space of contextual language models."",
}
@article{troiano-etal-2023-dimensional,
    title = ""Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction"",
    author = {Troiano, Enrica  and
      Oberl{\""a}nder, Laura  and
      Klinger, Roman},
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""1"",
    month = mar,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-1.1"",
    doi = ""10.1162/coli_a_00461"",
    pages = ""1--72"",
    abstract = ""The most prominent tasks in emotion analysis are to assign emotions to texts and to understand how emotions manifest in language. An important observation for natural language processing is that emotions can be communicated implicitly by referring to events alone, appealing to an empathetic, intersubjective understanding of events, even without explicitly mentioning an emotion name. In psychology, the class of emotion theories known as appraisal theories aims at explaining the link between events and emotions. Appraisals can be formalized as variables that measure a cognitive evaluation by people living through an event that they consider relevant. They include the assessment if an event is novel, if the person considers themselves to be responsible, if it is in line with their own goals, and so forth. Such appraisals explain which emotions are developed based on an event, for example, that a novel situation can induce surprise or one with uncertain consequences could evoke fear. We analyze the suitability of appraisal theories for emotion analysis in text with the goal of understanding if appraisal concepts can reliably be reconstructed by annotators, if they can be predicted by text classifiers, and if appraisal concepts help to identify emotion categories. To achieve that, we compile a corpus by asking people to textually describe events that triggered particular emotions and to disclose their appraisals. Then, we ask readers to reconstruct emotions and appraisals from the text. This set-up allows us to measure if emotions and appraisals can be recovered purely from text and provides a human baseline to judge a model{'}s performance measures. Our comparison of text classification methods to human annotators shows that both can reliably detect emotions and appraisals with similar performance. Therefore, appraisals constitute an alternative computational emotion analysis paradigm and further improve the categorization of emotions in text with joint models."",
}
@article{wysocki-etal-2023-transformers,
    title = ""Transformers and the Representation of Biomedical Background Knowledge"",
    author = ""Wysocki, Oskar  and
      Zhou, Zili  and
      O{'}Regan, Paul  and
      Ferreira, Deborah  and
      Wysocka, Magdalena  and
      Landers, D{\'o}nal  and
      Freitas, Andr{\'e}"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""1"",
    month = mar,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-1.2"",
    doi = ""10.1162/coli_a_00462"",
    pages = ""73--115"",
    abstract = ""Specialized transformers-based models (such as BioBERT and BioMegatron) are adapted for the biomedical domain based on publicly available biomedical corpora. As such, they have the potential to encode large-scale biological knowledge. We investigate the encoding and representation of biological knowledge in these models, and its potential utility to support inference in cancer precision medicine{---}namely, the interpretation of the clinical significance of genomic alterations. We compare the performance of different transformer baselines; we use probing to determine the consistency of encodings for distinct entities; and we use clustering methods to compare and contrast the internal properties of the embeddings for genes, variants, drugs, and diseases. We show that these models do indeed encode biological knowledge, although some of this is lost in fine-tuning for specific tasks. Finally, we analyze how the models behave with regard to biases and imbalances in the dataset."",
}
@article{tang-surdeanu-2023-takes,
    title = ""It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers"",
    author = ""Tang, Zheng  and
      Surdeanu, Mihai"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""1"",
    month = mar,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-1.3"",
    doi = ""10.1162/coli_a_00463"",
    pages = ""117--156"",
    abstract = ""We propose an explainable approach for relation extraction that mitigates the tension between generalization and explainability by jointly training for the two goals. Our approach uses a multi-task learning architecture, which jointly trains a classifier for relation extraction, and a sequence model that labels words in the context of the relations that explain the decisions of the relation classifier. We also convert the model outputs to rules to bring global explanations to this approach. This sequence model is trained using a hybrid strategy: supervised, when supervision from pre-existing patterns is available, and semi-supervised otherwise. In the latter situation, we treat the sequence model{'}s labels as latent variables, and learn the best assignment that maximizes the performance of the relation classifier. We evaluate the proposed approach on the two datasets and show that the sequence model provides labels that serve as accurate explanations for the relation classifier{'}s decisions, and, importantly, that the joint training generally improves the performance of the relation classifier. We also evaluate the performance of the generated rules and show that the new rules are a great add-on to the manual rules and bring the rule-based system much closer to the neural models."",
}
@article{klie-etal-2023-annotation,
    title = ""Annotation Error Detection: Analyzing the Past and Present for a More Coherent Future"",
    author = ""Klie, Jan-Christoph  and
      Webber, Bonnie  and
      Gurevych, Iryna"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""1"",
    month = mar,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-1.4"",
    doi = ""10.1162/coli_a_00464"",
    pages = ""157--198"",
    abstract = ""Annotated data is an essential ingredient in natural language processing for training and evaluating machine learning models. It is therefore very desirable for the annotations to be of high quality. Recent work, however, has shown that several popular datasets contain a surprising number of annotation errors or inconsistencies. To alleviate this issue, many methods for annotation error detection have been devised over the years. While researchers show that their approaches work well on their newly introduced datasets, they rarely compare their methods to previous work or on the same datasets. This raises strong concerns on methods{'} general performance and makes it difficult to assess their strengths and weaknesses. We therefore reimplement 18 methods for detecting potential annotation errors and evaluate them on 9 English datasets for text classification as well as token and span labeling. In addition, we define a uniform evaluation setup including a new formalization of the annotation error detection task, evaluation protocol, and general best practices. To facilitate future research and reproducibility, we release our datasets and implementations in an easy-to-use and open source software package.1"",
}
@article{kalouli-etal-2023-curing,
    title = ""Curing the {SICK} and Other {NLI} Maladies"",
    author = ""Kalouli, Aikaterini-Lida  and
      Hu, Hai  and
      Webb, Alexander F.  and
      Moss, Lawrence S.  and
      de Paiva, Valeria"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""1"",
    month = mar,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-1.5"",
    doi = ""10.1162/coli_a_00465"",
    pages = ""199--243"",
    abstract = ""Against the backdrop of the ever-improving Natural Language Inference (NLI) models, recent efforts have focused on the suitability of the current NLI datasets and on the feasibility of the NLI task as it is currently approached. Many of the recent studies have exposed the inherent human disagreements of the inference task and have proposed a shift from categorical labels to human subjective probability assessments, capturing human uncertainty. In this work, we show how neither the current task formulation nor the proposed uncertainty gradient are entirely suitable for solving the NLI challenges. Instead, we propose an ordered sense space annotation, which distinguishes between logical and common-sense inference. One end of the space captures non-sensical inferences, while the other end represents strictly logical scenarios. In the middle of the space, we find a continuum of common-sense, namely, the subjective and graded opinion of a {``}person on the street.{''} To arrive at the proposed annotation scheme, we perform a careful investigation of the SICK corpus and we create a taxonomy of annotation issues and guidelines. We re-annotate the corpus with the proposed annotation scheme, utilizing four symbolic inference systems, and then perform a thorough evaluation of the scheme by fine-tuning and testing commonly used pre-trained language models on the re-annotated SICK within various settings. We also pioneer a crowd annotation of a small portion of the MultiNLI corpus, showcasing that it is possible to adapt our scheme for annotation by non-experts on another NLI corpus. Our work shows the efficiency and benefits of the proposed mechanism and opens the way for a careful NLI task refinement."",
}
@article{de-santo-2023-finite,
    title = ""Finite-State Text Processing"",
    author = ""De Santo, Aniello"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""1"",
    month = mar,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-1.6"",
    doi = ""10.1162/coli_r_00466"",
    pages = ""245--247"",
}
@article{futrell-2023-validity,
    title = ""Validity, Reliability, and Significance: Empirical Methods for {NLP} and Data Science"",
    author = ""Futrell, Richard"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""1"",
    month = mar,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-1.7"",
    doi = ""10.1162/coli_r_00467"",
    pages = ""249--251"",
}
@article{verberne-2023-pretrained,
    title = ""Pretrained Transformers for Text Ranking: {BERT} and Beyond"",
    author = ""Verberne, Suzan"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""1"",
    month = mar,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-1.8"",
    doi = ""10.1162/coli_r_00468"",
    pages = ""253--255"",
}
@article{seminck-2023-conversational,
    title = ""Conversational {AI}: Dialogue Systems, Conversational Agents, and Chatbots by {M}ichael {M}c{T}ear"",
    author = ""Seminck, Olga"",
    journal = ""Computational Linguistics"",
    volume = ""49"",
    number = ""1"",
    month = mar,
    year = ""2023"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2023.cl-1.9"",
    doi = ""10.1162/coli_r_00470"",
    pages = ""257--259"",
}
@",medical domain,natural language process,natural languag,language process,translat,nlp,infer,generat,relation extract,relation classif,semant,annotat,challeng,evalu,assess
" ""Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints"","," ""Processing information locked within clinical health records is a challenging task that remains an active area of research in biomedical NLP. In this work, we evaluate a broad set of machine learning techniques ranging from simple RNNs to specialised transformers such as BioBERT on a dataset containing clinical notes along with a set of annotations indicating whether a sample is cancer-related or not. Furthermore, we specifically employ efficient fine-tuning methods from NLP, namely, bottleneck adapters and prompt tuning, to adapt the models to our specialised task. Our evaluations suggest that fine-tuning a frozen BERT model pre-trained on natural language and with bottleneck adapters outperforms all other strategies, including full fine-tuning of the specialised BioBERT model. Based on our findings, we suggest that using bottleneck adapters in low-resource situations with limited access to labelled data or processing capacity could be a viable strategy in biomedical text mining."",","{rohanian-etal-2023-using,
    title = ""Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints"",
    author = ""Rohanian, Omid  and
      Jauncey, Hannah  and
      Nouriborji, Mohammadmahdi  and
      Kumar, Vinod  and
      Gonalves, Bronner P.  and
      Kartsonaki, Christiana  and
      Clinical Characterisation Group, Isaric  and
      Merson, Laura  and
      Clifton, David"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.5"",
    doi = ""10.18653/v1/2023.bionlp-1.5"",
    pages = ""62--78"",
    abstract = ""Processing information locked within clinical health records is a challenging task that remains an active area of research in biomedical NLP. In this work, we evaluate a broad set of machine learning techniques ranging from simple RNNs to specialised transformers such as BioBERT on a dataset containing clinical notes along with a set of annotations indicating whether a sample is cancer-related or not. Furthermore, we specifically employ efficient fine-tuning methods from NLP, namely, bottleneck adapters and prompt tuning, to adapt the models to our specialised task. Our evaluations suggest that fine-tuning a frozen BERT model pre-trained on natural language and with bottleneck adapters outperforms all other strategies, including full fine-tuning of the specialised BioBERT model. Based on our findings, we suggest that using bottleneck adapters in low-resource situations with limited access to labelled data or processing capacity could be a viable strategy in biomedical text mining."",
}
@",health record,clinical not,medical text,natural language process,natural languag,language process,nlp,shared task,annotat,challeng,evalu
" ""Is the ranking of {P}ub{M}ed similar articles good enough? An evaluation of text similarity methods for three datasets"","," ""The use of seed articles in information retrieval provides many advantages, such as a longercontext and more details about the topic being searched for. Given a seed article (i.e., a PMID), PubMed provides a pre-compiled list of similar articles to support the user in finding equivalent papers in the biomedical literature. We aimed at performing a quantitative evaluation of the PubMed Similar Articles based on three existing biomedical text similarity datasets, namely, RELISH, TREC-COVID, and SMAFIRA-c. Further, we carried out a survey and an evaluation of various text similarity methods on these three datasets. Our experiments considered the original title and abstract from PubMed as well as automatically detected sections and manually annotated relevant sentences. We provide an overview about which methods better performfor each dataset and compare them to the ranking in PubMed similar articles. While resultsvaried considerably among the datasets, we were able to obtain a better performance thanPubMed for all of them. Datasets and source codes are available at: \url{https://github.com/mariananeves/reranking}"",","{neves-etal-2023-ranking,
    title = ""Is the ranking of {P}ub{M}ed similar articles good enough? An evaluation of text similarity methods for three datasets"",
    author = ""Neves, Mariana  and
      Schadock, Ines  and
      Eusemann, Beryl  and
      Schnfelder, Gilbert  and
      Bert, Bettina  and
      Butzke, Daniel"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.11"",
    doi = ""10.18653/v1/2023.bionlp-1.11"",
    pages = ""133--144"",
    abstract = ""The use of seed articles in information retrieval provides many advantages, such as a longercontext and more details about the topic being searched for. Given a seed article (i.e., a PMID), PubMed provides a pre-compiled list of similar articles to support the user in finding equivalent papers in the biomedical literature. We aimed at performing a quantitative evaluation of the PubMed Similar Articles based on three existing biomedical text similarity datasets, namely, RELISH, TREC-COVID, and SMAFIRA-c. Further, we carried out a survey and an evaluation of various text similarity methods on these three datasets. Our experiments considered the original title and abstract from PubMed as well as automatically detected sections and manually annotated relevant sentences. We provide an overview about which methods better performfor each dataset and compare them to the ranking in PubMed similar articles. While resultsvaried considerably among the datasets, we were able to obtain a better performance thanPubMed for all of them. Datasets and source codes are available at: \url{https://github.com/mariananeves/reranking}"",
}
@",medical text,natural language process,natural languag,language process,nlp,information retriev,shared task,annotat,evalu
" ""How Much do Knowledge Graphs Impact Transformer Models for Extracting Biomedical Events?"","," ""Biomedical event extraction can be divided into three main subtasks; (1) biomedical event trigger detection, (2) biomedical argument identification and (3) event construction. This work focuses in the two first subtasks. For the first subtask we analyze a set of transformer language models that are commonly used in the biomedical domain to evaluate and compare their capacity for event trigger detection. We fine-tune the models using seven manually annotated corpora to assess their performance in different biomedical subdomains. SciBERT emerged as the highest performing model, presenting a slight improvement compared to baseline models. Then, for the second subtask we construct a knowledge graph (KG) from the biomedical corpora and integrate its KG embeddings to SciBERT to enrich its semantic information. We demonstrate that adding the KG embeddings to the model improves the argument identification performance by around 20 {\%}, and by around 15 {\%} compared to two baseline models. Our results suggest that fine-tuning a transformer model that is pretrained from scratch with biomedical and general data allows to detect event triggers and identify arguments covering different biomedical subdomains, and therefore improving its generalization. Furthermore, the integration of KG embeddings into the model can significantly improve the performance of biomedical event argument identification, outperforming the results of baseline models."",","{zanella-toussaint-2023-much,
    title = ""How Much do Knowledge Graphs Impact Transformer Models for Extracting Biomedical Events?"",
    author = ""Zanella, Laura  and
      Toussaint, Yannick"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.12"",
    doi = ""10.18653/v1/2023.bionlp-1.12"",
    pages = ""145--155"",
    abstract = ""Biomedical event extraction can be divided into three main subtasks; (1) biomedical event trigger detection, (2) biomedical argument identification and (3) event construction. This work focuses in the two first subtasks. For the first subtask we analyze a set of transformer language models that are commonly used in the biomedical domain to evaluate and compare their capacity for event trigger detection. We fine-tune the models using seven manually annotated corpora to assess their performance in different biomedical subdomains. SciBERT emerged as the highest performing model, presenting a slight improvement compared to baseline models. Then, for the second subtask we construct a knowledge graph (KG) from the biomedical corpora and integrate its KG embeddings to SciBERT to enrich its semantic information. We demonstrate that adding the KG embeddings to the model improves the argument identification performance by around 20 {\%}, and by around 15 {\%} compared to two baseline models. Our results suggest that fine-tuning a transformer model that is pretrained from scratch with biomedical and general data allows to detect event triggers and identify arguments covering different biomedical subdomains, and therefore improving its generalization. Furthermore, the integration of KG embeddings into the model can significantly improve the performance of biomedical event argument identification, outperforming the results of baseline models."",
}
@",medical domain,natural language process,natural languag,language process,nlp,semant,shared task,annotat,evalu,assess
" ""Large Language Models as Instructors: A Study on Multilingual Clinical Entity Extraction"","," ""In clinical and other specialized domains, data are scarce due to their confidential nature. This lack of data is a major problem when fine-tuning language models. Nevertheless, very large language models (LLMs) are promising for the medical domain but cannot be used directly in healthcare facilities due to data confidentiality issues. We explore an approach of annotating training data with LLMs to train smaller models more adapted to our problem. We show that this method yields promising results for information extraction tasks."",","{meoni-etal-2023-large,
    title = ""Large Language Models as Instructors: A Study on Multilingual Clinical Entity Extraction"",
    author = ""Meoni, Simon  and
      De la Clergerie, Eric  and
      Ryffel, Theo"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.15"",
    doi = ""10.18653/v1/2023.bionlp-1.15"",
    pages = ""178--190"",
    abstract = ""In clinical and other specialized domains, data are scarce due to their confidential nature. This lack of data is a major problem when fine-tuning language models. Nevertheless, very large language models (LLMs) are promising for the medical domain but cannot be used directly in healthcare facilities due to data confidentiality issues. We explore an approach of annotating training data with LLMs to train smaller models more adapted to our problem. We show that this method yields promising results for information extraction tasks."",
}
@",medical domain,natural language process,natural languag,language process,nlp,shared task,annotat
" ""Event-independent temporal positioning: application to {F}rench clinical text"","," ""Extracting temporal relations usually entails identifying and classifying the relation between two mentions. However, the definition of temporal mentions strongly depends on the text type and the application domain. Clinical text in particular is complex. It may describe events that occurred at different times, contain redundant information and a variety of domain-specific temporal expressions. In this paper, we propose a novel event-independent representation of temporal relations that is task-independent and, therefore, domain-independent. We are interested in identifying homogeneous text portions from a temporal standpoint and classifying the relation between each text portion and the document creation time. Temporal relation extraction is cast as a sequence labeling task and evaluated on oncology notes. We further evaluate our temporal representation by the temporal positioning of toxicity events of chemotherapy administrated to colon and lung cancer patients described in French clinical reports. An overall macro F-measure of 0.86 is obtained for temporal relation extraction by a neural token classification model trained on clinical texts written in French. Our results suggest that the toxicity event extraction task can be performed successfully by automatically identifying toxicity events and placing them within the patient timeline (F-measure .62). The proposed system has the potential to assist clinicians in the preparation of tumor board meetings."",","{bannour-etal-2023-event,
    title = ""Event-independent temporal positioning: application to {F}rench clinical text"",
    author = ""Bannour, Nesrine  and
      Rance, Bastien  and
      Tannier, Xavier  and
      Neveol, Aurelie"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.16"",
    doi = ""10.18653/v1/2023.bionlp-1.16"",
    pages = ""191--205"",
    abstract = ""Extracting temporal relations usually entails identifying and classifying the relation between two mentions. However, the definition of temporal mentions strongly depends on the text type and the application domain. Clinical text in particular is complex. It may describe events that occurred at different times, contain redundant information and a variety of domain-specific temporal expressions. In this paper, we propose a novel event-independent representation of temporal relations that is task-independent and, therefore, domain-independent. We are interested in identifying homogeneous text portions from a temporal standpoint and classifying the relation between each text portion and the document creation time. Temporal relation extraction is cast as a sequence labeling task and evaluated on oncology notes. We further evaluate our temporal representation by the temporal positioning of toxicity events of chemotherapy administrated to colon and lung cancer patients described in French clinical reports. An overall macro F-measure of 0.86 is obtained for temporal relation extraction by a neural token classification model trained on clinical texts written in French. Our results suggest that the toxicity event extraction task can be performed successfully by automatically identifying toxicity events and placing them within the patient timeline (F-measure .62). The proposed system has the potential to assist clinicians in the preparation of tumor board meetings."",
}
@",clinical text,natural language process,natural languag,language process,nlp,relation extract,sequence label,shared task,evalu
" ""{ADEQA}: A Question Answer based approach for joint {ADE}-Suspect Extraction using Sequence-To-Sequence Transformers"","," ""Early identification of Adverse Drug Events (ADE) is critical for taking prompt actions while introducing new drugs into the market. These ADEs information are available through various unstructured data sources like clinical study reports, patient health records, social media posts, etc. Extracting ADEs and the related suspect drugs using machine learning is a challenging task due to the complex linguistic relations between drug ADE pairs in textual data and unavailability of large corpus of labelled datasets. This paper introduces ADEQA, a question- answer(QA) based approach using quasi supervised labelled data and sequence-to-sequence transformers to extract ADEs, drug suspects and the relationships between them. Unlike traditional QA models, natural language generation (NLG) based models don{'}t require extensive token level labelling and thereby reduces the adoption barrier significantly. On a public ADE corpus, we were able to achieve state-of-the-art results with an F1 score of 94{\%} on establishing the relationships between ADEs and the respective suspects."",","{arannil-etal-2023-adeqa,
    title = ""{ADEQA}: A Question Answer based approach for joint {ADE}-Suspect Extraction using Sequence-To-Sequence Transformers"",
    author = ""Arannil, Vinayak  and
      Deb, Tomal  and
      Roy, Atanu"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.17"",
    doi = ""10.18653/v1/2023.bionlp-1.17"",
    pages = ""206--214"",
    abstract = ""Early identification of Adverse Drug Events (ADE) is critical for taking prompt actions while introducing new drugs into the market. These ADEs information are available through various unstructured data sources like clinical study reports, patient health records, social media posts, etc. Extracting ADEs and the related suspect drugs using machine learning is a challenging task due to the complex linguistic relations between drug ADE pairs in textual data and unavailability of large corpus of labelled datasets. This paper introduces ADEQA, a question- answer(QA) based approach using quasi supervised labelled data and sequence-to-sequence transformers to extract ADEs, drug suspects and the relationships between them. Unlike traditional QA models, natural language generation (NLG) based models don{'}t require extensive token level labelling and thereby reduces the adoption barrier significantly. On a public ADE corpus, we were able to achieve state-of-the-art results with an F1 score of 94{\%} on establishing the relationships between ADEs and the respective suspects."",
}
@",health record,natural language process,natural languag,language process,nlp,generat,shared task,challeng
" ""{A}li{BERT}: A Pre-trained Language Model for {F}rench Biomedical Text"","," ""Over the past few years, domain specific pretrained language models have been investigated and have shown remarkable achievements in different downstream tasks, especially in biomedical domain. These achievements stem on the well known BERT architecture which uses an attention based self-supervision for context learning of textual documents. However, these domain specific biomedical pretrained language models mainly use English corpora. Therefore, non-English, domain-specific pretrained models remain quite rare, both of these requirements being hard to achieve. In this work, we proposed AliBERT, a biomedical pretrained language model for French and investigated different learning strategies. AliBERT is trained using regularized Unigram based tokenizer trained for this purpose. AliBERT has achieved state of the art F1 and accuracy scores in different down-stream biomedical tasks. Our pretrained model manages to outperform some French non domain-specific models such as CamemBERT and FlauBERT on diverse down-stream tasks, with less pretraining and training time and with much smaller corpora."",","{berhe-etal-2023-alibert,
    title = ""{A}li{BERT}: A Pre-trained Language Model for {F}rench Biomedical Text"",
    author = ""Berhe, Aman  and
      Draznieks, Guillaume  and
      Martenot, Vincent  and
      Masdeu, Valentin  and
      Davy, Lucas  and
      Zucker, Jean-Daniel"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.19"",
    doi = ""10.18653/v1/2023.bionlp-1.19"",
    pages = ""223--236"",
    abstract = ""Over the past few years, domain specific pretrained language models have been investigated and have shown remarkable achievements in different downstream tasks, especially in biomedical domain. These achievements stem on the well known BERT architecture which uses an attention based self-supervision for context learning of textual documents. However, these domain specific biomedical pretrained language models mainly use English corpora. Therefore, non-English, domain-specific pretrained models remain quite rare, both of these requirements being hard to achieve. In this work, we proposed AliBERT, a biomedical pretrained language model for French and investigated different learning strategies. AliBERT is trained using regularized Unigram based tokenizer trained for this purpose. AliBERT has achieved state of the art F1 and accuracy scores in different down-stream biomedical tasks. Our pretrained model manages to outperform some French non domain-specific models such as CamemBERT and FlauBERT on diverse down-stream tasks, with less pretraining and training time and with much smaller corpora."",
}
@",medical domain,medical text,natural language process,natural languag,language process,nlp,shared task
" ""Building a Corpus for Biomedical Relation Extraction of Species Mentions"","," ""We present a manually annotated new corpus, Species-Species Interaction (SSI), for extracting meaningful binary relations between species, in biomedical texts, at sentence level, with a focus on the gut microbiota. The corpus leverages PubTator to annotate species in full-text articles after evaluating different NER species taggers. Our first results are promising for extracting relations between species using BERT and its biomedical variants."",","{el-khettari-etal-2023-building,
    title = ""Building a Corpus for Biomedical Relation Extraction of Species Mentions"",
    author = ""El Khettari, Oumaima  and
      Quiniou, Solen  and
      Chaffron, Samuel"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.21"",
    doi = ""10.18653/v1/2023.bionlp-1.21"",
    pages = ""248--254"",
    abstract = ""We present a manually annotated new corpus, Species-Species Interaction (SSI), for extracting meaningful binary relations between species, in biomedical texts, at sentence level, with a focus on the gut microbiota. The corpus leverages PubTator to annotate species in full-text articles after evaluating different NER species taggers. Our first results are promising for extracting relations between species using BERT and its biomedical variants."",
}
@",medical text,natural language process,natural languag,language process,nlp,relation extract,shared task,annotat,evalu
" ""Automatic Glossary of Clinical Terminology: a Large-Scale Dictionary of Biomedical Definitions Generated from Ontological Knowledge"","," ""Background: More than 400.000 biomedical concepts and some of their relationships are contained in SnomedCT, a comprehensive biomedical ontology. However, their concept names are not always readily interpretable by non-experts, or patients looking at their own electronic health records (EHR). Clear definitions or descriptions in understandable language or often not available. Therefore, generating human-readable definitions for biomedical concepts might help make the information they encode more accessible and understandable to a wider public. Objective: In this article, we introduce the Automatic Glossary of Clinical Terminology (AGCT), a large-scale biomedical dictionary of clinical concepts generated using high-quality information extracted from the biomedical knowledge contained in SnomedCT.Methods: We generate a novel definition for every SnomedCT concept, after prompting the OpenAI Turbo model, a variant of GPT 3.5, using a high-quality verbalization of the SnomedCT relationships of the to-be-defined concept. A significant subset of the generated definitions was subsequently evaluated by NLP researchers with biomedical expertise on 5-point scales along the following three axes: factuality, insight, and fluency. Results: AGCT contains 422,070 computer-generated definitions for SnomedCT concepts, covering various domains such as diseases, procedures, drugs, and anatomy. The average length of the definitions is 49 words. The definitions were assigned average scores of over 4.5 out of 5 on all three axes, indicating a majority of factual, insightful, and fluent definitions. Conclusion: AGCT is a novel and valuable resource for biomedical tasks that require human-readable definitions for SnomedCT concepts. It can also serve as a base for developing robust biomedical retrieval models or other applications that leverage natural language understanding of biomedical knowledge."",","{remy-etal-2023-automatic,
    title = ""Automatic Glossary of Clinical Terminology: a Large-Scale Dictionary of Biomedical Definitions Generated from Ontological Knowledge"",
    author = ""Remy, Fran{\c{c}}ois  and
      Demuynck, Kris  and
      Demeester, Thomas"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.23"",
    doi = ""10.18653/v1/2023.bionlp-1.23"",
    pages = ""265--272"",
    abstract = ""Background: More than 400.000 biomedical concepts and some of their relationships are contained in SnomedCT, a comprehensive biomedical ontology. However, their concept names are not always readily interpretable by non-experts, or patients looking at their own electronic health records (EHR). Clear definitions or descriptions in understandable language or often not available. Therefore, generating human-readable definitions for biomedical concepts might help make the information they encode more accessible and understandable to a wider public. Objective: In this article, we introduce the Automatic Glossary of Clinical Terminology (AGCT), a large-scale biomedical dictionary of clinical concepts generated using high-quality information extracted from the biomedical knowledge contained in SnomedCT.Methods: We generate a novel definition for every SnomedCT concept, after prompting the OpenAI Turbo model, a variant of GPT 3.5, using a high-quality verbalization of the SnomedCT relationships of the to-be-defined concept. A significant subset of the generated definitions was subsequently evaluated by NLP researchers with biomedical expertise on 5-point scales along the following three axes: factuality, insight, and fluency. Results: AGCT contains 422,070 computer-generated definitions for SnomedCT concepts, covering various domains such as diseases, procedures, drugs, and anatomy. The average length of the definitions is 49 words. The definitions were assigned average scores of over 4.5 out of 5 on all three axes, indicating a majority of factual, insightful, and fluent definitions. Conclusion: AGCT is a novel and valuable resource for biomedical tasks that require human-readable definitions for SnomedCT concepts. It can also serve as a base for developing robust biomedical retrieval models or other applications that leverage natural language understanding of biomedical knowledge."",
}
@",electronic health record,health record,natural language process,natural languag,language process,nlp,generat,shared task,evalu
" ""Resolving Elliptical Compounds in {G}erman Medical Text"","," ""Elliptical coordinated compound noun phrases (ECCNPs), a special kind of coordination ellipsis, are a common phenomenon in German medical texts. As their presence is known to affect the performance in downstream tasks such as entity extraction and disambiguation, their resolution can be a useful preprocessing step in information extraction pipelines. In this work, we present a new comprehensive dataset of more than 4,000 manually annotated ECCNPs in German medical text, along with the respective ground truth resolutions. Based on this data, we propose a generative encoder-decoder Transformer model, allowing for a simple end-to-end resolution of ECCNPs from raw input strings with very high accuracy (90.5{\%} exact match score). We compare our approach to an elaborate rule-based baseline, which the generative model outperforms by a large margin. We further investigate different scenarios for prompting large language models (LLM) to resolve ECCNPs. In a zero-shot setting, performance is remarkably poor (21.6{\%} exact matches), as the LLM tends to apply complex changes to the inputs unrelated to our specific task. We also find no improvement over the generative model when using the LLM for post-filtering of generated candidate resolutions."",","{kammer-etal-2023-resolving,
    title = ""Resolving Elliptical Compounds in {G}erman Medical Text"",
    author = ""Kammer, Niklas  and
      Borchert, Florian  and
      Winkler, Silvia  and
      de Melo, Gerard  and
      Schapranow, Matthieu-P."",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.26"",
    doi = ""10.18653/v1/2023.bionlp-1.26"",
    pages = ""292--305"",
    abstract = ""Elliptical coordinated compound noun phrases (ECCNPs), a special kind of coordination ellipsis, are a common phenomenon in German medical texts. As their presence is known to affect the performance in downstream tasks such as entity extraction and disambiguation, their resolution can be a useful preprocessing step in information extraction pipelines. In this work, we present a new comprehensive dataset of more than 4,000 manually annotated ECCNPs in German medical text, along with the respective ground truth resolutions. Based on this data, we propose a generative encoder-decoder Transformer model, allowing for a simple end-to-end resolution of ECCNPs from raw input strings with very high accuracy (90.5{\%} exact match score). We compare our approach to an elaborate rule-based baseline, which the generative model outperforms by a large margin. We further investigate different scenarios for prompting large language models (LLM) to resolve ECCNPs. In a zero-shot setting, performance is remarkably poor (21.6{\%} exact matches), as the LLM tends to apply complex changes to the inputs unrelated to our specific task. We also find no improvement over the generative model when using the LLM for post-filtering of generated candidate resolutions."",
}
@",medical text,natural language process,natural languag,language process,nlp,generat,shared task,annotat
" ""End-to-end clinical temporal information extraction with multi-head attention"","," ""Understanding temporal relationships in text from electronic health records can be valuable for many important downstream clinical applications. Since Clinical TempEval 2017, there has been little work on end-to-end systems for temporal relation extraction, with most work focused on the setting where gold standard events and time expressions are given. In this work, we make use of a novel multi-headed attention mechanism on top of a pre-trained transformer encoder to allow the learning process to attend to multiple aspects of the contextualized embeddings. Our system achieves state of the art results on the THYME corpus by a wide margin, in both the in-domain and cross-domain settings."",","{miller-etal-2023-end,
    title = ""End-to-end clinical temporal information extraction with multi-head attention"",
    author = ""Miller, Timothy  and
      Bethard, Steven  and
      Dligach, Dmitriy  and
      Savova, Guergana"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.28"",
    doi = ""10.18653/v1/2023.bionlp-1.28"",
    pages = ""313--319"",
    abstract = ""Understanding temporal relationships in text from electronic health records can be valuable for many important downstream clinical applications. Since Clinical TempEval 2017, there has been little work on end-to-end systems for temporal relation extraction, with most work focused on the setting where gold standard events and time expressions are given. In this work, we make use of a novel multi-headed attention mechanism on top of a pre-trained transformer encoder to allow the learning process to attend to multiple aspects of the contextualized embeddings. Our system achieves state of the art results on the THYME corpus by a wide margin, in both the in-domain and cross-domain settings."",
}
@",electronic health record,health record,natural language process,natural languag,language process,nlp,relation extract,shared task
" ""Intermediate Domain Finetuning for Weakly Supervised Domain-adaptive Clinical {NER}"","," ""Accurate human-annotated data for real-worlduse cases can be scarce and expensive to obtain. In the clinical domain, obtaining such data is evenmore difficult due to privacy concerns which notonly restrict open access to quality data but also require that the annotation be done by domain experts. In this paper, we propose a novel framework - InterDAPT - that leverages Intermediate Domain Finetuning to allow language models to adapt to narrow domains with small, noisy datasets. By making use of peripherally-related, unlabeled datasets,this framework circumvents domain-specific datascarcity issues. Our results show that this weaklysupervised framework provides performance improvements in downstream clinical named entityrecognition tasks."",","{suresh-etal-2023-intermediate,
    title = ""Intermediate Domain Finetuning for Weakly Supervised Domain-adaptive Clinical {NER}"",
    author = ""Suresh, Shilpa  and
      Tavabi, Nazgol  and
      Golchin, Shahriar  and
      Gilreath, Leah  and
      Garcia-Andujar, Rafael  and
      Kim, Alexander  and
      Murray, Joseph  and
      Bacevich, Blake  and
      Kiapour, Ata"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.29"",
    doi = ""10.18653/v1/2023.bionlp-1.29"",
    pages = ""320--325"",
    abstract = ""Accurate human-annotated data for real-worlduse cases can be scarce and expensive to obtain. In the clinical domain, obtaining such data is evenmore difficult due to privacy concerns which notonly restrict open access to quality data but also require that the annotation be done by domain experts. In this paper, we propose a novel framework - InterDAPT - that leverages Intermediate Domain Finetuning to allow language models to adapt to narrow domains with small, noisy datasets. By making use of peripherally-related, unlabeled datasets,this framework circumvents domain-specific datascarcity issues. Our results show that this weaklysupervised framework provides performance improvements in downstream clinical named entityrecognition tasks."",
}
@",clinical domain,natural language process,natural languag,language process,nlp,shared task,annotat
" ""Evaluation of {C}hat{GPT} on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers"","," ""ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT{'}s pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that lack large annotated data."",","{jahan-etal-2023-evaluation,
    title = ""Evaluation of {C}hat{GPT} on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers"",
    author = ""Jahan, Israt  and
      Laskar, Md Tahmid Rahman  and
      Peng, Chun  and
      Huang, Jimmy"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.30"",
    doi = ""10.18653/v1/2023.bionlp-1.30"",
    pages = ""326--336"",
    abstract = ""ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT{'}s pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that lack large annotated data."",
}
@",medical domain,natural language process,natural languag,language process,nlp,generat,relation extract,summar,shared task,annotat,benchmark,evalu
" ""{BIO}ptimus: Pre-training an Optimal Biomedical Language Model with Curriculum Learning for Named Entity Recognition"","," ""Using language models (LMs) pre-trained in a self-supervised setting on large corpora and then fine-tuning for a downstream task has helped to deal with the problem of limited label data for supervised learning tasks such as Named Entity Recognition (NER). Recent research in biomedical language processing has offered a number of biomedical LMs pre-trained using different methods and techniques that advance results on many BioNLP tasks, including NER. However, there is still a lack of a comprehensive comparison of pre-training approaches that would work more optimally in the biomedical domain. This paper aims to investigate different pre-training methods, such as pre-training the biomedical LM from scratch and pre-training it in a continued fashion. We compare existing methods with our proposed pre-training method of initializing weights for new tokens by distilling existing weights from the BERT model inside the context where the tokens were found. The method helps to speed up the pre-training stage and improve performance on NER. In addition, we compare how masking rate, corruption strategy, and masking strategies impact the performance of the biomedical LM. Finally, using the insights from our experiments, we introduce a new biomedical LM (BIOptimus), which is pre-trained using Curriculum Learning (CL) and contextualized weight distillation method. Our model sets new states of the art on several biomedical Named Entity Recognition (NER) tasks. We release our code and all pre-trained models."",","{pavlova-makhlouf-2023-bioptimus,
    title = ""{BIO}ptimus: Pre-training an Optimal Biomedical Language Model with Curriculum Learning for Named Entity Recognition"",
    author = ""Pavlova, Vera  and
      Makhlouf, Mohammed"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.31"",
    doi = ""10.18653/v1/2023.bionlp-1.31"",
    pages = ""337--349"",
    abstract = ""Using language models (LMs) pre-trained in a self-supervised setting on large corpora and then fine-tuning for a downstream task has helped to deal with the problem of limited label data for supervised learning tasks such as Named Entity Recognition (NER). Recent research in biomedical language processing has offered a number of biomedical LMs pre-trained using different methods and techniques that advance results on many BioNLP tasks, including NER. However, there is still a lack of a comprehensive comparison of pre-training approaches that would work more optimally in the biomedical domain. This paper aims to investigate different pre-training methods, such as pre-training the biomedical LM from scratch and pre-training it in a continued fashion. We compare existing methods with our proposed pre-training method of initializing weights for new tokens by distilling existing weights from the BERT model inside the context where the tokens were found. The method helps to speed up the pre-training stage and improve performance on NER. In addition, we compare how masking rate, corruption strategy, and masking strategies impact the performance of the biomedical LM. Finally, using the insights from our experiments, we introduce a new biomedical LM (BIOptimus), which is pre-trained using Curriculum Learning (CL) and contextualized weight distillation method. Our model sets new states of the art on several biomedical Named Entity Recognition (NER) tasks. We release our code and all pre-trained models."",
}
@",medical domain,natural language process,natural languag,language process,nlp,entity recognit,shared task
" ""{B}io{NART}: A Biomedical Non-{A}uto{R}egressive Transformer for Natural Language Generation"","," ""We propose a novel Biomedical domain-specific Non-AutoRegressive Transformer model for natural language generation: BioNART. Our BioNART is based on an encoder-decoder model, and both encoder and decoder are compatible with widely used BERT architecture, which allows benefiting from publicly available pre-trained biomedical language model checkpoints. We performed additional pre-training and fine-tuned BioNART on biomedical summarization and doctor-patient dialogue tasks. Experimental results show that our BioNART achieves about 94{\%} of the ROUGE score to the pre-trained autoregressive model while realizing an 18 times faster inference speed on the iCliniq dataset."",","{asada-miwa-2023-bionart,
    title = ""{B}io{NART}: A Biomedical Non-{A}uto{R}egressive Transformer for Natural Language Generation"",
    author = ""Asada, Masaki  and
      Miwa, Makoto"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.34"",
    doi = ""10.18653/v1/2023.bionlp-1.34"",
    pages = ""369--376"",
    abstract = ""We propose a novel Biomedical domain-specific Non-AutoRegressive Transformer model for natural language generation: BioNART. Our BioNART is based on an encoder-decoder model, and both encoder and decoder are compatible with widely used BERT architecture, which allows benefiting from publicly available pre-trained biomedical language model checkpoints. We performed additional pre-training and fine-tuned BioNART on biomedical summarization and doctor-patient dialogue tasks. Experimental results show that our BioNART achieves about 94{\%} of the ROUGE score to the pre-trained autoregressive model while realizing an 18 times faster inference speed on the iCliniq dataset."",
}
@",medical domain,natural language process,natural languag,language process,nlp,infer,generat,summar,shared task
" ""Biomedical Relation Extraction with Entity Type Markers and Relation-specific Question Answering"","," ""Recently, several methods have tackled the relation extraction task with QA and have shown successful results. However, the effectiveness of existing methods in specific domains, such as the biomedical domain, is yet to be verified. When there are multiple entity pairs that share an entity in a sentence, a QA-based relation extraction model that outputs only one single answer to a given question may not extract desired relations. In addition, these methods employ QA models that are not tuned for relation extraction. To address these issues, we first extend and apply a span QA-based relation extraction method to the drug-protein relation extraction by creating question templates and incorporating entity type markers. We further propose a binary QA-based method that directly uses the entity information available in the relation extraction task. The experimental results on the DrugProt dataset show that our QA-based methods, especially the proposed binary QA method, are effective for drug-protein relation extraction."",","{yamada-etal-2023-biomedical,
    title = ""Biomedical Relation Extraction with Entity Type Markers and Relation-specific Question Answering"",
    author = ""Yamada, Koshi  and
      Miwa, Makoto  and
      Sasaki, Yutaka"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.35"",
    doi = ""10.18653/v1/2023.bionlp-1.35"",
    pages = ""377--384"",
    abstract = ""Recently, several methods have tackled the relation extraction task with QA and have shown successful results. However, the effectiveness of existing methods in specific domains, such as the biomedical domain, is yet to be verified. When there are multiple entity pairs that share an entity in a sentence, a QA-based relation extraction model that outputs only one single answer to a given question may not extract desired relations. In addition, these methods employ QA models that are not tuned for relation extraction. To address these issues, we first extend and apply a span QA-based relation extraction method to the drug-protein relation extraction by creating question templates and incorporating entity type markers. We further propose a binary QA-based method that directly uses the entity information available in the relation extraction task. The experimental results on the DrugProt dataset show that our QA-based methods, especially the proposed binary QA method, are effective for drug-protein relation extraction."",
}
@",medical domain,natural language process,natural languag,language process,nlp,relation extract,shared task
" ""Biomedical Document Classification with Literature Graph Representations of Bibliographies and Entities"","," ""This paper proposes a new document classification method that incorporates the representations of a literature graph created from bibliographic and entity information. Recently, document classification performance has been significantly improved with large pre-trained language models; however, there still remain documents that are difficult to classify. External information, such as bibliographic information, citation links, descriptions of entities, and medical taxonomies, has been considered one of the keys to dealing with such documents in document classification. Although several document classification methods using external information have been proposed, they only consider limited relationships, e.g., word co-occurrence and citation relationships. However, there are multiple types of external information. To overcome the limitation of the conventional use of external information, we propose a document classification model that simultaneously considers bibliographic and entity information to deeply model the relationships among documents using the representations of the literature graph. The experimental results show that our proposed method outperforms existing methods on two document classification datasets in the biomedical domain with the help of the literature graph."",","{ida-etal-2023-biomedical,
    title = ""Biomedical Document Classification with Literature Graph Representations of Bibliographies and Entities"",
    author = ""Ida, Ryuki  and
      Miwa, Makoto  and
      Sasaki, Yutaka"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.36"",
    doi = ""10.18653/v1/2023.bionlp-1.36"",
    pages = ""385--395"",
    abstract = ""This paper proposes a new document classification method that incorporates the representations of a literature graph created from bibliographic and entity information. Recently, document classification performance has been significantly improved with large pre-trained language models; however, there still remain documents that are difficult to classify. External information, such as bibliographic information, citation links, descriptions of entities, and medical taxonomies, has been considered one of the keys to dealing with such documents in document classification. Although several document classification methods using external information have been proposed, they only consider limited relationships, e.g., word co-occurrence and citation relationships. However, there are multiple types of external information. To overcome the limitation of the conventional use of external information, we propose a document classification model that simultaneously considers bibliographic and entity information to deeply model the relationships among documents using the representations of the literature graph. The experimental results show that our proposed method outperforms existing methods on two document classification datasets in the biomedical domain with the help of the literature graph."",
}
@",medical domain,natural language process,natural languag,language process,nlp,shared task
" ""Hospital Discharge Summarization Data Provenance"","," ""Summarization of medical notes has been studied for decades with hospital discharge summaries garnering recent interest in the research community. While methods for summarizing these notes have been the focus, there has been little work in understanding the feasibility of this task. We believe this effort is warranted given the notes{'} length and complexity, and that they are often riddled with poorly formatted structured data and redundancy in copy and pasted text. In this work, we investigate the feasibility of the summarization task by finding the origin, or data provenance, of the discharge summary{'}s source text. As a motivation to understanding the data challenges of the summarization task, we present DSProv, a new dataset of 51 hospital admissions annotated by clinical informatics physicians. The dataset is analyzed for semantics and the extent of copied text from human authored electronic health record (EHR) notes. We also present a novel unsupervised method of matching notes used in discharge summaries, and release our annotation dataset1 and source code to the community."",","{landes-etal-2023-hospital,
    title = ""Hospital Discharge Summarization Data Provenance"",
    author = ""Landes, Paul  and
      Chaise, Aaron  and
      Patel, Kunal  and
      Huang, Sean  and
      Di Eugenio, Barbara"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.41"",
    doi = ""10.18653/v1/2023.bionlp-1.41"",
    pages = ""439--448"",
    abstract = ""Summarization of medical notes has been studied for decades with hospital discharge summaries garnering recent interest in the research community. While methods for summarizing these notes have been the focus, there has been little work in understanding the feasibility of this task. We believe this effort is warranted given the notes{'} length and complexity, and that they are often riddled with poorly formatted structured data and redundancy in copy and pasted text. In this work, we investigate the feasibility of the summarization task by finding the origin, or data provenance, of the discharge summary{'}s source text. As a motivation to understanding the data challenges of the summarization task, we present DSProv, a new dataset of 51 hospital admissions annotated by clinical informatics physicians. The dataset is analyzed for semantics and the extent of copied text from human authored electronic health record (EHR) notes. We also present a novel unsupervised method of matching notes used in discharge summaries, and release our annotation dataset1 and source code to the community."",
}
@",electronic health record,health record,discharge summar,natural language process,natural languag,language process,nlp,summar,semant,shared task,annotat,challeng
" ""{R}ad{A}dapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models"","," ""We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS). Specifically, we focus on domain adaptation via pretraining (on natural language, biomedical text, or clinical text) and via discrete prompting or parameter-efficient fine-tuning. Our results consistently achieve best performance by maximally adapting to the task via pretraining on clinical text and fine-tuning on RRS examples. Importantly, this method fine-tunes a mere 0.32{\%} of parameters throughout the model, in contrast to end-to-end fine-tuning (100{\%} of parameters). Additionally, we study the effect of in-context examples and out-of-distribution (OOD) training before concluding with a radiologist reader study and qualitative analysis. Our findings highlight the importance of domain adaptation in RRS and provide valuable insights toward developing effective natural language processing solutions for clinical tasks."",","{van-veen-etal-2023-radadapt,
    title = ""{R}ad{A}dapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models"",
    author = ""Van Veen, Dave  and
      Van Uden, Cara  and
      Attias, Maayane  and
      Pareek, Anuj  and
      Bluethgen, Christian  and
      Polacin, Malgorzata  and
      Chiu, Wah  and
      Delbrouck, Jean-Benoit  and
      Zambrano Chaves, Juan  and
      Langlotz, Curtis  and
      Chaudhari, Akshay  and
      Pauly, John"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.42"",
    doi = ""10.18653/v1/2023.bionlp-1.42"",
    pages = ""449--460"",
    abstract = ""We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS). Specifically, we focus on domain adaptation via pretraining (on natural language, biomedical text, or clinical text) and via discrete prompting or parameter-efficient fine-tuning. Our results consistently achieve best performance by maximally adapting to the task via pretraining on clinical text and fine-tuning on RRS examples. Importantly, this method fine-tunes a mere 0.32{\%} of parameters throughout the model, in contrast to end-to-end fine-tuning (100{\%} of parameters). Additionally, we study the effect of in-context examples and out-of-distribution (OOD) training before concluding with a radiologist reader study and qualitative analysis. Our findings highlight the importance of domain adaptation in RRS and provide valuable insights toward developing effective natural language processing solutions for clinical tasks."",
}
@",clinical text,medical text,natural language process,natural languag,language process,nlp,summar,shared task
" ""Overview of the Problem List Summarization ({P}rob{S}um) 2023 Shared Task on Summarizing Patients{'} Active Diagnoses and Problems from Electronic Health Record Progress Notes"","," ""The BioNLP Workshop 2023 initiated the launch of a shared task on Problem List Summarization (ProbSum) in January 2023. The aim of this shared task is to attract future research efforts in building NLP models for real-world diagnostic decision support applications, where a system generating relevant and accurate diagnoses will augment the healthcare providers{'} decision-making process and improve the quality of care for patients. The goal for participants is to develop models that generated a list of diagnoses and problems using input from the daily care notes collected from the hospitalization of critically ill patients. Eight teams submitted their final systems to the shared task leaderboard. In this paper, we describe the tasks, datasets, evaluation metrics, and baseline systems. Additionally, the techniques and results of the evaluation of the different approaches tried by the participating teams are summarized."",","{gao-etal-2023-overview,
    title = ""Overview of the Problem List Summarization ({P}rob{S}um) 2023 Shared Task on Summarizing Patients{'} Active Diagnoses and Problems from Electronic Health Record Progress Notes"",
    author = ""Gao, Yanjun  and
      Dligach, Dmitriy  and
      Miller, Timothy  and
      Afshar, Majid"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.43"",
    doi = ""10.18653/v1/2023.bionlp-1.43"",
    pages = ""461--467"",
    abstract = ""The BioNLP Workshop 2023 initiated the launch of a shared task on Problem List Summarization (ProbSum) in January 2023. The aim of this shared task is to attract future research efforts in building NLP models for real-world diagnostic decision support applications, where a system generating relevant and accurate diagnoses will augment the healthcare providers{'} decision-making process and improve the quality of care for patients. The goal for participants is to develop models that generated a list of diagnoses and problems using input from the daily care notes collected from the hospitalization of critically ill patients. Eight teams submitted their final systems to the shared task leaderboard. In this paper, we describe the tasks, datasets, evaluation metrics, and baseline systems. Additionally, the techniques and results of the evaluation of the different approaches tried by the participating teams are summarized."",
}
@",electronic health record,health record,natural language process,natural languag,language process,nlp,generat,summar,shared task,evalu
" ""{TALP}-{UPC} at {P}rob{S}um 2023: Fine-tuning and Data Augmentation Strategies for {NER}"","," ""This paper describes the submission of the TALP-UPC team to the Problem List Summarization task from the BioNLP 2023 workshop. This task consists of automatically extracting a list of health issues from the e-health medical record of a given patient. Our submission combines additional steps of data annotationwith finetuning of BERT pre-trained language models. Our experiments focus on the impact of finetuning on different datasets as well as the addition of data augmentation techniques to delay overfitting."",","{torrero-etal-2023-talp,
    title = ""{TALP}-{UPC} at {P}rob{S}um 2023: Fine-tuning and Data Augmentation Strategies for {NER}"",
    author = ""Torrero, Neil  and
      Sant, Gerard  and
      Escolano, Carlos"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.48"",
    doi = ""10.18653/v1/2023.bionlp-1.48"",
    pages = ""497--502"",
    abstract = ""This paper describes the submission of the TALP-UPC team to the Problem List Summarization task from the BioNLP 2023 workshop. This task consists of automatically extracting a list of health issues from the e-health medical record of a given patient. Our submission combines additional steps of data annotationwith finetuning of BERT pre-trained language models. Our experiments focus on the impact of finetuning on different datasets as well as the addition of data augmentation techniques to delay overfitting."",
}
@",medical record,natural language process,natural languag,language process,nlp,summar,shared task,annotat
" ""Team Converge at {P}rob{S}um 2023: Abstractive Text Summarization of Patient Progress Notes"","," ""In this paper, we elaborate on our approach for the shared task 1A issued by BioNLP Workshop 2023 titled Problem List Summarization. With an increase in the digitization of health records, a need arises for quick and precise summarization of large amounts of records. With the help of summarization, medical professionals can sieve through multiple records in a short span of time without overlooking any crucial point. We use abstractive text summarization for this task and experiment with multiple state-of-the-art models like Pegasus, BART, and T5, along with various pre-processing and data augmentation techniques to generate summaries from patients{'} progress notes. For this task, the metric used was the ROUGE-L score. From our experiments, we conclude that Pegasus is the best-performing model on the dataset, achieving a ROUGE-L F1 score of 0.2744 on the test dataset (3rd rank on the leaderboard)."",","{kolhatkar-etal-2023-team,
    title = ""Team Converge at {P}rob{S}um 2023: Abstractive Text Summarization of Patient Progress Notes"",
    author = ""Kolhatkar, Gaurav  and
      Paranjape, Aditya  and
      Gokhale, Omkar  and
      Kadam, Dipali"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.50"",
    doi = ""10.18653/v1/2023.bionlp-1.50"",
    pages = ""510--515"",
    abstract = ""In this paper, we elaborate on our approach for the shared task 1A issued by BioNLP Workshop 2023 titled Problem List Summarization. With an increase in the digitization of health records, a need arises for quick and precise summarization of large amounts of records. With the help of summarization, medical professionals can sieve through multiple records in a short span of time without overlooking any crucial point. We use abstractive text summarization for this task and experiment with multiple state-of-the-art models like Pegasus, BART, and T5, along with various pre-processing and data augmentation techniques to generate summaries from patients{'} progress notes. For this task, the metric used was the ROUGE-L score. From our experiments, we conclude that Pegasus is the best-performing model on the dataset, achieving a ROUGE-L F1 score of 0.2744 on the test dataset (3rd rank on the leaderboard)."",
}
@",health record,natural language process,natural languag,language process,nlp,generat,summar,shared task
" ""{SINAI} at {R}ad{S}um23: Radiology Report Summarization Based on Domain-Specific Sequence-To-Sequence Transformer Model"","," ""This paper covers participation of the SINAI team in the shared task 1B: Radiology Report Summarization at the BioNLP workshop held on ACL 2023. Our proposal follows a sequence-to-sequence approach which leverages pre-trained multilingual general domain and monolingual biomedical domain pre-trained language models. The best performing system based on domain-specific model reached 33.96 F1RadGraph score which is the fourth best result among the challenge participants. This model was made publicly available on HuggingFace. We also describe an attempt of Proximal Policy Optimization Reinforcement Learning that was made in order to improve the factual correctness measured with F1RadGraph but did not lead to satisfactory results."",","{chizhikova-etal-2023-sinai,
    title = ""{SINAI} at {R}ad{S}um23: Radiology Report Summarization Based on Domain-Specific Sequence-To-Sequence Transformer Model"",
    author = ""Chizhikova, Mariia  and
      Diaz-Galiano, Manuel  and
      Urena-Lopez, L. Alfonso  and
      Martin-Valdivia, M. Teresa"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.53"",
    doi = ""10.18653/v1/2023.bionlp-1.53"",
    pages = ""530--534"",
    abstract = ""This paper covers participation of the SINAI team in the shared task 1B: Radiology Report Summarization at the BioNLP workshop held on ACL 2023. Our proposal follows a sequence-to-sequence approach which leverages pre-trained multilingual general domain and monolingual biomedical domain pre-trained language models. The best performing system based on domain-specific model reached 33.96 F1RadGraph score which is the fourth best result among the challenge participants. This model was made publicly available on HuggingFace. We also describe an attempt of Proximal Policy Optimization Reinforcement Learning that was made in order to improve the factual correctness measured with F1RadGraph but did not lead to satisfactory results."",
}
@",medical domain,natural language process,natural languag,language process,nlp,summar,shared task,challeng
" ""shs-nlp at {R}ad{S}um23: Domain-Adaptive Pre-training of Instruction-tuned {LLM}s for Radiology Report Impression Generation"","," ""Instruction-tuned generative large language models (LLMs), such as ChatGPT and Bloomz, possess excellent generalization abilities. However, they face limitations in understanding radiology reports, particularly when generating the IMPRESSIONS section from the FINDINGS section. These models tend to produce either verbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to medical text data during training. We present a system that leverages large-scale medical text data for domain-adaptive pre-training of instruction-tuned LLMs, enhancing their medical knowledge and performance on specific medical tasks. We demonstrate that this system performs better in a zero-shot setting compared to several pretrain-and-finetune adaptation methods on the IMPRESSIONS generation task. Furthermore, it ranks 1st among participating systems in Task 1B: Radiology Report Summarization."",","{karn-etal-2023-shs,
    title = ""shs-nlp at {R}ad{S}um23: Domain-Adaptive Pre-training of Instruction-tuned {LLM}s for Radiology Report Impression Generation"",
    author = ""Karn, Sanjeev Kumar  and
      Ghosh, Rikhiya  and
      P, Kusuma  and
      Farri, Oladimeji"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.57"",
    doi = ""10.18653/v1/2023.bionlp-1.57"",
    pages = ""550--556"",
    abstract = ""Instruction-tuned generative large language models (LLMs), such as ChatGPT and Bloomz, possess excellent generalization abilities. However, they face limitations in understanding radiology reports, particularly when generating the IMPRESSIONS section from the FINDINGS section. These models tend to produce either verbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to medical text data during training. We present a system that leverages large-scale medical text data for domain-adaptive pre-training of instruction-tuned LLMs, enhancing their medical knowledge and performance on specific medical tasks. We demonstrate that this system performs better in a zero-shot setting compared to several pretrain-and-finetune adaptation methods on the IMPRESSIONS generation task. Furthermore, it ranks 1st among participating systems in Task 1B: Radiology Report Summarization."",
}
@",medical text,natural language process,natural languag,language process,nlp,generat,summar,shared task
" ""{VBD}-{NLP} at {B}io{L}ay{S}umm Task 1: Explicit and Implicit Key Information Selection for Lay Summarization on Biomedical Long Documents"","," ""We describe our systems participated in the BioLaySumm 2023 Task 1, which aims at automatically generating lay summaries of scientific articles in a simplified way so that its content becomes easier to comprehend for non-expert readers. Our approaches are based on selecting key information by both explicit and implicit strategies. For explicit selection strategies, we conduct extractive summarization based on selecting key sentences for training abstractive summarization models. For implicit selection strategies, we utilize a method based on a factorized energy-based model, which is able to extract important information from long documents to generate summaries and achieve promising results. We build our systems using sequence-to-sequence models, which enable us to leverage powerful and biomedical domain pre-trained language models and apply different strategies to generate lay summaries from long documents. We conducted various experiments to carefully investigate the effects of different aspects of this long-document summarization task such as extracting different document lengths and utilizing different pre-trained language models. We achieve the third rank in the shared task (and the second rank excluding the baseline submission of the organizers)."",","{phan-etal-2023-vbd,
    title = ""{VBD}-{NLP} at {B}io{L}ay{S}umm Task 1: Explicit and Implicit Key Information Selection for Lay Summarization on Biomedical Long Documents"",
    author = ""Phan, Phuc  and
      Tran, Tri  and
      Trieu, Hai-Long"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.60"",
    doi = ""10.18653/v1/2023.bionlp-1.60"",
    pages = ""574--578"",
    abstract = ""We describe our systems participated in the BioLaySumm 2023 Task 1, which aims at automatically generating lay summaries of scientific articles in a simplified way so that its content becomes easier to comprehend for non-expert readers. Our approaches are based on selecting key information by both explicit and implicit strategies. For explicit selection strategies, we conduct extractive summarization based on selecting key sentences for training abstractive summarization models. For implicit selection strategies, we utilize a method based on a factorized energy-based model, which is able to extract important information from long documents to generate summaries and achieve promising results. We build our systems using sequence-to-sequence models, which enable us to leverage powerful and biomedical domain pre-trained language models and apply different strategies to generate lay summaries from long documents. We conducted various experiments to carefully investigate the effects of different aspects of this long-document summarization task such as extracting different document lengths and utilizing different pre-trained language models. We achieve the third rank in the shared task (and the second rank excluding the baseline submission of the organizers)."",
}
@",medical domain,natural language process,natural languag,language process,nlp,generat,summar,shared task
" ""{MDC} at {B}io{L}ay{S}umm Task 1: Evaluating {GPT} Models for Biomedical Lay Summarization"","," ""This paper presents our approach to the BioLaySumm Task 1 shared task, held at the BioNLP 2023 Workshop. The effective communication of scientific knowledge to the general public is often limited by the technical language used in research, making it difficult for non-experts to comprehend. To address this issue, lay summaries can be used to explain research findings to non-experts in an accessible form. We conduct an evaluation of autoregressive language models, both general and specialized for the biomedical domain, to generate lay summaries from biomedical research article abstracts. Our findings demonstrate that a GPT-3.5 model combined with a straightforward few-shot prompt produces lay summaries that achieve significantly relevance and factuality compared to those generated by a fine-tuned BioGPT model. However, the summaries generated by the BioGPT model exhibit better readability. Notably, our submission for the shared task achieved 1st place in the competition."",","{turbitt-etal-2023-mdc,
    title = ""{MDC} at {B}io{L}ay{S}umm Task 1: Evaluating {GPT} Models for Biomedical Lay Summarization"",
    author = ""Turbitt, Ois{\'\i}n  and
      Bevan, Robert  and
      Aboshokor, Mouhamad"",
    editor = ""Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin"",
    booktitle = ""The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bionlp-1.65"",
    doi = ""10.18653/v1/2023.bionlp-1.65"",
    pages = ""611--619"",
    abstract = ""This paper presents our approach to the BioLaySumm Task 1 shared task, held at the BioNLP 2023 Workshop. The effective communication of scientific knowledge to the general public is often limited by the technical language used in research, making it difficult for non-experts to comprehend. To address this issue, lay summaries can be used to explain research findings to non-experts in an accessible form. We conduct an evaluation of autoregressive language models, both general and specialized for the biomedical domain, to generate lay summaries from biomedical research article abstracts. Our findings demonstrate that a GPT-3.5 model combined with a straightforward few-shot prompt produces lay summaries that achieve significantly relevance and factuality compared to those generated by a fine-tuned BioGPT model. However, the summaries generated by the BioGPT model exhibit better readability. Notably, our submission for the shared task achieved 1st place in the competition."",
}
@",medical domain,natural language process,natural languag,language process,nlp,generat,summar,shared task,evalu
" ""{ACTA}: Short-Answer Grading in High-Stakes Medical Exams"","," ""This paper presents the ACTA system, which performs automated short-answer grading in the domain of high-stakes medical exams. The system builds upon previous work on neural similarity-based grading approaches by applying these to the medical domain and utilizing contrastive learning as a means to optimize the similarity metric. ACTA is evaluated against three strong baselines and is developed in alignment with operational needs, where low-confidence responses are flagged for human review. Learning curves are explored to understand the effects of training data on performance. The results demonstrate that ACTA leads to substantially lower number of responses being flagged for human review, while maintaining high classification accuracy."",","{suen-etal-2023-acta,
    title = ""{ACTA}: Short-Answer Grading in High-Stakes Medical Exams"",
    author = ""Suen, King Yiu  and
      Yaneva, Victoria  and
      Ha, Le An  and
      Mee, Janet  and
      Zhou, Yiyun  and
      Harik, Polina"",
    editor = {Kochmar, Ekaterina  and
      Burstein, Jill  and
      Horbach, Andrea  and
      Laarmann-Quante, Ronja  and
      Madnani, Nitin  and
      Tack, Ana{\""\i}s  and
      Yaneva, Victoria  and
      Yuan, Zheng  and
      Zesch, Torsten},
    booktitle = ""Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.bea-1.36"",
    doi = ""10.18653/v1/2023.bea-1.36"",
    pages = ""443--447"",
    abstract = ""This paper presents the ACTA system, which performs automated short-answer grading in the domain of high-stakes medical exams. The system builds upon previous work on neural similarity-based grading approaches by applying these to the medical domain and utilizing contrastive learning as a means to optimize the similarity metric. ACTA is evaluated against three strong baselines and is developed in alignment with operational needs, where low-confidence responses are flagged for human review. Learning curves are explored to understand the effects of training data on performance. The results demonstrate that ACTA leads to substantially lower number of responses being flagged for human review, while maintaining high classification accuracy."",
}
@",medical domain,nlp,evalu
" ""Efficient Diagnosis Assignment Using Unstructured Clinical Notes"","," ""Electronic phenotyping entails using electronic health records (EHRs) to identify patients with specific health outcomes and determine when those outcomes occurred. Unstructured clinical notes, which contain a vast amount of information, are a valuable resource for electronic phenotyping. However, traditional methods, such as rule-based labeling functions or neural networks, require significant manual effort to tune and may not generalize well to multiple indications. To address these challenges, we propose \textit{HyDE} (hybrid diagnosis extractor). HyDE is a simple framework for electronic phenotyping that integrates labeling functions and a disease-agnostic neural network to assign diagnoses to patients. By training HyDE{'}s model to correct predictions made by labeling functions, we are able to disambiguate hypertension true positives and false positives with a supervised area under the precision-recall curve (AUPRC) of 0.85. We extend this hypertension-trained model to zero-shot evaluation of four other diseases, generating AUPRC values ranging from 0.82 - 0.95 and outperforming a labeling function baseline by 44 points in F1 score and a Word2Vec baseline by 24 points in F1 score on average. Furthermore, we demonstrate a speedup of {\textgreater}4x by pruning the length of inputs into our language model to {\textasciitilde}2.3{\%} of the full clinical notes, with negligible impact to the AUPRC. HyDE has the potential to improve the efficiency and efficacy of interpreting large-scale unstructured clinical notes for accurate EHR phenotyping."",","{blankemeier-etal-2023-efficient,
    title = ""Efficient Diagnosis Assignment Using Unstructured Clinical Notes"",
    author = ""Blankemeier, Louis  and
      Fries, Jason  and
      Tinn, Robert  and
      Preston, Joseph  and
      Shah, Nigam  and
      Chaudhari, Akshay"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-short.42"",
    doi = ""10.18653/v1/2023.acl-short.42"",
    pages = ""485--494"",
    abstract = ""Electronic phenotyping entails using electronic health records (EHRs) to identify patients with specific health outcomes and determine when those outcomes occurred. Unstructured clinical notes, which contain a vast amount of information, are a valuable resource for electronic phenotyping. However, traditional methods, such as rule-based labeling functions or neural networks, require significant manual effort to tune and may not generalize well to multiple indications. To address these challenges, we propose \textit{HyDE} (hybrid diagnosis extractor). HyDE is a simple framework for electronic phenotyping that integrates labeling functions and a disease-agnostic neural network to assign diagnoses to patients. By training HyDE{'}s model to correct predictions made by labeling functions, we are able to disambiguate hypertension true positives and false positives with a supervised area under the precision-recall curve (AUPRC) of 0.85. We extend this hypertension-trained model to zero-shot evaluation of four other diseases, generating AUPRC values ranging from 0.82 - 0.95 and outperforming a labeling function baseline by 44 points in F1 score and a Word2Vec baseline by 24 points in F1 score on average. Furthermore, we demonstrate a speedup of {\textgreater}4x by pruning the length of inputs into our language model to {\textasciitilde}2.3{\%} of the full clinical notes, with negligible impact to the AUPRC. HyDE has the potential to improve the efficiency and efficacy of interpreting large-scale unstructured clinical notes for accurate EHR phenotyping."",
}
@",electronic health record,health record,clinical not,generat,challeng,evalu
" ""Clinical Note Owns its Hierarchy: Multi-Level Hypergraph Neural Networks for Patient-Level Representation Learning"","," ""Leveraging knowledge from electronic health records (EHRs) to predict a patient{'}s condition is essential to the effective delivery of appropriate care. Clinical notes of patient EHRs contain valuable information from healthcare professionals, but have been underused due to their difficult contents and complex hierarchies. Recently, hypergraph-based methods have been proposed for document classifications. Directly adopting existing hypergraph methods on clinical notes cannot sufficiently utilize the hierarchy information of the patient, which can degrade clinical semantic information by (1) frequent neutral words and (2) hierarchies with imbalanced distribution. Thus, we propose a taxonomy-aware multi-level hypergraph neural network (TM-HGNN), where multi-level hypergraphs assemble useful neutral words with rare keywords via note and taxonomy level hyperedges to retain the clinical semantic information. The constructed patient hypergraphs are fed into hierarchical message passing layers for learning more balanced multi-level knowledge at the note and taxonomy levels. We validate the effectiveness of TM-HGNN by conducting extensive experiments with MIMIC-III dataset on benchmark in-hospital-mortality prediction."",","{kim-etal-2023-clinical,
    title = ""Clinical Note Owns its Hierarchy: Multi-Level Hypergraph Neural Networks for Patient-Level Representation Learning"",
    author = ""Kim, Nayeon  and
      Piao, Yinhua  and
      Kim, Sun"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.305"",
    doi = ""10.18653/v1/2023.acl-long.305"",
    pages = ""5559--5573"",
    abstract = ""Leveraging knowledge from electronic health records (EHRs) to predict a patient{'}s condition is essential to the effective delivery of appropriate care. Clinical notes of patient EHRs contain valuable information from healthcare professionals, but have been underused due to their difficult contents and complex hierarchies. Recently, hypergraph-based methods have been proposed for document classifications. Directly adopting existing hypergraph methods on clinical notes cannot sufficiently utilize the hierarchy information of the patient, which can degrade clinical semantic information by (1) frequent neutral words and (2) hierarchies with imbalanced distribution. Thus, we propose a taxonomy-aware multi-level hypergraph neural network (TM-HGNN), where multi-level hypergraphs assemble useful neutral words with rare keywords via note and taxonomy level hyperedges to retain the clinical semantic information. The constructed patient hypergraphs are fed into hierarchical message passing layers for learning more balanced multi-level knowledge at the note and taxonomy levels. We validate the effectiveness of TM-HGNN by conducting extensive experiments with MIMIC-III dataset on benchmark in-hospital-mortality prediction."",
}
@",electronic health record,health record,clinical not,semant,benchmark
" ""{DICE}: Data-Efficient Clinical Event Extraction with Generative Models"","," ""Event extraction for the clinical domain is an under-explored research area. The lack of training data along with the high volume of domain-specific terminologies with vague entity boundaries makes the task especially challenging. In this paper, we introduce DICE, a robust and data-efficient generative model for clinical event extraction. DICE frames event extraction as a conditional generation problem and introduces a contrastive learning objective to accurately decide the boundaries of biomedical mentions. DICE also trains an auxiliary mention identification task jointly with event extraction tasks to better identify entity mention boundaries, and further introduces special markers to incorporate identified entity mentions as trigger and argument candidates for their respective tasks. To benchmark clinical event extraction, we compose MACCROBAT-EE, the first clinical event extraction dataset with argument annotation, based on an existing clinical information extraction dataset MACCROBAT. Our experiments demonstrate state-of-the-art performances of DICE for clinical and news domain event extraction, especially under low data settings."",","{ma-etal-2023-dice,
    title = ""{DICE}: Data-Efficient Clinical Event Extraction with Generative Models"",
    author = ""Ma, Mingyu Derek  and
      Taylor, Alexander  and
      Wang, Wei  and
      Peng, Nanyun"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.886"",
    doi = ""10.18653/v1/2023.acl-long.886"",
    pages = ""15898--15917"",
    abstract = ""Event extraction for the clinical domain is an under-explored research area. The lack of training data along with the high volume of domain-specific terminologies with vague entity boundaries makes the task especially challenging. In this paper, we introduce DICE, a robust and data-efficient generative model for clinical event extraction. DICE frames event extraction as a conditional generation problem and introduces a contrastive learning objective to accurately decide the boundaries of biomedical mentions. DICE also trains an auxiliary mention identification task jointly with event extraction tasks to better identify entity mention boundaries, and further introduces special markers to incorporate identified entity mentions as trigger and argument candidates for their respective tasks. To benchmark clinical event extraction, we compose MACCROBAT-EE, the first clinical event extraction dataset with argument annotation, based on an existing clinical information extraction dataset MACCROBAT. Our experiments demonstrate state-of-the-art performances of DICE for clinical and news domain event extraction, especially under low data settings."",
}
@",clinical domain,generat,annotat,challeng,benchmark
" ""{D}r{BERT}: A Robust Pre-trained Model in {F}rench for Biomedical and Clinical domains"","," ""In recent years, pre-trained language models (PLMs) achieve the best performance on a wide range of natural language processing (NLP) tasks. While the first models were trained on general domain data, specialized ones have emerged to more effectively treat specific domains. In this paper, we propose an original study of PLMs in the medical domain on French language. We compare, for the first time, the performance of PLMs trained on both public data from the web and private data from healthcare establishments. We also evaluate different learning strategies on a set of biomedical tasks. In particular, we show that we can take advantage of already existing biomedical PLMs in a foreign language by further pre-train it on our targeted data. Finally, we release the first specialized PLMs for the biomedical field in French, called DrBERT, as well as the largest corpus of medical data under free license on which these models are trained."",","{labrak-etal-2023-drbert,
    title = ""{D}r{BERT}: A Robust Pre-trained Model in {F}rench for Biomedical and Clinical domains"",
    author = ""Labrak, Yanis  and
      Bazoge, Adrien  and
      Dufour, Richard  and
      Rouvier, Mickael  and
      Morin, Emmanuel  and
      Daille, B{\'e}atrice  and
      Gourraud, Pierre-Antoine"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.896"",
    doi = ""10.18653/v1/2023.acl-long.896"",
    pages = ""16207--16221"",
    abstract = ""In recent years, pre-trained language models (PLMs) achieve the best performance on a wide range of natural language processing (NLP) tasks. While the first models were trained on general domain data, specialized ones have emerged to more effectively treat specific domains. In this paper, we propose an original study of PLMs in the medical domain on French language. We compare, for the first time, the performance of PLMs trained on both public data from the web and private data from healthcare establishments. We also evaluate different learning strategies on a set of biomedical tasks. In particular, we show that we can take advantage of already existing biomedical PLMs in a foreign language by further pre-train it on our targeted data. Finally, we release the first specialized PLMs for the biomedical field in French, called DrBERT, as well as the largest corpus of medical data under free license on which these models are trained."",
}
@",clinical domain,medical domain,natural language process,natural languag,language process,nlp,evalu,public data
" ""Disease Network Constructor: a Pathway Extraction and Visualization"","," ""We present Disease Network Constructor (DNC), a system that extracts and visualizes a disease network, in which nodes are entities such as diseases, proteins, and genes, and edges represent regulation relation. We focused on the disease network derived through regulation events found in scientific articles on idiopathic pulmonary fibrosis (IPF). The front-end web-base user interface of DNC includes two-dimensional (2D) and 3D visualizations of the constructed disease network. The back-end system of DNC includes several natural language processing (NLP) techniques to process biomedical text including BERT-based tokenization on the basis of Bidirectional Encoder Representations from Transformers (BERT), flat and nested named entity recognition (NER), candidate generation and candidate ranking for entity linking (EL) or, relation extraction (RE), and event extraction (EE) tasks. We evaluated the end-to-end EL and end-to-end nested EE systems to determine the DNC{'}s back-endimplementation performance. To the best of our knowledge, this is the first attempt that addresses neural NER, EL, RE, and EE tasks in an end-to-end manner that constructs a path-way visualization from events, which we name Disease Network Constructor. The demonstration video can be accessed from \url{https://youtu.be/rFhWwAgcXE8}. We release an online system for end users and the source code is available at \url{https://github.com/aistairc/PRISM-APIs/}."",","{sohrab-etal-2023-disease,
    title = ""Disease Network Constructor: a Pathway Extraction and Visualization"",
    author = ""Sohrab, Mohammad Golam  and
      Duong, Khoa  and
      Topi{\'c}, Goran  and
      Ikeda, Masami  and
      Nagano, Nozomi  and
      Natsume-Kitatani, Yayoi  and
      Kuroda, Masakata  and
      Itoh, Mari  and
      Takamura, Hiroya"",
    editor = ""Bollegala, Danushka  and
      Huang, Ruihong  and
      Ritter, Alan"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-demo.53"",
    doi = ""10.18653/v1/2023.acl-demo.53"",
    pages = ""549--557"",
    abstract = ""We present Disease Network Constructor (DNC), a system that extracts and visualizes a disease network, in which nodes are entities such as diseases, proteins, and genes, and edges represent regulation relation. We focused on the disease network derived through regulation events found in scientific articles on idiopathic pulmonary fibrosis (IPF). The front-end web-base user interface of DNC includes two-dimensional (2D) and 3D visualizations of the constructed disease network. The back-end system of DNC includes several natural language processing (NLP) techniques to process biomedical text including BERT-based tokenization on the basis of Bidirectional Encoder Representations from Transformers (BERT), flat and nested named entity recognition (NER), candidate generation and candidate ranking for entity linking (EL) or, relation extraction (RE), and event extraction (EE) tasks. We evaluated the end-to-end EL and end-to-end nested EE systems to determine the DNC{'}s back-endimplementation performance. To the best of our knowledge, this is the first attempt that addresses neural NER, EL, RE, and EE tasks in an end-to-end manner that constructs a path-way visualization from events, which we name Disease Network Constructor. The demonstration video can be accessed from \url{https://youtu.be/rFhWwAgcXE8}. We release an online system for end users and the source code is available at \url{https://github.com/aistairc/PRISM-APIs/}."",
}
@",medical text,natural language process,natural languag,language process,nlp,generat,relation extract,entity recognit,evalu
" ""Findings of the {WMT} 2022 Biomedical Translation Shared Task: Monolingual Clinical Case Reports"","," ""In the seventh edition of the WMT Biomedical Task, we addressed a total of seven languagepairs, namely English/German, English/French, English/Spanish, English/Portuguese, English/Chinese, English/Russian, English/Italian. This year{'}s test sets covered three types of biomedical text genre. In addition to scientific abstracts and terminology items used in previous editions, we released test sets of clinical cases. The evaluation of clinical cases translations were given special attention by involving clinicians in the preparation of reference translations and manual evaluation. For the main MEDLINE test sets, we received a total of 609 submissions from 37 teams. For the ClinSpEn sub-task, we had the participation of five teams."",","{neves-etal-2022-findings,
    title = ""Findings of the {WMT} 2022 Biomedical Translation Shared Task: Monolingual Clinical Case Reports"",
    author = ""Neves, Mariana  and
      Jimeno Yepes, Antonio  and
      Siu, Amy  and
      Roller, Roland  and
      Thomas, Philippe  and
      Vicente Navarro, Maika  and
      Yeganova, Lana  and
      Wiemann, Dina  and
      Di Nunzio, Giorgio Maria  and
      Vezzani, Federica  and
      Gerardin, Christel  and
      Bawden, Rachel  and
      Estrada, Darryl Johan  and
      Lima-lopez, Salvador  and
      Farre-maduel, Eulalia  and
      Krallinger, Martin  and
      Grozea, Cristian  and
      Neveol, Aurelie"",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\""\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = ""Proceedings of the Seventh Conference on Machine Translation (WMT)"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.wmt-1.69"",
    pages = ""694--723"",
    abstract = ""In the seventh edition of the WMT Biomedical Task, we addressed a total of seven languagepairs, namely English/German, English/French, English/Spanish, English/Portuguese, English/Chinese, English/Russian, English/Italian. This year{'}s test sets covered three types of biomedical text genre. In addition to scientific abstracts and terminology items used in previous editions, we released test sets of clinical cases. The evaluation of clinical cases translations were given special attention by involving clinicians in the preparation of reference translations and manual evaluation. For the main MEDLINE test sets, we received a total of 609 submissions from 37 teams. For the ClinSpEn sub-task, we had the participation of five teams."",
}
@",medical text,translat,shared task,evalu
" ""Optum{'}s Submission to {WMT}22 Biomedical Translation Tasks"","," ""This paper describes Optum{'}s submission to the Biomedical Translation task of the seventh conference on Machine Translation (WMT22). The task aims at promoting the development and evaluation of machine translation systems in their ability to handle challenging domain-specific biomedical data. We made submissions to two sub-tracks of ClinSpEn 2022, namely, ClinSpEn-CC (clinical cases) and ClinSpEn-OC (ontology concepts). These sub-tasks aim to test translation from English to Spanish. Our approach involves fine-tuning a pre-trained transformer model using in-house clinical domain data and the biomedical data provided by WMT. The fine-tuned model results in a test BLEU score of 38.12 in the ClinSpEn-CC (clinical cases) subtask, which is a gain of 1.23 BLEU compared to the pre-trained model."",","{manchanda-bhagwat-2022-optums,
    title = ""Optum{'}s Submission to {WMT}22 Biomedical Translation Tasks"",
    author = ""Manchanda, Sahil  and
      Bhagwat, Saurabh"",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\""\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = ""Proceedings of the Seventh Conference on Machine Translation (WMT)"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.wmt-1.86"",
    pages = ""925--929"",
    abstract = ""This paper describes Optum{'}s submission to the Biomedical Translation task of the seventh conference on Machine Translation (WMT22). The task aims at promoting the development and evaluation of machine translation systems in their ability to handle challenging domain-specific biomedical data. We made submissions to two sub-tracks of ClinSpEn 2022, namely, ClinSpEn-CC (clinical cases) and ClinSpEn-OC (ontology concepts). These sub-tasks aim to test translation from English to Spanish. Our approach involves fine-tuning a pre-trained transformer model using in-house clinical domain data and the biomedical data provided by WMT. The fine-tuned model results in a test BLEU score of 38.12 in the ClinSpEn-CC (clinical cases) subtask, which is a gain of 1.23 BLEU compared to the pre-trained model."",
}
@",clinical domain,translat,challeng,evalu,track
" ""Leveraging knowledge graphs to update scientific word embeddings using latent semantic imputation"","," ""The most interesting words in scientific texts will often be novel or rare. This presents a challenge for scientific word embedding models to determine quality embedding vectors for useful terms that are infrequent or newly emerging. We demonstrate how Latent Semantic Imputation (LSI) can address this problem by imputing embeddings for domain-specific words from up-to-date knowledge graphs while otherwise preserving the original word embedding model. We use the MeSH knowledge graph to impute embedding vectors for biomedical terminology without retraining and evaluate the resulting embedding model on a domain-specific word-pair similarity task. We show that LSI can produce reliable embedding vectors for rare and out-of-vocabulary terms in the biomedical domain."",","{hoelscher-obermaier-etal-2022-leveraging,
    title = ""Leveraging knowledge graphs to update scientific word embeddings using latent semantic imputation"",
    author = ""Hoelscher-Obermaier, Jason  and
      Stevinson, Edward  and
      Stauber, Valentin  and
      Zhelev, Ivaylo  and
      Botev, Viktor  and
      Wu, Ronin  and
      Minton, Jeremy"",
    editor = ""Ghosal, Tirthankar  and
      Blanco-Cuaresma, Sergi  and
      Accomazzi, Alberto  and
      Patton, Robert M.  and
      Grezes, Felix  and
      Allen, Thomas"",
    booktitle = ""Proceedings of the first Workshop on Information Extraction from Scientific Publications"",
    month = nov,
    year = ""2022"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.wiesp-1.6"",
    pages = ""43--53"",
    abstract = ""The most interesting words in scientific texts will often be novel or rare. This presents a challenge for scientific word embedding models to determine quality embedding vectors for useful terms that are infrequent or newly emerging. We demonstrate how Latent Semantic Imputation (LSI) can address this problem by imputing embeddings for domain-specific words from up-to-date knowledge graphs while otherwise preserving the original word embedding model. We use the MeSH knowledge graph to impute embedding vectors for biomedical terminology without retraining and evaluate the resulting embedding model on a domain-specific word-pair similarity task. We show that LSI can produce reliable embedding vectors for rare and out-of-vocabulary terms in the biomedical domain."",
}
@",medical domain,semant,challeng,evalu
" ""Proceedings of The Third Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)"","," ""The goal of information-seeking dialogue is to respond to seeker queries with natural language utterances that are grounded on knowledge sources. However, dialogue systems often produce unsupported utterances, a phenomenon known as hallucination. To mitigate this behavior, we adopt a data-centric solution and create FaithDial, a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe that FaithDial is more faithful than WoW while also maintaining engaging conversations. We show that FaithDial can serve as training signal for: i) a hallucination critic, which discriminates whether an utterance is faithful or not, and boosts the performance by 12.8 F1 score on the BEGIN benchmark compared to existing datasets for dialogue coherence; ii) high-quality dialogue generation. We benchmark a series of state-of-the-art models and propose an auxiliary contrastive objective that achieves the highest level of faithfulness and abstractiveness based on several automated metrics. Further, we find that the benefits of FaithDial generalize to zero-shot transfer on other datasets, such as CMU-Dog and TopicalChat. Finally, human evaluation reveals that responses generated by models trained on FaithDial are perceived as more interpretable, cooperative, and engaging."",","{berhe-etal-2022-survey,
    title = ""Survey on Narrative Structure: from Linguistic Theories to Automatic Extraction Approaches"",
    author = ""Berhe, Aman  and
      Guinaudeau, Camille  and
      Barras, Claude"",
    editor = ""Fabre, C{\'e}cile  and
      Morin, Emmanuel  and
      Rosset, Sophie  and
      S{\'e}billot, Pascale"",
    booktitle = ""Traitement Automatique des Langues, Volume 63, Num{\'e}ro 1 : Varia [Varia]"",
    year = ""2022"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2022.tal-1.3"",
    pages = ""63--87"",
}
@article{chang-bergen-2022-word,
    title = ""Word Acquisition in Neural Language Models"",
    author = ""Chang, Tyler A.  and
      Bergen, Benjamin K."",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.1"",
    doi = ""10.1162/tacl_a_00444"",
    pages = ""1--16"",
    abstract = ""We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words{'} ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models."",
}
@article{gantt-etal-2022-decomposing,
    title = ""Decomposing and Recomposing Event Structure"",
    author = ""Gantt, William  and
      Glass, Lelia  and
      White, Aaron Steven"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.2"",
    doi = ""10.1162/tacl_a_00445"",
    pages = ""17--34"",
    abstract = ""We present an event structure classification empirically derived from inferential properties annotated on sentence- and document-level Universal Decompositional Semantics (UDS) graphs. We induce this classification jointly with semantic role, entity, and event-event relation classifications using a document-level generative model structured by these graphs. To support this induction, we augment existing annotations found in the UDS1.0 dataset, which covers the entirety of the English Web Treebank, with an array of inferential properties capturing fine-grained aspects of the temporal and aspectual structure of events. The resulting dataset (available at decomp.io) is the largest annotation of event structure and (partial) event coreference to date."",
}
@article{nan-etal-2022-fetaqa,
    title = ""{F}e{T}a{QA}: Free-form Table Question Answering"",
    author = ""Nan, Linyong  and
      Hsieh, Chiachun  and
      Mao, Ziming  and
      Lin, Xi Victoria  and
      Verma, Neha  and
      Zhang, Rui  and
      Kry{\'s}ci{\'n}ski, Wojciech  and
      Schoelkopf, Hailey  and
      Kong, Riley  and
      Tang, Xiangru  and
      Mutuma, Mutethia  and
      Rosand, Ben  and
      Trindade, Isabel  and
      Bandaru, Renusree  and
      Cunningham, Jacob  and
      Xiong, Caiming  and
      Radev, Dragomir  and
      Radev, Dragomir"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.3"",
    doi = ""10.1162/tacl_a_00446"",
    pages = ""35--49"",
    abstract = ""Existing table question answering datasets contain abundant factual questions that primarily evaluate a QA system{'}s comprehension of query and tabular data. However, restricted by their short-form answers, these datasets fail to include question{--}answer interactions that represent more advanced and naturally occurring information needs: questions that ask for reasoning and integration of information pieces retrieved from a structured knowledge source. To complement the existing datasets and to reveal the challenging nature of the table-based question answering task, we introduce FeTaQA, a new dataset with 10K Wikipedia-based table, question, free-form answer, supporting table cells pairs. FeTaQA is collected from noteworthy descriptions of Wikipedia tables that contain information people tend to seek; generation of these descriptions requires advanced processing that humans perform on a daily basis: Understand the question and table, retrieve, integrate, infer, and conduct text planning and surface realization to generate an answer. We provide two benchmark methods for the proposed task: a pipeline method based on semantic parsing-based QA systems and an end-to-end method based on large pretrained text generation models, and show that FeTaQA poses a challenge for both methods."",
}
@article{kreutzer-etal-2022-quality,
    title = ""Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets"",
    author = {Kreutzer, Julia  and
      Caswell, Isaac  and
      Wang, Lisa  and
      Wahab, Ahsan  and
      van Esch, Daan  and
      Ulzii-Orshikh, Nasanbayar  and
      Tapo, Allahsera  and
      Subramani, Nishant  and
      Sokolov, Artem  and
      Sikasote, Claytone  and
      Setyawan, Monang  and
      Sarin, Supheakmungkol  and
      Samb, Sokhar  and
      Sagot, Beno{\^\i}t  and
      Rivera, Clara  and
      Rios, Annette  and
      Papadimitriou, Isabel  and
      Osei, Salomey  and
      Suarez, Pedro Ortiz  and
      Orife, Iroro  and
      Ogueji, Kelechi  and
      Rubungo, Andre Niyongabo  and
      Nguyen, Toan Q.  and
      M{\""u}ller, Mathias  and
      M{\""u}ller, Andr{\'e}  and
      Muhammad, Shamsuddeen Hassan  and
      Muhammad, Nanda  and
      Mnyakeni, Ayanda  and
      Mirzakhalov, Jamshidbek  and
      Matangira, Tapiwanashe  and
      Leong, Colin  and
      Lawson, Nze  and
      Kudugunta, Sneha  and
      Jernite, Yacine  and
      Jenny, Mathias  and
      Firat, Orhan  and
      Dossou, Bonaventure F. P.  and
      Dlamini, Sakhile  and
      de Silva, Nisansa  and
      {\c{C}}abuk Ball{\i}, Sakine  and
      Biderman, Stella  and
      Battisti, Alessia  and
      Baruwa, Ahmed  and
      Bapna, Ankur  and
      Baljekar, Pallavi  and
      Azime, Israel Abebe  and
      Awokoya, Ayodele  and
      Ataman, Duygu  and
      Ahia, Orevaoghene  and
      Ahia, Oghenefego  and
      Agrawal, Sweta  and
      Adeyemi, Mofetoluwa},
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.4"",
    doi = ""10.1162/tacl_a_00447"",
    pages = ""50--72"",
    abstract = ""With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, Web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50{\%} sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases."",
}
@article{clark-etal-2022-canine,
    title = ""Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"",
    author = ""Clark, Jonathan H.  and
      Garrette, Dan  and
      Turc, Iulia  and
      Wieting, John"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.5"",
    doi = ""10.1162/tacl_a_00448"",
    pages = ""73--91"",
    abstract = ""Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model{'}s ability to adapt. In this paper, we present Canine, a neural encoder that operates directly on character sequences{---}without explicit tokenization or vocabulary{---}and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, Canine combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. Canine outperforms a comparable mBert model by 5.7 F1 on TyDi QA, a challenging multilingual benchmark, despite having fewer model parameters."",
}
@article{davani-etal-2022-dealing,
    title = ""Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations"",
    author = ""Mostafazadeh Davani, Aida  and
      D{\'\i}az, Mark  and
      Prabhakaran, Vinodkumar"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.6"",
    doi = ""10.1162/tacl_a_00449"",
    pages = ""92--110"",
    abstract = ""Majority voting and averaging are common approaches used to resolve annotator disagreements and derive single ground truth labels from multiple annotations. However, annotators may systematically disagree with one another, often reflecting their individual biases and values, especially in the case of subjective tasks such as detecting affect, aggression, and hate speech. Annotator disagreements may capture important nuances in such tasks that are often ignored while aggregating annotations to a single ground truth. In order to address this, we investigate the efficacy of multi-annotator models. In particular, our multi-task based approach treats predicting each annotators{'} judgements as separate subtasks, while sharing a common learned representation of the task. We show that this approach yields same or better performance than aggregating labels in the data prior to training across seven different binary classification tasks. Our approach also provides a way to estimate uncertainty in predictions, which we demonstrate better correlate with annotation disagreements than traditional methods. Being able to model uncertainty is especially useful in deployment scenarios where knowing when not to make a prediction is important."",
}
@article{geva-etal-2022-break,
    title = ""Break, Perturb, Build: Automatic Perturbation of Reasoning Paths Through Question Decomposition"",
    author = ""Geva, Mor  and
      Wolfson, Tomer  and
      Berant, Jonathan"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.7"",
    doi = ""10.1162/tacl_a_00450"",
    pages = ""111--126"",
    abstract = ""Recent efforts to create challenge benchmarks that test the abilities of natural language understanding models have largely depended on human annotations. In this work, we introduce the {``}Break, Perturb, Build{''} (BPB) framework for automatic reasoning-oriented perturbation of question-answer pairs. BPB represents a question by decomposing it into the reasoning steps that are required to answer it, symbolically perturbs the decomposition, and then generates new question-answer pairs. We demonstrate the effectiveness of BPB by creating evaluation sets for three reading comprehension (RC) benchmarks, generating thousands of high-quality examples without human intervention. We evaluate a range of RC models on our evaluation sets, which reveals large performance gaps on generated examples compared to the original data. Moreover, symbolic perturbations enable fine-grained analysis of the strengths and limitations of models. Last, augmenting the training data with examples generated by BPB helps close the performance gaps, without any drop on the original data distribution."",
}
@article{nishida-matsumoto-2022-domain,
    title = ""Out-of-Domain Discourse Dependency Parsing via Bootstrapping: An Empirical Analysis on Its Effectiveness and Limitation"",
    author = ""Nishida, Noriki  and
      Matsumoto, Yuji"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.8"",
    doi = ""10.1162/tacl_a_00451"",
    pages = ""127--144"",
    abstract = ""Discourse parsing has been studied for decades. However, it still remains challenging to utilize discourse parsing for real-world applications because the parsing accuracy degrades significantly on out-of-domain text. In this paper, we report and discuss the effectiveness and limitations of bootstrapping methods for adapting modern BERT-based discourse dependency parsers to out-of-domain text without relying on additional human supervision. Specifically, we investigate self-training, co-training, tri-training, and asymmetric tri-training of graph-based and transition-based discourse dependency parsing models, as well as confidence measures and sample selection criteria in two adaptation scenarios: monologue adaptation between scientific disciplines and dialogue genre adaptation. We also release COVID-19 Discourse Dependency Treebank (COVID19-DTB), a new manually annotated resource for discourse dependency parsing of biomedical paper abstracts. The experimental results show that bootstrapping is significantly and consistently effective for unsupervised domain adaptation of discourse dependency parsing, but the low coverage of accurately predicted pseudo labels is a bottleneck for further improvement. We show that active learning can mitigate this limitation."",
}
@article{ramesh-etal-2022-samanantar,
    title = ""Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 {I}ndic Languages"",
    author = ""Ramesh, Gowtham  and
      Doddapaneni, Sumanth  and
      Bheemaraj, Aravinth  and
      Jobanputra, Mayank  and
      AK, Raghavan  and
      Sharma, Ajitesh  and
      Sahoo, Sujit  and
      Diddee, Harshita  and
      J, Mahalakshmi  and
      Kakwani, Divyanshu  and
      Kumar, Navneet  and
      Pradeep, Aswin  and
      Nagaraj, Srihari  and
      Deepak, Kumar  and
      Raghavan, Vivek  and
      Kunchukuttan, Anoop  and
      Kumar, Pratyush  and
      Khapra, Mitesh Shantadevi"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.9"",
    doi = ""10.1162/tacl_a_00452"",
    pages = ""145--162"",
    abstract = ""We present Samanantar, the largest publicly available parallel corpora collection for Indic languages. The collection contains a total of 49.7 million sentence pairs between English and 11 Indic languages (from two language families). Specifically, we compile 12.4 million sentence pairs from existing, publicly available parallel corpora, and additionally mine 37.4 million sentence pairs from the Web, resulting in a 4{\mbox{$\times$}} increase. We mine the parallel sentences from the Web by combining many corpora, tools, and methods: (a) Web-crawled monolingual corpora, (b) document OCR for extracting sentences from scanned documents, (c) multilingual representation models for aligning sentences, and (d) approximate nearest neighbor search for searching in a large collection of sentences. Human evaluation of samples from the newly mined corpora validate the high quality of the parallel sentences across 11 languages. Further, we extract 83.4 million sentence pairs between all 55 Indic language pairs from the English-centric parallel corpus using English as the pivot language. We trained multilingual NMT models spanning all these languages on Samanantar which outperform existing models and baselines on publicly available benchmarks, such as FLORES, establishing the utility of Samanantar. Our data and models are available publicly at Samanantar and we hope they will help advance research in NMT and multilingual NLP for Indic languages."",
}
@article{laban-etal-2022-summac,
    title = ""{S}umma{C}: Re-Visiting {NLI}-based Models for Inconsistency Detection in Summarization"",
    author = ""Laban, Philippe  and
      Schnabel, Tobias  and
      Bennett, Paul N.  and
      Hearst, Marti A."",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.10"",
    doi = ""10.1162/tacl_a_00453"",
    pages = ""163--177"",
    abstract = ""In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SummaC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4{\%}, a 5{\%} improvement compared with prior work."",
}
@article{guo-etal-2022-survey,
    title = ""A Survey on Automated Fact-Checking"",
    author = ""Guo, Zhijiang  and
      Schlichtkrull, Michael  and
      Vlachos, Andreas"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.11"",
    doi = ""10.1162/tacl_a_00454"",
    pages = ""178--206"",
    abstract = ""Fact-checking has become increasingly important due to the speed with which both information and misinformation can spread in the modern media ecosystem. Therefore, researchers have been exploring how fact-checking can be automated, using techniques based on natural language processing, machine learning, knowledge representation, and databases to automatically predict the veracity of claims. In this paper, we survey automated fact-checking stemming from natural language processing, and discuss its connections to related tasks and disciplines. In this process, we present an overview of existing datasets and models, aiming to unify the various definitions given and identify common concepts. Finally, we highlight challenges for future research."",
}
@article{singhania-etal-2022-predicting,
    title = ""Predicting Document Coverage for Relation Extraction"",
    author = ""Singhania, Sneha  and
      Razniewski, Simon  and
      Weikum, Gerhard"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.12"",
    doi = ""10.1162/tacl_a_00456"",
    pages = ""207--223"",
    abstract = ""This paper presents a new task of predicting the coverage of a text document for relation extraction (RE): Does the document contain many relational tuples for a given entity? Coverage predictions are useful in selecting the best documents for knowledge base construction with large input corpora. To study this problem, we present a dataset of 31,366 diverse documents for 520 entities. We analyze the correlation of document coverage with features like length, entity mention frequency, Alexa rank, language complexity, and information retrieval scores. Each of these features has only moderate predictive power. We employ methods combining features with statistical models like TF-IDF and language models like BERT. The model combining features and BERT, HERB, achieves an F1 score of up to 46{\%}. We demonstrate the utility of coverage predictions on two use cases: KB construction and claim refutation."",
}
@article{macavaney-etal-2022-abnirml,
    title = ""{ABNIRML}: Analyzing the Behavior of Neural {IR} Models"",
    author = ""MacAvaney, Sean  and
      Feldman, Sergey  and
      Goharian, Nazli  and
      Downey, Doug  and
      Cohan, Arman"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.13"",
    doi = ""10.1162/tacl_a_00457"",
    pages = ""224--239"",
    abstract = ""Pretrained contextualized language models such as BERT and T5 have established a new state-of-the-art for ad-hoc search. However, it is not yet well understood why these methods are so effective, what makes some variants more effective than others, and what pitfalls they may have. We present a new comprehensive framework for Analyzing the Behavior of Neural IR ModeLs (ABNIRML), which includes new types of diagnostic probes that allow us to test several characteristics{---}such as writing styles, factuality, sensitivity to paraphrasing and word order{---}that are not addressed by previous techniques. To demonstrate the value of the framework, we conduct an extensive empirical study that yields insights into the factors that contribute to the neural model{'}s gains, and identify potential unintended biases the models exhibit. Some of our results confirm conventional wisdom, for example, that recent neural ranking models rely less on exact term overlap with the query, and instead leverage richer linguistic information, evidenced by their higher sensitivity to word and sentence order. Other results are more surprising, such as that some models (e.g., T5 and ColBERT) are biased towards factually correct (rather than simply relevant) texts. Further, some characteristics vary even for the same base language model, and other characteristics can appear due to random variations during model training.1"",
}
@article{feng-etal-2022-neuro,
    title = ""Neuro-symbolic Natural Logic with Introspective Revision for Natural Language Inference"",
    author = ""Feng, Yufei  and
      Yang, Xiaoyu  and
      Zhu, Xiaodan  and
      Greenspan, Michael"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.14"",
    doi = ""10.1162/tacl_a_00458"",
    pages = ""240--256"",
    abstract = ""We introduce a neuro-symbolic natural logic framework based on reinforcement learning with introspective revision. The model samples and rewards specific reasoning paths through policy gradient, in which the introspective revision algorithm modifies intermediate symbolic reasoning steps to discover reward-earning operations as well as leverages external knowledge to alleviate spurious reasoning and training inefficiency. The framework is supported by properly designed local relation models to avoid input entangling, which helps ensure the interpretability of the proof paths. The proposed model has built-in interpretability and shows superior capability in monotonicity inference, systematic generalization, and interpretability, compared with previous models on the existing datasets."",
}
@article{dhingra-etal-2022-time,
    title = ""Time-Aware Language Models as Temporal Knowledge Bases"",
    author = ""Dhingra, Bhuwan  and
      Cole, Jeremy R.  and
      Eisenschlos, Julian Martin  and
      Gillick, Daniel  and
      Eisenstein, Jacob  and
      Cohen, William W."",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.15"",
    doi = ""10.1162/tacl_a_00459"",
    pages = ""257--273"",
    abstract = ""Many facts come with an expiration date, from the name of the President to the basketball team Lebron James plays for. However, most language models (LMs) are trained on snapshots of data collected at a specific moment in time. This can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize. We introduce a diagnostic dataset aimed at probing LMs for factual knowledge that changes over time and highlight problems with LMs at either end of the spectrum{---}those trained on specific slices of temporal data, as well as those trained on a wide range of temporal data. To mitigate these problems, we propose a simple technique for jointly modeling text with its timestamp. This improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods. We also show that models trained with temporal context can be efficiently {``}refreshed{''} as new data arrives, without the need for retraining from scratch."",
}
@article{de-cao-etal-2022-multilingual,
    title = ""Multilingual Autoregressive Entity Linking"",
    author = ""De Cao, Nicola  and
      Wu, Ledell  and
      Popat, Kashyap  and
      Artetxe, Mikel  and
      Goyal, Naman  and
      Plekhanov, Mikhail  and
      Zettlemoyer, Luke  and
      Cancedda, Nicola  and
      Riedel, Sebastian  and
      Petroni, Fabio"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.16"",
    doi = ""10.1162/tacl_a_00460"",
    pages = ""274--290"",
    abstract = ""We present mGENRE, a sequence-to- sequence system for the Multilingual Entity Linking (MEL) problem{---}the task of resolving language-specific mentions to a multilingual Knowledge Base (KB). For a mention in a given language, mGENRE predicts the name of the target entity left-to-right, token-by-token in an autoregressive fashion. The autoregressive formulation allows us to effectively cross-encode mention string and entity names to capture more interactions than the standard dot product between mention and entity vectors. It also enables fast search within a large KB even for mentions that do not appear in mention tables and with no need for large-scale vector indices. While prior MEL works use a single representation for each entity, we match against entity names of as many languages as possible, which allows exploiting language connections between source input and target name. Moreover, in a zero-shot setting on languages with no training data at all, mGENRE treats the target language as a latent variable that is marginalized at prediction time. This leads to over 50{\%} improvements in average accuracy. We show the efficacy of our approach through extensive evaluation including experiments on three popular MEL benchmarks where we establish new state-of-the-art results. Source code available at \url{https://github.com/facebookresearch/GENRE}."",
}
@article{xue-etal-2022-byt5,
    title = ""{B}y{T}5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"",
    author = ""Xue, Linting  and
      Barua, Aditya  and
      Constant, Noah  and
      Al-Rfou, Rami  and
      Narang, Sharan  and
      Kale, Mihir  and
      Roberts, Adam  and
      Raffel, Colin"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.17"",
    doi = ""10.1162/tacl_a_00461"",
    pages = ""291--306"",
    abstract = ""Most widely used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: They can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Because byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.1"",
}
@article{raifer-etal-2022-designing,
    title = ""Designing an Automatic Agent for Repeated Language{--}based Persuasion Games"",
    author = ""Raifer, Maya  and
      Rotman, Guy  and
      Apel, Reut  and
      Tennenholtz, Moshe  and
      Reichart, Roi"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.18"",
    doi = ""10.1162/tacl_a_00462"",
    pages = ""307--324"",
    abstract = ""Persuasion games are fundamental in economics and AI research and serve as the basis for important applications. However, work on this setup assumes communication with stylized messages that do not consist of rich human language. In this paper we consider a repeated sender (expert) {--} receiver (decision maker) game, where the sender is fully informed about the state of the world and aims to persuade the receiver to accept a deal by sending one of several possible natural language reviews. We design an automatic expert that plays this repeated game, aiming to achieve the maximal payoff. Our expert is implemented within the Monte Carlo Tree Search (MCTS) algorithm, with deep learning models that exploit behavioral and linguistic signals in order to predict the next action of the decision maker, and the future payoff of the expert given the state of the game and a candidate review. We demonstrate the superiority of our expert over strong baselines and its adaptability to different decision makers and potential proposed deals.1"",
}
@article{saparov-mitchell-2022-towards,
    title = ""Towards General Natural Language Understanding with Probabilistic Worldbuilding"",
    author = ""Saparov, Abulhair  and
      Mitchell, Tom M."",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.19"",
    doi = ""10.1162/tacl_a_00463"",
    pages = ""325--342"",
    abstract = ""We introduce the Probabilistic Worldbuilding Model (PWM), a new fully symbolic Bayesian model of semantic parsing and reasoning, as a first step in a research program toward more domain- and task-general NLU and AI. Humans create internal mental models of their observations that greatly aid in their ability to understand and reason about a large variety of problems. In PWM, the meanings of sentences, acquired facts about the world, and intermediate steps in reasoning are all expressed in a human-readable formal language, with the design goal of interpretability. PWM is Bayesian, designed specifically to be able to generalize to new domains and new tasks. We derive and implement an inference algorithm that reads sentences by parsing and abducing updates to its latent world model that capture the semantics of those sentences, and evaluate it on two out-of-domain question-answering datasets: (1) ProofWriter and (2) a new dataset we call FictionalGeoQA, designed to be more representative of real language but still simple enough to focus on evaluating reasoning ability, while being robust against heuristics. Our method outperforms baselines on both, thereby demonstrating its value as a proof-of-concept."",
}
@article{somayajula-etal-2022-multi,
    title = ""A Multi-Level Optimization Framework for End-to-End Text Augmentation"",
    author = ""Somayajula, Sai Ashish  and
      Song, Linfeng  and
      Xie, Pengtao"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.20"",
    doi = ""10.1162/tacl_a_00464"",
    pages = ""343--358"",
    abstract = ""Text augmentation is an effective technique in alleviating overfitting in NLP tasks. In existing methods, text augmentation and downstream tasks are mostly performed separately. As a result, the augmented texts may not be optimal to train the downstream model. To address this problem, we propose a three-level optimization framework to perform text augmentation and the downstream task end-to- end. The augmentation model is trained in a way tailored to the downstream task. Our framework consists of three learning stages. A text summarization model is trained to perform data augmentation at the first stage. Each summarization example is associated with a weight to account for its domain difference with the text classification data. At the second stage, we use the model trained at the first stage to perform text augmentation and train a text classification model on the augmented texts. At the third stage, we evaluate the text classification model trained at the second stage and update weights of summarization examples by minimizing the validation loss. These three stages are performed end-to-end. We evaluate our method on several text classification datasets where the results demonstrate the effectiveness of our method. Code is available at \url{https://github.com/Sai-Ashish/End-to-End-Text-Augmentation}."",
}
@article{pruthi-etal-2022-evaluating,
    title = ""Evaluating Explanations: How Much Do Explanations from the Teacher Aid Students?"",
    author = ""Pruthi, Danish  and
      Bansal, Rachit  and
      Dhingra, Bhuwan  and
      Baldini Soares, Livio  and
      Collins, Michael  and
      Lipton, Zachary C.  and
      Neubig, Graham  and
      Cohen, William W."",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.21"",
    doi = ""10.1162/tacl_a_00465"",
    pages = ""359--375"",
    abstract = ""While many methods purport to explain predictions by highlighting salient features, what aims these explanations serve and how they ought to be evaluated often go unstated. In this work, we introduce a framework to quantify the value of explanations via the accuracy gains that they confer on a student model trained to simulate a teacher model. Crucially, the explanations are available to the student during training, but are not available at test time. Compared with prior proposals, our approach is less easily gamed, enabling principled, automatic, model-agnostic evaluation of attributions. Using our framework, we compare numerous attribution methods for text classification and question answering, and observe quantitative differences that are consistent (to a moderate to high degree) across different student model architectures and learning strategies.1"",
}
@article{shen-etal-2022-vila,
    title = ""{VILA}: Improving Structured Content Extraction from Scientific {PDF}s Using Visual Layout Groups"",
    author = ""Shen, Zejiang  and
      Lo, Kyle  and
      Wang, Lucy Lu  and
      Kuehl, Bailey  and
      Weld, Daniel S.  and
      Downey, Doug"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.22"",
    doi = ""10.1162/tacl_a_00466"",
    pages = ""376--392"",
    abstract = ""Accurately extracting structured content from PDFs is a critical first step for NLP over scientific papers. Recent work has improved extraction accuracy by incorporating elementary layout information, for example, each token{'}s 2D position on the page, into language model pretraining. We introduce new methods that explicitly model VIsual LAyout (VILA) groups, that is, text lines or text blocks, to further improve performance. In our I-VILA approach, we show that simply inserting special tokens denoting layout group boundaries into model inputs can lead to a 1.9{\%} Macro F1 improvement in token classification. In the H-VILA approach, we show that hierarchical encoding of layout-groups can result in up to 47{\%} inference time reduction with less than 0.8{\%} Macro F1 loss. Unlike prior layout-aware approaches, our methods do not require expensive additional pretraining, only fine-tuning, which we show can reduce training cost by up to 95{\%}. Experiments are conducted on a newly curated evaluation suite, S2-VLUE, that unifies existing automatically labeled datasets and includes a new dataset of manual annotations covering diverse papers from 19 scientific disciplines. Pre-trained weights, benchmark datasets, and source code are available at \url{https://github.com/allenai/VILA}."",
}
@article{liu-prudhommeaux-2022-data,
    title = ""Data-driven Model Generalizability in Crosslinguistic Low-resource Morphological Segmentation"",
    author = ""Liu, Zoey  and
      Prud{'}hommeaux, Emily"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.23"",
    doi = ""10.1162/tacl_a_00467"",
    pages = ""393--413"",
    abstract = ""Common designs of model evaluation typically focus on monolingual settings, where different models are compared according to their performance on a single data set that is assumed to be representative of all possible data for the task at hand. While this may be reasonable for a large data set, this assumption is difficult to maintain in low-resource scenarios, where artifacts of the data collection can yield data sets that are outliers, potentially making conclusions about model performance coincidental. To address these concerns, we investigate model generalizability in crosslinguistic low-resource scenarios. Using morphological segmentation as the test case, we compare three broad classes of models with different parameterizations, taking data from 11 languages across 6 language families. In each experimental setting, we evaluate all models on a first data set, then examine their performance consistency when introducing new randomly sampled data sets with the same size and when applying the trained models to unseen test sets of varying sizes. The results demonstrate that the extent of model generalization depends on the characteristics of the data set, and does not necessarily rely heavily on the data set size. Among the characteristics that we studied, the ratio of morpheme overlap and that of the average number of morphemes per word between the training and test sets are the two most prominent factors. Our findings suggest that future work should adopt random sampling to construct data sets with different sizes in order to make more responsible claims about model evaluation."",
}
@article{ben-david-etal-2022-pada,
    title = ""{PADA}: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains"",
    author = ""Ben-David, Eyal  and
      Oved, Nadav  and
      Reichart, Roi"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.24"",
    doi = ""10.1162/tacl_a_00468"",
    pages = ""414--433"",
    abstract = ""Natural Language Processing algorithms have made incredible progress, but they still struggle when applied to out-of-distribution examples. We address a challenging and underexplored version of this domain adaptation problem, where an algorithm is trained on several source domains, and then applied to examples from unseen domains that are unknown at training time. Particularly, no examples, labeled or unlabeled, or any other knowledge about the target domain are available to the algorithm at training time. We present PADA: An example-based autoregressive Prompt learning algorithm for on-the-fly Any-Domain Adaptation, based on the T5 language model. Given a test example, PADA first generates a unique prompt for it and then, conditioned on this prompt, labels the example with respect to the NLP prediction task. PADA is trained to generate a prompt that is a token sequence of unrestricted length, consisting of Domain Related Features (DRFs) that characterize each of the source domains. Intuitively, the generated prompt is a unique signature that maps the test example to a semantic space spanned by the source domains. In experiments with 3 tasks (text classification and sequence tagging), for a total of 14 multi-source adaptation scenarios, PADA substantially outperforms strong baselines.1"",
}
@article{guan-etal-2022-lot,
    title = ""{LOT}: A Story-Centric Benchmark for Evaluating {C}hinese Long Text Understanding and Generation"",
    author = ""Guan, Jian  and
      Feng, Zhuoer  and
      Chen, Yamei  and
      He, Ruilin  and
      Mao, Xiaoxi  and
      Fan, Changjie  and
      Huang, Minlie"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.25"",
    doi = ""10.1162/tacl_a_00469"",
    pages = ""434--451"",
    abstract = ""Standard multi-task benchmarks are essential for developing pretraining models that can generalize to various downstream tasks. Existing benchmarks for natural language processing (NLP) usually focus only on understanding or generating short texts. However, long text modeling requires many distinct abilities in contrast to short texts, such as the modeling of long-range discourse and commonsense relations, and the coherence and controllability of generation. The lack of standardized benchmarks makes it difficult to assess these abilities of a model and fairly compare different models, especially Chinese models. Therefore, we propose a story-centric benchmark named LOT for evaluating Chinese long text modeling, which aggregates two understanding tasks and two generation tasks. We construct new datasets for these tasks based on human-written Chinese stories with hundreds of words. Furthermore, we release an encoder-decoder-based Chinese long text pretraining model named LongLM with up to 1 billion parameters. We pretrain LongLM on 120G Chinese novels with two generative tasks including text infilling and conditional continuation. Extensive experiments show that LongLM outperforms similar-sized pretraining models substantially on both the understanding and generation tasks in LOT."",
}
@article{naplava-etal-2022-czech,
    title = ""{C}zech Grammar Error Correction with a Large and Diverse Corpus"",
    author = ""N{\'a}plava, Jakub  and
      Straka, Milan  and
      Strakov{\'a}, Jana  and
      Rosen, Alexandr"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.26"",
    doi = ""10.1162/tacl_a_00470"",
    pages = ""452--467"",
    abstract = ""We introduce a large and diverse Czech corpus annotated for grammatical error correction (GEC) with the aim to contribute to the still scarce data resources in this domain for languages other than English. The Grammar Error Correction Corpus for Czech (GECCC) offers a variety of four domains, covering error distributions ranging from high error density essays written by non-native speakers, to website texts, where errors are expected to be much less common. We compare several Czech GEC systems, including several Transformer-based ones, setting a strong baseline to future research. Finally, we meta-evaluate common GEC metrics against human judgments on our data. We make the new Czech GEC corpus publicly available under the CC BY-SA 4.0 license at \url{http://hdl.handle.net/11234/1-4639}."",
}
@article{adlakha-etal-2022-topiocqa,
    title = ""{T}opi{OCQA}: Open-domain Conversational Question Answering with Topic Switching"",
    author = ""Adlakha, Vaibhav  and
      Dhuliawala, Shehzaad  and
      Suleman, Kaheer  and
      de Vries, Harm  and
      Reddy, Siva"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.27"",
    doi = ""10.1162/tacl_a_00471"",
    pages = ""468--483"",
    abstract = ""In a conversational question answering scenario, a questioner seeks to extract information about a topic through a series of interdependent questions and answers. As the conversation progresses, they may switch to related topics, a phenomenon commonly observed in information-seeking search sessions. However, current datasets for conversational question answering are limiting in two ways: 1) they do not contain topic switches; and 2) they assume the reference text for the conversation is given, that is, the setting is not open-domain. We introduce TopiOCQA (pronounced Tapioca), an open-domain conversational dataset with topic switches based on Wikipedia. TopiOCQA contains 3,920 conversations with information-seeking questions and free-form answers. On average, a conversation in our dataset spans 13 question-answer turns and involves four topics (documents). TopiOCQA poses a challenging test-bed for models, where efficient retrieval is required on multiple turns of the same conversation, in conjunction with constructing valid responses using conversational history. We evaluate several baselines, by combining state-of-the-art document retrieval methods with neural reader models. Our best model achieves F1 of 55.8, falling short of human performance by 14.2 points, indicating the difficulty of our dataset. Our dataset and code are available at \url{https://mcgill-nlp.github.io/topiocqa}."",
}
@article{sarwar-etal-2022-neighborhood,
    title = ""A Neighborhood Framework for Resource-Lean Content Flagging"",
    author = ""Sarwar, Sheikh Muhammad  and
      Zlatkova, Dimitrina  and
      Hardalov, Momchil  and
      Dinkov, Yoan  and
      Augenstein, Isabelle  and
      Nakov, Preslav"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.28"",
    doi = ""10.1162/tacl_a_00472"",
    pages = ""484--502"",
    abstract = ""We propose a novel framework for cross- lingual content flagging with limited target- language data, which significantly outperforms prior work in terms of predictive performance. The framework is based on a nearest-neighbor architecture. It is a modern instantiation of the vanilla k-nearest neighbor model, as we use Transformer representations in all its components. Our framework can adapt to new source- language instances, without the need to be retrained from scratch. Unlike prior work on neighborhood-based approaches, we encode the neighborhood information based on query{--} neighbor interactions. We propose two encoding schemes and we show their effectiveness using both qualitative and quantitative analysis. Our evaluation results on eight languages from two different datasets for abusive language detection show sizable improvements of up to 9.5 F1 points absolute (for Italian) over strong baselines. On average, we achieve 3.6 absolute F1 points of improvement for the three languages in the Jigsaw Multilingual dataset and 2.14 points for the WUL dataset."",
}
@article{geigle-etal-2022-retrieve,
    title = ""Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval"",
    author = ""Geigle, Gregor  and
      Pfeiffer, Jonas  and
      Reimers, Nils  and
      Vuli{\'c}, Ivan  and
      Gurevych, Iryna"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.29"",
    doi = ""10.1162/tacl_a_00473"",
    pages = ""503--521"",
    abstract = ""Current state-of-the-art approaches to cross- modal retrieval process text and visual input jointly, relying on Transformer-based architectures with cross-attention mechanisms that attend over all words and objects in an image. While offering unmatched retrieval performance, such models: 1) are typically pretrained from scratch and thus less scalable, 2) suffer from huge retrieval latency and inefficiency issues, which makes them impractical in realistic applications. To address these crucial gaps towards both improved and efficient cross- modal retrieval, we propose a novel fine-tuning framework that turns any pretrained text-image multi-modal model into an efficient retrieval model. The framework is based on a cooperative retrieve-and-rerank approach that combines: 1) twin networks (i.e., a bi-encoder) to separately encode all items of a corpus, enabling efficient initial retrieval, and 2) a cross-encoder component for a more nuanced (i.e., smarter) ranking of the retrieved small set of items. We also propose to jointly fine- tune the two components with shared weights, yielding a more parameter-efficient model. Our experiments on a series of standard cross-modal retrieval benchmarks in monolingual, multilingual, and zero-shot setups, demonstrate improved accuracy and huge efficiency benefits over the state-of-the-art cross- encoders.1"",
}
@article{goyal-etal-2022-flores,
    title = ""The {F}lores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"",
    author = ""Goyal, Naman  and
      Gao, Cynthia  and
      Chaudhary, Vishrav  and
      Chen, Peng-Jen  and
      Wenzek, Guillaume  and
      Ju, Da  and
      Krishnan, Sanjana  and
      Ranzato, Marc{'}Aurelio  and
      Guzm{\'a}n, Francisco  and
      Fan, Angela"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.30"",
    doi = ""10.1162/tacl_a_00474"",
    pages = ""522--538"",
    abstract = ""One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the Flores-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are fully aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond."",
}
@article{trivedi-etal-2022-musique,
    title = ""♫ {M}u{S}i{Q}ue: Multihop Questions via Single-hop Question Composition"",
    author = ""Trivedi, Harsh  and
      Balasubramanian, Niranjan  and
      Khot, Tushar  and
      Sabharwal, Ashish"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.31"",
    doi = ""10.1162/tacl_a_00475"",
    pages = ""539--554"",
    abstract = ""Multihop reasoning remains an elusive goal as existing multihop benchmarks are known to be largely solvable via shortcuts. Can we create a question answering (QA) dataset that, by construction, requires proper multihop reasoning? To this end, we introduce a bottom{--}up approach that systematically selects composable pairs of single-hop questions that are connected, that is, where one reasoning step critically relies on information from another. This bottom{--}up methodology lets us explore a vast space of questions and add stringent filters as well as other mechanisms targeting connected reasoning. It provides fine-grained control over the construction process and the properties of the resulting k-hop questions. We use this methodology to create MuSiQue-Ans, a new multihop QA dataset with 25K 2{--}4 hop questions. Relative to existing datasets, MuSiQue-Ans is more difficult overall (3{\mbox{$\times$}} increase in human{--}machine gap), and harder to cheat via disconnected reasoning (e.g., a single-hop model has a 30-point drop in F1). We further add unanswerable contrast questions to produce a more stringent dataset, MuSiQue-Full. We hope our datasets will help the NLP community develop models that perform genuine multihop reasoning.1"",
}
@article{liu-etal-2022-relational,
    title = ""Relational Memory-Augmented Language Models"",
    author = ""Liu, Qi  and
      Yogatama, Dani  and
      Blunsom, Phil"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.32"",
    doi = ""10.1162/tacl_a_00476"",
    pages = ""555--572"",
    abstract = ""We present a memory-augmented approach to condition an autoregressive language model on a knowledge graph. We represent the graph as a collection of relation triples and retrieve relevant relations for a given context to improve text generation. Experiments on WikiText-103, WMT19, and enwik8 English datasets demonstrate that our approach produces a better language model in terms of perplexity and bits per character. We also show that relational memory improves coherence, is complementary to token-based memory, and enables causal interventions. Our model provides a simple yet effective way to combine an autoregressive language model and a knowledge graph for more coherent and logical generation."",
}
@article{sun-etal-2022-sentence,
    title = ""Sentence Similarity Based on Contexts"",
    author = ""Sun, Xiaofei  and
      Meng, Yuxian  and
      Ao, Xiang  and
      Wu, Fei  and
      Zhang, Tianwei  and
      Li, Jiwei  and
      Fan, Chun"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.33"",
    doi = ""10.1162/tacl_a_00477"",
    pages = ""573--588"",
    abstract = ""Existing methods to measure sentence similarity are faced with two challenges: (1) labeled datasets are usually limited in size, making them insufficient to train supervised neural models; and (2) there is a training-test gap for unsupervised language modeling (LM) based models to compute semantic scores between sentences, since sentence-level semantics are not explicitly modeled at training. This results in inferior performances in this task. In this work, we propose a new framework to address these two issues. The proposed framework is based on the core idea that the meaning of a sentence should be defined by its contexts, and that sentence similarity can be measured by comparing the probabilities of generating two sentences given the same context. The proposed framework is able to generate high-quality, large-scale dataset with semantic similarity scores between two sentences in an unsupervised manner, with which the train-test gap can be largely bridged. Extensive experiments show that the proposed framework achieves significant performance boosts over existing baselines under both the supervised and unsupervised settings across different datasets."",
}
@article{chakrabarty-etal-2022-rocket,
    title = ""It{'}s not Rocket Science: Interpreting Figurative Language in Narratives"",
    author = ""Chakrabarty, Tuhin  and
      Choi, Yejin  and
      Shwartz, Vered"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.34"",
    doi = ""10.1162/tacl_a_00478"",
    pages = ""589--606"",
    abstract = ""Figurative language is ubiquitous in English. Yet, the vast majority of NLP research focuses on literal language. Existing text representations by design rely on compositionality, while figurative language is often non- compositional. In this paper, we study the interpretation of two non-compositional figurative languages (idioms and similes). We collected datasets of fictional narratives containing a figurative expression along with crowd-sourced plausible and implausible continuations relying on the correct interpretation of the expression. We then trained models to choose or generate the plausible continuation. Our experiments show that models based solely on pre-trained language models perform substantially worse than humans on these tasks. We additionally propose knowledge-enhanced models, adopting human strategies for interpreting figurative language types: inferring meaning from the context and relying on the constituent words{'} literal meanings. The knowledge-enhanced models improve the performance on both the discriminative and generative tasks, further bridging the gap from human performance."",
}
@article{li-etal-2022-ultra,
    title = ""Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference"",
    author = ""Li, Bangzheng  and
      Yin, Wenpeng  and
      Chen, Muhao"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.35"",
    doi = ""10.1162/tacl_a_00479"",
    pages = ""607--622"",
    abstract = ""The task of ultra-fine entity typing (UFET) seeks to predict diverse and free-form words or phrases that describe the appropriate types of entities mentioned in sentences. A key challenge for this task lies in the large number of types and the scarcity of annotated data per type. Existing systems formulate the task as a multi-way classification problem and train directly or distantly supervised classifiers. This causes two issues: (i) the classifiers do not capture the type semantics because types are often converted into indices; (ii) systems developed in this way are limited to predicting within a pre-defined type set, and often fall short of generalizing to types that are rarely seen or unseen in training. This work presents LITE🍻, a new approach that formulates entity typing as a natural language inference (NLI) problem, making use of (i) the indirect supervision from NLI to infer type information meaningfully represented as textual hypotheses and alleviate the data scarcity issue, as well as (ii) a learning-to-rank objective to avoid the pre-defining of a type set. Experiments show that, with limited training data, LITE obtains state-of-the-art performance on the UFET task. In addition, LITE demonstrates its strong generalizability by not only yielding best results on other fine-grained entity typing benchmarks, more importantly, a pre-trained LITE system works well on new data containing unseen types.1"",
}
@article{xu-lapata-2022-document,
    title = ""Document Summarization with Latent Queries"",
    author = ""Xu, Yumo  and
      Lapata, Mirella"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.36"",
    doi = ""10.1162/tacl_a_00480"",
    pages = ""623--638"",
    abstract = ""The availability of large-scale datasets has driven the development of neural models that create generic summaries for single or multiple documents. For query-focused summarization (QFS), labeled training data in the form of queries, documents, and summaries is not readily available. We provide a unified modeling framework for any kind of summarization, under the assumption that all summaries are a response to a query, which is observed in the case of QFS and latent in the case of generic summarization. We model queries as discrete latent variables over document tokens, and learn representations compatible with observed and unobserved query verbalizations. Our framework formulates summarization as a generative process, and jointly optimizes a latent query model and a conditional language model. Despite learning from generic summarization data only, our approach outperforms strong comparison systems across benchmarks, query types, document settings, and target domains.1"",
}
@article{morio-etal-2022-end,
    title = ""End-to-end Argument Mining with Cross-corpora Multi-task Learning"",
    author = ""Morio, Gaku  and
      Ozaki, Hiroaki  and
      Morishita, Terufumi  and
      Yanai, Kohsuke"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.37"",
    doi = ""10.1162/tacl_a_00481"",
    pages = ""639--658"",
    abstract = ""Mining an argument structure from text is an important step for tasks such as argument search and summarization. While studies on argument(ation) mining have proposed promising neural network models, they usually suffer from a shortage of training data. To address this issue, we expand the training data with various auxiliary argument mining corpora and propose an end-to-end cross-corpus training method called Multi-Task Argument Mining (MT-AM). To evaluate our approach, we conducted experiments for the main argument mining tasks on several well-established argument mining corpora. The results demonstrate that MT-AM generally outperformed the models trained on a single corpus. Also, the smaller the target corpus was, the better the MT-AM performed. Our extensive analyses suggest that the improvement of MT-AM depends on several factors of transferability among auxiliary and target corpora."",
}
@article{gupta-etal-2022-model,
    title = ""Is My Model Using the Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning"",
    author = ""Gupta, Vivek  and
      Bhat, Riyaz A.  and
      Ghosal, Atreya  and
      Shrivastava, Manish  and
      Singh, Maneesh  and
      Srikumar, Vivek"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.38"",
    doi = ""10.1162/tacl_a_00482"",
    pages = ""659--679"",
    abstract = ""Neural models command state-of-the-art performance across NLP tasks, including ones involving {``}reasoning{''}. Models claiming to reason about the evidence presented to them should attend to the correct parts of the input while avoiding spurious patterns therein, be self-consistent in their predictions across inputs, and be immune to biases derived from their pre-training in a nuanced, context- sensitive fashion. Do the prevalent *BERT- family of models do so? In this paper, we study this question using the problem of reasoning on tabular data. Tabular inputs are especially well-suited for the study{---}they admit systematic probes targeting the properties listed above. Our experiments demonstrate that a RoBERTa-based model, representative of the current state-of-the-art, fails at reasoning on the following counts: it (a) ignores relevant parts of the evidence, (b) is over- sensitive to annotation artifacts, and (c) relies on the knowledge encoded in the pre-trained language model rather than the evidence presented in its tabular inputs. Finally, through inoculation experiments, we show that fine- tuning the model on perturbed data does not help it overcome the above challenges."",
}
@article{wang-etal-2022-uncertainty,
    title = ""Uncertainty Estimation and Reduction of Pre-trained Models for Text Regression"",
    author = ""Wang, Yuxia  and
      Beck, Daniel  and
      Baldwin, Timothy  and
      Verspoor, Karin"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.39"",
    doi = ""10.1162/tacl_a_00483"",
    pages = ""680--696"",
    abstract = ""State-of-the-art classification and regression models are often not well calibrated, and cannot reliably provide uncertainty estimates, limiting their utility in safety-critical applications such as clinical decision-making. While recent work has focused on calibration of classifiers, there is almost no work in NLP on calibration in a regression setting. In this paper, we quantify the calibration of pre- trained language models for text regression, both intrinsically and extrinsically. We further apply uncertainty estimates to augment training data in low-resource domains. Our experiments on three regression tasks in both self-training and active-learning settings show that uncertainty estimation can be used to increase overall performance and enhance model generalization."",
}
@article{puduppully-etal-2022-data,
    title = ""Data-to-text Generation with Variational Sequential Planning"",
    author = ""Puduppully, Ratish  and
      Fu, Yao  and
      Lapata, Mirella"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.40"",
    doi = ""10.1162/tacl_a_00484"",
    pages = ""697--715"",
    abstract = ""We consider the task of data-to-text generation, which aims to create textual output from non-linguistic input. We focus on generating long-form text, that is, documents with multiple paragraphs, and propose a neural model enhanced with a planning component responsible for organizing high-level information in a coherent and meaningful way. We infer latent plans sequentially with a structured variational model, while interleaving the steps of planning and generation. Text is generated by conditioning on previous variational decisions and previously generated text. Experiments on two data-to-text benchmarks (RotoWire and MLB) show that our model outperforms strong baselines and is sample-efficient in the face of limited training data (e.g., a few hundred instances)."",
}
@article{schick-schutze-2022-true,
    title = ""True Few-Shot Learning with {P}rompts{---}{A} Real-World Perspective"",
    author = {Schick, Timo  and
      Sch{\""u}tze, Hinrich},
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.41"",
    doi = ""10.1162/tacl_a_00485"",
    pages = ""716--731"",
    abstract = ""Prompt-based approaches excel at few-shot learning. However, Perez et al. (2021) recently cast doubt on their performance as they had difficulty getting good results in a {``}true{''} few-shot setting in which prompts and hyperparameters cannot be tuned on a dev set. In view of this, we conduct an extensive study of Pet, a method that combines textual instructions with example-based finetuning. We show that, if correctly configured, Pet performs strongly in true few-shot settings without a dev set. Crucial for this strong performance is a number of design choices, including Pet{'}s ability to intelligently handle multiple prompts. We put our findings to a real-world test by running Pet on RAFT, a benchmark of tasks taken from realistic NLP applications for which no labeled dev or test sets are available. Pet achieves a new state of the art on RAFT and performs close to non-expert humans for 7 out of 11 tasks. These results demonstrate that prompt-based learners can successfully be applied in true few-shot settings and underpin our belief that learning from instructions will play an important role on the path towards human-like few-shot learning capabilities."",
}
@article{sridhar-etal-2022-heterogeneous,
    title = ""Heterogeneous Supervised Topic Models"",
    author = ""Sridhar, Dhanya  and
      Daum{\'e} III, Hal  and
      Blei, David"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.42"",
    doi = ""10.1162/tacl_a_00487"",
    pages = ""732--745"",
    abstract = ""Researchers in the social sciences are often interested in the relationship between text and an outcome of interest, where the goal is to both uncover latent patterns in the text and predict outcomes for unseen texts. To this end, this paper develops the heterogeneous supervised topic model (HSTM), a probabilistic approach to text analysis and prediction. HSTMs posit a joint model of text and outcomes to find heterogeneous patterns that help with both text analysis and prediction. The main benefit of HSTMs is that they capture heterogeneity in the relationship between text and the outcome across latent topics. To fit HSTMs, we develop a variational inference algorithm based on the auto-encoding variational Bayes framework. We study the performance of HSTMs on eight datasets and find that they consistently outperform related methods, including fine-tuned black-box models. Finally, we apply HSTMs to analyze news articles labeled with pro- or anti-tone. We find evidence of differing language used to signal a pro- and anti-tone."",
}
@article{atanasova-etal-2022-fact,
    title = ""Fact Checking with Insufficient Evidence"",
    author = ""Atanasova, Pepa  and
      Simonsen, Jakob Grue  and
      Lioma, Christina  and
      Augenstein, Isabelle"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.43"",
    doi = ""10.1162/tacl_a_00486"",
    pages = ""746--763"",
    abstract = ""Automating the fact checking (FC) process relies on information obtained from external sources. In this work, we posit that it is crucial for FC models to make veracity predictions only when there is sufficient evidence and otherwise indicate when it is not enough. To this end, we are the first to study what information FC models consider sufficient by introducing a novel task and advancing it with three main contributions. First, we conduct an in-depth empirical analysis of the task with a new fluency-preserving method for omitting information from the evidence at the constituent and sentence level. We identify when models consider the remaining evidence (in)sufficient for FC, based on three trained models with different Transformer architectures and three FC datasets. Second, we ask annotators whether the omitted evidence was important for FC, resulting in a novel diagnostic dataset, SufficientFacts1, for FC with omitted evidence. We find that models are least successful in detecting missing evidence when adverbial modifiers are omitted (21{\%} accuracy), whereas it is easiest for omitted date modifiers (63{\%} accuracy). Finally, we propose a novel data augmentation strategy for contrastive self-learning of missing evidence by employing the proposed omission method combined with tri-training. It improves performance for Evidence Sufficiency Prediction by up to 17.8 F1 score, which in turn improves FC performance by up to 2.6 F1 score."",
}
@article{elazar-etal-2022-text,
    title = ""Text-based {NP} Enrichment"",
    author = ""Elazar, Yanai  and
      Basmov, Victoria  and
      Goldberg, Yoav  and
      Tsarfaty, Reut"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.44"",
    doi = ""10.1162/tacl_a_00488"",
    pages = ""764--784"",
    abstract = ""Understanding the relations between entities denoted by NPs in a text is a critical part of human-like natural language understanding. However, only a fraction of such relations is covered by standard NLP tasks and benchmarks nowadays. In this work, we propose a novel task termed text-based NP enrichment (TNE), in which we aim to enrich each NP in a text with all the preposition-mediated relations{---}either explicit or implicit{---}that hold between it and other NPs in the text. The relations are represented as triplets, each denoted by two NPs related via a preposition. Humans recover such relations seamlessly, while current state-of-the-art models struggle with them due to the implicit nature of the problem. We build the first large-scale dataset for the problem, provide the formal framing and scope of annotation, analyze the data, and report the results of fine-tuned language models on the task, demonstrating the challenge it poses to current technology. A webpage with a data-exploration UI, a demo, and links to the code, models, and leaderboard, to foster further research into this challenging problem can be found at: yanaiela.github.io/TNE/."",
}
@article{lan-etal-2022-minimum,
    title = ""Minimum Description Length Recurrent Neural Networks"",
    author = ""Lan, Nur  and
      Geyer, Michal  and
      Chemla, Emmanuel  and
      Katzir, Roni"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.45"",
    doi = ""10.1162/tacl_a_00489"",
    pages = ""785--799"",
    abstract = ""We train neural networks to optimize a Minimum Description Length score, that is, to balance between the complexity of the network and its accuracy at a task. We show that networks optimizing this objective function master tasks involving memory challenges and go beyond context-free languages. These learners master languages such as anbn, anbncn, anb2n, anbmcn +m, and they perform addition. Moreover, they often do so with 100{\%} accuracy. The networks are small, and their inner workings are transparent. We thus provide formal proofs that their perfect accuracy holds not only on a given test set, but for any input sequence. To our knowledge, no other connectionist model has been shown to capture the underlying grammars for these languages in full generality."",
}
@article{hao-etal-2022-formal,
    title = ""Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity"",
    author = ""Hao, Yiding  and
      Angluin, Dana  and
      Frank, Robert"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.46"",
    doi = ""10.1162/tacl_a_00490"",
    pages = ""800--810"",
    abstract = ""This paper analyzes three formal models of Transformer encoders that differ in the form of their self-attention mechanism: unique hard attention (UHAT); generalized unique hard attention (GUHAT), which generalizes UHAT; and averaging hard attention (AHAT). We show that UHAT and GUHAT Transformers, viewed as string acceptors, can only recognize formal languages in the complexity class AC0, the class of languages recognizable by families of Boolean circuits of constant depth and polynomial size. This upper bound subsumes Hahn{'}s (2020) results that GUHAT cannot recognize the DYCK languages or the PARITY language, since those languages are outside AC0 (Furst et al., 1984). In contrast, the non-AC0 languages MAJORITY and DYCK-1 are recognizable by AHAT networks, implying that AHAT can recognize languages that UHAT and GUHAT cannot."",
}
@article{freitag-etal-2022-high,
    title = ""High Quality Rather than High Model Probability: Minimum {B}ayes Risk Decoding with Neural Metrics"",
    author = ""Freitag, Markus  and
      Grangier, David  and
      Tan, Qijun  and
      Liang, Bowen"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.47"",
    doi = ""10.1162/tacl_a_00491"",
    pages = ""811--825"",
    abstract = ""In Neural Machine Translation, it is typically assumed that the sentence with the highest estimated probability should also be the translation with the highest quality as measured by humans. In this work, we question this assumption and show that model estimates and translation quality only vaguely correlate. We apply Minimum Bayes Risk (MBR) decoding on unbiased samples to optimize diverse automated metrics of translation quality as an alternative inference strategy to beam search. Instead of targeting the hypotheses with the highest model probability, MBR decoding extracts the hypotheses with the highest estimated quality. Our experiments show that the combination of a neural translation model with a neural reference-based metric, Bleurt, results in significant improvement in human evaluations. This improvement is obtained with translations different from classical beam-search output: These translations have much lower model likelihood and are less favored by surface metrics like Bleu."",
}
@article{he-etal-2022-generate,
    title = ""Generate, Annotate, and Learn: {NLP} with Synthetic Text"",
    author = ""He, Xuanli  and
      Nassar, Islam  and
      Kiros, Jamie  and
      Haffari, Gholamreza  and
      Norouzi, Mohammad"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.48"",
    doi = ""10.1162/tacl_a_00492"",
    pages = ""826--842"",
    abstract = ""This paper studies the use of language models as a source of synthetic unlabeled text for NLP. We formulate a general framework called {``}generate, annotate, and learn (GAL){''} to take advantage of synthetic text within knowledge distillation, self-training, and few-shot learning applications. To generate high-quality task-specific text, we either fine-tune LMs on inputs from the task of interest, or prompt large LMs with few examples. We use the best available classifier to annotate synthetic text with soft pseudo labels for knowledge distillation and self-training, and use LMs to obtain hard labels for few-shot learning. We train new supervised models on the combination of labeled and pseudo-labeled data, which results in significant gains across several applications. We investigate key components of GAL and present theoretical and empirical arguments against the use of class-conditional LMs to generate synthetic labeled text instead of unlabeled text. GAL achieves new state-of-the-art knowledge distillation results for 6-layer transformers on the GLUE leaderboard."",
}
@article{merrill-etal-2022-saturated,
    title = ""Saturated Transformers are Constant-Depth Threshold Circuits"",
    author = ""Merrill, William  and
      Sabharwal, Ashish  and
      Smith, Noah A."",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.49"",
    doi = ""10.1162/tacl_a_00493"",
    pages = ""843--856"",
    abstract = ""Transformers have become a standard neural network architecture for many NLP problems, motivating theoretical analysis of their power in terms of formal languages. Recent work has shown that transformers with hard attention are quite limited in power (Hahn, 2020), as they can be simulated by constant-depth AND/OR circuits (Hao et al., 2022). However, hard attention is a strong assumption, which may complicate the relevance of these results in practice. In this work, we analyze the circuit complexity of transformers with saturated attention: a generalization of hard attention that more closely captures the attention patterns learnable in practical transformers. We first show that saturated transformers transcend the known limitations of hard-attention transformers. We then prove saturated transformers with floating-point values can be simulated by constant-depth threshold circuits, giving the class TC0 as an upper bound on the formal languages they recognize."",
}
@article{mielke-etal-2022-reducing,
    title = ""Reducing Conversational Agents{'} Overconfidence Through Linguistic Calibration"",
    author = ""Mielke, Sabrina J.  and
      Szlam, Arthur  and
      Dinan, Emily  and
      Boureau, Y-Lan"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.50"",
    doi = ""10.1162/tacl_a_00494"",
    pages = ""857--872"",
    abstract = ""While improving neural dialogue agents{'} factual accuracy is the object of much research, another important aspect of communication, less studied in the setting of neural dialogue, is transparency about ignorance. In this work, we analyze to what extent state-of-the-art chit-chat models are linguistically calibrated in the sense that their verbalized expression of doubt (or confidence) matches the likelihood that the model{'}s responses are factually incorrect (or correct). We find that these models are poorly calibrated, yet we show that likelihood of correctness can accurately be predicted. By incorporating such metacognitive features into the training of a controllable generation model, we obtain a dialogue agent with greatly improved linguistic calibration."",
}
@article{osborne-etal-2022-survey,
    title = ""A Survey of Text Games for Reinforcement Learning Informed by Natural Language"",
    author = ""Osborne, Philip  and
      N{\~o}mm, Heido  and
      Freitas, Andr{\'e}"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.51"",
    doi = ""10.1162/tacl_a_00495"",
    pages = ""873--887"",
    abstract = ""Reinforcement Learning has shown success in a number of complex virtual environments. However, many challenges still exist towards solving problems with natural language as a core component. Interactive Fiction Games (or Text Games) are one such problem type that offer a set of safe, partially observable environments where natural language is required as part of the Reinforcement Learning solution. Therefore, this survey{'}s aim is to assist in the development of new Text Game problem settings and solutions for Reinforcement Learning informed by natural language. Specifically, this survey: 1) introduces the challenges in Text Game Reinforcement Learning problems, 2) outlines the generation tools for rendering Text Games and the subsequent environments generated, and 3) compares the agent architectures currently applied to provide a systematic review of benchmark methodologies and opportunities for future researchers."",
}
@article{dary-etal-2022-dependency,
    title = ""Dependency Parsing with Backtracking using Deep Reinforcement Learning"",
    author = ""Dary, Franck  and
      Petit, Maxime  and
      Nasr, Alexis"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.52"",
    doi = ""10.1162/tacl_a_00496"",
    pages = ""888--903"",
    abstract = ""Greedy algorithms for NLP such as transition-based parsing are prone to error propagation. One way to overcome this problem is to allow the algorithm to backtrack and explore an alternative solution in cases where new evidence contradicts the solution explored so far. In order to implement such a behavior, we use reinforcement learning and let the algorithm backtrack in cases where such an action gets a better reward than continuing to explore the current solution. We test this idea on both POS tagging and dependency parsing and show that backtracking is an effective means to fight against error propagation."",
}
@article{agarwal-nenkova-2022-temporal,
    title = ""Temporal Effects on Pre-trained Models for Language Processing Tasks"",
    author = ""Agarwal, Oshin  and
      Nenkova, Ani"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.53"",
    doi = ""10.1162/tacl_a_00497"",
    pages = ""904--921"",
    abstract = ""Keeping the performance of language technologies optimal as time passes is of great practical interest. We study temporal effects on model performance on downstream language tasks, establishing a nuanced terminology for such discussion and identifying factors essential to conduct a robust study. We present experiments for several tasks in English where the label correctness is not dependent on time and demonstrate the importance of distinguishing between temporal model deterioration and temporal domain adaptation for systems using pre-trained representations. We find that, depending on the task, temporal model deterioration is not necessarily a concern. Temporal domain adaptation, however, is beneficial in all cases, with better performance for a given time period possible when the system is trained on temporally more recent data. Therefore, we also examine the efficacy of two approaches for temporal domain adaptation without human annotations on new data. Self-labeling shows consistent improvement and notably, for named entity recognition, leads to better temporal adaptation than even human annotations."",
}
@article{nikolaus-etal-2022-learning,
    title = ""Learning {E}nglish with {P}eppa {P}ig"",
    author = ""Nikolaus, Mitja  and
      Alishahi, Afra  and
      Chrupa{\l}a, Grzegorz"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.54"",
    doi = ""10.1162/tacl_a_00498"",
    pages = ""922--936"",
    abstract = ""Recent computational models of the acquisition of spoken language via grounding in perception exploit associations between spoken and visual modalities and learn to represent speech and visual data in a joint vector space. A major unresolved issue from the point of ecological validity is the training data, typically consisting of images or videos paired with spoken descriptions of what is depicted. Such a setup guarantees an unrealistically strong correlation between speech and the visual data. In the real world the coupling between the linguistic and the visual modality is loose, and often confounded by correlations with non-semantic aspects of the speech signal. Here we address this shortcoming by using a dataset based on the children{'}s cartoon Peppa Pig. We train a simple bi-modal architecture on the portion of the data consisting of dialog between characters, and evaluate on segments containing descriptive narrations. Despite the weak and confounded signal in this training data, our model succeeds at learning aspects of the visual semantics of spoken language."",
}
@article{cui-etal-2022-compositional,
    title = ""Compositional Generalization in Multilingual Semantic Parsing over {W}ikidata"",
    author = ""Cui, Ruixiang  and
      Aralikatte, Rahul  and
      Lent, Heather  and
      Hershcovich, Daniel"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.55"",
    doi = ""10.1162/tacl_a_00499"",
    pages = ""937--955"",
    abstract = ""Semantic parsing (SP) allows humans to leverage vast knowledge resources through natural interaction. However, parsers are mostly designed for and evaluated on English resources, such as CFQ (Keysers et al., 2020), the current standard benchmark based on English data generated from grammar rules and oriented towards Freebase, an outdated knowledge base. We propose a method for creating a multilingual, parallel dataset of question-query pairs, grounded in Wikidata. We introduce such a dataset, which we call Multilingual Compositional Wikidata Questions (MCWQ), and use it to analyze the compositional generalization of semantic parsers in Hebrew, Kannada, Chinese, and English. While within- language generalization is comparable across languages, experiments on zero-shot cross- lingual transfer demonstrate that cross-lingual compositional generalization fails, even with state-of-the-art pretrained multilingual encoders. Furthermore, our methodology, dataset, and results will facilitate future research on SP in more realistic and diverse settings than has been possible with existing resources."",
}
@article{naik-etal-2022-adapting,
    title = ""Adapting to the Long Tail: A Meta-Analysis of Transfer Learning Research for Language Understanding Tasks"",
    author = ""Naik, Aakanksha  and
      Lehman, Jill  and
      Ros{\'e}, Carolyn"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.56"",
    doi = ""10.1162/tacl_a_00500"",
    pages = ""956--980"",
    abstract = ""Natural language understanding (NLU) has made massive progress driven by large benchmarks, but benchmarks often leave a long tail of infrequent phenomena underrepresented. We reflect on the question: Have transfer learning methods sufficiently addressed the poor performance of benchmark-trained models on the long tail? We conceptualize the long tail using macro-level dimensions (underrepresented genres, topics, etc.), and perform a qualitative meta-analysis of 100 representative papers on transfer learning research for NLU. Our analysis asks three questions: (i) Which long tail dimensions do transfer learning studies target? (ii) Which properties of adaptation methods help improve performance on the long tail? (iii) Which methodological gaps have greatest negative impact on long tail performance? Our answers highlight major avenues for future research in transfer learning for the long tail. Lastly, using our meta-analysis framework, we perform a case study comparing the performance of various adaptation methods on clinical narratives, which provides interesting insights that may enable us to make progress along these future avenues."",
}
@article{mickus-etal-2022-dissect,
    title = ""How to Dissect a {M}uppet: The Structure of Transformer Embedding Spaces"",
    author = ""Mickus, Timothee  and
      Paperno, Denis  and
      Constant, Mathieu"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.57"",
    doi = ""10.1162/tacl_a_00501"",
    pages = ""981--996"",
    abstract = ""Pretrained embeddings based on the Transformer architecture have taken the NLP community by storm. We show that they can mathematically be reframed as a sum of vector factors and showcase how to use this reframing to study the impact of each component. We provide evidence that multi-head attentions and feed-forwards are not equally useful in all downstream applications, as well as a quantitative overview of the effects of finetuning on the overall embedding space. This approach allows us to draw connections to a wide range of previous studies, from vector space anisotropy to attention weights."",
}
@article{wiher-etal-2022-decoding,
    title = ""On Decoding Strategies for Neural Text Generators"",
    author = ""Wiher, Gian  and
      Meister, Clara  and
      Cotterell, Ryan"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.58"",
    doi = ""10.1162/tacl_a_00502"",
    pages = ""997--1012"",
    abstract = ""When generating text from probabilistic models, the chosen decoding strategy has a profound effect on the resulting text. Yet the properties elicited by various decoding strategies do not always transfer across natural language generation tasks. For example, while mode-seeking methods like beam search perform remarkably well for machine translation, they have been observed to lead to incoherent and repetitive text in story generation. Despite such observations, the effectiveness of decoding strategies is often assessed on only a single task. This work{---}in contrast{---}provides a comprehensive analysis of the interaction between language generation tasks and decoding strategies. Specifically, we measure changes in attributes of generated text as a function of both decoding strategy and task using human and automatic evaluation. Our results reveal both previously observed and novel findings. For example, the nature of the diversity{--}quality trade-off in language generation is very task-specific; the length bias often attributed to beam search is not constant across tasks. \url{https://github.com/gianwiher/decoding-NLG}"",
}
@article{krishna-etal-2022-proofver,
    title = ""{P}roo{FV}er: Natural Logic Theorem Proving for Fact Verification"",
    author = ""Krishna, Amrith  and
      Riedel, Sebastian  and
      Vlachos, Andreas"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.59"",
    doi = ""10.1162/tacl_a_00503"",
    pages = ""1013--1030"",
    abstract = ""Fact verification systems typically rely on neural network classifiers for veracity prediction, which lack explainability. This paper proposes ProoFVer, which uses a seq2seq model to generate natural logic-based inferences as proofs. These proofs consist of lexical mutations between spans in the claim and the evidence retrieved, each marked with a natural logic operator. Claim veracity is determined solely based on the sequence of these operators. Hence, these proofs are faithful explanations, and this makes ProoFVer faithful by construction. Currently, ProoFVer has the highest label accuracy and the second best score in the FEVER leaderboard. Furthermore, it improves by 13.21{\%} points over the next best model on a dataset with counterfactual instances, demonstrating its robustness. As explanations, the proofs show better overlap with human rationales than attention-based highlights and the proofs help humans predict model decisions correctly more often than using the evidence directly.1"",
}
@article{sinclair-etal-2022-structural,
    title = ""Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations"",
    author = ""Sinclair, Arabella  and
      Jumelet, Jaap  and
      Zuidema, Willem  and
      Fern{\'a}ndez, Raquel"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.60"",
    doi = ""10.1162/tacl_a_00504"",
    pages = ""1031--1050"",
    abstract = ""We investigate the extent to which modern neural language models are susceptible to structural priming, the phenomenon whereby the structure of a sentence makes the same structure more probable in a follow-up sentence. We explore how priming can be used to study the potential of these models to learn abstract structural information, which is a prerequisite for good performance on tasks that require natural language understanding skills. We introduce a novel metric and release Prime-LM, a large corpus where we control for various linguistic factors that interact with priming strength. We find that Transformer models indeed show evidence of structural priming, but also that the generalizations they learned are to some extent modulated by semantic information. Our experiments also show that the representations acquired by the models may not only encode abstract sequential structure but involve certain level of hierarchical syntactic information. More generally, our study shows that the priming paradigm is a useful, additional tool for gaining insights into the capacities of language models and opens the door to future priming-based investigations that probe the model{'}s internal states.1"",
}
@article{algayres-etal-2022-dp,
    title = ""{DP}-Parse: Finding Word Boundaries from Raw Speech with an Instance Lexicon"",
    author = ""Algayres, Robin  and
      Ricoul, Tristan  and
      Karadayi, Julien  and
      Lauren{\c{c}}on, Hugo  and
      Zaiem, Salah  and
      Mohamed, Abdelrahman  and
      Sagot, Beno{\^\i}t  and
      Dupoux, Emmanuel"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.61"",
    doi = ""10.1162/tacl_a_00505"",
    pages = ""1051--1065"",
    abstract = ""Finding word boundaries in continuous speech is challenging as there is little or no equivalent of a {`}space{'} delimiter between words. Popular Bayesian non-parametric models for text segmentation (Goldwater et al., 2006, 2009) use a Dirichlet process to jointly segment sentences and build a lexicon of word types. We introduce DP-Parse, which uses similar principles but only relies on an instance lexicon of word tokens, avoiding the clustering errors that arise with a lexicon of word types. On the Zero Resource Speech Benchmark 2017, our model sets a new speech segmentation state-of-the-art in 5 languages. The algorithm monotonically improves with better input representations, achieving yet higher scores when fed with weakly supervised inputs. Despite lacking a type lexicon, DP-Parse can be pipelined to a language model and learn semantic and syntactic representations as assessed by a new spoken word embedding benchmark. 1"",
}
@article{dziri-etal-2022-evaluating,
    title = ""Evaluating Attribution in Dialogue Systems: The {BEGIN} Benchmark"",
    author = ""Dziri, Nouha  and
      Rashkin, Hannah  and
      Linzen, Tal  and
      Reitter, David"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.62"",
    doi = ""10.1162/tacl_a_00506"",
    pages = ""1066--1083"",
    abstract = ""Knowledge-grounded dialogue systems powered by large language models often generate responses that, while fluent, are not attributable to a relevant source of information. Progress towards models that do not exhibit this issue requires evaluation metrics that can quantify its prevalence. To this end, we introduce the Benchmark for Evaluation of Grounded INteraction (Begin), comprising 12k dialogue turns generated by neural dialogue systems trained on three knowledge-grounded dialogue corpora. We collect human annotations assessing the extent to which the models{'} responses can be attributed to the given background information. We then use Begin to analyze eight evaluation metrics. We find that these metrics rely on spurious correlations, do not reliably distinguish attributable abstractive responses from unattributable ones, and perform substantially worse when the knowledge source is longer. Our findings underscore the need for more sophisticated and robust evaluation metrics for knowledge-grounded dialogue. We make Begin publicly available at \url{https://github.com/google/BEGIN-dataset}."",
}
@article{sicilia-etal-2022-modeling,
    title = ""Modeling Non-Cooperative Dialogue: Theoretical and Empirical Insights"",
    author = ""Sicilia, Anthony  and
      Maidment, Tristan  and
      Healy, Pat  and
      Alikhani, Malihe"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.63"",
    doi = ""10.1162/tacl_a_00507"",
    pages = ""1084--1102"",
    abstract = ""Investigating cooperativity of interlocutors is central in studying pragmatics of dialogue. Models of conversation that only assume cooperative agents fail to explain the dynamics of strategic conversations. Thus, we investigate the ability of agents to identify non-cooperative interlocutors while completing a concurrent visual-dialogue task. Within this novel setting, we study the optimality of communication strategies for achieving this multi-task objective. We use the tools of learning theory to develop a theoretical model for identifying non-cooperative interlocutors and apply this theory to analyze different communication strategies. We also introduce a corpus of non-cooperative conversations about images in the GuessWhat?! dataset proposed by De Vries et al. (2017). We use reinforcement learning to implement multiple communication strategies in this context and find that empirical results validate our theory."",
}
@article{thayaparan-etal-2022-diff,
    title = ""Diff-Explainer: Differentiable Convex Optimization for Explainable Multi-hop Inference"",
    author = ""Thayaparan, Mokanarangan  and
      Valentino, Marco  and
      Ferreira, Deborah  and
      Rozanova, Julia  and
      Freitas, Andr{\'e}"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.64"",
    doi = ""10.1162/tacl_a_00508"",
    pages = ""1103--1119"",
    abstract = ""This paper presents Diff-Explainer, the first hybrid framework for explainable multi-hop inference that integrates explicit constraints with neural architectures through differentiable convex optimization. Specifically, Diff- Explainer allows for the fine-tuning of neural representations within a constrained optimization framework to answer and explain multi-hop questions in natural language. To demonstrate the efficacy of the hybrid framework, we combine existing ILP-based solvers for multi-hop Question Answering (QA) with Transformer-based representations. An extensive empirical evaluation on scientific and commonsense QA tasks demonstrates that the integration of explicit constraints in a end-to-end differentiable framework can significantly improve the performance of non- differentiable ILP solvers (8.91{\%}{--}13.3{\%}). Moreover, additional analysis reveals that Diff-Explainer is able to achieve strong performance when compared to standalone Transformers and previous multi-hop approaches while still providing structured explanations in support of its predictions."",
}
@article{zeng-bhat-2022-getting,
    title = ""Getting {BART} to Ride the Idiomatic Train: Learning to Represent Idiomatic Expressions"",
    author = ""Zeng, Ziheng  and
      Bhat, Suma"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.65"",
    doi = ""10.1162/tacl_a_00510"",
    pages = ""1120--1137"",
    abstract = ""Idiomatic expressions (IEs), characterized by their non-compositionality, are an important part of natural language. They have been a classical challenge to NLP, including pre-trained language models that drive today{'}s state-of-the-art. Prior work has identified deficiencies in their contextualized representation stemming from the underlying compositional paradigm of representation. In this work, we take a first-principles approach to build idiomaticity into BART using an adapter as a lightweight non-compositional language expert trained on idiomatic sentences. The improved capability over baselines (e.g., BART) is seen via intrinsic and extrinsic methods, where idiom embeddings score 0.19 points higher in homogeneity score for embedding clustering, and up to 25{\%} higher sequence accuracy on the idiom processing tasks of IE sense disambiguation and span detection."",
}
@article{feder-etal-2022-causal,
    title = ""Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond"",
    author = ""Feder, Amir  and
      Keith, Katherine A.  and
      Manzoor, Emaad  and
      Pryzant, Reid  and
      Sridhar, Dhanya  and
      Wood-Doughty, Zach  and
      Eisenstein, Jacob  and
      Grimmer, Justin  and
      Reichart, Roi  and
      Roberts, Margaret E.  and
      Stewart, Brandon M.  and
      Veitch, Victor  and
      Yang, Diyi"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.66"",
    doi = ""10.1162/tacl_a_00511"",
    pages = ""1138--1158"",
    abstract = ""A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the challenges and opportunities in the application of causal inference to the textual domain, with its unique properties. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects with text, encompassing settings where text is used as an outcome, treatment, or to address confounding. In addition, we explore potential uses of causal inference to improve the robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the NLP community.1"",
}
@article{chowdhury-chaturvedi-2022-learning,
    title = ""Learning Fair Representations via Rate-Distortion Maximization"",
    author = ""Chowdhury, Somnath Basu Roy  and
      Chaturvedi, Snigdha"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.67"",
    doi = ""10.1162/tacl_a_00512"",
    pages = ""1159--1174"",
    abstract = ""Text representations learned by machine learning models often encode undesirable demographic information of the user. Predictive models based on these representations can rely on such information, resulting in biased decisions. We present a novel debiasing technique, Fairness-aware Rate Maximization (FaRM), that removes protected information by making representations of instances belonging to the same protected attribute class uncorrelated, using the rate-distortion function. FaRM is able to debias representations with or without a target task at hand. FaRM can also be adapted to remove information about multiple protected attributes simultaneously. Empirical evaluations show that FaRM achieves state-of-the-art performance on several datasets, and learned representations leak significantly less protected attribute information against an attack by a non-linear probing network."",
}
@article{heck-etal-2022-robust,
    title = ""Robust Dialogue State Tracking with Weak Supervision and Sparse Data"",
    author = ""Heck, Michael  and
      Lubis, Nurul  and
      van Niekerk, Carel  and
      Feng, Shutong  and
      Geishauser, Christian  and
      Lin, Hsien-Chin  and
      Ga{\v{s}}i{\'c}, Milica"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.68"",
    doi = ""10.1162/tacl_a_00513"",
    pages = ""1175--1192"",
    abstract = ""Generalizing dialogue state tracking (DST) to new data is especially challenging due to the strong reliance on abundant and fine-grained supervision during training. Sample sparsity, distributional shift, and the occurrence of new concepts and topics frequently lead to severe performance degradation during inference. In this paper we propose a training strategy to build extractive DST models without the need for fine-grained manual span labels. Two novel input-level dropout methods mitigate the negative impact of sample sparsity. We propose a new model architecture with a unified encoder that supports value as well as slot independence by leveraging the attention mechanism. We combine the strengths of triple copy strategy DST and value matching to benefit from complementary predictions without violating the principle of ontology independence. Our experiments demonstrate that an extractive DST model can be trained without manual span labels. Our architecture and training strategies improve robustness towards sample sparsity, new concepts, and topics, leading to state-of-the-art performance on a range of benchmarks. We further highlight our model{'}s ability to effectively learn from non-dialogue data."",
}
@article{lovering-pavlick-2022-unit,
    title = ""Unit Testing for Concepts in Neural Networks"",
    author = ""Lovering, Charles  and
      Pavlick, Ellie"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.69"",
    doi = ""10.1162/tacl_a_00514"",
    pages = ""1193--1208"",
    abstract = ""Many complex problems are naturally understood in terms of symbolic concepts. For example, our concept of {``}cat{''} is related to our concepts of {``}ears{''} and {``}whiskers{''} in a non-arbitrary way. Fodor (1998) proposes one theory of concepts, which emphasizes symbolic representations related via constituency structures. Whether neural networks are consistent with such a theory is open for debate. We propose unit tests for evaluating whether a system{'}s behavior is consistent with several key aspects of Fodor{'}s criteria. Using a simple visual concept learning task, we evaluate several modern neural architectures against this specification. We find that models succeed on tests of groundedness, modularity, and reusability of concepts, but that important questions about causality remain open. Resolving these will require new methods for analyzing models{'} internal states."",
}
@article{rotman-reichart-2022-multi,
    title = ""Multi-task Active Learning for Pre-trained Transformer-based Models"",
    author = ""Rotman, Guy  and
      Reichart, Roi"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.70"",
    doi = ""10.1162/tacl_a_00515"",
    pages = ""1209--1228"",
    abstract = ""Multi-task learning, in which several tasks are jointly learned by a single model, allows NLP models to share information from multiple annotations and may facilitate better predictions when the tasks are inter-related. This technique, however, requires annotating the same text with multiple annotation schemes, which may be costly and laborious. Active learning (AL) has been demonstrated to optimize annotation processes by iteratively selecting unlabeled examples whose annotation is most valuable for the NLP model. Yet, multi-task active learning (MT-AL) has not been applied to state-of-the-art pre-trained Transformer-based NLP models. This paper aims to close this gap. We explore various multi-task selection criteria in three realistic multi-task scenarios, reflecting different relations between the participating tasks, and demonstrate the effectiveness of multi-task compared to single-task selection. Our results suggest that MT-AL can be effectively used in order to minimize annotation efforts for multi-task NLP models.1"",
}
@article{bilal-etal-2022-template,
    title = ""Template-based Abstractive Microblog Opinion Summarization"",
    author = ""Bilal, Iman Munire  and
      Wang, Bo  and
      Tsakalidis, Adam  and
      Nguyen, Dong  and
      Procter, Rob  and
      Liakata, Maria"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.71"",
    doi = ""10.1162/tacl_a_00516"",
    pages = ""1229--1248"",
    abstract = ""We introduce the task of microblog opinion summarization (MOS) and share a dataset of 3100 gold-standard opinion summaries to facilitate research in this domain. The dataset contains summaries of tweets spanning a 2-year period and covers more topics than any other public Twitter summarization dataset. Summaries are abstractive in nature and have been created by journalists skilled in summarizing news articles following a template separating factual information (main story) from author opinions. Our method differs from previous work on generating gold-standard summaries from social media, which usually involves selecting representative posts and thus favors extractive summarization models. To showcase the dataset{'}s utility and challenges, we benchmark a range of abstractive and extractive state-of-the-art summarization models and achieve good performance, with the former outperforming the latter. We also show that fine-tuning is necessary to improve performance and investigate the benefits of using different sample sizes."",
}
@article{hou-etal-2022-meta,
    title = ""Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation"",
    author = ""Hou, Zejiang  and
      Salazar, Julian  and
      Polovets, George"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.72"",
    doi = ""10.1162/tacl_a_00517"",
    pages = ""1249--1265"",
    abstract = ""Large pretrained language models (PLMs) are often domain- or task-adapted via finetuning or prompting. Finetuning requires modifying all of the parameters and having enough data to avoid overfitting while prompting requires no training and few examples but limits performance. Instead, we prepare PLMs for data- and parameter-efficient adaptation by learning to learn the difference between general and adapted PLMs. This difference is expressed in terms of model weights and sublayer structure through our proposed dynamic low-rank reparameterization and learned architecture controller. Experiments on few-shot dialogue completion, low-resource abstractive summarization, and multi-domain language modeling show improvements in adaptation time and performance over direct finetuning or preparation via domain-adaptive pretraining. Ablations show our task-adaptive reparameterization (TARP) and model search (TAMS) components individually improve on other parameter-efficient transfer like adapters and structure-learning methods like learned sparsification."",
}
@article{yanaka-mineshima-2022-compositional,
    title = ""Compositional Evaluation on {J}apanese Textual Entailment and Similarity"",
    author = ""Yanaka, Hitomi  and
      Mineshima, Koji"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.73"",
    doi = ""10.1162/tacl_a_00518"",
    pages = ""1266--1284"",
    abstract = ""Natural Language Inference (NLI) and Semantic Textual Similarity (STS) are widely used benchmark tasks for compositional evaluation of pre-trained language models. Despite growing interest in linguistic universals, most NLI/STS studies have focused almost exclusively on English. In particular, there are no available multilingual NLI/STS datasets in Japanese, which is typologically different from English and can shed light on the currently controversial behavior of language models in matters such as sensitivity to word order and case particles. Against this background, we introduce JSICK, a Japanese NLI/STS dataset that was manually translated from the English dataset SICK. We also present a stress-test dataset for compositional inference, created by transforming syntactic structures of sentences in JSICK to investigate whether language models are sensitive to word order and case particles. We conduct baseline experiments on different pre-trained language models and compare the performance of multilingual models when applied to Japanese and other languages. The results of the stress-test experiments suggest that the current pre-trained language models are insensitive to word order and case marking."",
}
@article{sajjad-etal-2022-neuron,
    title = ""Neuron-level Interpretation of Deep {NLP} Models: A Survey"",
    author = ""Sajjad, Hassan  and
      Durrani, Nadir  and
      Dalvi, Fahim"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.74"",
    doi = ""10.1162/tacl_a_00519"",
    pages = ""1285--1303"",
    abstract = ""The proliferation of Deep Neural Networks in various domains has seen an increased need for interpretability of these models. Preliminary work done along this line, and papers that surveyed such, are focused on high-level representation analysis. However, a recent branch of work has concentrated on interpretability at a more granular level of analyzing neurons within these models. In this paper, we survey the work done on neuron analysis including: i) methods to discover and understand neurons in a network; ii) evaluation methods; iii) major findings including cross architectural comparisons that neuron analysis has unraveled; iv) applications of neuron probing such as: controlling the model, domain adaptation, and so forth; and v) a discussion on open issues and future research directions."",
}
@article{wang-etal-2022-survey,
    title = ""A Survey on Cross-Lingual Summarization"",
    author = ""Wang, Jiaan  and
      Meng, Fandong  and
      Zheng, Duo  and
      Liang, Yunlong  and
      Li, Zhixu  and
      Qu, Jianfeng  and
      Zhou, Jie"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.75"",
    doi = ""10.1162/tacl_a_00520"",
    pages = ""1304--1323"",
    abstract = ""Cross-lingual summarization is the task of generating a summary in one language (e.g., English) for the given document(s) in a different language (e.g., Chinese). Under the globalization background, this task has attracted increasing attention of the computational linguistics community. Nevertheless, there still remains a lack of comprehensive review for this task. Therefore, we present the first systematic critical review on the datasets, approaches, and challenges in this field. Specifically, we carefully organize existing datasets and approaches according to different construction methods and solution paradigms, respectively. For each type of dataset or approach, we thoroughly introduce and summarize previous efforts and further compare them with each other to provide deeper analyses. In the end, we also discuss promising directions and offer our thoughts to facilitate future research. This survey is for both beginners and experts in cross-lingual summarization, and we hope it will serve as a starting point as well as a source of new ideas for researchers and engineers interested in this area."",
}
@article{fang-xie-2022-end,
    title = ""An End-to-End Contrastive Self-Supervised Learning Framework for Language Understanding"",
    author = ""Fang, Hongchao  and
      Xie, Pengtao"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.76"",
    doi = ""10.1162/tacl_a_00521"",
    pages = ""1324--1340"",
    abstract = ""Self-supervised learning (SSL) methods such as Word2vec, BERT, and GPT have shown great effectiveness in language understanding. Contrastive learning, as a recent SSL approach, has attracted increasing attention in NLP. Contrastive learning learns data representations by predicting whether two augmented data instances are generated from the same original data example. Previous contrastive learning methods perform data augmentation and contrastive learning separately. As a result, the augmented data may not be optimal for contrastive learning. To address this problem, we propose a four-level optimization framework that performs data augmentation and contrastive learning end-to-end, to enable the augmented data to be tailored to the contrastive learning task. This framework consists of four learning stages, including training machine translation models for sentence augmentation, pretraining a text encoder using contrastive learning, finetuning a text classification model, and updating weights of translation data by minimizing the validation loss of the classification model, which are performed in a unified way. Experiments on datasets in the GLUE benchmark (Wang et al., 2018a) and on datasets used in Gururangan et al. (2020) demonstrate the effectiveness of our method."",
}
@article{lachmy-etal-2022-draw,
    title = ""Draw Me a Flower: Processing and Grounding Abstraction in Natural Language"",
    author = ""Lachmy, Royi  and
      Pyatkin, Valentina  and
      Manevich, Avshalom  and
      Tsarfaty, Reut"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.77"",
    doi = ""10.1162/tacl_a_00522"",
    pages = ""1341--1356"",
    abstract = ""Abstraction is a core tenet of human cognition and communication. When composing natural language instructions, humans naturally evoke abstraction to convey complex procedures in an efficient and concise way. Yet, interpreting and grounding abstraction expressed in NL has not yet been systematically studied in NLP, with no accepted benchmarks specifically eliciting abstraction in NL. In this work, we set the foundation for a systematic study of processing and grounding abstraction in NLP. First, we deliver a novel abstraction elicitation method and present Hexagons, a 2D instruction-following game. Using Hexagons we collected over 4k naturally occurring visually-grounded instructions rich with diverse types of abstractions. From these data, we derive an instruction-to-execution task and assess different types of neural models. Our results show that contemporary models and modeling practices are substantially inferior to human performance, and that model performance is inversely correlated with the level of abstraction, showing less satisfying performance on higher levels of abstraction. These findings are consistent across models and setups, confirming that abstraction is a challenging phenomenon deserving further attention and study in NLP/AI research."",
}
@article{jiang-marneffe-2022-investigating,
    title = ""Investigating Reasons for Disagreement in Natural Language Inference"",
    author = ""Jiang, Nan-Jiang  and
      de Marneffe, Marie-Catherine"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.78"",
    doi = ""10.1162/tacl_a_00523"",
    pages = ""1357--1374"",
    abstract = ""We investigate how disagreement in natural language inference (NLI) annotation arises. We developed a taxonomy of disagreement sources with 10 categories spanning 3 high- level classes. We found that some disagreements are due to uncertainty in the sentence meaning, others to annotator biases and task artifacts, leading to different interpretations of the label distribution. We explore two modeling approaches for detecting items with potential disagreement: a 4-way classification with a {``}Complicated{''} label in addition to the three standard NLI labels, and a multilabel classification approach. We found that the multilabel classification is more expressive and gives better recall of the possible interpretations in the data."",
}
@article{bosc-vincent-2022-emergence,
    title = ""The Emergence of Argument Structure in Artificial Languages"",
    author = ""Bosc, Tom  and
      Vincent, Pascal"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.79"",
    doi = ""10.1162/tacl_a_00524"",
    pages = ""1375--1391"",
    abstract = ""Computational approaches to the study of language emergence can help us understand how natural languages are shaped by cognitive and sociocultural factors. Previous work focused on tasks where agents refer to a single entity. In contrast, we study how agents predicate, that is, how they express that some relation holds between several entities. We introduce a setup where agents talk about a variable number of entities that can be partially observed by the listener. In the presence of a least-effort pressure, they tend to discuss only entities that are not observed by the listener. Thus we can obtain artificial phrases that denote a single entity, as well as artificial sentences that denote several entities. In natural languages, if we ignore the verb, phrases are usually concatenated, either in a specific order or by adding case markers to form sentences. Our setup allows us to quantify how much this holds in emergent languages using a metric we call concatenability. We also measure transitivity, which quantifies the importance of word order. We demonstrate the usefulness of this new setup and metrics for studying factors that influence argument structure. We compare agents having access to input representations structured into pre-segmented objects with properties, versus unstructured representations. Our results indicate that the awareness of object structure yields a more natural sentence organization."",
}
@article{lauscher-etal-2022-scientia,
    title = ""Scientia Potentia {E}st{---}{O}n the Role of Knowledge in Computational Argumentation"",
    author = ""Lauscher, Anne  and
      Wachsmuth, Henning  and
      Gurevych, Iryna  and
      Glava{\v{s}}, Goran"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.80"",
    doi = ""10.1162/tacl_a_00525"",
    pages = ""1392--1422"",
    abstract = ""Despite extensive research efforts in recent years, computational argumentation (CA) remains one of the most challenging areas of natural language processing. The reason for this is the inherent complexity of the cognitive processes behind human argumentation, which integrate a plethora of different types of knowledge, ranging from topic-specific facts and common sense to rhetorical knowledge. The integration of knowledge from such a wide range in CA requires modeling capabilities far beyond many other natural language understanding tasks. Existing research on mining, assessing, reasoning over, and generating arguments largely acknowledges that much more knowledge is needed to accurately model argumentation computationally. However, a systematic overview of the types of knowledge introduced in existing CA models is missing, hindering targeted progress in the field. Adopting the operational definition of knowledge as any task-relevant normative information not provided as input, the survey paper at hand fills this gap by (1) proposing a taxonomy of types of knowledge required in CA tasks, (2) systematizing the large body of CA work according to the reliance on and exploitation of these knowledge types for the four main research areas in CA, and (3) outlining and discussing directions for future research efforts in CA."",
}
@article{sartran-etal-2022-transformer,
    title = ""Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale"",
    author = ""Sartran, Laurent  and
      Barrett, Samuel  and
      Kuncoro, Adhiguna  and
      Stanojevi{\'c}, Milo{\v{s}}  and
      Blunsom, Phil  and
      Dyer, Chris"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.81"",
    doi = ""10.1162/tacl_a_00526"",
    pages = ""1423--1439"",
    abstract = ""We introduce Transformer Grammars (TGs), a novel class of Transformer language models that combine (i) the expressive power, scalability, and strong performance of Transformers and (ii) recursive syntactic compositions, which here are implemented through a special attention mask and deterministic transformation of the linearized tree. We find that TGs outperform various strong baselines on sentence-level language modeling perplexity, as well as on multiple syntax-sensitive language modeling evaluation metrics. Additionally, we find that the recursive syntactic composition bottleneck which represents each sentence as a single vector harms perplexity on document-level language modeling, providing evidence that a different kind of memory mechanism{---}one that is independent of composed syntactic representations{---}plays an important role in current successful models of long text."",
}
@article{calabrese-etal-2022-explainable,
    title = ""Explainable Abuse Detection as Intent Classification and Slot Filling"",
    author = {Calabrese, Agostina  and
      Ross, Bj{\""o}rn  and
      Lapata, Mirella},
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.82"",
    doi = ""10.1162/tacl_a_00527"",
    pages = ""1440--1454"",
    abstract = ""To proactively offer social media users a safe online experience, there is a need for systems that can detect harmful posts and promptly alert platform moderators. In order to guarantee the enforcement of a consistent policy, moderators are provided with detailed guidelines. In contrast, most state-of-the-art models learn what abuse is from labeled examples and as a result base their predictions on spurious cues, such as the presence of group identifiers, which can be unreliable. In this work we introduce the concept of policy-aware abuse detection, abandoning the unrealistic expectation that systems can reliably learn which phenomena constitute abuse from inspecting the data alone. We propose a machine-friendly representation of the policy that moderators wish to enforce, by breaking it down into a collection of intents and slots. We collect and annotate a dataset of 3,535 English posts with such slots, and show how architectures for intent classification and slot filling can be used for abuse detection, while providing a rationale for model decisions.1"",
}
@article{goldman-tsarfaty-2022-morphology,
    title = ""Morphology Without Borders: Clause-Level Morphology"",
    author = ""Goldman, Omer  and
      Tsarfaty, Reut"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.83"",
    doi = ""10.1162/tacl_a_00528"",
    pages = ""1455--1472"",
    abstract = ""Morphological tasks use large multi-lingual datasets that organize words into inflection tables, which then serve as training and evaluation data for various tasks. However, a closer inspection of these data reveals profound cross-linguistic inconsistencies, which arise from the lack of a clear linguistic and operational definition of what is a word, and which severely impair the universality of the derived tasks. To overcome this deficiency, we propose to view morphology as a clause-level phenomenon, rather than word-level. It is anchored in a fixed yet inclusive set of features, that encapsulates all functions realized in a saturated clause. We deliver MightyMorph, a novel dataset for clause-level morphology covering 4 typologically different languages: English, German, Turkish, and Hebrew. We use this dataset to derive 3 clause-level morphological tasks: inflection, reinflection and analysis. Our experiments show that the clause-level tasks are substantially harder than the respective word-level tasks, while having comparable complexity across languages. Furthermore, redefining morphology to the clause-level provides a neat interface with contextualized language models (LMs) and allows assessing the morphological knowledge encoded in these models and their usability for morphological tasks. Taken together, this work opens up new horizons in the study of computational morphology, leaving ample space for studying neural morphology cross-linguistically."",
}
@article{dziri-etal-2022-faithdial,
    title = ""{F}aith{D}ial: A Faithful Benchmark for Information-Seeking Dialogue"",
    author = ""Dziri, Nouha  and
      Kamalloo, Ehsan  and
      Milton, Sivan  and
      Zaiane, Osmar  and
      Yu, Mo  and
      Ponti, Edoardo M.  and
      Reddy, Siva"",
    editor = ""Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""10"",
    year = ""2022"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2022.tacl-1.84"",
    doi = ""10.1162/tacl_a_00529"",
    pages = ""1473--1490"",
    abstract = ""The goal of information-seeking dialogue is to respond to seeker queries with natural language utterances that are grounded on knowledge sources. However, dialogue systems often produce unsupported utterances, a phenomenon known as hallucination. To mitigate this behavior, we adopt a data-centric solution and create FaithDial, a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe that FaithDial is more faithful than WoW while also maintaining engaging conversations. We show that FaithDial can serve as training signal for: i) a hallucination critic, which discriminates whether an utterance is faithful or not, and boosts the performance by 12.8 F1 score on the BEGIN benchmark compared to existing datasets for dialogue coherence; ii) high-quality dialogue generation. We benchmark a series of state-of-the-art models and propose an auxiliary contrastive objective that achieves the highest level of faithfulness and abstractiveness based on several automated metrics. Further, we find that the benefits of FaithDial generalize to zero-shot transfer on other datasets, such as CMU-Dog and TopicalChat. Finally, human evaluation reveals that responses generated by models trained on FaithDial are perceived as more interpretable, cooperative, and engaging."",
}
@proceedings{sustainlp-2022-simple,
    title = ""Proceedings of The Third Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)"",
    editor = {Fan, Angela  and
      Gurevych, Iryna  and
      Hou, Yufang  and
      Kozareva, Zornitsa  and
      Luccioni, Sasha  and
      Sadat Moosavi, Nafise  and
      Ravi, Sujith  and
      Kim, Gyuwan  and
      Schwartz, Roy  and
      R{\""u}ckl{\'e}, Andreas},
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.sustainlp-1.0"",
}
@",clinical narr,natural language process,natural languag,language process,translat,nlp,infer,generat,relation extract,entity recognit,relation classif,summar,information retriev,semant,annotat,question-answ,challeng,benchmark,evalu,assess,track,public data,public dataset,public dataset
" ""{NLP}-{CIC}-{WFU} at {S}ocial{D}is{NER}: Disease Mention Extraction in {S}panish Tweets Using Transfer Learning and Search by Propagation"","," ""Named entity recognition (e.g., disease mention extraction) is one of the most relevant tasks for data mining in the medical field. Although it is a well-known challenge, the bulk of the efforts to tackle this task have been made using clinical texts commonly written in English. In this work, we present our contribution to the SocialDisNER competition, which consists of a transfer learning approach to extracting disease mentions in a corpus from Twitter written in Spanish. We fine-tuned a model based on mBERT and applied post-processing using regular expressions to propagate the entities identified by the model and enhance disease mention extraction. Our system achieved a competitive strict F1 of 0.851 on the testing data set."",","{tamayo-etal-2022-nlp,
    title = ""{NLP}-{CIC}-{WFU} at {S}ocial{D}is{NER}: Disease Mention Extraction in {S}panish Tweets Using Transfer Learning and Search by Propagation"",
    author = ""Tamayo, Antonio  and
      Gelbukh, Alexander  and
      Burgos, Diego"",
    editor = ""Gonzalez-Hernandez, Graciela  and
      Weissenbacher, Davy"",
    booktitle = ""Proceedings of The Seventh Workshop on Social Media Mining for Health Applications, Workshop {\&} Shared Task"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.smm4h-1.6"",
    pages = ""19--22"",
    abstract = ""Named entity recognition (e.g., disease mention extraction) is one of the most relevant tasks for data mining in the medical field. Although it is a well-known challenge, the bulk of the efforts to tackle this task have been made using clinical texts commonly written in English. In this work, we present our contribution to the SocialDisNER competition, which consists of a transfer learning approach to extracting disease mentions in a corpus from Twitter written in Spanish. We fine-tuned a model based on mBERT and applied post-processing using regular expressions to propagate the entities identified by the model and enhance disease mention extraction. Our system achieved a competitive strict F1 of 0.851 on the testing data set."",
}
@",clinical text,nlp,entity recognit,shared task,challeng
" ""{READ}-{B}io{M}ed@{S}ocial{D}is{NER}: Adaptation of an Annotation System to {S}panish Tweets"","," ""We describe the work of the READ-BioMed team for the preparation of a submission to the SocialDisNER Disease Named Entity Recognition (NER) Task (Task 10) in 2022. We had developed a system for named entity recognition for identifying biomedical concepts in English MEDLINE citations and Spanish clinical text for the LivingNER 2022 challenge. Minimal adaptation of our system was required to perform named entity recognition in the Spanish tweets in the SocialDisNER task, given the availability of Spanish pre-trained language models and the SocialDisNER training data. Minor additions included treatment of emojis and entities in hashtags and Twitter account names."",","{jimeno-yepes-verspoor-2022-read,
    title = ""{READ}-{B}io{M}ed@{S}ocial{D}is{NER}: Adaptation of an Annotation System to {S}panish Tweets"",
    author = ""Jimeno Yepes, Antonio  and
      Verspoor, Karin"",
    editor = ""Gonzalez-Hernandez, Graciela  and
      Weissenbacher, Davy"",
    booktitle = ""Proceedings of The Seventh Workshop on Social Media Mining for Health Applications, Workshop {\&} Shared Task"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.smm4h-1.14"",
    pages = ""48--51"",
    abstract = ""We describe the work of the READ-BioMed team for the preparation of a submission to the SocialDisNER Disease Named Entity Recognition (NER) Task (Task 10) in 2022. We had developed a system for named entity recognition for identifying biomedical concepts in English MEDLINE citations and Spanish clinical text for the LivingNER 2022 challenge. Minimal adaptation of our system was required to perform named entity recognition in the Spanish tweets in the SocialDisNER task, given the availability of Spanish pre-trained language models and the SocialDisNER training data. Minor additions included treatment of emojis and entities in hashtags and Twitter account names."",
}
@",clinical text,entity recognit,shared task,annotat,challeng
" ""{FRE} at {S}ocial{D}is{NER}: Joint Learning of Language Models for Named Entity Recognition"","," ""This paper describes our followed methodology for the automatic extraction of disease mentions from tweets in Spanish as part of the SocialDisNER challenge within the 2022 Social Media Mining for Health Applications (SMM4H) Shared Task. We followed a Joint Learning ensemble architecture for the fine-tuning of top performing pre-trained language models in biomedical domain for Named Entity Recognition tasks. We used text generation techniques to augment training data. During practice phase of the challenge our approach showed results of 0.87 F1-Score."",","{cetina-garcia-santa-2022-fre,
    title = ""{FRE} at {S}ocial{D}is{NER}: Joint Learning of Language Models for Named Entity Recognition"",
    author = ""Cetina, Kendrick  and
      Garc{\'\i}a-Santa, Nuria"",
    editor = ""Gonzalez-Hernandez, Graciela  and
      Weissenbacher, Davy"",
    booktitle = ""Proceedings of The Seventh Workshop on Social Media Mining for Health Applications, Workshop {\&} Shared Task"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.smm4h-1.20"",
    pages = ""68--70"",
    abstract = ""This paper describes our followed methodology for the automatic extraction of disease mentions from tweets in Spanish as part of the SocialDisNER challenge within the 2022 Social Media Mining for Health Applications (SMM4H) Shared Task. We followed a Joint Learning ensemble architecture for the fine-tuning of top performing pre-trained language models in biomedical domain for Named Entity Recognition tasks. We used text generation techniques to augment training data. During practice phase of the challenge our approach showed results of 0.87 F1-Score."",
}
@",medical domain,generat,entity recognit,shared task,challeng
" ""{LED} down the rabbit hole: exploring the potential of global attention for biomedical multi-document summarisation"","," ""In this paper we report the experiments performed for the submission to the Multidocument summarisation for Literature Review (MSLR) Shared Task. In particular, we adopt Primera model to the biomedical domain by placing global attention on important biomedical entities in several ways. We analyse the outputs of 23 resulting models and report some patterns related to the presence of additional global attention, number of training steps and the inputs configuration."",","{otmakhova-etal-2022-led,
    title = ""{LED} down the rabbit hole: exploring the potential of global attention for biomedical multi-document summarisation"",
    author = ""Otmakhova, Yulia  and
      Truong, Thinh Hung  and
      Baldwin, Timothy  and
      Cohn, Trevor  and
      Verspoor, Karin  and
      Lau, Jey Han"",
    editor = ""Cohan, Arman  and
      Feigenblat, Guy  and
      Freitag, Dayne  and
      Ghosal, Tirthankar  and
      Herrmannova, Drahomira  and
      Knoth, Petr  and
      Lo, Kyle  and
      Mayr, Philipp  and
      Shmueli-Scheuer, Michal  and
      de Waard, Anita  and
      Wang, Lucy Lu"",
    booktitle = ""Proceedings of the Third Workshop on Scholarly Document Processing"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.sdp-1.21"",
    pages = ""181--187"",
    abstract = ""In this paper we report the experiments performed for the submission to the Multidocument summarisation for Literature Review (MSLR) Shared Task. In particular, we adopt Primera model to the biomedical domain by placing global attention on important biomedical entities in several ways. We analyse the outputs of 23 resulting models and report some patterns related to the presence of additional global attention, number of training steps and the inputs configuration."",
}
@",medical domain,summar,summaris,shared task
" ""Exploring the limits of a base {BART} for multi-document summarization in the medical domain"","," ""This paper is a description of our participation in the Multi-document Summarization for Literature Review (MSLR) Shared Task, in which we explore summarization models to create an automatic review of scientific results. Rather than maximizing the metrics using expensive computational models, we placed ourselves in a situation of scarce computational resources and explore the limits of a base sequence to sequence models (thus with a limited input length) to the task. Although we explore methods to feed the abstractive model with salient sentences only (using a first extractive step), we find the results still need some improvements."",","{obonyo-etal-2022-exploring,
    title = ""Exploring the limits of a base {BART} for multi-document summarization in the medical domain"",
    author = ""Obonyo, Ishmael  and
      Casola, Silvia  and
      Saggion, Horacio"",
    editor = ""Cohan, Arman  and
      Feigenblat, Guy  and
      Freitag, Dayne  and
      Ghosal, Tirthankar  and
      Herrmannova, Drahomira  and
      Knoth, Petr  and
      Lo, Kyle  and
      Mayr, Philipp  and
      Shmueli-Scheuer, Michal  and
      de Waard, Anita  and
      Wang, Lucy Lu"",
    booktitle = ""Proceedings of the Third Workshop on Scholarly Document Processing"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.sdp-1.23"",
    pages = ""193--198"",
    abstract = ""This paper is a description of our participation in the Multi-document Summarization for Literature Review (MSLR) Shared Task, in which we explore summarization models to create an automatic review of scientific results. Rather than maximizing the metrics using expensive computational models, we placed ourselves in a situation of scarce computational resources and explore the limits of a base sequence to sequence models (thus with a limited input length) to the task. Although we explore methods to feed the abstractive model with salient sentences only (using a first extractive step), we find the results still need some improvements."",
}
@",medical domain,summar,shared task
" ""Abstractive Approaches To Multidocument Summarization Of Medical Literature Reviews"","," ""Text summarization has been a trending domain of research in NLP in the past few decades. The medical domain is no exception to the same. Medical documents often contain a lot of jargon pertaining to certain domains, and performing an abstractive summarization on the same remains a challenge. This paper presents a summary of the findings that we obtained based on the shared task of Multidocument Summarization for Literature Review (MSLR). We stood fourth in the leaderboards for evaluation on the MS{\^{}}2 and Cochrane datasets. We finetuned pre-trained models such as BART-large, DistilBART and T5-base on both these datasets. These models{'} accuracy was later tested with a part of the same dataset using ROUGE scores as the evaluation metrics."",","{tangsali-etal-2022-abstractive,
    title = ""Abstractive Approaches To Multidocument Summarization Of Medical Literature Reviews"",
    author = ""Tangsali, Rahul  and
      Vyawahare, Aditya Jagdish  and
      Mandke, Aditya Vyankatesh  and
      Litake, Onkar Rupesh  and
      Kadam, Dipali Dattatray"",
    editor = ""Cohan, Arman  and
      Feigenblat, Guy  and
      Freitag, Dayne  and
      Ghosal, Tirthankar  and
      Herrmannova, Drahomira  and
      Knoth, Petr  and
      Lo, Kyle  and
      Mayr, Philipp  and
      Shmueli-Scheuer, Michal  and
      de Waard, Anita  and
      Wang, Lucy Lu"",
    booktitle = ""Proceedings of the Third Workshop on Scholarly Document Processing"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.sdp-1.24"",
    pages = ""199--203"",
    abstract = ""Text summarization has been a trending domain of research in NLP in the past few decades. The medical domain is no exception to the same. Medical documents often contain a lot of jargon pertaining to certain domains, and performing an abstractive summarization on the same remains a challenge. This paper presents a summary of the findings that we obtained based on the shared task of Multidocument Summarization for Literature Review (MSLR). We stood fourth in the leaderboards for evaluation on the MS{\^{}}2 and Cochrane datasets. We finetuned pre-trained models such as BART-large, DistilBART and T5-base on both these datasets. These models{'} accuracy was later tested with a part of the same dataset using ROUGE scores as the evaluation metrics."",
}
@",medical domain,nlp,summar,shared task,challeng,evalu
" ""An Extractive-Abstractive Approach for Multi-document Summarization of Scientific Articles for Literature Review"","," ""Research in the biomedical domain is con- stantly challenged by its large amount of ever- evolving textual information. Biomedical re- searchers are usually required to conduct a lit- erature review before any medical interven- tion to assess the effectiveness of the con- cerned research. However, the process is time- consuming, and therefore, automation to some extent would help reduce the accompanying information overload. Multi-document sum- marization of scientific articles for literature reviews is one approximation of such automa- tion. Here in this paper, we describe our pipelined approach for the aforementioned task. We design a BERT-based extractive method followed by a BigBird PEGASUS-based ab- stractive pipeline for generating literature re- view summaries from the abstracts of biomedi- cal trial reports as part of the Multi-document Summarization for Literature Review (MSLR) shared task1 in the Scholarly Document Pro- cessing (SDP) workshop 20222. Our proposed model achieves the best performance on the MSLR-Cochrane leaderboard3 on majority of the evaluation metrics. Human scrutiny of our automatically generated summaries indicates that our approach is promising to yield readable multi-article summaries for conducting such lit- erature reviews."",","{shinde-etal-2022-extractive,
    title = ""An Extractive-Abstractive Approach for Multi-document Summarization of Scientific Articles for Literature Review"",
    author = ""Shinde, Kartik  and
      Roy, Trinita  and
      Ghosal, Tirthankar"",
    editor = ""Cohan, Arman  and
      Feigenblat, Guy  and
      Freitag, Dayne  and
      Ghosal, Tirthankar  and
      Herrmannova, Drahomira  and
      Knoth, Petr  and
      Lo, Kyle  and
      Mayr, Philipp  and
      Shmueli-Scheuer, Michal  and
      de Waard, Anita  and
      Wang, Lucy Lu"",
    booktitle = ""Proceedings of the Third Workshop on Scholarly Document Processing"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.sdp-1.25"",
    pages = ""204--209"",
    abstract = ""Research in the biomedical domain is con- stantly challenged by its large amount of ever- evolving textual information. Biomedical re- searchers are usually required to conduct a lit- erature review before any medical interven- tion to assess the effectiveness of the con- cerned research. However, the process is time- consuming, and therefore, automation to some extent would help reduce the accompanying information overload. Multi-document sum- marization of scientific articles for literature reviews is one approximation of such automa- tion. Here in this paper, we describe our pipelined approach for the aforementioned task. We design a BERT-based extractive method followed by a BigBird PEGASUS-based ab- stractive pipeline for generating literature re- view summaries from the abstracts of biomedi- cal trial reports as part of the Multi-document Summarization for Literature Review (MSLR) shared task1 in the Scholarly Document Pro- cessing (SDP) workshop 20222. Our proposed model achieves the best performance on the MSLR-Cochrane leaderboard3 on majority of the evaluation metrics. Human scrutiny of our automatically generated summaries indicates that our approach is promising to yield readable multi-article summaries for conducting such lit- erature reviews."",
}
@",medical domain,generat,summar,shared task,challeng,evalu,assess
" ""{SCU}-{MESCL}ab at {ROCLING}-2022 Shared Task: Named Entity Recognition Using {BERT} Classifier"","," ""In this study, named entity recognition is constructed and applied in the medical domain. Data is labeled in BIO format. For example, {``}muscle{''} would be labeled {``}B-BODY{''} and {``}I-BODY{''}, and {``}cough{''} would be {``}B-SYMP{''} and {``}I-SYMP{''}. All words outside the category are marked with {``}O{''}. The Chinese HealthNER Corpus contains 30,692 sentences, of which 2531 sentences are divided into the validation set (dev) for this evaluation, and the conference finally provides another 3204 sentences for the test set (test). We use BLSTM{\_}CRF, Roberta+BLSTM{\_}CRF and BERT Classifier to submit three prediction results respectively. Finally, the BERT Classifier system submitted as RUN3 achieved the best prediction performance, with an accuracy of 80.18{\%}, a recall rate of 78.3{\%}, and an F1-score of 79.23."",","{yang-etal-2022-scu,
    title = ""{SCU}-{MESCL}ab at {ROCLING}-2022 Shared Task: Named Entity Recognition Using {BERT} Classifier"",
    author = ""Yang, Tsung-Hsien  and
      Su, Ruei-Cyuan  and
      Su, Tzu-En  and
      Chong, Sing-Seong  and
      Su, Ming-Hsiang"",
    editor = ""Chang, Yung-Chun  and
      Huang, Yi-Chin"",
    booktitle = ""Proceedings of the 34th Conference on Computational Linguistics and Speech Processing (ROCLING 2022)"",
    month = nov,
    year = ""2022"",
    address = ""Taipei, Taiwan"",
    publisher = ""The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)"",
    url = ""https://aclanthology.org/2022.rocling-1.41"",
    pages = ""329--334"",
    abstract = ""In this study, named entity recognition is constructed and applied in the medical domain. Data is labeled in BIO format. For example, {``}muscle{''} would be labeled {``}B-BODY{''} and {``}I-BODY{''}, and {``}cough{''} would be {``}B-SYMP{''} and {``}I-SYMP{''}. All words outside the category are marked with {``}O{''}. The Chinese HealthNER Corpus contains 30,692 sentences, of which 2531 sentences are divided into the validation set (dev) for this evaluation, and the conference finally provides another 3204 sentences for the test set (test). We use BLSTM{\_}CRF, Roberta+BLSTM{\_}CRF and BERT Classifier to submit three prediction results respectively. Finally, the BERT Classifier system submitted as RUN3 achieved the best prediction performance, with an accuracy of 80.18{\%}, a recall rate of 78.3{\%}, and an F1-score of 79.23."",
    language = ""Chinese"",
}
@",medical domain,language process,entity recognit,shared task,evalu
" ""In-Domain Pre-Training Improves Clinical Note Generation from Doctor-Patient Conversations"","," ""Summarization of doctor-patient conversations into clinical notes by medical scribes is an essential process for effective clinical care. Pre-trained transformer models have shown a great amount of success in this area, but the domain shift from standard NLP tasks to the medical domain continues to present challenges. We build upon several recent works to show that additional pre-training with in-domain medical conversations leads to performance gains for clinical summarization. In addition to conventional evaluation metrics, we also explore a clinical named entity recognition model for concept-based evaluation. Finally, we contrast long-sequence transformers with a common transformer model, BART. Overall, our findings corroborate research in non-medical domains and suggest that in-domain pre-training combined with transformers for long sequences are effective strategies for summarizing clinical encounters."",","{grambow-etal-2022-domain,
    title = ""In-Domain Pre-Training Improves Clinical Note Generation from Doctor-Patient Conversations"",
    author = ""Grambow, Colin  and
      Zhang, Longxiang  and
      Schaaf, Thomas"",
    editor = ""Krahmer, Emiel  and
      McCoy, Kathy  and
      Reiter, Ehud"",
    booktitle = ""Proceedings of the First Workshop on Natural Language Generation in Healthcare"",
    month = jul,
    year = ""2022"",
    address = ""Waterville, Maine, USA and virtual meeting"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.nlg4health-1.2"",
    pages = ""9--22"",
    abstract = ""Summarization of doctor-patient conversations into clinical notes by medical scribes is an essential process for effective clinical care. Pre-trained transformer models have shown a great amount of success in this area, but the domain shift from standard NLP tasks to the medical domain continues to present challenges. We build upon several recent works to show that additional pre-training with in-domain medical conversations leads to performance gains for clinical summarization. In addition to conventional evaluation metrics, we also explore a clinical named entity recognition model for concept-based evaluation. Finally, we contrast long-sequence transformers with a common transformer model, BART. Overall, our findings corroborate research in non-medical domains and suggest that in-domain pre-training combined with transformers for long sequences are effective strategies for summarizing clinical encounters."",
}
@",medical domain,clinical not,natural languag,nlp,generat,entity recognit,summar,challeng,evalu
" ""Sentence-Level Resampling for Named Entity Recognition"","," ""As a fundamental task in natural language processing, named entity recognition (NER) aims to locate and classify named entities in unstructured text. However, named entities are always the minority among all tokens in the text. This data imbalance problem presents a challenge to machine learning models as their learning objective is usually dominated by the majority of non-entity tokens. To alleviate data imbalance, we propose a set of sentence-level resampling methods where the importance of each training sentence is computed based on its tokens and entities. We study the generalizability of these resampling methods on a wide variety of NER models (CRF, Bi-LSTM, and BERT) across corpora from diverse domains (general, social, and medical texts). Extensive experiments show that the proposed methods improve span-level macro F1-scores of the evaluated NER models on multiple corpora, frequently outperforming sub-sentence-level resampling, data augmentation, and special loss functions such as focal and Dice loss."",","{wang-wang-2022-sentence,
    title = ""Sentence-Level Resampling for Named Entity Recognition"",
    author = ""Wang, Xiaochen  and
      Wang, Yue"",
    editor = ""Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir"",
    booktitle = ""Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jul,
    year = ""2022"",
    address = ""Seattle, United States"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.naacl-main.156"",
    doi = ""10.18653/v1/2022.naacl-main.156"",
    pages = ""2151--2165"",
    abstract = ""As a fundamental task in natural language processing, named entity recognition (NER) aims to locate and classify named entities in unstructured text. However, named entities are always the minority among all tokens in the text. This data imbalance problem presents a challenge to machine learning models as their learning objective is usually dominated by the majority of non-entity tokens. To alleviate data imbalance, we propose a set of sentence-level resampling methods where the importance of each training sentence is computed based on its tokens and entities. We study the generalizability of these resampling methods on a wide variety of NER models (CRF, Bi-LSTM, and BERT) across corpora from diverse domains (general, social, and medical texts). Extensive experiments show that the proposed methods improve span-level macro F1-scores of the evaluated NER models on multiple corpora, frequently outperforming sub-sentence-level resampling, data augmentation, and special loss functions such as focal and Dice loss."",
}
@",medical text,natural language process,natural languag,language process,entity recognit,challeng,evalu
" ""Improving Neural Models for Radiology Report Retrieval with Lexicon-based Automated Annotation"","," ""Many clinical informatics tasks that are based on electronic health records (EHR) need relevant patient cohorts to be selected based on findings, symptoms and diseases. Frequently, these conditions are described in radiology reports which can be retrieved using information retrieval (IR) methods. The latest of these techniques utilize neural IR models such as BERT trained on clinical text. However, these methods still lack semantic understanding of the underlying clinical conditions as well as ruled out findings, resulting in poor precision during retrieval. In this paper we combine clinical finding detection with supervised query match learning. Specifically, we use lexicon-driven concept detection to detect relevant findings in sentences. These findings are used as queries to train a Sentence-BERT (SBERT) model using triplet loss on matched and unmatched query-sentence pairs. We show that the proposed supervised training task remarkably improves the retrieval performance of SBERT. The trained model generalizes well to unseen queries and reports from different collections."",","{shi-etal-2022-improving,
    title = ""Improving Neural Models for Radiology Report Retrieval with Lexicon-based Automated Annotation"",
    author = ""Shi, Luyao  and
      Syeda-mahmood, Tanveer  and
      Baldwin, Tyler"",
    editor = ""Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir"",
    booktitle = ""Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jul,
    year = ""2022"",
    address = ""Seattle, United States"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.naacl-main.253"",
    doi = ""10.18653/v1/2022.naacl-main.253"",
    pages = ""3457--3463"",
    abstract = ""Many clinical informatics tasks that are based on electronic health records (EHR) need relevant patient cohorts to be selected based on findings, symptoms and diseases. Frequently, these conditions are described in radiology reports which can be retrieved using information retrieval (IR) methods. The latest of these techniques utilize neural IR models such as BERT trained on clinical text. However, these methods still lack semantic understanding of the underlying clinical conditions as well as ruled out findings, resulting in poor precision during retrieval. In this paper we combine clinical finding detection with supervised query match learning. Specifically, we use lexicon-driven concept detection to detect relevant findings in sentences. These findings are used as queries to train a Sentence-BERT (SBERT) model using triplet loss on matched and unmatched query-sentence pairs. We show that the proposed supervised training task remarkably improves the retrieval performance of SBERT. The trained model generalizes well to unseen queries and reports from different collections."",
}
@",electronic health record,health record,clinical text,information retriev,semant,annotat
" ""Conceptualizing Treatment Leakage in Text-based Causal Inference"","," ""Causal inference methods that control for text-based confounders are becoming increasingly important in the social sciences and other disciplines where text is readily available. However, these methods rely on a critical assumption that there is no treatment leakage: that is, the text only contains information about the confounder and no information about treatment assignment. When this assumption does not hold, methods that control for text to adjust for confounders face the problem of post-treatment (collider) bias. However, the assumption that there is no treatment leakage may be unrealistic in real-world situations involving text, as human language is rich and flexible. Language appearing in a public policy document or health records may refer to the future and the past simultaneously, and thereby reveal information about the treatment assignment. In this article, we define the treatment-leakage problem, and discuss the identification as well as the estimation challenges it raises. Second, we delineate the conditions under which leakage can be addressed by removing the treatment-related signal from the text in a pre-processing step we define as text distillation. Lastly, using simulation, we show how treatment leakage introduces a bias in estimates of the average treatment effect (ATE) and how text distillation can mitigate this bias."",","{daoud-etal-2022-conceptualizing,
    title = ""Conceptualizing Treatment Leakage in Text-based Causal Inference"",
    author = ""Daoud, Adel  and
      Jerzak, Connor  and
      Johansson, Richard"",
    editor = ""Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir"",
    booktitle = ""Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jul,
    year = ""2022"",
    address = ""Seattle, United States"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.naacl-main.413"",
    doi = ""10.18653/v1/2022.naacl-main.413"",
    pages = ""5638--5645"",
    abstract = ""Causal inference methods that control for text-based confounders are becoming increasingly important in the social sciences and other disciplines where text is readily available. However, these methods rely on a critical assumption that there is no treatment leakage: that is, the text only contains information about the confounder and no information about treatment assignment. When this assumption does not hold, methods that control for text to adjust for confounders face the problem of post-treatment (collider) bias. However, the assumption that there is no treatment leakage may be unrealistic in real-world situations involving text, as human language is rich and flexible. Language appearing in a public policy document or health records may refer to the future and the past simultaneously, and thereby reveal information about the treatment assignment. In this article, we define the treatment-leakage problem, and discuss the identification as well as the estimation challenges it raises. Second, we delineate the conditions under which leakage can be addressed by removing the treatment-related signal from the text in a pre-processing step we define as text distillation. Lastly, using simulation, we show how treatment leakage introduces a bias in estimates of the average treatment effect (ATE) and how text distillation can mitigate this bias."",
}
@",health record,infer,challeng
" ""{KIMERA}: Injecting Domain Knowledge into Vacant Transformer Heads"","," ""Training transformer language models requires vast amounts of text and computational resources. This drastically limits the usage of these models in niche domains for which they are not optimized, or where domain-specific training data is scarce. We focus here on the clinical domain because of its limited access to training data in common tasks, while structured ontological data is often readily available. Recent observations in model compression of transformer models show optimization potential in improving the representation capacity of attention heads. We propose KIMERA (Knowledge Injection via Mask Enforced Retraining of Attention) for detecting, retraining and instilling attention heads with complementary structured domain knowledge. Our novel multi-task training scheme effectively identifies and targets individual attention heads that are least useful for a given downstream task and optimizes their representation with information from structured data. KIMERA generalizes well, thereby building the basis for an efficient fine-tuning. KIMERA achieves significant performance boosts on seven datasets in the medical domain in Information Retrieval and Clinical Outcome Prediction settings. We apply KIMERA to BERT-base to evaluate the extent of the domain transfer and also improve on the already strong results of BioBERT in the clinical domain."",","{winter-etal-2022-kimera,
    title = ""{KIMERA}: Injecting Domain Knowledge into Vacant Transformer Heads"",
    author = {Winter, Benjamin  and
      Rosero, Alexei Figueroa  and
      L{\""o}ser, Alexander  and
      Gers, Felix Alexander  and
      Siu, Amy},
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Thirteenth Language Resources and Evaluation Conference"",
    month = jun,
    year = ""2022"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2022.lrec-1.38"",
    pages = ""363--373"",
    abstract = ""Training transformer language models requires vast amounts of text and computational resources. This drastically limits the usage of these models in niche domains for which they are not optimized, or where domain-specific training data is scarce. We focus here on the clinical domain because of its limited access to training data in common tasks, while structured ontological data is often readily available. Recent observations in model compression of transformer models show optimization potential in improving the representation capacity of attention heads. We propose KIMERA (Knowledge Injection via Mask Enforced Retraining of Attention) for detecting, retraining and instilling attention heads with complementary structured domain knowledge. Our novel multi-task training scheme effectively identifies and targets individual attention heads that are least useful for a given downstream task and optimizes their representation with information from structured data. KIMERA generalizes well, thereby building the basis for an efficient fine-tuning. KIMERA achieves significant performance boosts on seven datasets in the medical domain in Information Retrieval and Clinical Outcome Prediction settings. We apply KIMERA to BERT-base to evaluate the extent of the domain transfer and also improve on the already strong results of BioBERT in the clinical domain."",
}
@",clinical domain,medical domain,information retriev,evalu
" ""Language Resources to Support Language Diversity {--} the {ELRA} Achievements"","," ""This article highlights ELRA{'}s latest achievements in the field of Language Resources (LRs) identification, sharing and production. It also reports on ELRA{'}s involvement in several national and international projects, as well as in the organization of events for the support of LRs and related Language Technologies, including for under-resourced languages. Over the past few years, ELRA, together with its operational agency ELDA, has continued to increase its catalogue offer of LRs, establishing worldwide partnerships for the production of various types of LRs (SMS, tweets, crawled data, MT aligned data, speech LRs, sentiment-based data, etc.). Through their consistent involvement in EU-funded projects, ELRA and ELDA have contributed to improve the access to multilingual information in the context of the pandemic, develop tools for the de-identification of texts in the legal and medical domains, support the EU eTranslation Machine Translation system, and set up a European platform providing access to both resources and services. In December 2019, ELRA co-organized the LT4All conference, whose main topics were Language Technologies for enabling linguistic diversity and multilingualism worldwide. Moreover, although LREC was cancelled in 2020, ELRA published the LREC 2020 proceedings for the Main conference and Workshops papers, and carried on its dissemination activities while targeting the new LREC edition for 2022."",","{mapelli-etal-2022-language,
    title = ""Language Resources to Support Language Diversity {--} the {ELRA} Achievements"",
    author = ""Mapelli, Val{\'e}rie  and
      Arranz, Victoria  and
      Choukri, Khalid  and
      Mazo, H{\'e}l{\`e}ne"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Thirteenth Language Resources and Evaluation Conference"",
    month = jun,
    year = ""2022"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2022.lrec-1.58"",
    pages = ""551--558"",
    abstract = ""This article highlights ELRA{'}s latest achievements in the field of Language Resources (LRs) identification, sharing and production. It also reports on ELRA{'}s involvement in several national and international projects, as well as in the organization of events for the support of LRs and related Language Technologies, including for under-resourced languages. Over the past few years, ELRA, together with its operational agency ELDA, has continued to increase its catalogue offer of LRs, establishing worldwide partnerships for the production of various types of LRs (SMS, tweets, crawled data, MT aligned data, speech LRs, sentiment-based data, etc.). Through their consistent involvement in EU-funded projects, ELRA and ELDA have contributed to improve the access to multilingual information in the context of the pandemic, develop tools for the de-identification of texts in the legal and medical domains, support the EU eTranslation Machine Translation system, and set up a European platform providing access to both resources and services. In December 2019, ELRA co-organized the LT4All conference, whose main topics were Language Technologies for enabling linguistic diversity and multilingualism worldwide. Moreover, although LREC was cancelled in 2020, ELRA published the LREC 2020 proceedings for the Main conference and Workshops papers, and carried on its dissemination activities while targeting the new LREC edition for 2022."",
}
@",medical domain,translat,sentiment,evalu
" ""Cross-Lingual Knowledge Transfer for Clinical Phenotyping"","," ""Clinical phenotyping enables the automatic extraction of clinical conditions from patient records, which can be beneficial to doctors and clinics worldwide. However, current state-of-the-art models are mostly applicable to clinical notes written in English. We therefore investigate cross-lingual knowledge transfer strategies to execute this task for clinics that do not use the English language and have a small amount of in-domain data available. Our results reveal two strategies that outperform the state-of-the-art: Translation-based methods in combination with domain-specific encoders and cross-lingual encoders plus adapters. We find that these strategies perform especially well for classifying rare phenotypes and we advise on which method to prefer in which situation. Our results show that using multilingual data overall improves clinical phenotyping models and can compensate for data sparseness."",","{papaioannou-etal-2022-cross,
    title = ""Cross-Lingual Knowledge Transfer for Clinical Phenotyping"",
    author = ""Papaioannou, Jens-Michalis  and
      Grundmann, Paul  and
      van Aken, Betty  and
      Samaras, Athanasios  and
      Kyparissidis, Ilias  and
      Giannakoulas, George  and
      Gers, Felix  and
      Loeser, Alexander"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Thirteenth Language Resources and Evaluation Conference"",
    month = jun,
    year = ""2022"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2022.lrec-1.95"",
    pages = ""900--909"",
    abstract = ""Clinical phenotyping enables the automatic extraction of clinical conditions from patient records, which can be beneficial to doctors and clinics worldwide. However, current state-of-the-art models are mostly applicable to clinical notes written in English. We therefore investigate cross-lingual knowledge transfer strategies to execute this task for clinics that do not use the English language and have a small amount of in-domain data available. Our results reveal two strategies that outperform the state-of-the-art: Translation-based methods in combination with domain-specific encoders and cross-lingual encoders plus adapters. We find that these strategies perform especially well for classifying rare phenotypes and we advise on which method to prefer in which situation. Our results show that using multilingual data overall improves clinical phenotyping models and can compensate for data sparseness."",
}
@",clinical not,translat,evalu
" ""A Named Entity Recognition Corpus for {V}ietnamese Biomedical Texts to Support Tuberculosis Treatment"","," ""Named Entity Recognition (NER) is an important task in information extraction. However, due to the lack of labelled corpora, biomedical NER has scarcely been studied in Vietnamese compared to English. To address this situation, we have constructed VietBioNER, a labelled NER corpus of Vietnamese academic biomedical text. The corpus focuses specifically on supporting tuberculosis surveillance, and was constructed by collecting scientific papers and grey literature related to tuberculosis symptoms and diagnostics. We manually annotated a small set of the collected documents with five categories of named entities: Organisation, Location, Date and Time, Symptom and Disease, and Diagnostic Procedure. Inter-annotator agreement ranges from 70.59{\%} and 95.89{\%} F-score according to entity category. In this paper, we make available two splits of the corpus, corresponding to traditional supervised learning and few-shot learning settings. We also provide baseline results for both of these settings, in addition to a dictionary-based approach, as a means to stimulate further research into Vietnamese biomedical NER. Although supervised methods produce results that are far superior to the other two approaches, the fact that even one-shot learning can outperform the dictionary-based method provides evidence that further research into few-shot learning on this text type would be worthwhile."",","{phan-etal-2022-named,
    title = ""A Named Entity Recognition Corpus for {V}ietnamese Biomedical Texts to Support Tuberculosis Treatment"",
    author = ""Phan, Uyen  and
      Nguyen, Phuong N.V  and
      Nguyen, Nhung"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Thirteenth Language Resources and Evaluation Conference"",
    month = jun,
    year = ""2022"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2022.lrec-1.385"",
    pages = ""3601--3609"",
    abstract = ""Named Entity Recognition (NER) is an important task in information extraction. However, due to the lack of labelled corpora, biomedical NER has scarcely been studied in Vietnamese compared to English. To address this situation, we have constructed VietBioNER, a labelled NER corpus of Vietnamese academic biomedical text. The corpus focuses specifically on supporting tuberculosis surveillance, and was constructed by collecting scientific papers and grey literature related to tuberculosis symptoms and diagnostics. We manually annotated a small set of the collected documents with five categories of named entities: Organisation, Location, Date and Time, Symptom and Disease, and Diagnostic Procedure. Inter-annotator agreement ranges from 70.59{\%} and 95.89{\%} F-score according to entity category. In this paper, we make available two splits of the corpus, corresponding to traditional supervised learning and few-shot learning settings. We also provide baseline results for both of these settings, in addition to a dictionary-based approach, as a means to stimulate further research into Vietnamese biomedical NER. Although supervised methods produce results that are far superior to the other two approaches, the fact that even one-shot learning can outperform the dictionary-based method provides evidence that further research into few-shot learning on this text type would be worthwhile."",
}
@",medical text,entity recognit,annotat,evalu
" ""A Cross-document Coreference Dataset for Longitudinal Tracking across Radiology Reports"","," ""This paper proposes a new cross-document coreference resolution (CDCR) dataset for identifying co-referring radiological findings and medical devices across a patient{'}s radiology reports. Our annotated corpus contains 5872 mentions (findings and devices) spanning 638 MIMIC-III radiology reports across 60 patients, covering multiple imaging modalities and anatomies. There are a total of 2292 mention chains. We describe the annotation process in detail, highlighting the complexities involved in creating a sizable and realistic dataset for radiology CDCR. We apply two baseline methods{--}string matching and transformer language models (BERT){--}to identify cross-report coreferences. Our results indicate the requirement of further model development targeting better understanding of domain language and context to address this challenging and unexplored task. This dataset can serve as a resource to develop more advanced natural language processing CDCR methods in the future. This is one of the first attempts focusing on CDCR in the clinical domain and holds potential in benefiting physicians and clinical research through long-term tracking of radiology findings."",","{datta-etal-2022-cross,
    title = ""A Cross-document Coreference Dataset for Longitudinal Tracking across Radiology Reports"",
    author = ""Datta, Surabhi  and
      Lam, Hio Cheng  and
      Pajouhi, Atieh  and
      Mogalla, Sunitha  and
      Roberts, Kirk"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Thirteenth Language Resources and Evaluation Conference"",
    month = jun,
    year = ""2022"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2022.lrec-1.393"",
    pages = ""3686--3695"",
    abstract = ""This paper proposes a new cross-document coreference resolution (CDCR) dataset for identifying co-referring radiological findings and medical devices across a patient{'}s radiology reports. Our annotated corpus contains 5872 mentions (findings and devices) spanning 638 MIMIC-III radiology reports across 60 patients, covering multiple imaging modalities and anatomies. There are a total of 2292 mention chains. We describe the annotation process in detail, highlighting the complexities involved in creating a sizable and realistic dataset for radiology CDCR. We apply two baseline methods{--}string matching and transformer language models (BERT){--}to identify cross-report coreferences. Our results indicate the requirement of further model development targeting better understanding of domain language and context to address this challenging and unexplored task. This dataset can serve as a resource to develop more advanced natural language processing CDCR methods in the future. This is one of the first attempts focusing on CDCR in the clinical domain and holds potential in benefiting physicians and clinical research through long-term tracking of radiology findings."",
}
@",clinical domain,natural language process,natural languag,language process,co-refer,annotat,challeng,evalu,track
" ""Decorate the Examples: A Simple Method of Prompt Design for Biomedical Relation Extraction"","," ""Relation extraction is a core problem for natural language processing in the biomedical domain. Recent research on relation extraction showed that prompt-based learning improves the performance on both fine-tuning on full training set and few-shot training. However, less effort has been made on domain-specific tasks where good prompt design can be even harder. In this paper, we investigate prompting for biomedical relation extraction, with experiments on the ChemProt dataset. We present a simple yet effective method to systematically generate comprehensive prompts that reformulate the relation extraction task as a cloze-test task under a simple prompt formulation. In particular, we experiment with different ranking scores for prompt selection. With BioMed-RoBERTa-base, our results show that prompting-based fine-tuning obtains gains by 14.21 F1 over its regular fine-tuning baseline, and 1.14 F1 over SciFive-Large, the current state-of-the-art on ChemProt. Besides, we find prompt-based learning requires fewer training examples to make reasonable predictions. The results demonstrate the potential of our methods in such a domain-specific relation extraction task."",","{yeh-etal-2022-decorate,
    title = ""Decorate the Examples: A Simple Method of Prompt Design for Biomedical Relation Extraction"",
    author = ""Yeh, Hui-Syuan  and
      Lavergne, Thomas  and
      Zweigenbaum, Pierre"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Thirteenth Language Resources and Evaluation Conference"",
    month = jun,
    year = ""2022"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2022.lrec-1.403"",
    pages = ""3780--3787"",
    abstract = ""Relation extraction is a core problem for natural language processing in the biomedical domain. Recent research on relation extraction showed that prompt-based learning improves the performance on both fine-tuning on full training set and few-shot training. However, less effort has been made on domain-specific tasks where good prompt design can be even harder. In this paper, we investigate prompting for biomedical relation extraction, with experiments on the ChemProt dataset. We present a simple yet effective method to systematically generate comprehensive prompts that reformulate the relation extraction task as a cloze-test task under a simple prompt formulation. In particular, we experiment with different ranking scores for prompt selection. With BioMed-RoBERTa-base, our results show that prompting-based fine-tuning obtains gains by 14.21 F1 over its regular fine-tuning baseline, and 1.14 F1 over SciFive-Large, the current state-of-the-art on ChemProt. Besides, we find prompt-based learning requires fewer training examples to make reasonable predictions. The results demonstrate the potential of our methods in such a domain-specific relation extraction task."",
}
@",medical domain,natural language process,natural languag,language process,generat,relation extract,evalu
" ""{CLISTER} : A Corpus for Semantic Textual Similarity in {F}rench Clinical Narratives"","," ""Modern Natural Language Processing relies on the availability of annotated corpora for training and evaluating models. Such resources are scarce, especially for specialized domains in languages other than English. In particular, there are very few resources for semantic similarity in the clinical domain in French. This can be useful for many biomedical natural language processing applications, including text generation. We introduce a definition of similarity that is guided by clinical facts and apply it to the development of a new French corpus of 1,000 sentence pairs manually annotated according to similarity scores. This new sentence similarity corpus is made freely available to the community. We further evaluate the corpus through experiments of automatic similarity measurement. We show that a model of sentence embeddings can capture similarity with state-of-the-art performance on the DEFT STS shared task evaluation data set (Spearman","{hiebel-etal-2022-clister-corpus,
    title = ""{CLISTER} : A Corpus for Semantic Textual Similarity in {F}rench Clinical Narratives"",
    author = {Hiebel, Nicolas  and
      Ferret, Olivier  and
      Fort, Kar{\""e}n  and
      N{\'e}v{\'e}ol, Aur{\'e}lie},
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Thirteenth Language Resources and Evaluation Conference"",
    month = jun,
    year = ""2022"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2022.lrec-1.459"",
    pages = ""4306--4315"",
    abstract = ""Modern Natural Language Processing relies on the availability of annotated corpora for training and evaluating models. Such resources are scarce, especially for specialized domains in languages other than English. In particular, there are very few resources for semantic similarity in the clinical domain in French. This can be useful for many biomedical natural language processing applications, including text generation. We introduce a definition of similarity that is guided by clinical facts and apply it to the development of a new French corpus of 1,000 sentence pairs manually annotated according to similarity scores. This new sentence similarity corpus is made freely available to the community. We further evaluate the corpus through experiments of automatic similarity measurement. We show that a model of sentence embeddings can capture similarity with state-of-the-art performance on the DEFT STS shared task evaluation data set (Spearman=0.8343). We also show that the corpus is complementary to DEFT STS."",
}
@",clinical narr,clinical domain,natural language process,natural languag,language process,generat,semant,shared task,annotat,evalu
" ""Recovering Patient Journeys: A Corpus of Biomedical Entities and Relations on {T}witter ({BEAR})"","," ""Text mining and information extraction for the medical domain has focused on scientific text generated by researchers. However, their access to individual patient experiences or patient-doctor interactions is limited. On social media, doctors, patients and their relatives also discuss medical information. Individual information provided by laypeople complements the knowledge available in scientific text. It reflects the patient{'}s journey making the value of this type of data twofold: It offers direct access to people{'}s perspectives, and it might cover information that is not available elsewhere, including self-treatment or self-diagnose. Named entity recognition and relation extraction are methods to structure information that is available in unstructured text. However, existing medical social media corpora focused on a comparably small set of entities and relations. In contrast, we provide rich annotation layers to model patients{'} experiences in detail. The corpus consists of medical tweets annotated with a fine-grained set of medical entities and relations between them, namely 14 entity (incl. environmental factors, diagnostics, biochemical processes, patients{'} quality-of-life descriptions, pathogens, medical conditions, and treatments) and 20 relation classes (incl. prevents, influences, interactions, causes). The dataset consists of 2,100 tweets with approx. 6,000 entities and 2,200 relations."",","{wuhrl-klinger-2022-recovering,
    title = ""Recovering Patient Journeys: A Corpus of Biomedical Entities and Relations on {T}witter ({BEAR})"",
    author = {W{\""u}hrl, Amelie  and
      Klinger, Roman},
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Thirteenth Language Resources and Evaluation Conference"",
    month = jun,
    year = ""2022"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2022.lrec-1.472"",
    pages = ""4439--4450"",
    abstract = ""Text mining and information extraction for the medical domain has focused on scientific text generated by researchers. However, their access to individual patient experiences or patient-doctor interactions is limited. On social media, doctors, patients and their relatives also discuss medical information. Individual information provided by laypeople complements the knowledge available in scientific text. It reflects the patient{'}s journey making the value of this type of data twofold: It offers direct access to people{'}s perspectives, and it might cover information that is not available elsewhere, including self-treatment or self-diagnose. Named entity recognition and relation extraction are methods to structure information that is available in unstructured text. However, existing medical social media corpora focused on a comparably small set of entities and relations. In contrast, we provide rich annotation layers to model patients{'} experiences in detail. The corpus consists of medical tweets annotated with a fine-grained set of medical entities and relations between them, namely 14 entity (incl. environmental factors, diagnostics, biochemical processes, patients{'} quality-of-life descriptions, pathogens, medical conditions, and treatments) and 20 relation classes (incl. prevents, influences, interactions, causes). The dataset consists of 2,100 tweets with approx. 6,000 entities and 2,200 relations."",
}
@",medical domain,generat,relation extract,entity recognit,annotat,evalu
" ""Modeling {D}utch Medical Texts for Detecting Functional Categories and Levels of {COVID}-19 Patients"","," ""Electronic Health Records contain a lot of information in natural language that is not expressed in the structured clinical data. Especially in the case of new diseases such as COVID-19, this information is crucial to get a better understanding of patient recovery patterns and factors that may play a role in it. However, the language in these records is very different from standard language and generic natural language processing tools cannot easily be applied out-of-the-box. In this paper, we present a fine-tuned Dutch language model specifically developed for the language in these health records that can determine the functional level of patients according to a standard coding framework from the World Health Organization. We provide evidence that our classification performs at a sufficient level to generate patient recovery patterns that can be used in the future to analyse factors that contribute to the rehabilitation of COVID-19 patients and to predict individual patient recovery of functioning."",","{kim-etal-2022-modeling,
    title = ""Modeling {D}utch Medical Texts for Detecting Functional Categories and Levels of {COVID}-19 Patients"",
    author = ""Kim, Jenia  and
      Verkijk, Stella  and
      Geleijn, Edwin  and
      van der Leeden, Marieke  and
      Meskers, Carel  and
      Meskers, Caroline  and
      van der Veen, Sabina  and
      Vossen, Piek  and
      Widdershoven, Guy"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Thirteenth Language Resources and Evaluation Conference"",
    month = jun,
    year = ""2022"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2022.lrec-1.488"",
    pages = ""4577--4585"",
    abstract = ""Electronic Health Records contain a lot of information in natural language that is not expressed in the structured clinical data. Especially in the case of new diseases such as COVID-19, this information is crucial to get a better understanding of patient recovery patterns and factors that may play a role in it. However, the language in these records is very different from standard language and generic natural language processing tools cannot easily be applied out-of-the-box. In this paper, we present a fine-tuned Dutch language model specifically developed for the language in these health records that can determine the functional level of patients according to a standard coding framework from the World Health Organization. We provide evidence that our classification performs at a sufficient level to generate patient recovery patterns that can be used in the future to analyse factors that contribute to the rehabilitation of COVID-19 patients and to predict individual patient recovery of functioning."",
}
@",electronic health record,health record,medical text,natural language process,natural languag,language process,generat,evalu
" ""Hierarchical Annotation for Building A Suite of Clinical Natural Language Processing Tasks: Progress Note Understanding"","," ""Applying methods in natural language processing on electronic health records (EHR) data has attracted rising interests. Existing corpus and annotation focus on modeling textual features and relation prediction. However, there are a paucity of annotated corpus built to model clinical diagnostic thinking, a processing involving text understanding, domain knowledge abstraction and reasoning. In this work, we introduce a hierarchical annotation schema with three stages to address clinical text understanding, clinical reasoning and summarization. We create an annotated corpus based on a large collection of publicly available daily progress notes, a type of EHR that is time-sensitive, problem-oriented, and well-documented by the format of Subjective, Objective, Assessment and Plan (SOAP). We also define a new suite of tasks, Progress Note Understanding, with three tasks utilizing the three annotation stages. This new suite aims at training and evaluating future NLP models for clinical text understanding, clinical knowledge representation, inference and summarization."",","{gao-etal-2022-hierarchical,
    title = ""Hierarchical Annotation for Building A Suite of Clinical Natural Language Processing Tasks: Progress Note Understanding"",
    author = ""Gao, Yanjun  and
      Dligach, Dmitriy  and
      Miller, Timothy  and
      Tesch, Samuel  and
      Laffin, Ryan  and
      Churpek, Matthew M.  and
      Afshar, Majid"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Thirteenth Language Resources and Evaluation Conference"",
    month = jun,
    year = ""2022"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2022.lrec-1.587"",
    pages = ""5484--5493"",
    abstract = ""Applying methods in natural language processing on electronic health records (EHR) data has attracted rising interests. Existing corpus and annotation focus on modeling textual features and relation prediction. However, there are a paucity of annotated corpus built to model clinical diagnostic thinking, a processing involving text understanding, domain knowledge abstraction and reasoning. In this work, we introduce a hierarchical annotation schema with three stages to address clinical text understanding, clinical reasoning and summarization. We create an annotated corpus based on a large collection of publicly available daily progress notes, a type of EHR that is time-sensitive, problem-oriented, and well-documented by the format of Subjective, Objective, Assessment and Plan (SOAP). We also define a new suite of tasks, Progress Note Understanding, with three tasks utilizing the three annotation stages. This new suite aims at training and evaluating future NLP models for clinical text understanding, clinical knowledge representation, inference and summarization."",
}
@",electronic health record,health record,clinical text,natural language process,natural languag,language process,nlp,infer,summar,annotat,evalu,assess
" ""{C}hi{MST}: A {C}hinese Medical Corpus for Word Segmentation and Medical Term Recognition"","," ""Chinese word segmentation (CWS) and named entity recognition (NER) are two important tasks in Chinese natural language processing. To achieve good model performance on these tasks, existing neural approaches normally require a large amount of labeled training data, which is often unavailable for specific domains such as the Chinese medical domain due to privacy and legal issues. To address this problem, we have developed a Chinese medical corpus named ChiMST which consists of question-answer pairs collected from an online medical healthcare platform and is annotated with word boundary and medical term information. For word boundary, we mainly follow the word segmentation guidelines for the Penn Chinese Treebank (Xia, 2000); for medical terms, we define 9 categories and 18 sub-categories after consulting medical experts. To provide baselines on this corpus, we train existing state-of-the-art models on it and achieve good performance. We believe that the corpus and the baseline systems will be a valuable resource for CWS and NER research on the medical domain."",","{tian-etal-2022-chimst,
    title = ""{C}hi{MST}: A {C}hinese Medical Corpus for Word Segmentation and Medical Term Recognition"",
    author = ""Tian, Yuanhe  and
      Qin, Han  and
      Xia, Fei  and
      Song, Yan"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Thirteenth Language Resources and Evaluation Conference"",
    month = jun,
    year = ""2022"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2022.lrec-1.607"",
    pages = ""5654--5664"",
    abstract = ""Chinese word segmentation (CWS) and named entity recognition (NER) are two important tasks in Chinese natural language processing. To achieve good model performance on these tasks, existing neural approaches normally require a large amount of labeled training data, which is often unavailable for specific domains such as the Chinese medical domain due to privacy and legal issues. To address this problem, we have developed a Chinese medical corpus named ChiMST which consists of question-answer pairs collected from an online medical healthcare platform and is annotated with word boundary and medical term information. For word boundary, we mainly follow the word segmentation guidelines for the Penn Chinese Treebank (Xia, 2000); for medical terms, we define 9 categories and 18 sub-categories after consulting medical experts. To provide baselines on this corpus, we train existing state-of-the-art models on it and achieve good performance. We believe that the corpus and the baseline systems will be a valuable resource for CWS and NER research on the medical domain."",
}
@",medical domain,natural language process,natural languag,language process,entity recognit,annotat,question-answ,evalu
" ""{S}u{M}e: A Dataset Towards Summarizing Biomedical Mechanisms"","," ""Can language models read biomedical texts and explain the biomedical mechanisms discussed? In this work we introduce a biomedical mechanism summarization task. Biomedical studies often investigate the mechanisms behind how one entity (e.g., a protein or a chemical) affects another in a biological context. The abstracts of these publications often include a focused set of sentences that present relevant supporting statements regarding such relationships, associated experimental evidence, and a concluding sentence that summarizes the mechanism underlying the relationship. We leverage this structure and create a summarization task, where the input is a collection of sentences and the main entities in an abstract, and the output includes the relationship and a sentence that summarizes the mechanism. Using a small amount of manually labeled mechanism sentences, we train a mechanism sentence classifier to filter a large biomedical abstract collection and create a summarization dataset with 22k instances. We also introduce conclusion sentence generation as a pretraining task with 611k instances. We benchmark the performance of large bio-domain language models. We find that while the pretraining task help improves performance, the best model produces acceptable mechanism outputs in only 32{\%} of the instances, which shows the task presents significant challenges in biomedical language understanding and summarization."",","{bastan-etal-2022-sume,
    title = ""{S}u{M}e: A Dataset Towards Summarizing Biomedical Mechanisms"",
    author = ""Bastan, Mohaddeseh  and
      Shankar, Nishant  and
      Surdeanu, Mihai  and
      Balasubramanian, Niranjan"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Thirteenth Language Resources and Evaluation Conference"",
    month = jun,
    year = ""2022"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2022.lrec-1.748"",
    pages = ""6922--6931"",
    abstract = ""Can language models read biomedical texts and explain the biomedical mechanisms discussed? In this work we introduce a biomedical mechanism summarization task. Biomedical studies often investigate the mechanisms behind how one entity (e.g., a protein or a chemical) affects another in a biological context. The abstracts of these publications often include a focused set of sentences that present relevant supporting statements regarding such relationships, associated experimental evidence, and a concluding sentence that summarizes the mechanism underlying the relationship. We leverage this structure and create a summarization task, where the input is a collection of sentences and the main entities in an abstract, and the output includes the relationship and a sentence that summarizes the mechanism. Using a small amount of manually labeled mechanism sentences, we train a mechanism sentence classifier to filter a large biomedical abstract collection and create a summarization dataset with 22k instances. We also introduce conclusion sentence generation as a pretraining task with 611k instances. We benchmark the performance of large bio-domain language models. We find that while the pretraining task help improves performance, the best model produces acceptable mechanism outputs in only 32{\%} of the instances, which shows the task presents significant challenges in biomedical language understanding and summarization."",
}
@",medical text,generat,summar,challeng,benchmark,evalu
" ""Assessing the Limits of Straightforward Models for Nested Named Entity Recognition in {S}panish Clinical Narratives"","," ""Nested Named Entity Recognition (NER) is an information extraction task that aims to identify entities that may be nested within other entity mentions. Despite the availability of several corpora with nested entities in the Spanish clinical domain, most previous work has overlooked them due to the lack of models and a clear annotation scheme for dealing with the task. To fill this gap, this paper provides an empirical study of straightforward methods for tackling the nested NER task on two Spanish clinical datasets, Clinical Trials, and the Chilean Waiting List. We assess the advantages and limitations of two sequence labeling approaches; one based on Multiple LSTM-CRF architectures and another on Joint labeling models. To better understand the differences between these models, we compute task-specific metrics that adequately measure the ability of models to detect nested entities and perform a fine-grained comparison across models. Our experimental results show that employing domain-specific language models trained from scratch significantly improves the performance obtained with strong domain-specific and general-domain baselines, achieving state-of-the-art results in both datasets. Specifically, we obtained F1 scores of 89.21 and 83.16 in Clinical Trials and the Chilean Waiting List, respectively. Interestingly enough, we observe that the task-specific metrics and analysis properly reflect the limitations of the models when recognizing nested entities. Finally, we perform a case study on an aggregated NER dataset created from several clinical corpora in Spanish. We highlight how entity length and the simultaneous recognition of inner and outer entities are the most critical variables for the nested NER task."",","{rojas-etal-2022-assessing,
    title = ""Assessing the Limits of Straightforward Models for Nested Named Entity Recognition in {S}panish Clinical Narratives"",
    author = ""Rojas, Matias  and
      Carrino, Casimiro Pio  and
      Gonzalez-Agirre, Aitor  and
      Dunstan, Jocelyn  and
      Villegas, Marta"",
    editor = ""Lavelli, Alberto  and
      Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI)"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.louhi-1.2"",
    doi = ""10.18653/v1/2022.louhi-1.2"",
    pages = ""14--25"",
    abstract = ""Nested Named Entity Recognition (NER) is an information extraction task that aims to identify entities that may be nested within other entity mentions. Despite the availability of several corpora with nested entities in the Spanish clinical domain, most previous work has overlooked them due to the lack of models and a clear annotation scheme for dealing with the task. To fill this gap, this paper provides an empirical study of straightforward methods for tackling the nested NER task on two Spanish clinical datasets, Clinical Trials, and the Chilean Waiting List. We assess the advantages and limitations of two sequence labeling approaches; one based on Multiple LSTM-CRF architectures and another on Joint labeling models. To better understand the differences between these models, we compute task-specific metrics that adequately measure the ability of models to detect nested entities and perform a fine-grained comparison across models. Our experimental results show that employing domain-specific language models trained from scratch significantly improves the performance obtained with strong domain-specific and general-domain baselines, achieving state-of-the-art results in both datasets. Specifically, we obtained F1 scores of 89.21 and 83.16 in Clinical Trials and the Chilean Waiting List, respectively. Interestingly enough, we observe that the task-specific metrics and analysis properly reflect the limitations of the models when recognizing nested entities. Finally, we perform a case study on an aggregated NER dataset created from several clinical corpora in Spanish. We highlight how entity length and the simultaneous recognition of inner and outer entities are the most critical variables for the nested NER task."",
}
@",clinical narr,clinical domain,entity recognit,sequence label,annotat,assess
" ""Can Current Explainability Help Provide References in Clinical Notes to Support Humans Annotate Medical Codes?"","," ""The medical codes prediction problem from clinical notes has received substantial interest in the NLP community, and several recent studies have shown the state-of-the-art (SOTA) code prediction results of full-fledged deep learning-based methods. However, most previous SOTA works based on deep learning are still in early stages in terms of providing textual references and explanations of the predicted codes, despite the fact that this level of explainability of the prediction outcomes is critical to gaining trust from professional medical coders. This raises the important question of how well current explainability methods apply to advanced neural network models such as transformers to predict correct codes and present references in clinical notes that support code prediction. First, we present an explainable Read, Attend, and Code (xRAC) framework and assess two approaches, attention score-based xRAC-ATTN and model-agnostic knowledge-distillation-based xRAC-KD, through simplified but thorough human-grounded evaluations with SOTA transformer-based model, RAC. We find that the supporting evidence text highlighted by xRAC-ATTN is of higher quality than xRAC-KD whereas xRAC-KD has potential advantages in production deployment scenarios. More importantly, we show for the first time that, given the current state of explainability methodologies, using the SOTA medical codes prediction system still requires the expertise and competencies of professional coders, even though its prediction accuracy is superior to that of human coders. This, we believe, is a very meaningful step toward developing explainable and accurate machine learning systems for fully autonomous medical code prediction from clinical notes."",","{kim-etal-2022-current,
    title = ""Can Current Explainability Help Provide References in Clinical Notes to Support Humans Annotate Medical Codes?"",
    author = ""Kim, Byung-Hak  and
      Deng, Zhongfen  and
      Yu, Philip  and
      Ganapathi, Varun"",
    editor = ""Lavelli, Alberto  and
      Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI)"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.louhi-1.3"",
    doi = ""10.18653/v1/2022.louhi-1.3"",
    pages = ""26--34"",
    abstract = ""The medical codes prediction problem from clinical notes has received substantial interest in the NLP community, and several recent studies have shown the state-of-the-art (SOTA) code prediction results of full-fledged deep learning-based methods. However, most previous SOTA works based on deep learning are still in early stages in terms of providing textual references and explanations of the predicted codes, despite the fact that this level of explainability of the prediction outcomes is critical to gaining trust from professional medical coders. This raises the important question of how well current explainability methods apply to advanced neural network models such as transformers to predict correct codes and present references in clinical notes that support code prediction. First, we present an explainable Read, Attend, and Code (xRAC) framework and assess two approaches, attention score-based xRAC-ATTN and model-agnostic knowledge-distillation-based xRAC-KD, through simplified but thorough human-grounded evaluations with SOTA transformer-based model, RAC. We find that the supporting evidence text highlighted by xRAC-ATTN is of higher quality than xRAC-KD whereas xRAC-KD has potential advantages in production deployment scenarios. More importantly, we show for the first time that, given the current state of explainability methodologies, using the SOTA medical codes prediction system still requires the expertise and competencies of professional coders, even though its prediction accuracy is superior to that of human coders. This, we believe, is a very meaningful step toward developing explainable and accurate machine learning systems for fully autonomous medical code prediction from clinical notes."",
}
@",clinical not,nlp,annotat,evalu,assess
" ""Building a Clinically-Focused Problem List From Medical Notes"","," ""Clinical notes often contain useful information not documented in structured data, but their unstructured nature can lead to critical patient-related information being missed. To increase the likelihood that this valuable information is utilized for patient care, algorithms that summarize notes into a problem list have been proposed. Focused on identifying medically-relevant entities in the free-form text, these solutions are often detached from a canonical ontology and do not allow downstream use of the detected text-spans. Mitigating these issues, we present here a system for generating a canonical problem list from medical notes, consisting of two major stages. At the first stage, annotation, we use a transformer model to detect all clinical conditions which are mentioned in a single note. These clinical conditions are then grounded to a predefined ontology, and are linked to spans in the text. At the second stage, summarization, we develop a novel algorithm that aggregates over the set of clinical conditions detected on all of the patient{'}s notes, and produce a concise patient summary that organizes their most important conditions."",","{feder-etal-2022-building,
    title = ""Building a Clinically-Focused Problem List From Medical Notes"",
    author = ""Feder, Amir  and
      Laish, Itay  and
      Agarwal, Shashank  and
      Lerner, Uri  and
      Atias, Avel  and
      Cheung, Cathy  and
      Clardy, Peter  and
      Peled-Cohen, Alon  and
      Fellinger, Rachana  and
      Liu, Hengrui  and
      Huong Nguyen, Lan  and
      Patel, Birju  and
      Potikha, Natan  and
      Taubenfeld, Amir  and
      Xu, Liwen  and
      Yang, Seung Doo  and
      Benjamini, Ayelet  and
      Hassidim, Avinatan"",
    editor = ""Lavelli, Alberto  and
      Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI)"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.louhi-1.8"",
    doi = ""10.18653/v1/2022.louhi-1.8"",
    pages = ""60--68"",
    abstract = ""Clinical notes often contain useful information not documented in structured data, but their unstructured nature can lead to critical patient-related information being missed. To increase the likelihood that this valuable information is utilized for patient care, algorithms that summarize notes into a problem list have been proposed. Focused on identifying medically-relevant entities in the free-form text, these solutions are often detached from a canonical ontology and do not allow downstream use of the detected text-spans. Mitigating these issues, we present here a system for generating a canonical problem list from medical notes, consisting of two major stages. At the first stage, annotation, we use a transformer model to detect all clinical conditions which are mentioned in a single note. These clinical conditions are then grounded to a predefined ontology, and are linked to spans in the text. At the second stage, summarization, we develop a novel algorithm that aggregates over the set of clinical conditions detected on all of the patient{'}s notes, and produce a concise patient summary that organizes their most important conditions."",
}
@",clinical not,generat,summar,annotat
" ""{B}io{S}im{CSE}: {B}io{M}edical Sentence Embeddings using Contrastive learning"","," ""Sentence embeddings in the form of fixed-size vectors that capture the information in the sentence as well as the context are critical components of Natural Language Processing systems. With transformer model based sentence encoders outperforming the other sentence embedding methods in the general domain, we explore the transformer based architectures to generate dense sentence embeddings in the biomedical domain. In this work, we present BioSimCSE, where we train sentence embeddings with domain specific transformer based models with biomedical texts. We assess our model{'}s performance with zero-shot and fine-tuned settings on Semantic Textual Similarity (STS) and Recognizing Question Entailment (RQE) tasks. Our BioSimCSE model using BioLinkBERT achieves state of the art (SOTA) performance on both tasks."",","{kanakarajan-etal-2022-biosimcse,
    title = ""{B}io{S}im{CSE}: {B}io{M}edical Sentence Embeddings using Contrastive learning"",
    author = ""Kanakarajan, Kamal raj  and
      Kundumani, Bhuvana  and
      Abraham, Abhijith  and
      Sankarasubbu, Malaikannan"",
    editor = ""Lavelli, Alberto  and
      Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI)"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.louhi-1.10"",
    doi = ""10.18653/v1/2022.louhi-1.10"",
    pages = ""81--86"",
    abstract = ""Sentence embeddings in the form of fixed-size vectors that capture the information in the sentence as well as the context are critical components of Natural Language Processing systems. With transformer model based sentence encoders outperforming the other sentence embedding methods in the general domain, we explore the transformer based architectures to generate dense sentence embeddings in the biomedical domain. In this work, we present BioSimCSE, where we train sentence embeddings with domain specific transformer based models with biomedical texts. We assess our model{'}s performance with zero-shot and fine-tuned settings on Semantic Textual Similarity (STS) and Recognizing Question Entailment (RQE) tasks. Our BioSimCSE model using BioLinkBERT achieves state of the art (SOTA) performance on both tasks."",
}
@",medical domain,medical text,natural language process,natural languag,language process,generat,semant,assess
" ""Parameter Efficient Transfer Learning for Suicide Attempt and Ideation Detection"","," ""Pre-trained language models (LMs) have been deployed as the state-of-the-art natural language processing (NLP) approaches for multiple clinical applications. Model generalisability is important in clinical domain due to the low available resources. In this study, we evaluated transfer learning techniques for an important clinical application: detecting suicide attempt (SA) and suicide ideation (SI) in electronic health records (EHRs). Using the annotation guideline provided by the authors of ScAN, we annotated two EHR datasets from different hospitals. We then fine-tuned ScANER, a publicly available SA and SI detection model, to evaluate five different parameter efficient transfer learning techniques, such as adapter-based learning and soft-prompt tuning, on the two datasets. Without any fine-tuning, ScANER achieve macro F1-scores of 0.85 and 0.87 for SA and SI evidence detection across the two datasets. We observed that by fine-tuning less than {\textasciitilde}2{\%} of ScANER{'}s parameters, we were able to further improve the macro F1-score for SA-SI evidence detection by 3{\%} and 5{\%} for the two EHR datasets. Our results show that parameter-efficient transfer learning methods can help improve the performance of publicly available clinical models on new hospital datasets with few annotations."",","{singh-rawat-yu-2022-parameter,
    title = ""Parameter Efficient Transfer Learning for Suicide Attempt and Ideation Detection"",
    author = ""Singh Rawat, Bhanu Pratap  and
      Yu, Hong"",
    editor = ""Lavelli, Alberto  and
      Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI)"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.louhi-1.13"",
    doi = ""10.18653/v1/2022.louhi-1.13"",
    pages = ""108--115"",
    abstract = ""Pre-trained language models (LMs) have been deployed as the state-of-the-art natural language processing (NLP) approaches for multiple clinical applications. Model generalisability is important in clinical domain due to the low available resources. In this study, we evaluated transfer learning techniques for an important clinical application: detecting suicide attempt (SA) and suicide ideation (SI) in electronic health records (EHRs). Using the annotation guideline provided by the authors of ScAN, we annotated two EHR datasets from different hospitals. We then fine-tuned ScANER, a publicly available SA and SI detection model, to evaluate five different parameter efficient transfer learning techniques, such as adapter-based learning and soft-prompt tuning, on the two datasets. Without any fine-tuning, ScANER achieve macro F1-scores of 0.85 and 0.87 for SA and SI evidence detection across the two datasets. We observed that by fine-tuning less than {\textasciitilde}2{\%} of ScANER{'}s parameters, we were able to further improve the macro F1-score for SA-SI evidence detection by 3{\%} and 5{\%} for the two EHR datasets. Our results show that parameter-efficient transfer learning methods can help improve the performance of publicly available clinical models on new hospital datasets with few annotations."",
}
@",electronic health record,health record,clinical domain,natural language process,natural languag,language process,nlp,annotat,evalu
" ""Divide and Conquer: An Extreme Multi-Label Classification Approach for Coding Diseases and Procedures in {S}panish"","," ""Clinical coding is the task of transforming medical documents into structured codes following a standard ontology. Since these terminologies are composed of hundreds of codes, this problem can be considered an Extreme Multi-label Classification task. This paper proposes a novel neural network-based architecture for clinical coding. First, we take full advantage of the hierarchical nature of ontologies to create clusters based on semantic relations. Then, we use a Matcher module to assign the probability of documents belonging to each cluster. Finally, the Ranker calculates the probability of each code considering only the documents in the cluster. This division allows a fine-grained differentiation within the cluster, which cannot be addressed using a single classifier. In addition, since most of the previous work has focused on solving this task in English, we conducted our experiments on three clinical coding corpora in Spanish. The experimental results demonstrate the effectiveness of our model, achieving state-of-the-art results on two of the three datasets. Specifically, we outperformed previous models on two subtasks of the CodiEsp shared task: CodiEsp-D (diseases) and CodiEsp-P (procedures). Automatic coding can profoundly impact healthcare by structuring critical information written in free text in electronic health records."",","{barros-etal-2022-divide,
    title = ""Divide and Conquer: An Extreme Multi-Label Classification Approach for Coding Diseases and Procedures in {S}panish"",
    author = ""Barros, Jose  and
      Rojas, Matias  and
      Dunstan, Jocelyn  and
      Abeliuk, Andres"",
    editor = ""Lavelli, Alberto  and
      Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI)"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.louhi-1.16"",
    doi = ""10.18653/v1/2022.louhi-1.16"",
    pages = ""138--147"",
    abstract = ""Clinical coding is the task of transforming medical documents into structured codes following a standard ontology. Since these terminologies are composed of hundreds of codes, this problem can be considered an Extreme Multi-label Classification task. This paper proposes a novel neural network-based architecture for clinical coding. First, we take full advantage of the hierarchical nature of ontologies to create clusters based on semantic relations. Then, we use a Matcher module to assign the probability of documents belonging to each cluster. Finally, the Ranker calculates the probability of each code considering only the documents in the cluster. This division allows a fine-grained differentiation within the cluster, which cannot be addressed using a single classifier. In addition, since most of the previous work has focused on solving this task in English, we conducted our experiments on three clinical coding corpora in Spanish. The experimental results demonstrate the effectiveness of our model, achieving state-of-the-art results on two of the three datasets. Specifically, we outperformed previous models on two subtasks of the CodiEsp shared task: CodiEsp-D (diseases) and CodiEsp-P (procedures). Automatic coding can profoundly impact healthcare by structuring critical information written in free text in electronic health records."",
}
@",electronic health record,health record,semant,shared task
" ""A Knowledge-Graph-Based Intrinsic Test for Benchmarking Medical Concept Embeddings and Pretrained Language Models"","," ""Using language models created from large data sources has improved the performance of several deep learning-based architectures, obtaining state-of-the-art results in several NLP extrinsic tasks. However, little research is related to creating intrinsic tests that allow us to compare the quality of different language models when obtaining contextualized embeddings. This gap increases even more when working on specific domains in languages other than English. This paper proposes a novel graph-based intrinsic test that allows us to measure the quality of different language models in clinical and biomedical domains in Spanish. Our results show that our intrinsic test performs better for clinical and biomedical language models than a general one. Also, it correlates with better outcomes for a NER task using a probing model over contextualized embeddings. We hope our work will help the clinical NLP research community to evaluate and compare new language models in other languages and find the most suitable models for solving downstream tasks."",","{aracena-etal-2022-knowledge,
    title = ""A Knowledge-Graph-Based Intrinsic Test for Benchmarking Medical Concept Embeddings and Pretrained Language Models"",
    author = ""Aracena, Claudio  and
      Villena, Fabi{\'a}n  and
      Rojas, Matias  and
      Dunstan, Jocelyn"",
    editor = ""Lavelli, Alberto  and
      Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI)"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.louhi-1.22"",
    doi = ""10.18653/v1/2022.louhi-1.22"",
    pages = ""197--206"",
    abstract = ""Using language models created from large data sources has improved the performance of several deep learning-based architectures, obtaining state-of-the-art results in several NLP extrinsic tasks. However, little research is related to creating intrinsic tests that allow us to compare the quality of different language models when obtaining contextualized embeddings. This gap increases even more when working on specific domains in languages other than English. This paper proposes a novel graph-based intrinsic test that allows us to measure the quality of different language models in clinical and biomedical domains in Spanish. Our results show that our intrinsic test performs better for clinical and biomedical language models than a general one. Also, it correlates with better outcomes for a NER task using a probing model over contextualized embeddings. We hope our work will help the clinical NLP research community to evaluate and compare new language models in other languages and find the most suitable models for solving downstream tasks."",
}
@",medical domain,nlp,benchmark,evalu
" ""Augmented Bio-{SBERT}: Improving Performance for Pairwise Sentence Tasks in Bio-medical Domain"","," ""One of the modern challenges in AI is the access to high-quality and annotated data, especially in NLP; that is why augmentation is gaining importance. In computer vision, where image data augmentation is standard, text data augmentation in NLP is complex due to the high complexity of language. Moreover, we have seen the advantages of augmentation where there are fewer data available, which can significantly improve the model{'}s accuracy and performance. We have implemented Augmentation in Pairwise sentence scoring in the biomedical domain. By experimenting with our approach to downstream tasks on biomedical data, we have looked into the solution to improve Bi-encoders{'} sentence transformer performance using an augmented dataset generated by cross-encoders fine-tuned on Biosses and MedNLI on the pre-trained Bio-BERT model. It has significantly improved the results with respect to the model only trained on Gold data for the respective tasks."",","{pankaj-gautam-2022-augmented,
    title = ""Augmented Bio-{SBERT}: Improving Performance for Pairwise Sentence Tasks in Bio-medical Domain"",
    author = ""Pankaj, Sonam  and
      Gautam, Amit"",
    editor = ""Ojha, Atul Kr.  and
      Liu, Chao-Hong  and
      Vylomova, Ekaterina  and
      Abbott, Jade  and
      Washington, Jonathan  and
      Oco, Nathaniel  and
      Pirinen, Tommi A  and
      Malykh, Valentin  and
      Logacheva, Varvara  and
      Zhao, Xiaobing"",
    booktitle = ""Proceedings of the Fifth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2022)"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.loresmt-1.6"",
    pages = ""43--47"",
    abstract = ""One of the modern challenges in AI is the access to high-quality and annotated data, especially in NLP; that is why augmentation is gaining importance. In computer vision, where image data augmentation is standard, text data augmentation in NLP is complex due to the high complexity of language. Moreover, we have seen the advantages of augmentation where there are fewer data available, which can significantly improve the model{'}s accuracy and performance. We have implemented Augmentation in Pairwise sentence scoring in the biomedical domain. By experimenting with our approach to downstream tasks on biomedical data, we have looked into the solution to improve Bi-encoders{'} sentence transformer performance using an augmented dataset generated by cross-encoders fine-tuned on Biosses and MedNLI on the pre-trained Bio-BERT model. It has significantly improved the results with respect to the model only trained on Gold data for the respective tasks."",
}
@",medical domain,translat,nlp,generat,annotat,challeng
" ""Cross-Clinic De-Identification of {S}wedish Electronic Health Records: Nuances and Caveats"","," ""Privacy preservation of sensitive information is one of the main concerns in clinical text mining. Due to the inherent privacy risks of handling clinical data, the clinical corpora used to create the clinical Named Entity Recognition (NER) models underlying clinical de-identification systems cannot be shared. This situation implies that clinical NER models are trained and tested on data originating from the same institution since it is rarely possible to evaluate them on data belonging to a different organization. These restrictions on sharing make it very difficult to assess whether a clinical NER model has overfitted the data or if it has learned any undetected biases. This paper presents the results of the first-ever cross-institution evaluation of a Swedish de-identification system on Swedish clinical data. Alongside the encouraging results, we discuss differences and similarities across EHR naming conventions and NER tagsets."",","{bridal-etal-2022-cross,
    title = ""Cross-Clinic De-Identification of {S}wedish Electronic Health Records: Nuances and Caveats"",
    author = ""Bridal, Olle  and
      Vakili, Thomas  and
      Santini, Marina"",
    editor = ""Siegert, Ingo  and
      Rigault, Mickael  and
      Arranz, Victoria"",
    booktitle = ""Proceedings of the Workshop on Ethical and Legal Issues in Human Language Technologies and Multilingual De-Identification of Sensitive Data In Language Resources within the 13th Language Resources and Evaluation Conference"",
    month = jun,
    year = ""2022"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2022.legal-1.10"",
    pages = ""49--52"",
    abstract = ""Privacy preservation of sensitive information is one of the main concerns in clinical text mining. Due to the inherent privacy risks of handling clinical data, the clinical corpora used to create the clinical Named Entity Recognition (NER) models underlying clinical de-identification systems cannot be shared. This situation implies that clinical NER models are trained and tested on data originating from the same institution since it is rarely possible to evaluate them on data belonging to a different organization. These restrictions on sharing make it very difficult to assess whether a clinical NER model has overfitted the data or if it has learned any undetected biases. This paper presents the results of the first-ever cross-institution evaluation of a Swedish de-identification system on Swedish clinical data. Alongside the encouraging results, we discuss differences and similarities across EHR naming conventions and NER tagsets."",
}
@",electronic health record,health record,clinical text,entity recognit,evalu,assess
" ""Efficient Joint Learning for Clinical Named Entity Recognition and Relation Extraction Using {F}ourier Networks:A Use Case in Adverse Drug Events"","," ""Current approaches for clinical information extraction are inefficient in terms of computational costs and memory consumption, hindering their application to process large-scale electronic health records (EHRs). We propose an efficient end-to-end model, the Joint-NER-RE-Fourier (JNRF), to jointly learn the tasks of named entity recognition and relation extraction for documents of variable length. The architecture uses positional encoding and unitary batch sizes to process variable length documents and uses a weight-shared Fourier network layer for low-complexity token mixing. Finally, we reach the theoretical computational complexity lower bound for relation extraction using a selective pooling strategy and distance-aware attention weights with trainable polynomial distance functions. We evaluated the JNRF architecture using the 2018 N2C2 ADE benchmark to jointly extract medication-related entities and relations in variable-length EHR summaries. JNRF outperforms rolling window BERT with selective pooling by 0.42{\%}, while being twice as fast to train. Compared to state-of-the-art BiLSTM-CRF architectures on the N2C2 ADE benchmark, results show that the proposed approach trains 22 times faster and reduces GPU memory consumption by 1.75 folds, with a reasonable performance tradeoff of 90{\%}, without the use of external tools, hand-crafted rules or post-processing. Given the significant carbon footprint of deep learning models and the current energy crises, these methods could support efficient and cleaner information extraction in EHRs and other types of large-scale document databases."",","{yazdani-etal-2022-efficient,
    title = ""Efficient Joint Learning for Clinical Named Entity Recognition and Relation Extraction Using {F}ourier Networks:A Use Case in Adverse Drug Events"",
    author = ""Yazdani, Anthony  and
      Proios, Dimitrios  and
      Rouhizadeh, Hossein  and
      Teodoro, Douglas"",
    editor = ""Akhtar, Md. Shad  and
      Chakraborty, Tanmoy"",
    booktitle = ""Proceedings of the 19th International Conference on Natural Language Processing (ICON)"",
    month = dec,
    year = ""2022"",
    address = ""New Delhi, India"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.icon-main.27"",
    pages = ""212--223"",
    abstract = ""Current approaches for clinical information extraction are inefficient in terms of computational costs and memory consumption, hindering their application to process large-scale electronic health records (EHRs). We propose an efficient end-to-end model, the Joint-NER-RE-Fourier (JNRF), to jointly learn the tasks of named entity recognition and relation extraction for documents of variable length. The architecture uses positional encoding and unitary batch sizes to process variable length documents and uses a weight-shared Fourier network layer for low-complexity token mixing. Finally, we reach the theoretical computational complexity lower bound for relation extraction using a selective pooling strategy and distance-aware attention weights with trainable polynomial distance functions. We evaluated the JNRF architecture using the 2018 N2C2 ADE benchmark to jointly extract medication-related entities and relations in variable-length EHR summaries. JNRF outperforms rolling window BERT with selective pooling by 0.42{\%}, while being twice as fast to train. Compared to state-of-the-art BiLSTM-CRF architectures on the N2C2 ADE benchmark, results show that the proposed approach trains 22 times faster and reduces GPU memory consumption by 1.75 folds, with a reasonable performance tradeoff of 90{\%}, without the use of external tools, hand-crafted rules or post-processing. Given the significant carbon footprint of deep learning models and the current energy crises, these methods could support efficient and cleaner information extraction in EHRs and other types of large-scale document databases."",
}
@",electronic health record,health record,natural language process,natural languag,language process,relation extract,entity recognit,summar,benchmark,evalu
" ""Knowledge-Rich Self-Supervision for Biomedical Entity Linking"","," ""Entity linking faces significant challenges such as prolific variations and prevalent ambiguities, especially in high-value domains with myriad entities. Standard classification approaches suffer from the annotation bottleneck and cannot effectively handle unseen entities. Zero-shot entity linking has emerged as a promising direction for generalizing to new entities, but it still requires example gold entity mentions during training and canonical descriptions for all entities, both of which are rarely available outside of Wikipedia. In this paper, we explore Knowledge-RIch Self-Supervision (KRISS) for biomedical entity linking, by leveraging readily available domain knowledge. In training, it generates self-supervised mention examples on unlabeled text using a domain ontology and trains a contextual encoder using contrastive learning. For inference, it samples self-supervised mentions as prototypes for each entity and conducts linking by mapping the test mention to the most similar prototype. Our approach can easily incorporate entity descriptions and gold mention labels if available. We conducted extensive experiments on seven standard datasets spanning biomedical literature and clinical notes. Without using any labeled information, our method produces KRISSBERT, a universal entity linker for four million UMLS entities that attains new state of the art, outperforming prior self-supervised methods by as much as 20 absolute points in accuracy. We released KRISSBERT at \url{https://aka.ms/krissbert}."",","{zhang-etal-2022-knowledge,
    title = ""Knowledge-Rich Self-Supervision for Biomedical Entity Linking"",
    author = ""Zhang, Sheng  and
      Cheng, Hao  and
      Vashishth, Shikhar  and
      Wong, Cliff  and
      Xiao, Jinfeng  and
      Liu, Xiaodong  and
      Naumann, Tristan  and
      Gao, Jianfeng  and
      Poon, Hoifung"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2022"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.findings-emnlp.61"",
    doi = ""10.18653/v1/2022.findings-emnlp.61"",
    pages = ""868--880"",
    abstract = ""Entity linking faces significant challenges such as prolific variations and prevalent ambiguities, especially in high-value domains with myriad entities. Standard classification approaches suffer from the annotation bottleneck and cannot effectively handle unseen entities. Zero-shot entity linking has emerged as a promising direction for generalizing to new entities, but it still requires example gold entity mentions during training and canonical descriptions for all entities, both of which are rarely available outside of Wikipedia. In this paper, we explore Knowledge-RIch Self-Supervision (KRISS) for biomedical entity linking, by leveraging readily available domain knowledge. In training, it generates self-supervised mention examples on unlabeled text using a domain ontology and trains a contextual encoder using contrastive learning. For inference, it samples self-supervised mentions as prototypes for each entity and conducts linking by mapping the test mention to the most similar prototype. Our approach can easily incorporate entity descriptions and gold mention labels if available. We conducted extensive experiments on seven standard datasets spanning biomedical literature and clinical notes. Without using any labeled information, our method produces KRISSBERT, a universal entity linker for four million UMLS entities that attains new state of the art, outperforming prior self-supervised methods by as much as 20 absolute points in accuracy. We released KRISSBERT at \url{https://aka.ms/krissbert}."",
}
@",clinical not,nlp,infer,generat,annotat,challeng
" ""Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot {ICD} Coding"","," ""Automatic International Classification of Diseases (ICD) coding aims to assign multiple ICD codes to a medical note with average length of 3,000+ tokens. This task is challenging due to a high-dimensional space of multi-label assignment (tens of thousands of ICD codes) and the long-tail challenge: only a few codes (common diseases) are frequently assigned while most codes (rare diseases) are infrequently assigned. This study addresses the long-tail challenge by adapting a prompt-based fine-tuning technique with label semantics, which has been shown to be effective under few-shot setting. To further enhance the performance in medical domain, we propose a knowledge-enhanced longformer by injecting three domain-specific knowledge: hierarchy, synonym, and abbreviation with additional pretraining using contrastive learning. Experiments on MIMIC-III-full, a benchmark dataset of code assignment, show that our proposed method outperforms previous state-of-the-art method in 14.5{\%} in marco F1 (from 10.3 to 11.8, P{\textless}0.001). To further test our model on few-shot setting, we created a new rare diseases coding dataset, MIMIC-III-rare50, on which our model improves marco F1 from 17.1 to 30.4 and micro F1 from 17.2 to 32.6 compared to previous method."",","{yang-etal-2022-knowledge-injected,
    title = ""Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot {ICD} Coding"",
    author = ""Yang, Zhichao  and
      Wang, Shufan  and
      Rawat, Bhanu Pratap Singh  and
      Mitra, Avijit  and
      Yu, Hong"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2022"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.findings-emnlp.127"",
    doi = ""10.18653/v1/2022.findings-emnlp.127"",
    pages = ""1767--1781"",
    abstract = ""Automatic International Classification of Diseases (ICD) coding aims to assign multiple ICD codes to a medical note with average length of 3,000+ tokens. This task is challenging due to a high-dimensional space of multi-label assignment (tens of thousands of ICD codes) and the long-tail challenge: only a few codes (common diseases) are frequently assigned while most codes (rare diseases) are infrequently assigned. This study addresses the long-tail challenge by adapting a prompt-based fine-tuning technique with label semantics, which has been shown to be effective under few-shot setting. To further enhance the performance in medical domain, we propose a knowledge-enhanced longformer by injecting three domain-specific knowledge: hierarchy, synonym, and abbreviation with additional pretraining using contrastive learning. Experiments on MIMIC-III-full, a benchmark dataset of code assignment, show that our proposed method outperforms previous state-of-the-art method in 14.5{\%} in marco F1 (from 10.3 to 11.8, P{\textless}0.001). To further test our model on few-shot setting, we created a new rare diseases coding dataset, MIMIC-III-rare50, on which our model improves marco F1 from 17.1 to 30.4 and micro F1 from 17.2 to 32.6 compared to previous method."",
}
@",medical domain,nlp,semant,challeng,benchmark
" ""P$\text{M}^2\text{F}^2${N}: Patient Multi-view Multi-modal Feature Fusion Networks for Clinical Outcome Prediction"","," ""Clinical outcome prediction is critical to the condition prediction of patients and management of hospital capacities. There are two kinds of medical data, including time series signals recorded by various devices and clinical notes in electronic health records (EHR), which are used for two common prediction targets: mortality and length of stay. Traditional methods focused on utilizing time series data but ignored clinical notes. With the development of deep learning, natural language processing (NLP) and multi-modal learning methods are exploited to jointly model the time series and clinical notes with different modals. However, the existing methods failed to fuse the multi-modal features of patients from different views. Therefore, we propose the patient multi-view multi-modal feature fusion networks for clinical outcome prediction. Firstly, from patient inner view, we propose to utilize the co-attention module to enhance the fine-grained feature interaction between time series and clinical notes from each patient. Secondly, the patient outer view is the correlation between patients, which can be reflected by the structural knowledge in clinical notes. We exploit the structural information extracted from clinical notes to construct the patient correlation graph, and fuse patients{'} multi-modal features by graph neural networks (GNN). The experimental results on MIMIC-III benchmark demonstrate the superiority of our method."",","{zhang-etal-2022-pm2f2n,
    title = ""P$\text{M}^2\text{F}^2${N}: Patient Multi-view Multi-modal Feature Fusion Networks for Clinical Outcome Prediction"",
    author = ""Zhang, Ying  and
      Zhou, Baohang  and
      Song, Kehui  and
      Sui, Xuhui  and
      Zhao, Guoqing  and
      Jiang, Ning  and
      Yuan, Xiaojie"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2022"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.findings-emnlp.144"",
    doi = ""10.18653/v1/2022.findings-emnlp.144"",
    pages = ""1985--1994"",
    abstract = ""Clinical outcome prediction is critical to the condition prediction of patients and management of hospital capacities. There are two kinds of medical data, including time series signals recorded by various devices and clinical notes in electronic health records (EHR), which are used for two common prediction targets: mortality and length of stay. Traditional methods focused on utilizing time series data but ignored clinical notes. With the development of deep learning, natural language processing (NLP) and multi-modal learning methods are exploited to jointly model the time series and clinical notes with different modals. However, the existing methods failed to fuse the multi-modal features of patients from different views. Therefore, we propose the patient multi-view multi-modal feature fusion networks for clinical outcome prediction. Firstly, from patient inner view, we propose to utilize the co-attention module to enhance the fine-grained feature interaction between time series and clinical notes from each patient. Secondly, the patient outer view is the correlation between patients, which can be reflected by the structural knowledge in clinical notes. We exploit the structural information extracted from clinical notes to construct the patient correlation graph, and fuse patients{'} multi-modal features by graph neural networks (GNN). The experimental results on MIMIC-III benchmark demonstrate the superiority of our method."",
}
@",electronic health record,health record,clinical not,natural language process,natural languag,language process,nlp,benchmark
" ""{M}3: Multi-level dataset for Multi-document summarisation of Medical studies"","," ""We present M3 (Multi-level dataset for Multi-document summarisation of Medical studies), a benchmark dataset for evaluating the quality of summarisation systems in the biomedical domain. The dataset contains sets of multiple input documents and target summaries of three levels of complexity: documents, sentences, and propositions. The dataset also includes several levels of annotation, including biomedical entities, direction, and strength of relations between them, and the discourse relationships between the input documents ({``}contradiction{''} or {``}agreement{''}). We showcase usage scenarios of the dataset by testing 10 generic and domain-specific summarisation models in a zero-shot setting, and introduce a probing task based on counterfactuals to test if models are aware of the direction and strength of the conclusions generated from input studies."",","{otmakhova-etal-2022-m3,
    title = ""{M}3: Multi-level dataset for Multi-document summarisation of Medical studies"",
    author = ""Otmakhova, Yulia  and
      Verspoor, Karin  and
      Baldwin, Timothy  and
      Jimeno Yepes, Antonio  and
      Lau, Jey Han"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2022"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.findings-emnlp.286"",
    doi = ""10.18653/v1/2022.findings-emnlp.286"",
    pages = ""3887--3901"",
    abstract = ""We present M3 (Multi-level dataset for Multi-document summarisation of Medical studies), a benchmark dataset for evaluating the quality of summarisation systems in the biomedical domain. The dataset contains sets of multiple input documents and target summaries of three levels of complexity: documents, sentences, and propositions. The dataset also includes several levels of annotation, including biomedical entities, direction, and strength of relations between them, and the discourse relationships between the input documents ({``}contradiction{''} or {``}agreement{''}). We showcase usage scenarios of the dataset by testing 10 generic and domain-specific summarisation models in a zero-shot setting, and introduce a probing task based on counterfactuals to test if models are aware of the direction and strength of the conclusions generated from input studies."",
}
@",medical domain,nlp,generat,summar,summaris,annotat,benchmark,evalu
" ""Learning to Revise References for Faithful Summarization"","," ""In real-world scenarios with naturally occurring datasets, reference summaries are noisy and may contain information that cannot be inferred from the source text. On large news corpora, removing low quality samples has been shown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora, filtering is detrimental to performance. To improve reference quality while retaining all data, we propose a new approach: to selectively re-write unsupported reference sentences to better reflect source data. We automatically generate a synthetic dataset of positive and negative revisions by corrupting supported sentences and learn to revise reference sentences with contrastive learning. The intensity of revisions is treated as a controllable attribute so that, at inference, diverse candidates can be over-generated-then-rescored to balance faithfulness and abstraction. To test our methods, we extract noisy references from publicly available MIMIC-III discharge summaries for the task of hospital-course summarization, and vary the data on which models are trained. According to metrics and human evaluation, models trained on revised clinical references are much more faithful, informative, and fluent than models trained on original or filtered data."",","{adams-etal-2022-learning,
    title = ""Learning to Revise References for Faithful Summarization"",
    author = ""Adams, Griffin  and
      Shing, Han-Chin  and
      Sun, Qing  and
      Winestock, Christopher  and
      McKeown, Kathleen  and
      Elhadad, No{\'e}mie"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2022"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.findings-emnlp.296"",
    doi = ""10.18653/v1/2022.findings-emnlp.296"",
    pages = ""4009--4027"",
    abstract = ""In real-world scenarios with naturally occurring datasets, reference summaries are noisy and may contain information that cannot be inferred from the source text. On large news corpora, removing low quality samples has been shown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora, filtering is detrimental to performance. To improve reference quality while retaining all data, we propose a new approach: to selectively re-write unsupported reference sentences to better reflect source data. We automatically generate a synthetic dataset of positive and negative revisions by corrupting supported sentences and learn to revise reference sentences with contrastive learning. The intensity of revisions is treated as a controllable attribute so that, at inference, diverse candidates can be over-generated-then-rescored to balance faithfulness and abstraction. To test our methods, we extract noisy references from publicly available MIMIC-III discharge summaries for the task of hospital-course summarization, and vary the data on which models are trained. According to metrics and human evaluation, models trained on revised clinical references are much more faithful, informative, and fluent than models trained on original or filtered data."",
}
@",discharge summar,nlp,infer,generat,summar,evalu
" ""Readability Controllable Biomedical Document Summarization"","," ""Different from general documents, it is recognised that the ease with which people can understand a biomedical text is eminently varied, owing to the highly technical nature of biomedical documents and the variance of readers{'} domain knowledge. However, existing biomedical document summarization systems have paid little attention to readability control, leaving users with summaries that are incompatible with their levels of expertise. In recognition of this urgent demand, we introduce a new task of readability controllable summarization for biomedical documents, which aims to recognise users{'} readability demands and generate summaries that better suit their needs: technical summaries for experts and plain language summaries (PLS) for laymen. To establish this task, we construct a corpus consisting of biomedical papers with technical summaries and PLSs written by the authors, and benchmark multiple advanced controllable abstractive and extractive summarization models based on pre-trained language models (PLMs) with prevalent controlling and generation techniques. Moreover, we propose a novel masked language model (MLM) based metric and its variant to effectively evaluate the readability discrepancy between lay and technical summaries. Experimental results from automated and human evaluations show that though current control techniques allow for a certain degree of readability adjustment during generation, the performance of existing controllable summarization methods is far from desirable in this task."",","{luo-etal-2022-readability,
    title = ""Readability Controllable Biomedical Document Summarization"",
    author = ""Luo, Zheheng  and
      Xie, Qianqian  and
      Ananiadou, Sophia"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2022"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.findings-emnlp.343"",
    doi = ""10.18653/v1/2022.findings-emnlp.343"",
    pages = ""4667--4680"",
    abstract = ""Different from general documents, it is recognised that the ease with which people can understand a biomedical text is eminently varied, owing to the highly technical nature of biomedical documents and the variance of readers{'} domain knowledge. However, existing biomedical document summarization systems have paid little attention to readability control, leaving users with summaries that are incompatible with their levels of expertise. In recognition of this urgent demand, we introduce a new task of readability controllable summarization for biomedical documents, which aims to recognise users{'} readability demands and generate summaries that better suit their needs: technical summaries for experts and plain language summaries (PLS) for laymen. To establish this task, we construct a corpus consisting of biomedical papers with technical summaries and PLSs written by the authors, and benchmark multiple advanced controllable abstractive and extractive summarization models based on pre-trained language models (PLMs) with prevalent controlling and generation techniques. Moreover, we propose a novel masked language model (MLM) based metric and its variant to effectively evaluate the readability discrepancy between lay and technical summaries. Experimental results from automated and human evaluations show that though current control techniques allow for a certain degree of readability adjustment during generation, the performance of existing controllable summarization methods is far from desirable in this task."",
}
@",medical text,nlp,generat,summar,benchmark,evalu
" ""{B}io{NLI}: Generating a Biomedical {NLI} Dataset Using Lexico-semantic Constraints for Adversarial Examples"","," ""Natural language inference (NLI) is critical in many domains requiring complex decision-making, such as the biomedical domain. We introduce a novel semi-supervised procedure that bootstraps biomedical NLI datasets from positive entailment examples present in abstracts of biomedical publications. We focus on challenging texts where the hypothesis includes mechanistic information such as biochemical interactions between two entities. A key contribution of this work is automating the creation of negative examples that are informative without being simplistic. We generate a range of negative examples using nine strategies that manipulate the structure of the underlying mechanisms both with rules, e.g., flip the roles of the entities in the interaction, and, more importantly, by imposing the perturbed conditions as logical constraints in a neuro-logical decoding system (CITATION).We use this procedure to create a novel dataset for NLI in the biomedical domain, called . The accuracy of neural classifiers on this dataset is in the mid 70s F1, which indicates that this NLI task remains to be solved. Critically, we observe that the performance on the different classes of negative examples varies widely, from 97{\%} F1 on the simple negative examples that change the role of the entities in the hypothesis, to barely better than chance on the negative examples generated using neuro-logic decoding."",","{bastan-etal-2022-bionli,
    title = ""{B}io{NLI}: Generating a Biomedical {NLI} Dataset Using Lexico-semantic Constraints for Adversarial Examples"",
    author = ""Bastan, Mohaddeseh  and
      Surdeanu, Mihai  and
      Balasubramanian, Niranjan"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2022"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.findings-emnlp.374"",
    doi = ""10.18653/v1/2022.findings-emnlp.374"",
    pages = ""5093--5104"",
    abstract = ""Natural language inference (NLI) is critical in many domains requiring complex decision-making, such as the biomedical domain. We introduce a novel semi-supervised procedure that bootstraps biomedical NLI datasets from positive entailment examples present in abstracts of biomedical publications. We focus on challenging texts where the hypothesis includes mechanistic information such as biochemical interactions between two entities. A key contribution of this work is automating the creation of negative examples that are informative without being simplistic. We generate a range of negative examples using nine strategies that manipulate the structure of the underlying mechanisms both with rules, e.g., flip the roles of the entities in the interaction, and, more importantly, by imposing the perturbed conditions as logical constraints in a neuro-logical decoding system (CITATION).We use this procedure to create a novel dataset for NLI in the biomedical domain, called . The accuracy of neural classifiers on this dataset is in the mid 70s F1, which indicates that this NLI task remains to be solved. Critically, we observe that the performance on the different classes of negative examples varies widely, from 97{\%} F1 on the simple negative examples that change the role of the entities in the hypothesis, to barely better than chance on the negative examples generated using neuro-logic decoding."",
}
@",medical domain,natural languag,nlp,infer,generat,semant,challeng
" ""{C}linical{T}5: A Generative Language Model for Clinical Text"","," ""In the past few years, large pre-trained language models (PLMs) have been widely adopted in different areas and have made fundamental improvements over a variety of downstream tasks in natural language processing (NLP). Meanwhile, domain-specific variants of PLMs are being proposed to address the needs of domains that demonstrate a specific pattern of writing and vocabulary, e.g., BioBERT for the biomedical domain and ClinicalBERT for the clinical domain. Recently, generative language models like BART and T5 are gaining popularity with their competitive performance on text generation as well as on tasks cast as generative problems. However, in the clinical domain, such domain-specific generative variants are still underexplored. To address this need, our work introduces a T5-based text-to-text transformer model pre-trained on clinical text, i.e., ClinicalT5. We evaluate the proposed model both intrinsically and extrinsically over a diverse set of tasks across multiple datasets, and show that ClinicalT5 dramatically outperforms T5 in the domain-specific tasks and compares favorably with its close baselines."",","{lu-etal-2022-clinicalt5,
    title = ""{C}linical{T}5: A Generative Language Model for Clinical Text"",
    author = ""Lu, Qiuhao  and
      Dou, Dejing  and
      Nguyen, Thien"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2022"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.findings-emnlp.398"",
    doi = ""10.18653/v1/2022.findings-emnlp.398"",
    pages = ""5436--5443"",
    abstract = ""In the past few years, large pre-trained language models (PLMs) have been widely adopted in different areas and have made fundamental improvements over a variety of downstream tasks in natural language processing (NLP). Meanwhile, domain-specific variants of PLMs are being proposed to address the needs of domains that demonstrate a specific pattern of writing and vocabulary, e.g., BioBERT for the biomedical domain and ClinicalBERT for the clinical domain. Recently, generative language models like BART and T5 are gaining popularity with their competitive performance on text generation as well as on tasks cast as generative problems. However, in the clinical domain, such domain-specific generative variants are still underexplored. To address this need, our work introduces a T5-based text-to-text transformer model pre-trained on clinical text, i.e., ClinicalT5. We evaluate the proposed model both intrinsically and extrinsically over a diverse set of tasks across multiple datasets, and show that ClinicalT5 dramatically outperforms T5 in the domain-specific tasks and compares favorably with its close baselines."",
}
@",clinical domain,medical domain,clinical text,natural language process,natural languag,language process,nlp,generat,evalu
" ""Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem"","," ""We propose an autoregressive entity linking model, that is trained with two auxiliary tasks, and learns to re-rank generated samples at inference time. Our proposed novelties address two weaknesses in the literature. First, a recent method proposes to learn mention detection and then entity candidate selection, but relies on predefined sets of candidates. We use encoder-decoder autoregressive entity linking in order to bypass this need, and propose to train mention detection as an auxiliary task instead. Second, previous work suggests that re-ranking could help correct prediction errors. We add a new, auxiliary task, match prediction, to learn re-ranking. Without the use of a knowledge base or candidate sets, our model sets a new state of the art in two benchmark datasets of entity linking: COMETA in the biomedical domain, and AIDA-CoNLL in the news domain. We show through ablation studies that each of the two auxiliary tasks increases performance, and that re-ranking is an important factor to the increase. Finally, our low-resource experimental results suggest that performance on the main task benefits from the knowledge learned by the auxiliary tasks, and not just from the additional training data."",","{mrini-etal-2022-detection,
    title = ""Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem"",
    author = ""Mrini, Khalil  and
      Nie, Shaoliang  and
      Gu, Jiatao  and
      Wang, Sinong  and
      Sanjabi, Maziar  and
      Firooz, Hamed"",
    editor = ""Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline"",
    booktitle = ""Findings of the Association for Computational Linguistics: ACL 2022"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.findings-acl.156"",
    doi = ""10.18653/v1/2022.findings-acl.156"",
    pages = ""1972--1983"",
    abstract = ""We propose an autoregressive entity linking model, that is trained with two auxiliary tasks, and learns to re-rank generated samples at inference time. Our proposed novelties address two weaknesses in the literature. First, a recent method proposes to learn mention detection and then entity candidate selection, but relies on predefined sets of candidates. We use encoder-decoder autoregressive entity linking in order to bypass this need, and propose to train mention detection as an auxiliary task instead. Second, previous work suggests that re-ranking could help correct prediction errors. We add a new, auxiliary task, match prediction, to learn re-ranking. Without the use of a knowledge base or candidate sets, our model sets a new state of the art in two benchmark datasets of entity linking: COMETA in the biomedical domain, and AIDA-CoNLL in the news domain. We show through ablation studies that each of the two auxiliary tasks increases performance, and that re-ranking is an important factor to the increase. Finally, our low-resource experimental results suggest that performance on the main task benefits from the knowledge learned by the auxiliary tasks, and not just from the additional training data."",
}
@",medical domain,infer,generat,benchmark
" ""Large language models are few-shot clinical information extractors"","," ""A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines."",","{agrawal-etal-2022-large,
    title = ""Large language models are few-shot clinical information extractors"",
    author = ""Agrawal, Monica  and
      Hegselmann, Stefan  and
      Lang, Hunter  and
      Kim, Yoon  and
      Sontag, David"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.130"",
    doi = ""10.18653/v1/2022.emnlp-main.130"",
    pages = ""1998--2022"",
    abstract = ""A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines."",
}
@",clinical domain,clinical not,clinical text,natural language process,natural languag,language process,nlp,generat,relation extract,annotat,benchmark,evalu
" ""That{'}s the Wrong Lung! Evaluating and Improving the Interpretability of Unsupervised Multimodal Encoders for Medical Data"","," ""Pretraining multimodal models on Electronic Health Records (EHRs) provides a means of learning representations that can transfer to downstream tasks with minimal supervision. Recent multimodal models induce soft local alignments between image regions and sentences. This is of particular interest in the medical domain, where alignments might highlight regions in an image relevant to specific phenomena described in free-text. While past work has suggested that attention {``}heatmaps{''} can be interpreted in this manner, there has been little evaluation of such alignments. We compare alignments from a state-of-the-art multimodal (image and text) model for EHR with human annotations that link image regions to sentences. Our main finding is that the text has an often weak or unintuitive influence on attention; alignments do not consistently reflect basic anatomical information. Moreover, synthetic modifications {---} such as substituting {``}left{''} for {``}right{''} {---} do not substantially influence highlights. Simple techniques such as allowing the model to opt out of attending to the image and few-shot finetuning show promise in terms of their ability to improve alignments with very little or no supervision. We make our code and checkpoints open-source."",","{mcinerney-etal-2022-thats,
    title = ""That{'}s the Wrong Lung! Evaluating and Improving the Interpretability of Unsupervised Multimodal Encoders for Medical Data"",
    author = ""McInerney, Jered  and
      Young, Geoffrey  and
      van de Meent, Jan-Willem  and
      Wallace, Byron"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.238"",
    doi = ""10.18653/v1/2022.emnlp-main.238"",
    pages = ""3626--3648"",
    abstract = ""Pretraining multimodal models on Electronic Health Records (EHRs) provides a means of learning representations that can transfer to downstream tasks with minimal supervision. Recent multimodal models induce soft local alignments between image regions and sentences. This is of particular interest in the medical domain, where alignments might highlight regions in an image relevant to specific phenomena described in free-text. While past work has suggested that attention {``}heatmaps{''} can be interpreted in this manner, there has been little evaluation of such alignments. We compare alignments from a state-of-the-art multimodal (image and text) model for EHR with human annotations that link image regions to sentences. Our main finding is that the text has an often weak or unintuitive influence on attention; alignments do not consistently reflect basic anatomical information. Moreover, synthetic modifications {---} such as substituting {``}left{''} for {``}right{''} {---} do not substantially influence highlights. Simple techniques such as allowing the model to opt out of attending to the image and few-shot finetuning show promise in terms of their ability to improve alignments with very little or no supervision. We make our code and checkpoints open-source."",
}
@",electronic health record,health record,medical domain,natural language process,natural languag,language process,nlp,annotat,evalu
" ""Hardness-guided domain adaptation to recognise biomedical named entities under low-resource scenarios"","," ""Domain adaptation is an effective solution to data scarcity in low-resource scenarios. However, when applied to token-level tasks such as bioNER, domain adaptation methods often suffer from the challenging linguistic characteristics that clinical narratives possess, which leads to unsatsifactory performance. In this paper, we present a simple yet effective hardness-guided domain adaptation framework for bioNER tasks that can effectively leverage the domain hardness information to improve the adaptability of the learnt model in the low-resource scenarios. Experimental results on biomedical datasets show that our model can achieve significant performance improvement over the recently published state-of-the-art (SOTA) MetaNER model."",","{nguyen-etal-2022-hardness,
    title = ""Hardness-guided domain adaptation to recognise biomedical named entities under low-resource scenarios"",
    author = ""Nguyen, Ngoc Dang  and
      Du, Lan  and
      Buntine, Wray  and
      Chen, Changyou  and
      Beare, Richard"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.271"",
    doi = ""10.18653/v1/2022.emnlp-main.271"",
    pages = ""4063--4071"",
    abstract = ""Domain adaptation is an effective solution to data scarcity in low-resource scenarios. However, when applied to token-level tasks such as bioNER, domain adaptation methods often suffer from the challenging linguistic characteristics that clinical narratives possess, which leads to unsatsifactory performance. In this paper, we present a simple yet effective hardness-guided domain adaptation framework for bioNER tasks that can effectively leverage the domain hardness information to improve the adaptability of the learnt model in the low-resource scenarios. Experimental results on biomedical datasets show that our model can achieve significant performance improvement over the recently published state-of-the-art (SOTA) MetaNER model."",
}
@",clinical narr,natural language process,natural languag,language process,nlp,challeng
" ""A Speaker-Aware Co-Attention Framework for Medical Dialogue Information Extraction"","," ""With the development of medical digitization, the extraction and structuring of Electronic Medical Records (EMRs) have become challenging but fundamental tasks. How to accurately and automatically extract structured information from medical dialogues is especially difficult because the information needs to be inferred from complex interactions between the doctor and the patient. To this end, in this paper, we propose a speaker-aware co-attention framework for medical dialogue information extraction. To better utilize the pre-trained language representation model to perceive the semantics of the utterance and the candidate item, we develop a speaker-aware dialogue encoder with multi-task learning, which considers the speaker{'}s identity into account. To deal with complex interactions between different utterances and the correlations between utterances and candidate items, we propose a co-attention fusion network to aggregate the utterance information. We evaluate our framework on the public medical dialogue extraction datasets to demonstrate the superiority of our method, which can outperform the state-of-the-art methods by a large margin. Codes will be publicly available upon acceptance."",","{xia-etal-2022-speaker,
    title = ""A Speaker-Aware Co-Attention Framework for Medical Dialogue Information Extraction"",
    author = ""Xia, Yuan  and
      Shi, Zhenhui  and
      Zhou, Jingbo  and
      Xu, Jiayu  and
      Lu, Chao  and
      Yang, Yehui  and
      Wang, Lei  and
      Huang, Haifeng  and
      Zhang, Xia  and
      Liu, Junwei"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.315"",
    doi = ""10.18653/v1/2022.emnlp-main.315"",
    pages = ""4777--4786"",
    abstract = ""With the development of medical digitization, the extraction and structuring of Electronic Medical Records (EMRs) have become challenging but fundamental tasks. How to accurately and automatically extract structured information from medical dialogues is especially difficult because the information needs to be inferred from complex interactions between the doctor and the patient. To this end, in this paper, we propose a speaker-aware co-attention framework for medical dialogue information extraction. To better utilize the pre-trained language representation model to perceive the semantics of the utterance and the candidate item, we develop a speaker-aware dialogue encoder with multi-task learning, which considers the speaker{'}s identity into account. To deal with complex interactions between different utterances and the correlations between utterances and candidate items, we propose a co-attention fusion network to aggregate the utterance information. We evaluate our framework on the public medical dialogue extraction datasets to demonstrate the superiority of our method, which can outperform the state-of-the-art methods by a large margin. Codes will be publicly available upon acceptance."",
}
@",medical record,natural language process,natural languag,language process,nlp,infer,semant,challeng,evalu
" ""{M}ed{JE}x: A Medical Jargon Extraction Model with {W}iki{'}s Hyperlink Span and Contextualized Masked Language Model Score"","," ""This paper proposes a new natural language processing (NLP) application for identifying medical jargon terms potentially difficult for patients to comprehend from electronic health record (EHR) notes. We first present a novel and publicly available dataset with expert-annotated medical jargon terms from 18K+ EHR note sentences (MedJ). Then, we introduce a novel medical jargon extraction (MedJEx) model which has been shown to outperform existing state-of-the-art NLP models. First, MedJEx improved the overall performance when it was trained on an auxiliary Wikipedia hyperlink span dataset, where hyperlink spans provide additional Wikipedia articles to explain the spans (or terms), and then fine-tuned on the annotated MedJ data. Secondly, we found that a contextualized masked language model score was beneficial for detecting domain-specific unfamiliar jargon terms. Moreover, our results show that training on the auxiliary Wikipedia hyperlink span datasets improved six out of eight biomedical named entity recognition benchmark datasets. MedJEx is publicly available."",","{kwon-etal-2022-medjex,
    title = ""{M}ed{JE}x: A Medical Jargon Extraction Model with {W}iki{'}s Hyperlink Span and Contextualized Masked Language Model Score"",
    author = ""Kwon, Sunjae  and
      Yao, Zonghai  and
      Jordan, Harmon  and
      Levy, David  and
      Corner, Brian  and
      Yu, Hong"",
    editor = ""Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.805"",
    doi = ""10.18653/v1/2022.emnlp-main.805"",
    pages = ""11733--11751"",
    abstract = ""This paper proposes a new natural language processing (NLP) application for identifying medical jargon terms potentially difficult for patients to comprehend from electronic health record (EHR) notes. We first present a novel and publicly available dataset with expert-annotated medical jargon terms from 18K+ EHR note sentences (MedJ). Then, we introduce a novel medical jargon extraction (MedJEx) model which has been shown to outperform existing state-of-the-art NLP models. First, MedJEx improved the overall performance when it was trained on an auxiliary Wikipedia hyperlink span dataset, where hyperlink spans provide additional Wikipedia articles to explain the spans (or terms), and then fine-tuned on the annotated MedJ data. Secondly, we found that a contextualized masked language model score was beneficial for detecting domain-specific unfamiliar jargon terms. Moreover, our results show that training on the auxiliary Wikipedia hyperlink span datasets improved six out of eight biomedical named entity recognition benchmark datasets. MedJEx is publicly available."",
}
@",electronic health record,health record,natural language process,natural languag,language process,nlp,entity recognit,annotat,benchmark,publicly available dataset,publicly available dataset
" ""Improving Precancerous Case Characterization via Transformer-based Ensemble Learning"","," ""The application of natural language processing (NLP) to cancer pathology reports has been focused on detecting cancer cases, largely ignoring precancerous cases. Improving the characterization of precancerous adenomas assists in developing diagnostic tests for early cancer detection and prevention, especially for colorectal cancer (CRC). Here we developed transformer-based deep neural network NLP models to perform the CRC phenotyping, with the goal of extracting precancerous lesion attributes and distinguishing cancer and precancerous cases. We achieved 0.914 macro-F1 scores for classifying patients into negative, non-advanced adenoma, advanced adenoma and CRC. We further improved the performance to 0.923 using an ensemble of classifiers for cancer status classification and lesion size named-entity recognition (NER). Our results demonstrated the potential of using NLP to leverage real-world health record data to facilitate the development of diagnostic tests for early cancer prevention."",","{zhong-etal-2022-improving-precancerous,
    title = ""Improving Precancerous Case Characterization via Transformer-based Ensemble Learning"",
    author = ""Zhong, Yizhen  and
      Xiao, Jiajie  and
      Vetterli, Thomas  and
      Matin, Mahan  and
      Loo, Ellen  and
      Lin, Jimmy  and
      Bourgon, Richard  and
      Shapira, Ofer"",
    editor = ""Li, Yunyao  and
      Lazaridou, Angeliki"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, UAE"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-industry.38"",
    doi = ""10.18653/v1/2022.emnlp-industry.38"",
    pages = ""379--389"",
    abstract = ""The application of natural language processing (NLP) to cancer pathology reports has been focused on detecting cancer cases, largely ignoring precancerous cases. Improving the characterization of precancerous adenomas assists in developing diagnostic tests for early cancer detection and prevention, especially for colorectal cancer (CRC). Here we developed transformer-based deep neural network NLP models to perform the CRC phenotyping, with the goal of extracting precancerous lesion attributes and distinguishing cancer and precancerous cases. We achieved 0.914 macro-F1 scores for classifying patients into negative, non-advanced adenoma, advanced adenoma and CRC. We further improved the performance to 0.923 using an ensemble of classifiers for cancer status classification and lesion size named-entity recognition (NER). Our results demonstrated the potential of using NLP to leverage real-world health record data to facilitate the development of diagnostic tests for early cancer prevention."",
}
@",health record,natural language process,natural languag,language process,nlp,entity recognit,track
" ""{A}n{EMIC}: A Framework for Benchmarking {ICD} Coding Models"","," ""Diagnostic coding, or ICD coding, is the task of assigning diagnosis codes defined by the ICD (International Classification of Diseases) standard to patient visits based on clinical notes. The current process of manual ICD coding is time-consuming and often error-prone, which suggests the need for automatic ICD coding. However, despite the long history of automatic ICD coding, there have been no standardized frameworks for benchmarking ICD coding models. We open-source an easy-to-use tool named \textit{AnEMIC}, which provides a streamlined pipeline for preprocessing, training, and evaluating for automatic ICD coding. We correct errors in preprocessing by existing works, and provide key models and weights trained on the correctly preprocessed datasets. We also provide an interactive demo performing real-time inference from custom inputs, and visualizations drawn from explainable AI to analyze the models. We hope the framework helps move the research of ICD coding forward and helps professionals explore the potential of ICD coding. The framework and the associated code are available here."",","{kim-etal-2022-anemic,
    title = ""{A}n{EMIC}: A Framework for Benchmarking {ICD} Coding Models"",
    author = ""Kim, Juyong  and
      Sharma, Abheesht  and
      Shanbhogue, Suhas  and
      Weiss, Jeremy  and
      Ravikumar, Pradeep"",
    editor = ""Che, Wanxiang  and
      Shutova, Ekaterina"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, UAE"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-demos.11"",
    doi = ""10.18653/v1/2022.emnlp-demos.11"",
    pages = ""109--120"",
    abstract = ""Diagnostic coding, or ICD coding, is the task of assigning diagnosis codes defined by the ICD (International Classification of Diseases) standard to patient visits based on clinical notes. The current process of manual ICD coding is time-consuming and often error-prone, which suggests the need for automatic ICD coding. However, despite the long history of automatic ICD coding, there have been no standardized frameworks for benchmarking ICD coding models. We open-source an easy-to-use tool named \textit{AnEMIC}, which provides a streamlined pipeline for preprocessing, training, and evaluating for automatic ICD coding. We correct errors in preprocessing by existing works, and provide key models and weights trained on the correctly preprocessed datasets. We also provide an interactive demo performing real-time inference from custom inputs, and visualizations drawn from explainable AI to analyze the models. We hope the framework helps move the research of ICD coding forward and helps professionals explore the potential of ICD coding. The framework and the associated code are available here."",
}
@",clinical not,natural language process,natural languag,language process,nlp,infer,benchmark,evalu
" ""Comparing and combining tagging with different decoding algorithms for back-translation in {NMT}: learnings from a low resource scenario"","," ""Back-translation is a well established approach to improve the performance of Neural Machine Translation (NMT) systems when large monolingual corpora of the target language and domain are available. Recently, diverse approaches have been proposed to get better automatic evaluation results of NMT models using back-translation, including the use of sampling instead of beam search as decoding algorithm for creating the synthetic corpus. Alternatively, it has been proposed to append a tag to the back-translated corpus for helping the NMT system to distinguish the synthetic bilingual corpus from the authentic one. However, not all the combinations of the previous approaches have been tested, and thus it is not clear which is the best approach for developing a given NMT system. In this work, we empirically compare and combine existing techniques for back-translation in a real low resource setting: the translation of clinical notes from Basque into Spanish. Apart from automatically evaluating the MT systems, we ask bilingual healthcare workers to perform a human evaluation, and analyze the different synthetic corpora by measuring their lexical diversity (LD). For reproducibility and generalizability, we repeat our experiments for German to English translation using public data. The results suggest that in lower resource scenarios tagging only helps when using sampling for decoding, in contradiction with the previous literature using bigger corpora from the news domain. When fine-tuning with a few thousand bilingual in-domain sentences, one of our proposed method (tagged restricted sampling) obtains the best results both in terms of automatic and human evaluation. We will publish the code upon acceptance."",","{soto-etal-2022-comparing,
    title = ""Comparing and combining tagging with different decoding algorithms for back-translation in {NMT}: learnings from a low resource scenario"",
    author = ""Soto, Xabier  and
      Perez-De-Vi{\~n}aspre, Olatz  and
      Labaka, Gorka  and
      Oronoz, Maite"",
    editor = {Moniz, Helena  and
      Macken, Lieve  and
      Rufener, Andrew  and
      Barrault, Lo{\""\i}c  and
      Costa-juss{\`a}, Marta R.  and
      Declercq, Christophe  and
      Koponen, Maarit  and
      Kemp, Ellie  and
      Pilos, Spyridon  and
      Forcada, Mikel L.  and
      Scarton, Carolina  and
      Van den Bogaert, Joachim  and
      Daems, Joke  and
      Tezcan, Arda  and
      Vanroy, Bram  and
      Fonteyne, Margot},
    booktitle = ""Proceedings of the 23rd Annual Conference of the European Association for Machine Translation"",
    month = jun,
    year = ""2022"",
    address = ""Ghent, Belgium"",
    publisher = ""European Association for Machine Translation"",
    url = ""https://aclanthology.org/2022.eamt-1.6"",
    pages = ""31--40"",
    abstract = ""Back-translation is a well established approach to improve the performance of Neural Machine Translation (NMT) systems when large monolingual corpora of the target language and domain are available. Recently, diverse approaches have been proposed to get better automatic evaluation results of NMT models using back-translation, including the use of sampling instead of beam search as decoding algorithm for creating the synthetic corpus. Alternatively, it has been proposed to append a tag to the back-translated corpus for helping the NMT system to distinguish the synthetic bilingual corpus from the authentic one. However, not all the combinations of the previous approaches have been tested, and thus it is not clear which is the best approach for developing a given NMT system. In this work, we empirically compare and combine existing techniques for back-translation in a real low resource setting: the translation of clinical notes from Basque into Spanish. Apart from automatically evaluating the MT systems, we ask bilingual healthcare workers to perform a human evaluation, and analyze the different synthetic corpora by measuring their lexical diversity (LD). For reproducibility and generalizability, we repeat our experiments for German to English translation using public data. The results suggest that in lower resource scenarios tagging only helps when using sampling for decoding, in contradiction with the previous literature using bigger corpora from the news domain. When fine-tuning with a few thousand bilingual in-domain sentences, one of our proposed method (tagged restricted sampling) obtains the best results both in terms of automatic and human evaluation. We will publish the code upon acceptance."",
}
@",clinical not,translat,evalu,public data
" ""Document-level Biomedical Relation Extraction Based on Multi-Dimensional Fusion Information and Multi-Granularity Logical Reasoning"","," ""Document-level biomedical relation extraction (Bio-DocuRE) is an important branch of biomedical text mining that aims to automatically extract all relation facts from the biomedical text. Since there are a considerable number of relations in biomedical documents that need to be judged by other existing relations, logical reasoning has become a research hotspot in the past two years. However, current models with reasoning are single-granularity only based on one element information, ignoring the complementary fact of different granularity reasoning information. In addition, obtaining rich document information is a prerequisite for logical reasoning, but most of the previous models cannot sufficiently utilize document information, which limits the reasoning ability of the model. In this paper, we propose a novel Bio-DocuRE model called FILR, based on Multi-Dimensional Fusion Information and Multi-Granularity Logical Reasoning. Specifically, FILR presents a multi-dimensional information fusion module MDIF to extract sufficient global document information. Then FILR proposes a multi-granularity reasoning module MGLR to obtain rich inference information through the reasoning of both entity-pairs and mention-pairs. We evaluate our FILR model on two widely used biomedical corpora CDR and GDA. Experimental results show that FILR achieves state-of-the-art performance."",","{li-etal-2022-document,
    title = ""Document-level Biomedical Relation Extraction Based on Multi-Dimensional Fusion Information and Multi-Granularity Logical Reasoning"",
    author = ""Li, Lishuang  and
      Lian, Ruiyuan  and
      Lu, Hongbin  and
      Tang, Jingyao"",
    editor = ""Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon"",
    booktitle = ""Proceedings of the 29th International Conference on Computational Linguistics"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""International Committee on Computational Linguistics"",
    url = ""https://aclanthology.org/2022.coling-1.183"",
    pages = ""2098--2107"",
    abstract = ""Document-level biomedical relation extraction (Bio-DocuRE) is an important branch of biomedical text mining that aims to automatically extract all relation facts from the biomedical text. Since there are a considerable number of relations in biomedical documents that need to be judged by other existing relations, logical reasoning has become a research hotspot in the past two years. However, current models with reasoning are single-granularity only based on one element information, ignoring the complementary fact of different granularity reasoning information. In addition, obtaining rich document information is a prerequisite for logical reasoning, but most of the previous models cannot sufficiently utilize document information, which limits the reasoning ability of the model. In this paper, we propose a novel Bio-DocuRE model called FILR, based on Multi-Dimensional Fusion Information and Multi-Granularity Logical Reasoning. Specifically, FILR presents a multi-dimensional information fusion module MDIF to extract sufficient global document information. Then FILR proposes a multi-granularity reasoning module MGLR to obtain rich inference information through the reasoning of both entity-pairs and mention-pairs. We evaluate our FILR model on two widely used biomedical corpora CDR and GDA. Experimental results show that FILR achieves state-of-the-art performance."",
}
@",medical text,infer,relation extract,evalu
" ""{M}ed{D}istant19: Towards an Accurate Benchmark for Broad-Coverage Biomedical Relation Extraction"","," ""Relation extraction in the biomedical domain is challenging due to the lack of labeled data and high annotation costs, needing domain experts. Distant supervision is commonly used to tackle the scarcity of annotated data by automatically pairing knowledge graph relationships with raw texts. Such a pipeline is prone to noise and has added challenges to scale for covering a large number of biomedical concepts. We investigated existing broad-coverage distantly supervised biomedical relation extraction benchmarks and found a significant overlap between training and test relationships ranging from 26{\%} to 86{\%}. Furthermore, we noticed several inconsistencies in the data construction process of these benchmarks, and where there is no train-test leakage, the focus is on interactions between narrower entity types. This work presents a more accurate benchmark MedDistant19 for broad-coverage distantly supervised biomedical relation extraction that addresses these shortcomings and is obtained by aligning the MEDLINE abstracts with the widely used SNOMED Clinical Terms knowledge base. Lacking thorough evaluation with domain-specific language models, we also conduct experiments validating general domain relation extraction findings to biomedical relation extraction."",","{amin-etal-2022-meddistant19,
    title = ""{M}ed{D}istant19: Towards an Accurate Benchmark for Broad-Coverage Biomedical Relation Extraction"",
    author = ""Amin, Saadullah  and
      Minervini, Pasquale  and
      Chang, David  and
      Stenetorp, Pontus  and
      Neumann, Guenter"",
    editor = ""Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon"",
    booktitle = ""Proceedings of the 29th International Conference on Computational Linguistics"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""International Committee on Computational Linguistics"",
    url = ""https://aclanthology.org/2022.coling-1.198"",
    pages = ""2259--2277"",
    abstract = ""Relation extraction in the biomedical domain is challenging due to the lack of labeled data and high annotation costs, needing domain experts. Distant supervision is commonly used to tackle the scarcity of annotated data by automatically pairing knowledge graph relationships with raw texts. Such a pipeline is prone to noise and has added challenges to scale for covering a large number of biomedical concepts. We investigated existing broad-coverage distantly supervised biomedical relation extraction benchmarks and found a significant overlap between training and test relationships ranging from 26{\%} to 86{\%}. Furthermore, we noticed several inconsistencies in the data construction process of these benchmarks, and where there is no train-test leakage, the focus is on interactions between narrower entity types. This work presents a more accurate benchmark MedDistant19 for broad-coverage distantly supervised biomedical relation extraction that addresses these shortcomings and is obtained by aligning the MEDLINE abstracts with the widely used SNOMED Clinical Terms knowledge base. Lacking thorough evaluation with domain-specific language models, we also conduct experiments validating general domain relation extraction findings to biomedical relation extraction."",
}
@",medical domain,relation extract,annotat,challeng,benchmark,evalu
" ""Method Entity Extraction from Biomedical Texts"","," ""In the field of Natural Language Processing (NLP), extracting method entities from biomedical text has been a challenging task. Scientific research papers commonly consist of complex keywords and domain-specific terminologies, and new terminologies are continuously appearing. In this research, we find method terminologies in biomedical text using both rule-based and machine learning techniques. We first use linguistic features to extract method sentence candidates from a large corpus of biomedical text. Then, we construct a silver standard biomedical corpus composed of these sentences. With a rule-based method that makes use of the Stanza dependency parsing module, we label the method entities in these sentences. Using this silver standard corpus we train two machine learning algorithms to automatically extract method entities from biomedical text. Our results show that it is possible to develop machine learning models that can automatically extract method entities to a reasonable accuracy without the need for a gold standard dataset."",","{kalim-mercer-2022-method,
    title = ""Method Entity Extraction from Biomedical Texts"",
    author = ""Kalim, Waqar Bin  and
      Mercer, Robert E."",
    editor = ""Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon"",
    booktitle = ""Proceedings of the 29th International Conference on Computational Linguistics"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""International Committee on Computational Linguistics"",
    url = ""https://aclanthology.org/2022.coling-1.207"",
    pages = ""2357--2362"",
    abstract = ""In the field of Natural Language Processing (NLP), extracting method entities from biomedical text has been a challenging task. Scientific research papers commonly consist of complex keywords and domain-specific terminologies, and new terminologies are continuously appearing. In this research, we find method terminologies in biomedical text using both rule-based and machine learning techniques. We first use linguistic features to extract method sentence candidates from a large corpus of biomedical text. Then, we construct a silver standard biomedical corpus composed of these sentences. With a rule-based method that makes use of the Stanza dependency parsing module, we label the method entities in these sentences. Using this silver standard corpus we train two machine learning algorithms to automatically extract method entities from biomedical text. Our results show that it is possible to develop machine learning models that can automatically extract method entities to a reasonable accuracy without the need for a gold standard dataset."",
}
@",medical text,natural language process,natural languag,language process,nlp,challeng
" ""Supporting Medical Relation Extraction via Causality-Pruned Semantic Dependency Forest"","," ""Medical Relation Extraction (MRE) task aims to extract relations between entities in medical texts. Traditional relation extraction methods achieve impressive success by exploring the syntactic information, e.g., dependency tree. However, the quality of the 1-best dependency tree for medical texts produced by an out-of-domain parser is relatively limited so that the performance of medical relation extraction method may degenerate. To this end, we propose a method to jointly model semantic and syntactic information from medical texts based on causal explanation theory. We generate dependency forests consisting of the semantic-embedded 1-best dependency tree. Then, a task-specific causal explainer is adopted to prune the dependency forests, which are further fed into a designed graph convolutional network to learn the corresponding representation for downstream task. Empirically, the various comparisons on benchmark medical datasets demonstrate the effectiveness of our model."",","{jin-etal-2022-supporting,
    title = ""Supporting Medical Relation Extraction via Causality-Pruned Semantic Dependency Forest"",
    author = ""Jin, Yifan  and
      Li, Jiangmeng  and
      Lian, Zheng  and
      Jiao, Chengbo  and
      Hu, Xiaohui"",
    editor = ""Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon"",
    booktitle = ""Proceedings of the 29th International Conference on Computational Linguistics"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""International Committee on Computational Linguistics"",
    url = ""https://aclanthology.org/2022.coling-1.216"",
    pages = ""2450--2460"",
    abstract = ""Medical Relation Extraction (MRE) task aims to extract relations between entities in medical texts. Traditional relation extraction methods achieve impressive success by exploring the syntactic information, e.g., dependency tree. However, the quality of the 1-best dependency tree for medical texts produced by an out-of-domain parser is relatively limited so that the performance of medical relation extraction method may degenerate. To this end, we propose a method to jointly model semantic and syntactic information from medical texts based on causal explanation theory. We generate dependency forests consisting of the semantic-embedded 1-best dependency tree. Then, a task-specific causal explainer is adopted to prune the dependency forests, which are further fed into a designed graph convolutional network to learn the corresponding representation for downstream task. Empirically, the various comparisons on benchmark medical datasets demonstrate the effectiveness of our model."",
}
@",medical text,generat,relation extract,semant,benchmark
" ""Text-to-Text Extraction and Verbalization of Biomedical Event Graphs"","," ""Biomedical events represent complex, graphical, and semantically rich interactions expressed in the scientific literature. Almost all contributions in the event realm orbit around semantic parsing, usually employing discriminative architectures and cumbersome multi-step pipelines limited to a small number of target interaction types. We present the first lightweight framework to solve both event extraction and event verbalization with a unified text-to-text approach, allowing us to fuse all the resources so far designed for different tasks. To this end, we present a new event graph linearization technique and release highly comprehensive event-text paired datasets, covering more than 150 event types from multiple biology subareas (English language). By streamlining parsing and generation to translations, we propose baseline transformer model results according to multiple biomedical text mining benchmarks and NLG metrics. Our extractive models achieve greater state-of-the-art performance than single-task competitors and show promising capabilities for the controlled generation of coherent natural language utterances from structured data."",","{frisoni-etal-2022-text,
    title = ""Text-to-Text Extraction and Verbalization of Biomedical Event Graphs"",
    author = ""Frisoni, Giacomo  and
      Moro, Gianluca  and
      Balzani, Lorenzo"",
    editor = ""Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon"",
    booktitle = ""Proceedings of the 29th International Conference on Computational Linguistics"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""International Committee on Computational Linguistics"",
    url = ""https://aclanthology.org/2022.coling-1.238"",
    pages = ""2692--2710"",
    abstract = ""Biomedical events represent complex, graphical, and semantically rich interactions expressed in the scientific literature. Almost all contributions in the event realm orbit around semantic parsing, usually employing discriminative architectures and cumbersome multi-step pipelines limited to a small number of target interaction types. We present the first lightweight framework to solve both event extraction and event verbalization with a unified text-to-text approach, allowing us to fuse all the resources so far designed for different tasks. To this end, we present a new event graph linearization technique and release highly comprehensive event-text paired datasets, covering more than 150 event types from multiple biology subareas (English language). By streamlining parsing and generation to translations, we propose baseline transformer model results according to multiple biomedical text mining benchmarks and NLG metrics. Our extractive models achieve greater state-of-the-art performance than single-task competitors and show promising capabilities for the controlled generation of coherent natural language utterances from structured data."",
}
@",medical text,natural languag,translat,generat,semant,benchmark
" ""Summarizing Patients{'} Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models"","," ""Automatically summarizing patients{'} main problems from daily progress notes using natural language processing methods helps to battle against information and cognitive overload in hospital settings and potentially assists providers with computerized diagnostic decision support. Problem list summarization requires a model to understand, abstract, and generate clinical documentation. In this work, we propose a new NLP task that aims to generate a list of problems in a patient{'}s daily care plan using input from the provider{'}s progress notes during hospitalization. We investigate the performance of T5 and BART, two state-of-the-art seq2seq transformer architectures, in solving this problem. We provide a corpus built on top of progress notes from publicly available electronic health record progress notes in the Medical Information Mart for Intensive Care (MIMIC)-III. T5 and BART are trained on general domain text, and we experiment with a data augmentation method and a domain adaptation pre-training method to increase exposure to medical vocabulary and knowledge. Evaluation methods include ROUGE, BERTScore, cosine similarity on sentence embedding, and F-score on medical concepts. Results show that T5 with domain adaptive pre-training achieves significant performance gains compared to a rule-based system and general domain pre-trained language models, indicating a promising direction for tackling the problem summarization task."",","{gao-etal-2022-summarizing,
    title = ""Summarizing Patients{'} Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models"",
    author = ""Gao, Yanjun  and
      Dligach, Dmitriy  and
      Miller, Timothy  and
      Xu, Dongfang  and
      Churpek, Matthew M. M.  and
      Afshar, Majid"",
    editor = ""Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon"",
    booktitle = ""Proceedings of the 29th International Conference on Computational Linguistics"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""International Committee on Computational Linguistics"",
    url = ""https://aclanthology.org/2022.coling-1.264"",
    pages = ""2979--2991"",
    abstract = ""Automatically summarizing patients{'} main problems from daily progress notes using natural language processing methods helps to battle against information and cognitive overload in hospital settings and potentially assists providers with computerized diagnostic decision support. Problem list summarization requires a model to understand, abstract, and generate clinical documentation. In this work, we propose a new NLP task that aims to generate a list of problems in a patient{'}s daily care plan using input from the provider{'}s progress notes during hospitalization. We investigate the performance of T5 and BART, two state-of-the-art seq2seq transformer architectures, in solving this problem. We provide a corpus built on top of progress notes from publicly available electronic health record progress notes in the Medical Information Mart for Intensive Care (MIMIC)-III. T5 and BART are trained on general domain text, and we experiment with a data augmentation method and a domain adaptation pre-training method to increase exposure to medical vocabulary and knowledge. Evaluation methods include ROUGE, BERTScore, cosine similarity on sentence embedding, and F-score on medical concepts. Results show that T5 with domain adaptive pre-training achieves significant performance gains compared to a rule-based system and general domain pre-trained language models, indicating a promising direction for tackling the problem summarization task."",
}
@",electronic health record,health record,natural language process,natural languag,language process,nlp,generat,summar,evalu
" ""Benchmarking Automated Clinical Language Simplification: Dataset, Algorithm, and Evaluation"","," ""Patients with low health literacy usually have difficulty understanding medical jargon and the complex structure of professional medical language. Although some studies are proposed to automatically translate expert language into layperson-understandable language, only a few of them focus on both accuracy and readability aspects simultaneously in the clinical domain. Thus, simplification of the clinical language is still a challenging task, but unfortunately, it is not yet fully addressed in previous work. To benchmark this task, we construct a new dataset named MedLane to support the development and evaluation of automated clinical language simplification approaches. Besides, we propose a new model called DECLARE that follows the human annotation procedure and achieves state-of-the-art performance compared with eight strong baselines. To fairly evaluate the performance, we also propose three specific evaluation metrics. Experimental results demonstrate the utility of the annotated MedLane dataset and the effectiveness of the proposed model DECLARE."",","{luo-etal-2022-benchmarking,
    title = ""Benchmarking Automated Clinical Language Simplification: Dataset, Algorithm, and Evaluation"",
    author = ""Luo, Junyu  and
      Lin, Junxian  and
      Lin, Chi  and
      Xiao, Cao  and
      Gui, Xinning  and
      Ma, Fenglong"",
    editor = ""Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon"",
    booktitle = ""Proceedings of the 29th International Conference on Computational Linguistics"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""International Committee on Computational Linguistics"",
    url = ""https://aclanthology.org/2022.coling-1.313"",
    pages = ""3550--3562"",
    abstract = ""Patients with low health literacy usually have difficulty understanding medical jargon and the complex structure of professional medical language. Although some studies are proposed to automatically translate expert language into layperson-understandable language, only a few of them focus on both accuracy and readability aspects simultaneously in the clinical domain. Thus, simplification of the clinical language is still a challenging task, but unfortunately, it is not yet fully addressed in previous work. To benchmark this task, we construct a new dataset named MedLane to support the development and evaluation of automated clinical language simplification approaches. Besides, we propose a new model called DECLARE that follows the human annotation procedure and achieves state-of-the-art performance compared with eight strong baselines. To fairly evaluate the performance, we also propose three specific evaluation metrics. Experimental results demonstrate the utility of the annotated MedLane dataset and the effectiveness of the proposed model DECLARE."",
}
@",clinical domain,translat,annotat,challeng,benchmark,evalu
" ""Generation of Patient After-Visit Summaries to Support Physicians"","," ""An after-visit summary (AVS) is a summary note given to patients after their clinical visit. It recaps what happened during their clinical visit and guides patients{'} disease self-management. Studies have shown that a majority of patients found after-visit summaries useful. However, many physicians face excessive workloads and do not have time to write clear and informative summaries. In this paper, we study the problem of automatic generation of after-visit summaries and examine whether those summaries can convey the gist of clinical visits. We report our findings on a new clinical dataset that contains a large number of electronic health record (EHR) notes and their associated summaries. Our results suggest that generation of lay language after-visit summaries remains a challenging task. Crucially, we introduce a feedback mechanism that alerts physicians when an automatic summary fails to capture the important details of the clinical notes or when it contains hallucinated facts that are potentially detrimental to the summary quality. Automatic and human evaluation demonstrates the effectiveness of our approach in providing writing feedback and supporting physicians."",","{cai-etal-2022-generation,
    title = ""Generation of Patient After-Visit Summaries to Support Physicians"",
    author = ""Cai, Pengshan  and
      Liu, Fei  and
      Bajracharya, Adarsha  and
      Sills, Joe  and
      Kapoor, Alok  and
      Liu, Weisong  and
      Berlowitz, Dan  and
      Levy, David  and
      Pradhan, Richeek  and
      Yu, Hong"",
    editor = ""Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon"",
    booktitle = ""Proceedings of the 29th International Conference on Computational Linguistics"",
    month = oct,
    year = ""2022"",
    address = ""Gyeongju, Republic of Korea"",
    publisher = ""International Committee on Computational Linguistics"",
    url = ""https://aclanthology.org/2022.coling-1.544"",
    pages = ""6234--6247"",
    abstract = ""An after-visit summary (AVS) is a summary note given to patients after their clinical visit. It recaps what happened during their clinical visit and guides patients{'} disease self-management. Studies have shown that a majority of patients found after-visit summaries useful. However, many physicians face excessive workloads and do not have time to write clear and informative summaries. In this paper, we study the problem of automatic generation of after-visit summaries and examine whether those summaries can convey the gist of clinical visits. We report our findings on a new clinical dataset that contains a large number of electronic health record (EHR) notes and their associated summaries. Our results suggest that generation of lay language after-visit summaries remains a challenging task. Crucially, we introduce a feedback mechanism that alerts physicians when an automatic summary fails to capture the important details of the clinical notes or when it contains hallucinated facts that are potentially detrimental to the summary quality. Automatic and human evaluation demonstrates the effectiveness of our approach in providing writing feedback and supporting physicians."",
}
@",electronic health record,health record,clinical not,generat,summar,challeng,evalu
" ""{PLM}-{ICD}: Automatic {ICD} Coding with Pretrained Language Models"","," ""Automatically classifying electronic health records (EHRs) into diagnostic codes has been challenging to the NLP community. State-of-the-art methods treated this problem as a multi-label classification problem and proposed various architectures to model this problem. However, these systems did not leverage the superb performance of pretrained language models, which achieved superb performance on natural language understanding tasks. Prior work has shown that pretrained language models underperformed on this task with the regular fine-tuning scheme. Therefore, this paper aims at analyzing the causes of the underperformance and developing a framework for automatic ICD coding with pretrained language models. We spotted three main issues through the experiments: 1) large label space, 2) long input sequences, and 3) domain mismatch between pretraining and fine-tuning. We propose PLM-ICD, a framework that tackles the challenges with various strategies. The experimental results show that our proposed framework can overcome the challenges and achieves state-of-the-art performance in terms of multiple metrics on the benchmark MIMIC data. Our source code is available at \url{https://github.com/MiuLab/PLM-ICD}."",","{huang-etal-2022-plm,
    title = ""{PLM}-{ICD}: Automatic {ICD} Coding with Pretrained Language Models"",
    author = ""Huang, Chao-Wei  and
      Tsai, Shang-Chi  and
      Chen, Yun-Nung"",
    editor = ""Naumann, Tristan  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 4th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2022"",
    address = ""Seattle, WA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.clinicalnlp-1.2"",
    doi = ""10.18653/v1/2022.clinicalnlp-1.2"",
    pages = ""10--20"",
    abstract = ""Automatically classifying electronic health records (EHRs) into diagnostic codes has been challenging to the NLP community. State-of-the-art methods treated this problem as a multi-label classification problem and proposed various architectures to model this problem. However, these systems did not leverage the superb performance of pretrained language models, which achieved superb performance on natural language understanding tasks. Prior work has shown that pretrained language models underperformed on this task with the regular fine-tuning scheme. Therefore, this paper aims at analyzing the causes of the underperformance and developing a framework for automatic ICD coding with pretrained language models. We spotted three main issues through the experiments: 1) large label space, 2) long input sequences, and 3) domain mismatch between pretraining and fine-tuning. We propose PLM-ICD, a framework that tackles the challenges with various strategies. The experimental results show that our proposed framework can overcome the challenges and achieves state-of-the-art performance in terms of multiple metrics on the benchmark MIMIC data. Our source code is available at \url{https://github.com/MiuLab/PLM-ICD}."",
}
@",electronic health record,health record,natural language process,natural languag,language process,nlp,challeng,benchmark
" ""Cross-Language Transfer of High-Quality Annotations: Combining Neural Machine Translation with Cross-Linguistic Span Alignment to Apply {NER} to Clinical Texts in a Low-Resource Language"","," {In this work, cross-linguistic span prediction based on contextualized word embedding models is used together with neural machine translation (NMT) to transfer and apply the state-of-the-art models in natural language processing (NLP) to a low-resource language clinical corpus. Two directions are evaluated: (a) English models can be applied to translated texts to subsequently transfer the predicted annotations to the source language and (b) existing high-quality annotations can be transferred beyond translation and then used to train NLP models in the target language. Effectiveness and loss of transmission is evaluated using the German Berlin-T{\""u}bingen-Oncology Corpus (BRONCO) dataset with transferred external data from NCBI disease, SemEval-2013 drug-drug interaction (DDI) and i2b2/VA 2010 data. The use of English models for translated clinical texts has always involved attempts to take full advantage of the benefits associated with them (large pre-trained biomedical word embeddings). To improve advances in this area, we provide a general-purpose pipeline to transfer any annotated BRAT or CoNLL format to various target languages. For the entity class medication, good results were obtained with 0.806 $F1$-score after re-alignment. Limited success occurred in the diagnosis and treatment class with results just below 0.5 $F1$-score due to differences in annotation guidelines.},","{schafer-etal-2022-cross,
    title = ""Cross-Language Transfer of High-Quality Annotations: Combining Neural Machine Translation with Cross-Linguistic Span Alignment to Apply {NER} to Clinical Texts in a Low-Resource Language"",
    author = {Sch{\""a}fer, Henning  and
      Idrissi-Yaghir, Ahmad  and
      Horn, Peter  and
      Friedrich, Christoph},
    editor = ""Naumann, Tristan  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 4th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2022"",
    address = ""Seattle, WA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.clinicalnlp-1.6"",
    doi = ""10.18653/v1/2022.clinicalnlp-1.6"",
    pages = ""53--62"",
    abstract = {In this work, cross-linguistic span prediction based on contextualized word embedding models is used together with neural machine translation (NMT) to transfer and apply the state-of-the-art models in natural language processing (NLP) to a low-resource language clinical corpus. Two directions are evaluated: (a) English models can be applied to translated texts to subsequently transfer the predicted annotations to the source language and (b) existing high-quality annotations can be transferred beyond translation and then used to train NLP models in the target language. Effectiveness and loss of transmission is evaluated using the German Berlin-T{\""u}bingen-Oncology Corpus (BRONCO) dataset with transferred external data from NCBI disease, SemEval-2013 drug-drug interaction (DDI) and i2b2/VA 2010 data. The use of English models for translated clinical texts has always involved attempts to take full advantage of the benefits associated with them (large pre-trained biomedical word embeddings). To improve advances in this area, we provide a general-purpose pipeline to transfer any annotated BRAT or CoNLL format to various target languages. For the entity class medication, good results were obtained with 0.806 $F1$-score after re-alignment. Limited success occurred in the diagnosis and treatment class with results just below 0.5 $F1$-score due to differences in annotation guidelines.},
}
@",clinical text,natural language process,natural languag,language process,translat,nlp,annotat,evalu
" ""What Do You See in this Patient? Behavioral Testing of Clinical {NLP} Models"","," ""Decision support systems based on clinical notes have the potential to improve patient care by pointing doctors towards overseen risks. Predicting a patient{'}s outcome is an essential part of such systems, for which the use of deep neural networks has shown promising results. However, the patterns learned by these networks are mostly opaque and previous work revealed both reproduction of systemic biases and unexpected behavior for out-of-distribution patients. For application in clinical practice it is crucial to be aware of such behavior. We thus introduce a testing framework that evaluates clinical models regarding certain changes in the input. The framework helps to understand learned patterns and their influence on model decisions. In this work, we apply it to analyse the change in behavior with regard to the patient characteristics gender, age and ethnicity. Our evaluation of three current clinical NLP models demonstrates the concrete effects of these characteristics on the models{'} decisions. They show that model behavior varies drastically even when fine-tuned on the same data with similar AUROC score. These results exemplify the need for a broader communication of model behavior in the clinical domain."",","{van-aken-etal-2022-see,
    title = ""What Do You See in this Patient? Behavioral Testing of Clinical {NLP} Models"",
    author = {Van Aken, Betty  and
      Herrmann, Sebastian  and
      L{\""o}ser, Alexander},
    editor = ""Naumann, Tristan  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna"",
    booktitle = ""Proceedings of the 4th Clinical Natural Language Processing Workshop"",
    month = jul,
    year = ""2022"",
    address = ""Seattle, WA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.clinicalnlp-1.7"",
    doi = ""10.18653/v1/2022.clinicalnlp-1.7"",
    pages = ""63--73"",
    abstract = ""Decision support systems based on clinical notes have the potential to improve patient care by pointing doctors towards overseen risks. Predicting a patient{'}s outcome is an essential part of such systems, for which the use of deep neural networks has shown promising results. However, the patterns learned by these networks are mostly opaque and previous work revealed both reproduction of systemic biases and unexpected behavior for out-of-distribution patients. For application in clinical practice it is crucial to be aware of such behavior. We thus introduce a testing framework that evaluates clinical models regarding certain changes in the input. The framework helps to understand learned patterns and their influence on model decisions. In this work, we apply it to analyse the change in behavior with regard to the patient characteristics gender, age and ethnicity. Our evaluation of three current clinical NLP models demonstrates the concrete effects of these characteristics on the models{'} decisions. They show that model behavior varies drastically even when fine-tuned on the same data with similar AUROC score. These results exemplify the need for a broader communication of model behavior in the clinical domain."",
}
@",clinical domain,clinical not,natural language process,natural languag,language process,nlp,evalu
" ""{TCM}-{SD}: A Benchmark for Probing Syndrome Differentiation via Natural Language Processing"","," ""{``}Traditional Chinese Medicine (TCM) is a natural, safe, and effective therapy that has spread and been applied worldwide. The unique TCM diagnosis and treatment system requires a comprehensive analysis of a patient{'}s symptoms hidden in the clinical record written in free text. Prior studies have shown that this system can be informationized and intelligentized with the aid of artificial intelligence (AI) technology, such as natural language processing (NLP). However, existing datasets are not of sufficient quality nor quantity to support the further development of data-driven AI technology in TCM. Therefore, in this paper, we focus on the core task of the TCM diagnosis and treatment system{---}syndrome differentiation (SD){---}and we introduce the first public large-scale benchmark for SD, called TCM-SD. Our benchmark contains 54,152 real-world clinical records covering 148 syndromes. Furthermore, we collect a large-scale unlabelled textual corpus in the field of TCM and propose a domain-specific pre-trained language model, called ZYBERT. We conducted experiments using deep neural networks to establish a strong performance baseline, reveal various challenges in SD, and prove the potential of domain-specific pre-trained language model. Our study and analysis reveal opportunities for incorporating computer science and linguistics knowledge to explore the empirical validity of TCM theories.{''}"",","{mucheng-etal-2022-tcm,
    title = ""{TCM}-{SD}: A Benchmark for Probing Syndrome Differentiation via Natural Language Processing"",
    author = ""Mucheng, Ren  and
      Heyan, Huang  and
      Yuxiang, Zhou  and
      Qianwen, Cao  and
      Yuan, Bu  and
      Yang, Gao"",
    editor = ""Sun, Maosong  and
      Liu, Yang  and
      Che, Wanxiang  and
      Feng, Yang  and
      Qiu, Xipeng  and
      Rao, Gaoqi  and
      Chen, Yubo"",
    booktitle = ""Proceedings of the 21st Chinese National Conference on Computational Linguistics"",
    month = oct,
    year = ""2022"",
    address = ""Nanchang, China"",
    publisher = ""Chinese Information Processing Society of China"",
    url = ""https://aclanthology.org/2022.ccl-1.80"",
    pages = ""908--920"",
    abstract = ""{``}Traditional Chinese Medicine (TCM) is a natural, safe, and effective therapy that has spread and been applied worldwide. The unique TCM diagnosis and treatment system requires a comprehensive analysis of a patient{'}s symptoms hidden in the clinical record written in free text. Prior studies have shown that this system can be informationized and intelligentized with the aid of artificial intelligence (AI) technology, such as natural language processing (NLP). However, existing datasets are not of sufficient quality nor quantity to support the further development of data-driven AI technology in TCM. Therefore, in this paper, we focus on the core task of the TCM diagnosis and treatment system{---}syndrome differentiation (SD){---}and we introduce the first public large-scale benchmark for SD, called TCM-SD. Our benchmark contains 54,152 real-world clinical records covering 148 syndromes. Furthermore, we collect a large-scale unlabelled textual corpus in the field of TCM and propose a domain-specific pre-trained language model, called ZYBERT. We conducted experiments using deep neural networks to establish a strong performance baseline, reveal various challenges in SD, and prove the potential of domain-specific pre-trained language model. Our study and analysis reveal opportunities for incorporating computer science and linguistics knowledge to explore the empirical validity of TCM theories.{''}"",
    language = ""English"",
}
@",clinical record,natural language process,natural languag,language process,nlp,challeng,benchmark
" ""Data Augmentation for Biomedical Factoid Question Answering"","," ""We study the effect of seven data augmentation (DA) methods in factoid question answering, focusing on the biomedical domain, where obtaining training instances is particularly difficult. We experiment with data from the BIOASQ challenge, which we augment with training instances obtained from an artificial biomedical machine reading comprehension dataset, or via back-translation, information retrieval, word substitution based on WORD2VEC embeddings, or masked language modeling, question generation, or extending the given passage with additional context. We show that DA can lead to very significant performance gains, even when using large pre-trained Transformers, contributing to a broader discussion of if/when DA benefits large pre-trained models. One of the simplest DA methods, WORD2VEC-based word substitution, performed best and is recommended. We release our artificial training instances and code."",","{pappas-etal-2022-data,
    title = ""Data Augmentation for Biomedical Factoid Question Answering"",
    author = ""Pappas, Dimitris  and
      Malakasiotis, Prodromos  and
      Androutsopoulos, Ion"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 21st Workshop on Biomedical Language Processing"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.bionlp-1.6"",
    doi = ""10.18653/v1/2022.bionlp-1.6"",
    pages = ""63--81"",
    abstract = ""We study the effect of seven data augmentation (DA) methods in factoid question answering, focusing on the biomedical domain, where obtaining training instances is particularly difficult. We experiment with data from the BIOASQ challenge, which we augment with training instances obtained from an artificial biomedical machine reading comprehension dataset, or via back-translation, information retrieval, word substitution based on WORD2VEC embeddings, or masked language modeling, question generation, or extending the given passage with additional context. We show that DA can lead to very significant performance gains, even when using large pre-trained Transformers, contributing to a broader discussion of if/when DA benefits large pre-trained models. One of the simplest DA methods, WORD2VEC-based word substitution, performed best and is recommended. We release our artificial training instances and code."",
}
@",medical domain,language process,translat,nlp,generat,information retriev,challeng
" ""Slot Filling for Biomedical Information Extraction"","," ""Information Extraction (IE) from text refers to the task of extracting structured knowledge from unstructured text. The task typically consists of a series of sub-tasks such as Named Entity Recognition and Relation Extraction. Sourcing entity and relation type specific training data is a major bottleneck in domains with limited resources such as biomedicine. In this work we present a slot filling approach to the task of biomedical IE, effectively replacing the need for entity and relation-specific training data, allowing us to deal with zero-shot settings. We follow the recently proposed paradigm of coupling a Tranformer-based bi-encoder, Dense Passage Retrieval, with a Transformer-based reading comprehension model to extract relations from biomedical text. We assemble a biomedical slot filling dataset for both retrieval and reading comprehension and conduct a series of experiments demonstrating that our approach outperforms a number of simpler baselines. We also evaluate our approach end-to-end for standard as well as zero-shot settings. Our work provides a fresh perspective on how to solve biomedical IE tasks, in the absence of relevant training data. Our code, models and datasets are available at \url{https://github.com/tba}."",","{papanikolaou-etal-2022-slot,
    title = ""Slot Filling for Biomedical Information Extraction"",
    author = ""Papanikolaou, Yannis  and
      Staib, Marlene  and
      Grace, Justin Joshua  and
      Bennett, Francine"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 21st Workshop on Biomedical Language Processing"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.bionlp-1.7"",
    doi = ""10.18653/v1/2022.bionlp-1.7"",
    pages = ""82--90"",
    abstract = ""Information Extraction (IE) from text refers to the task of extracting structured knowledge from unstructured text. The task typically consists of a series of sub-tasks such as Named Entity Recognition and Relation Extraction. Sourcing entity and relation type specific training data is a major bottleneck in domains with limited resources such as biomedicine. In this work we present a slot filling approach to the task of biomedical IE, effectively replacing the need for entity and relation-specific training data, allowing us to deal with zero-shot settings. We follow the recently proposed paradigm of coupling a Tranformer-based bi-encoder, Dense Passage Retrieval, with a Transformer-based reading comprehension model to extract relations from biomedical text. We assemble a biomedical slot filling dataset for both retrieval and reading comprehension and conduct a series of experiments demonstrating that our approach outperforms a number of simpler baselines. We also evaluate our approach end-to-end for standard as well as zero-shot settings. Our work provides a fresh perspective on how to solve biomedical IE tasks, in the absence of relevant training data. Our code, models and datasets are available at \url{https://github.com/tba}."",
}
@",medical text,language process,nlp,relation extract,entity recognit,evalu
" ""{B}io{BART}: Pretraining and Evaluation of A Biomedical Generative Language Model"","," ""Pretrained language models have served as important backbones for natural language processing. Recently, in-domain pretraining has been shown to benefit various domain-specific downstream tasks. In the biomedical domain, natural language generation (NLG) tasks are of critical importance, while understudied. Approaching natural language understanding (NLU) tasks as NLG achieves satisfying performance in the general domain through constrained language generation or language prompting. We emphasize the lack of in-domain generative language models and the unsystematic generative downstream benchmarks in the biomedical domain, hindering the development of the research community. In this work, we introduce the generative language model BioBART that adapts BART to the biomedical domain. We collate various biomedical language generation tasks including dialogue, summarization, entity linking, and named entity recognition. BioBART pretrained on PubMed abstracts has enhanced performance compared to BART and set strong baselines on several tasks. Furthermore, we conduct ablation studies on the pretraining tasks for BioBART and find that sentence permutation has negative effects on downstream tasks."",","{yuan-etal-2022-biobart,
    title = ""{B}io{BART}: Pretraining and Evaluation of A Biomedical Generative Language Model"",
    author = ""Yuan, Hongyi  and
      Yuan, Zheng  and
      Gan, Ruyi  and
      Zhang, Jiaxing  and
      Xie, Yutao  and
      Yu, Sheng"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 21st Workshop on Biomedical Language Processing"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.bionlp-1.9"",
    doi = ""10.18653/v1/2022.bionlp-1.9"",
    pages = ""97--109"",
    abstract = ""Pretrained language models have served as important backbones for natural language processing. Recently, in-domain pretraining has been shown to benefit various domain-specific downstream tasks. In the biomedical domain, natural language generation (NLG) tasks are of critical importance, while understudied. Approaching natural language understanding (NLU) tasks as NLG achieves satisfying performance in the general domain through constrained language generation or language prompting. We emphasize the lack of in-domain generative language models and the unsystematic generative downstream benchmarks in the biomedical domain, hindering the development of the research community. In this work, we introduce the generative language model BioBART that adapts BART to the biomedical domain. We collate various biomedical language generation tasks including dialogue, summarization, entity linking, and named entity recognition. BioBART pretrained on PubMed abstracts has enhanced performance compared to BART and set strong baselines on several tasks. Furthermore, we conduct ablation studies on the pretraining tasks for BioBART and find that sentence permutation has negative effects on downstream tasks."",
}
@",medical domain,natural language process,natural languag,language process,nlp,generat,entity recognit,summar,benchmark,evalu
" ""Auxiliary Learning for Named Entity Recognition with Multiple Auxiliary Biomedical Training Data"","," ""Named entity recognition (NER) is one of the elemental technologies, which has been used for knowledge extraction from biomedical text. As one of the NER improvement approaches, multi-task learning that learns a model from multiple training data has been used. Among multi-task learning, an auxiliary learning method, which uses an auxiliary task for improving its target task, has shown higher NER performance than conventional multi-task learning for improving all the tasks simultaneously by using only one auxiliary task in the auxiliary learning. We propose Multiple Utilization of NER Corpora Helpful for Auxiliary BLESsing (MUNCH ABLES). MUNCHABLES utilizes multiple training datasets as auxiliary training data by the following methods; the first one is to finetune the NER model of the target task by sequentially performing auxiliary learning for each auxiliary training dataset, and the other is to use all training datasets in one auxiliary learning. We evaluate MUNCHABLES on eight biomedical-related domain NER tasks, where seven training datasets are used as auxiliary training data. The experiment results show that MUNCHABLES achieves higher accuracy than conventional multi-task learning methods on average while showing state-of-the-art accuracy."",","{watanabe-etal-2022-auxiliary,
    title = ""Auxiliary Learning for Named Entity Recognition with Multiple Auxiliary Biomedical Training Data"",
    author = ""Watanabe, Taiki  and
      Ichikawa, Tomoya  and
      Tamura, Akihiro  and
      Iwakura, Tomoya  and
      Ma, Chunpeng  and
      Kato, Tsuneo"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 21st Workshop on Biomedical Language Processing"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.bionlp-1.13"",
    doi = ""10.18653/v1/2022.bionlp-1.13"",
    pages = ""130--139"",
    abstract = ""Named entity recognition (NER) is one of the elemental technologies, which has been used for knowledge extraction from biomedical text. As one of the NER improvement approaches, multi-task learning that learns a model from multiple training data has been used. Among multi-task learning, an auxiliary learning method, which uses an auxiliary task for improving its target task, has shown higher NER performance than conventional multi-task learning for improving all the tasks simultaneously by using only one auxiliary task in the auxiliary learning. We propose Multiple Utilization of NER Corpora Helpful for Auxiliary BLESsing (MUNCH ABLES). MUNCHABLES utilizes multiple training datasets as auxiliary training data by the following methods; the first one is to finetune the NER model of the target task by sequentially performing auxiliary learning for each auxiliary training dataset, and the other is to use all training datasets in one auxiliary learning. We evaluate MUNCHABLES on eight biomedical-related domain NER tasks, where seven training datasets are used as auxiliary training data. The experiment results show that MUNCHABLES achieves higher accuracy than conventional multi-task learning methods on average while showing state-of-the-art accuracy."",
}
@",medical text,language process,nlp,entity recognit,evalu
" ""Biomedical {NER} using Novel Schema and Distant Supervision"","," ""Biomedical Named Entity Recognition (BMNER) is one of the most important tasks in the field of biomedical text mining. Most work so far on this task has not focused on identification of discontinuous and overlapping entities, even though they are present in significant fractions in real-life biomedical datasets. In this paper, we introduce a novel annotation schema to capture complex entities, and explore the effects of distant supervision on our deep-learning sequence labelling model. For BMNER task, our annotation schema outperforms other BIO-based annotation schemes on the same model. We also achieve higher F1-scores than state-of-the-art models on multiple corpora without fine-tuning embeddings, highlighting the efficacy of neural feature extraction using our model."",","{khandelwal-etal-2022-biomedical,
    title = ""Biomedical {NER} using Novel Schema and Distant Supervision"",
    author = ""Khandelwal, Anshita  and
      Kar, Alok  and
      Chikka, Veera Raghavendra  and
      Karlapalem, Kamalakar"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 21st Workshop on Biomedical Language Processing"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.bionlp-1.15"",
    doi = ""10.18653/v1/2022.bionlp-1.15"",
    pages = ""155--160"",
    abstract = ""Biomedical Named Entity Recognition (BMNER) is one of the most important tasks in the field of biomedical text mining. Most work so far on this task has not focused on identification of discontinuous and overlapping entities, even though they are present in significant fractions in real-life biomedical datasets. In this paper, we introduce a novel annotation schema to capture complex entities, and explore the effects of distant supervision on our deep-learning sequence labelling model. For BMNER task, our annotation schema outperforms other BIO-based annotation schemes on the same model. We also achieve higher F1-scores than state-of-the-art models on multiple corpora without fine-tuning embeddings, highlighting the efficacy of neural feature extraction using our model."",
}
@",medical text,language process,nlp,entity recognit,sequence label,annotat
" ""Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts"","," ""Despite the advances in digital healthcare systems offering curated structured knowledge, much of the critical information still lies in large volumes of unlabeled and unstructured clinical texts. These texts, which often contain protected health information (PHI), are exposed to information extraction tools for downstream applications, risking patient identification. Existing works in de-identification rely on using large-scale annotated corpora in English, which often are not suitable in real-world multilingual settings. Pre-trained language models (LM) have shown great potential for cross-lingual transfer in low-resource settings. In this work, we empirically show the few-shot cross-lingual transfer property of LMs for named entity recognition (NER) and apply it to solve a low-resource and real-world challenge of code-mixed (Spanish-Catalan) clinical notes de-identification in the stroke domain. We annotate a gold evaluation dataset to assess few-shot setting performance where we only use a few hundred labeled examples for training. Our model improves the zero-shot F1-score from 73.7{\%} to 91.2{\%} on the gold evaluation set when adapting Multilingual BERT (mBERT) (CITATION) from the MEDDOCAN (CITATION) corpus with our few-shot cross-lingual target corpus. When generalized to an out-of-sample test set, the best model achieves a human-evaluation F1-score of 97.2{\%}."",","{amin-etal-2022-shot,
    title = ""Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts"",
    author = ""Amin, Saadullah  and
      Pokaratsiri Goldstein, Noon  and
      Wixted, Morgan  and
      Garcia-Rudolph, Alejandro  and
      Mart{\'\i}nez-Costa, Catalina  and
      Neumann, Guenter"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 21st Workshop on Biomedical Language Processing"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.bionlp-1.20"",
    doi = ""10.18653/v1/2022.bionlp-1.20"",
    pages = ""200--211"",
    abstract = ""Despite the advances in digital healthcare systems offering curated structured knowledge, much of the critical information still lies in large volumes of unlabeled and unstructured clinical texts. These texts, which often contain protected health information (PHI), are exposed to information extraction tools for downstream applications, risking patient identification. Existing works in de-identification rely on using large-scale annotated corpora in English, which often are not suitable in real-world multilingual settings. Pre-trained language models (LM) have shown great potential for cross-lingual transfer in low-resource settings. In this work, we empirically show the few-shot cross-lingual transfer property of LMs for named entity recognition (NER) and apply it to solve a low-resource and real-world challenge of code-mixed (Spanish-Catalan) clinical notes de-identification in the stroke domain. We annotate a gold evaluation dataset to assess few-shot setting performance where we only use a few hundred labeled examples for training. Our model improves the zero-shot F1-score from 73.7{\%} to 91.2{\%} on the gold evaluation set when adapting Multilingual BERT (mBERT) (CITATION) from the MEDDOCAN (CITATION) corpus with our few-shot cross-lingual target corpus. When generalized to an out-of-sample test set, the best model achieves a human-evaluation F1-score of 97.2{\%}."",
}
@",clinical not,clinical text,language process,nlp,entity recognit,annotat,challeng,evalu,assess
" ""Comparing Encoder-Only and Encoder-Decoder Transformers for Relation Extraction from Biomedical Texts: An Empirical Study on Ten Benchmark Datasets"","," ""Biomedical relation extraction, aiming to automatically discover high-quality and semantic relations between the entities from free text, is becoming a vital step for automated knowledge discovery. Pretrained language models have achieved impressive performance on various natural language processing tasks, including relation extraction. In this paper, we perform extensive empirical comparisons of encoder-only transformers with the encoder-decoder transformer, specifically T5, on ten public biomedical relation extraction datasets. We study the relation extraction task from four major biomedical tasks, namely chemical-protein relation extraction, disease-protein relation extraction, drug-drug interaction, and protein-protein interaction. We also explore the use of multi-task fine-tuning to investigate the correlation among major biomedical relation extraction tasks. We report performance (micro F-score) using T5, BioBERT and PubMedBERT, demonstrating that T5 and multi-task learning can improve the performance of the biomedical relation extraction task."",","{sarrouti-etal-2022-comparing,
    title = ""Comparing Encoder-Only and Encoder-Decoder Transformers for Relation Extraction from Biomedical Texts: An Empirical Study on Ten Benchmark Datasets"",
    author = ""Sarrouti, Mourad  and
      Tao, Carson  and
      Mamy Randriamihaja, Yoann"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 21st Workshop on Biomedical Language Processing"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.bionlp-1.37"",
    doi = ""10.18653/v1/2022.bionlp-1.37"",
    pages = ""376--382"",
    abstract = ""Biomedical relation extraction, aiming to automatically discover high-quality and semantic relations between the entities from free text, is becoming a vital step for automated knowledge discovery. Pretrained language models have achieved impressive performance on various natural language processing tasks, including relation extraction. In this paper, we perform extensive empirical comparisons of encoder-only transformers with the encoder-decoder transformer, specifically T5, on ten public biomedical relation extraction datasets. We study the relation extraction task from four major biomedical tasks, namely chemical-protein relation extraction, disease-protein relation extraction, drug-drug interaction, and protein-protein interaction. We also explore the use of multi-task fine-tuning to investigate the correlation among major biomedical relation extraction tasks. We report performance (micro F-score) using T5, BioBERT and PubMedBERT, demonstrating that T5 and multi-task learning can improve the performance of the biomedical relation extraction task."",
}
@",medical text,natural language process,natural languag,language process,nlp,relation extract,semant,benchmark
" ""Utility Preservation of Clinical Text After De-Identification"","," ""Electronic health records contain valuable information about symptoms, diagnosis, treatment and outcomes of the treatments of individual patients. However, the records may also contain information that can reveal the identity of the patients. Removing these identifiers - the Protected Health Information (PHI) - can protect the identity of the patient. Automatic de-identification is a process which employs machine learning techniques to detect and remove PHI. However, automatic techniques are imperfect in their precision and introduce noise into the data. This study examines the impact of this noise on the utility of Swedish de-identified clinical data by using human evaluators and by training and testing BERT models. Our results indicate that de-identification does not harm the utility for clinical NLP and that human evaluators are less sensitive to noise from de-identification than expected."",","{vakili-dalianis-2022-utility,
    title = ""Utility Preservation of Clinical Text After De-Identification"",
    author = ""Vakili, Thomas  and
      Dalianis, Hercules"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 21st Workshop on Biomedical Language Processing"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.bionlp-1.38"",
    doi = ""10.18653/v1/2022.bionlp-1.38"",
    pages = ""383--388"",
    abstract = ""Electronic health records contain valuable information about symptoms, diagnosis, treatment and outcomes of the treatments of individual patients. However, the records may also contain information that can reveal the identity of the patients. Removing these identifiers - the Protected Health Information (PHI) - can protect the identity of the patient. Automatic de-identification is a process which employs machine learning techniques to detect and remove PHI. However, automatic techniques are imperfect in their precision and introduce noise into the data. This study examines the impact of this noise on the utility of Swedish de-identified clinical data by using human evaluators and by training and testing BERT models. Our results indicate that de-identification does not harm the utility for clinical NLP and that human evaluators are less sensitive to noise from de-identification than expected."",
}
@",electronic health record,health record,clinical text,language process,nlp,evalu
" ""Model Distillation for Faithful Explanations of Medical Code Predictions"","," ""Machine learning models that offer excellent predictive performance often lack the interpretability necessary to support integrated human machine decision-making. In clinical medicine and other high-risk settings, domain experts may be unwilling to trust model predictions without explanations. Work in explainable AI must balance competing objectives along two different axes: 1) Models should ideally be both accurate and simple. 2) Explanations must balance faithfulness to the model{'}s decision-making with their plausibility to a domain expert. We propose to use knowledge distillation, or training a student model that mimics the behavior of a trained teacher model, as a technique to generate faithful and plausible explanations. We evaluate our approach on the task of assigning ICD codes to clinical notes to demonstrate that the student model is faithful to the teacher model{'}s behavior and produces quality natural language explanations."",","{wood-doughty-etal-2022-model,
    title = ""Model Distillation for Faithful Explanations of Medical Code Predictions"",
    author = ""Wood-Doughty, Zach  and
      Cachola, Isabel  and
      Dredze, Mark"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 21st Workshop on Biomedical Language Processing"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.bionlp-1.41"",
    doi = ""10.18653/v1/2022.bionlp-1.41"",
    pages = ""412--425"",
    abstract = ""Machine learning models that offer excellent predictive performance often lack the interpretability necessary to support integrated human machine decision-making. In clinical medicine and other high-risk settings, domain experts may be unwilling to trust model predictions without explanations. Work in explainable AI must balance competing objectives along two different axes: 1) Models should ideally be both accurate and simple. 2) Explanations must balance faithfulness to the model{'}s decision-making with their plausibility to a domain expert. We propose to use knowledge distillation, or training a student model that mimics the behavior of a trained teacher model, as a technique to generate faithful and plausible explanations. We evaluate our approach on the task of assigning ICD codes to clinical notes to demonstrate that the student model is faithful to the teacher model{'}s behavior and produces quality natural language explanations."",
}
@",clinical not,natural languag,language process,nlp,generat,evalu
" ""A Comparison of Strategies for Source-Free Domain Adaptation"","," ""Data sharing restrictions are common in NLP, especially in the clinical domain, but there is limited research on adapting models to new domains without access to the original training data, a setting known as source-free domain adaptation. We take algorithms that traditionally assume access to the source-domain training data{---}active learning, self-training, and data augmentation{---}and adapt them for source free domain adaptation. Then we systematically compare these different strategies across multiple tasks and domains. We find that active learning yields consistent gains across all SemEval 2021 Task 10 tasks and domains, but though the shared task saw successful self-trained and data augmented models, our systematic comparison finds these strategies to be unreliable for source-free domain adaptation."",","{su-etal-2022-comparison,
    title = ""A Comparison of Strategies for Source-Free Domain Adaptation"",
    author = ""Su, Xin  and
      Zhao, Yiyun  and
      Bethard, Steven"",
    editor = ""Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline"",
    booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.acl-long.572"",
    doi = ""10.18653/v1/2022.acl-long.572"",
    pages = ""8352--8367"",
    abstract = ""Data sharing restrictions are common in NLP, especially in the clinical domain, but there is limited research on adapting models to new domains without access to the original training data, a setting known as source-free domain adaptation. We take algorithms that traditionally assume access to the source-domain training data{---}active learning, self-training, and data augmentation{---}and adapt them for source free domain adaptation. Then we systematically compare these different strategies across multiple tasks and domains. We find that active learning yields consistent gains across all SemEval 2021 Task 10 tasks and domains, but though the shared task saw successful self-trained and data augmented models, our systematic comparison finds these strategies to be unreliable for source-free domain adaptation."",
}
@",clinical domain,nlp,shared task
" ""Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"","," ""Being able to train Named Entity Recognition (NER) models for emerging topics is crucial for many real-world applications especially in the medical domain where new topics are continuously evolving out of the scope of existing models and datasets. For a realistic evaluation setup, we introduce a novel COVID-19 news NER dataset (COVIDNEWS-NER) and release 3000 entries of hand annotated strongly labelled sentences and 13000 auto-generated weakly labelled sentences. Besides the dataset, we propose CONTROSTER, a recipe to strategically combine weak and strong labels in improving NER in an emerging topic through transfer learning. We show the effectiveness of CONTROSTER on COVIDNEWS-NER while providing analysis on combining weak and strong labels for training. Our key findings are: (1) Using weak data to formulate an initial backbone before tuning on strong data outperforms methods trained on only strong or weak data. (2) A combination of out-of-domain and in-domain weak label training is crucial and can overcome saturation when being training on weak labels from a single source."",","{ficek-etal-2022-tackle,
    title = ""How to tackle an emerging topic? Combining strong and weak labels for Covid news {NER}"",
    author = ""Ficek, Aleksander  and
      Liu, Fangyu  and
      Collier, Nigel"",
    editor = ""He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui"",
    booktitle = ""Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)"",
    month = nov,
    year = ""2022"",
    address = ""Online only"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.aacl-short.60"",
    pages = ""488--496"",
    abstract = ""Being able to train Named Entity Recognition (NER) models for emerging topics is crucial for many real-world applications especially in the medical domain where new topics are continuously evolving out of the scope of existing models and datasets. For a realistic evaluation setup, we introduce a novel COVID-19 news NER dataset (COVIDNEWS-NER) and release 3000 entries of hand annotated strongly labelled sentences and 13000 auto-generated weakly labelled sentences. Besides the dataset, we propose CONTROSTER, a recipe to strategically combine weak and strong labels in improving NER in an emerging topic through transfer learning. We show the effectiveness of CONTROSTER on COVIDNEWS-NER while providing analysis on combining weak and strong labels for training. Our key findings are: (1) Using weak data to formulate an initial backbone before tuning on strong data outperforms methods trained on only strong or weak data. (2) A combination of out-of-domain and in-domain weak label training is crucial and can overcome saturation when being training on weak labels from a single source."",
}
@proceedings{aacl-2022-asia,
    title = ""Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"",
    editor = ""He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui"",
    month = nov,
    year = ""2022"",
    address = ""Online only"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.aacl-main.0"",
}
@",medical domain,natural language process,natural languag,language process,generat,entity recognit,annotat,evalu
" ""This Patient Looks Like That Patient: Prototypical Networks for Interpretable Diagnosis Prediction from Clinical Text"","," ""The use of deep neural models for diagnosis prediction from clinical text has shown promising results. However, in clinical practice such models must not only be accurate, but provide doctors with interpretable and helpful results. We introduce ProtoPatient, a novel method based on prototypical networks and label-wise attention with both of these abilities. ProtoPatient makes predictions based on parts of the text that are similar to prototypical patients{---}providing justifications that doctors understand. We evaluate the model on two publicly available clinical datasets and show that it outperforms existing baselines. Quantitative and qualitative evaluations with medical doctors further demonstrate that the model provides valuable explanations for clinical decision support."",","{van-aken-etal-2022-patient,
    title = ""This Patient Looks Like That Patient: Prototypical Networks for Interpretable Diagnosis Prediction from Clinical Text"",
    author = ""van Aken, Betty  and
      Papaioannou, Jens-Michalis  and
      Naik, Marcel  and
      Eleftheriadis, Georgios  and
      Nejdl, Wolfgang  and
      Gers, Felix  and
      Loeser, Alexander"",
    editor = ""He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui"",
    booktitle = ""Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"",
    month = nov,
    year = ""2022"",
    address = ""Online only"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.aacl-main.14"",
    pages = ""172--184"",
    abstract = ""The use of deep neural models for diagnosis prediction from clinical text has shown promising results. However, in clinical practice such models must not only be accurate, but provide doctors with interpretable and helpful results. We introduce ProtoPatient, a novel method based on prototypical networks and label-wise attention with both of these abilities. ProtoPatient makes predictions based on parts of the text that are similar to prototypical patients{---}providing justifications that doctors understand. We evaluate the model on two publicly available clinical datasets and show that it outperforms existing baselines. Quantitative and qualitative evaluations with medical doctors further demonstrate that the model provides valuable explanations for clinical decision support."",
}
@",clinical text,natural language process,natural languag,language process,evalu
" ""Multimodal Generation of Radiology Reports using Knowledge-Grounded Extraction of Entities and Relations"","," ""Automated reporting has the potential to assist radiologists with the time-consuming procedure of generating text radiology reports. Most existing approaches generate the report directly from the radiology image, however we observe that the resulting reports exhibit realistic style but lack clinical accuracy. Therefore, we propose a two-step pipeline that subdivides the problem into factual triple extraction followed by free-text report generation. The first step comprises supervised extraction of clinically relevant structured information from the image, expressed as triples of the form (entity1, relation, entity2). In the second step, these triples are input to condition the generation of the radiology report. In particular, we focus our work on Chest X-Ray (CXR) radiology report generation. The proposed framework shows state-of-the-art results on the MIMIC-CXR dataset according to most of the standard text generation metrics that we employ (BLEU, METEOR, ROUGE) and to clinical accuracy metrics (recall, precision and F1 assessed using the CheXpert labeler), also giving a 23{\%} reduction in the total number of errors and a 29{\%} reduction in critical clinical errors as assessed by expert human evaluation. In future, this solution can easily integrate more advanced model architectures - to both improve the triple extraction and the report generation - and can be applied to other complex image captioning tasks, such as those found in the medical domain."",","{dalla-serra-etal-2022-multimodal,
    title = ""Multimodal Generation of Radiology Reports using Knowledge-Grounded Extraction of Entities and Relations"",
    author = ""Dalla Serra, Francesco  and
      Clackett, William  and
      MacKinnon, Hamish  and
      Wang, Chaoyang  and
      Deligianni, Fani  and
      Dalton, Jeff  and
      O{'}Neil, Alison Q."",
    editor = ""He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui"",
    booktitle = ""Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"",
    month = nov,
    year = ""2022"",
    address = ""Online only"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.aacl-main.47"",
    pages = ""615--624"",
    abstract = ""Automated reporting has the potential to assist radiologists with the time-consuming procedure of generating text radiology reports. Most existing approaches generate the report directly from the radiology image, however we observe that the resulting reports exhibit realistic style but lack clinical accuracy. Therefore, we propose a two-step pipeline that subdivides the problem into factual triple extraction followed by free-text report generation. The first step comprises supervised extraction of clinically relevant structured information from the image, expressed as triples of the form (entity1, relation, entity2). In the second step, these triples are input to condition the generation of the radiology report. In particular, we focus our work on Chest X-Ray (CXR) radiology report generation. The proposed framework shows state-of-the-art results on the MIMIC-CXR dataset according to most of the standard text generation metrics that we employ (BLEU, METEOR, ROUGE) and to clinical accuracy metrics (recall, precision and F1 assessed using the CheXpert labeler), also giving a 23{\%} reduction in the total number of errors and a 29{\%} reduction in critical clinical errors as assessed by expert human evaluation. In future, this solution can easily integrate more advanced model architectures - to both improve the triple extraction and the report generation - and can be applied to other complex image captioning tasks, such as those found in the medical domain."",
}
@",medical domain,natural language process,natural languag,language process,generat,evalu,assess
" ""{B}iomed{C}urator: Data Curation for Biomedical Literature"","," ""We present BiomedCurator1, a web application that extracts the structured data from scientific articles in PubMed and ClinicalTrials.gov. BiomedCurator uses state-of-the-art natural language processing techniques to fill the fields pre-selected by domain experts in the relevant biomedical area. The BiomedCurator web application includes: text generation based model for relation extraction, entity detection and recognition, text classification model for extracting several fields, information retrieval from external knowledge base to retrieve IDs, and a pattern-based extraction approach that can extract several fields using regular expressions over the PubMed and ClinicalTrials.gov datasets. Evaluation results show that different approaches of BiomedCurator web application system are effective for automatic data curation in the biomedical domain."",","{sohrab-etal-2022-biomedcurator,
    title = ""{B}iomed{C}urator: Data Curation for Biomedical Literature"",
    author = ""Sohrab, Mohammad Golam  and
      Duong, Khoa N.A.  and
      Masami, Ikeda  and
      Topi{\'c}, Goran  and
      Natsume-Kitatani, Yayoi  and
      Kuroda, Masakata  and
      Itoh, Mari Nogami  and
      Takamura, Hiroya"",
    editor = ""Buntine, Wray  and
      Liakata, Maria"",
    booktitle = ""Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: System Demonstrations"",
    month = nov,
    year = ""2022"",
    address = ""Taipei, Taiwan"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.aacl-demo.8"",
    pages = ""63--71"",
    abstract = ""We present BiomedCurator1, a web application that extracts the structured data from scientific articles in PubMed and ClinicalTrials.gov. BiomedCurator uses state-of-the-art natural language processing techniques to fill the fields pre-selected by domain experts in the relevant biomedical area. The BiomedCurator web application includes: text generation based model for relation extraction, entity detection and recognition, text classification model for extracting several fields, information retrieval from external knowledge base to retrieve IDs, and a pattern-based extraction approach that can extract several fields using regular expressions over the PubMed and ClinicalTrials.gov datasets. Evaluation results show that different approaches of BiomedCurator web application system are effective for automatic data curation in the biomedical domain."",
}
@",medical domain,natural language process,natural languag,language process,generat,relation extract,information retriev,evalu
" ""Description-based Label Attention Classifier for Explainable {ICD}-9 Classification"","," ""ICD-9 coding is a relevant clinical billing task, where unstructured texts with information about a patient{'}s diagnosis and treatments are annotated with multiple ICD-9 codes. Automated ICD-9 coding is an active research field, where CNN- and RNN-based model architectures represent the state-of-the-art approaches. In this work, we propose a description-based label attention classifier to improve the model explainability when dealing with noisy texts like clinical notes."",","{feucht-etal-2021-description,
    title = ""Description-based Label Attention Classifier for Explainable {ICD}-9 Classification"",
    author = ""Feucht, Malte  and
      Wu, Zhiliang  and
      Althammer, Sophia  and
      Tresp, Volker"",
    editor = ""Xu, Wei  and
      Ritter, Alan  and
      Baldwin, Tim  and
      Rahimi, Afshin"",
    booktitle = ""Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)"",
    month = nov,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.wnut-1.8"",
    doi = ""10.18653/v1/2021.wnut-1.8"",
    pages = ""62--66"",
    abstract = ""ICD-9 coding is a relevant clinical billing task, where unstructured texts with information about a patient{'}s diagnosis and treatments are annotated with multiple ICD-9 codes. Automated ICD-9 coding is an active research field, where CNN- and RNN-based model architectures represent the state-of-the-art approaches. In this work, we propose a description-based label attention classifier to improve the model explainability when dealing with noisy texts like clinical notes."",
}
@",clinical not,generat,annotat
" ""A Comparison between Named Entity Recognition Models in the Biomedical Domain"","," ""The domain-specialised application of Named Entity Recognition (NER) is known as Biomedical NER (BioNER), which aims to identify and classify biomedical concepts that are of interest to researchers, such as genes, proteins, chemical compounds, drugs, mutations, diseases, and so on. The BioNER task is very similar to general NER but recognising Biomedical Named Entities (BNEs) is more challenging than recognising proper names from newspapers due to the characteristics of biomedical nomenclature. In order to address the challenges posed by BioNER, seven machine learning models were implemented comparing a transfer learning approach based on fine-tuned BERT with Bi-LSTM based neural models and a CRF model used as baseline. Precision, Recall and F1-score were used as performance scores evaluating the models on two well-known biomedical corpora: JNLPBA and BIOCREATIVE IV (BC-IV). Strict and partial matching were considered as evaluation criteria. The reported results show that a transfer learning approach based on fine-tuned BERT outperforms all others methods achieving the highest scores for all metrics on both corpora."",","{cariello-etal-2021-comparison,
    title = ""A Comparison between Named Entity Recognition Models in the Biomedical Domain"",
    author = ""Cariello, Maria Carmela  and
      Lenci, Alessandro  and
      Mitkov, Ruslan"",
    editor = ""Mitkov, Ruslan  and
      Sosoni, Vilelmini  and
      Gigu{\`e}re, Julie Christine  and
      Murgolo, Elena  and
      Deysel, Elizabeth"",
    booktitle = ""Proceedings of the Translation and Interpreting Technology Online Conference"",
    month = jul,
    year = ""2021"",
    address = ""Held Online"",
    publisher = ""INCOMA Ltd."",
    url = ""https://aclanthology.org/2021.triton-1.9"",
    pages = ""76--84"",
    abstract = ""The domain-specialised application of Named Entity Recognition (NER) is known as Biomedical NER (BioNER), which aims to identify and classify biomedical concepts that are of interest to researchers, such as genes, proteins, chemical compounds, drugs, mutations, diseases, and so on. The BioNER task is very similar to general NER but recognising Biomedical Named Entities (BNEs) is more challenging than recognising proper names from newspapers due to the characteristics of biomedical nomenclature. In order to address the challenges posed by BioNER, seven machine learning models were implemented comparing a transfer learning approach based on fine-tuned BERT with Bi-LSTM based neural models and a CRF model used as baseline. Precision, Recall and F1-score were used as performance scores evaluating the models on two well-known biomedical corpora: JNLPBA and BIOCREATIVE IV (BC-IV). Strict and partial matching were considered as evaluation criteria. The reported results show that a transfer learning approach based on fine-tuned BERT outperforms all others methods achieving the highest scores for all metrics on both corpora."",
}
@",medical domain,translat,nlp,entity recognit,challeng,evalu
" ""Weakly Supervised Extractive Summarization with Attention"","," ""Automatic summarization aims to extract important information from large amounts of textual data in order to create a shorter version of the original texts while preserving its information. Training traditional extractive summarization models relies heavily on human-engineered labels such as sentence-level annotations of summary-worthiness. However, in many use cases, such human-engineered labels do not exist and manually annotating thousands of documents for the purpose of training models may not be feasible. On the other hand, indirect signals for summarization are often available, such as agent actions for customer service dialogues, headlines for news articles, diagnosis for Electronic Health Records, etc. In this paper, we develop a general framework that generates extractive summarization as a byproduct of supervised learning tasks for indirect signals via the help of attention mechanism. We test our models on customer service dialogues and experimental results demonstrated that our models can reliably select informative sentences and words for automatic summarization."",","{zhuang-etal-2021-weakly,
    title = ""Weakly Supervised Extractive Summarization with Attention"",
    author = ""Zhuang, Yingying  and
      Lu, Yichao  and
      Wang, Simi"",
    editor = ""Li, Haizhou  and
      Levow, Gina-Anne  and
      Yu, Zhou  and
      Gupta, Chitralekha  and
      Sisman, Berrak  and
      Cai, Siqi  and
      Vandyke, David  and
      Dethlefs, Nina  and
      Wu, Yan  and
      Li, Junyi Jessy"",
    booktitle = ""Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue"",
    month = jul,
    year = ""2021"",
    address = ""Singapore and Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.sigdial-1.54"",
    pages = ""520--529"",
    abstract = ""Automatic summarization aims to extract important information from large amounts of textual data in order to create a shorter version of the original texts while preserving its information. Training traditional extractive summarization models relies heavily on human-engineered labels such as sentence-level annotations of summary-worthiness. However, in many use cases, such human-engineered labels do not exist and manually annotating thousands of documents for the purpose of training models may not be feasible. On the other hand, indirect signals for summarization are often available, such as agent actions for customer service dialogues, headlines for news articles, diagnosis for Electronic Health Records, etc. In this paper, we develop a general framework that generates extractive summarization as a byproduct of supervised learning tasks for indirect signals via the help of attention mechanism. We test our models on customer service dialogues and experimental results demonstrated that our models can reliably select informative sentences and words for automatic summarization."",
}
@",electronic health record,health record,generat,summar,annotat
" ""{C}hinese Medical Speech Recognition with Punctuated Hypothesis"","," ""Automatic Speech Recognition (ASR) technology presents the possibility for medical professionals to document patient record, diagnosis, postoperative care, patrol records, and etc. that are now done manually. However, earlier research aimed on Chinese medical speech corpus (ChiMeS) has two shortcomings: first is the lack of punctuation, resulting in reduced readability of the output transcript, and second is the poor recognition error rate, affecting its application put to the fields. Accordingly, the contributions of this paper consist of: (1) A punctuated Chinese medical corpus psChiMeS-14 newly annotated from ChiMeS-14, which is the collection of 516 anonymized medical record readouts of 867 minutes long, recorded by 15 professional nursing staff from Taipei Hospital of the Ministry of Health and Welfare. psChiMeS-14 is manually punctuated with: colons, commas, and periods, ready for general end-to-end ASR models. (2) A self-attention based speech recognition solution by conformer networks. Trained by and tested on psChiMeS-14 corpus, the solutions deliver state-of-the-art recognition performance: CER (character error rate) 10.5{\%}, and KER (Keyword error rate) of 13.10{\%}, respectively, which is contrasted to the 15.70{\%} CER and the 22.50{\%} KER by an earlier reported Joint CTC/Attention architecture."",","{chung-etal-2021-chinese,
    title = ""{C}hinese Medical Speech Recognition with Punctuated Hypothesis"",
    author = ""Chung, Sheng-Luen  and
      Fan, Jin-Huan  and
      Ting, Hsien-Wei"",
    editor = ""Lee, Lung-Hao  and
      Chang, Chia-Hui  and
      Chen, Kuan-Yu"",
    booktitle = ""Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021)"",
    month = oct,
    year = ""2021"",
    address = ""Taoyuan, Taiwan"",
    publisher = ""The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)"",
    url = ""https://aclanthology.org/2021.rocling-1.9"",
    pages = ""63--71"",
    abstract = ""Automatic Speech Recognition (ASR) technology presents the possibility for medical professionals to document patient record, diagnosis, postoperative care, patrol records, and etc. that are now done manually. However, earlier research aimed on Chinese medical speech corpus (ChiMeS) has two shortcomings: first is the lack of punctuation, resulting in reduced readability of the output transcript, and second is the poor recognition error rate, affecting its application put to the fields. Accordingly, the contributions of this paper consist of: (1) A punctuated Chinese medical corpus psChiMeS-14 newly annotated from ChiMeS-14, which is the collection of 516 anonymized medical record readouts of 867 minutes long, recorded by 15 professional nursing staff from Taipei Hospital of the Ministry of Health and Welfare. psChiMeS-14 is manually punctuated with: colons, commas, and periods, ready for general end-to-end ASR models. (2) A self-attention based speech recognition solution by conformer networks. Trained by and tested on psChiMeS-14 corpus, the solutions deliver state-of-the-art recognition performance: CER (character error rate) 10.5{\%}, and KER (Keyword error rate) of 13.10{\%}, respectively, which is contrasted to the 15.70{\%} CER and the 22.50{\%} KER by an earlier reported Joint CTC/Attention architecture."",
}
@",medical record,language process,annotat
" ""Active Learning for Assisted Corpus Construction: A Case Study in Knowledge Discovery from Biomedical Text"","," ""This paper presents an active learning approach that aims to reduce the human effort required during the annotation of natural language corpora composed of entities and semantic relations. Our approach assists human annotators by intelligently selecting the most informative sentences to annotate and then pre-annotating them with a few highly accurate entities and semantic relations. We define an uncertainty-based query strategy with a weighted density factor, using similarity metrics based on sentence embeddings. As a case study, we evaluate our approach via simulation in a biomedical corpus and estimate the potential reduction in total annotation time. Experimental results suggest that the query strategy reduces by between 35{\%} and 40{\%} the number of sentences that must be manually annotated to develop systems able to reach a target F1 score, while the pre-annotation strategy produces an additional 24{\%} reduction in the total annotation time. Overall, our preliminary experiments suggest that as much as 60{\%} of the annotation time could be saved while producing corpora that have the same usefulness for training machine learning algorithms. An open-source computational tool that implements the aforementioned strategies is presented and published online for the research community."",","{canizares-diaz-etal-2021-active,
    title = ""Active Learning for Assisted Corpus Construction: A Case Study in Knowledge Discovery from Biomedical Text"",
    author = ""Ca{\~n}izares-D{\'\i}az, Hian  and
      Piad-Morffis, Alejandro  and
      Estevez-Velarde, Suilan  and
      Guti{\'e}rrez, Yoan  and
      Almeida Cruz, Yudivi{\'a}n  and
      Montoyo, Andres  and
      Mu{\~n}oz-Guillena, Rafael"",
    editor = ""Mitkov, Ruslan  and
      Angelova, Galia"",
    booktitle = ""Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)"",
    month = sep,
    year = ""2021"",
    address = ""Held Online"",
    publisher = ""INCOMA Ltd."",
    url = ""https://aclanthology.org/2021.ranlp-1.26"",
    pages = ""216--225"",
    abstract = ""This paper presents an active learning approach that aims to reduce the human effort required during the annotation of natural language corpora composed of entities and semantic relations. Our approach assists human annotators by intelligently selecting the most informative sentences to annotate and then pre-annotating them with a few highly accurate entities and semantic relations. We define an uncertainty-based query strategy with a weighted density factor, using similarity metrics based on sentence embeddings. As a case study, we evaluate our approach via simulation in a biomedical corpus and estimate the potential reduction in total annotation time. Experimental results suggest that the query strategy reduces by between 35{\%} and 40{\%} the number of sentences that must be manually annotated to develop systems able to reach a target F1 score, while the pre-annotation strategy produces an additional 24{\%} reduction in the total annotation time. Overall, our preliminary experiments suggest that as much as 60{\%} of the annotation time could be saved while producing corpora that have the same usefulness for training machine learning algorithms. An open-source computational tool that implements the aforementioned strategies is presented and published online for the research community."",
}
@",medical text,natural language process,natural languag,language process,nlp,semant,annotat,evalu
" ""Application of Deep Learning Methods to {SNOMED} {CT} Encoding of Clinical Texts: From Data Collection to Extreme Multi-Label Text-Based Classification"","," ""Concept normalization of clinical texts to standard medical classifications and ontologies is a task with high importance for healthcare and medical research. We attempt to solve this problem through automatic SNOMED CT encoding, where SNOMED CT is one of the most widely used and comprehensive clinical term ontologies. Applying basic Deep Learning models, however, leads to undesirable results due to the unbalanced nature of the data and the extreme number of classes. We propose a classification procedure that features a multiple-step workflow consisting of label clustering, multi-cluster classification, and clusters-to-labels mapping. For multi-cluster classification, BioBERT is fine-tuned over our custom dataset. The clusters-to-labels mapping is carried out by a one-vs-all classifier (SVC) applied to every single cluster. We also present the steps for automatic dataset generation of textual descriptions annotated with SNOMED CT codes based on public data and linked open data. In order to cope with the problem that our dataset is highly unbalanced, some data augmentation methods are applied. The results from the conducted experiments show high accuracy and reliability of our approach for prediction of SNOMED CT codes relevant to a clinical text."",","{hristov-etal-2021-application,
    title = ""Application of Deep Learning Methods to {SNOMED} {CT} Encoding of Clinical Texts: From Data Collection to Extreme Multi-Label Text-Based Classification"",
    author = ""Hristov, Anton  and
      Tahchiev, Aleksandar  and
      Papazov, Hristo  and
      Tulechki, Nikola  and
      Primov, Todor  and
      Boytcheva, Svetla"",
    editor = ""Mitkov, Ruslan  and
      Angelova, Galia"",
    booktitle = ""Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)"",
    month = sep,
    year = ""2021"",
    address = ""Held Online"",
    publisher = ""INCOMA Ltd."",
    url = ""https://aclanthology.org/2021.ranlp-1.63"",
    pages = ""557--565"",
    abstract = ""Concept normalization of clinical texts to standard medical classifications and ontologies is a task with high importance for healthcare and medical research. We attempt to solve this problem through automatic SNOMED CT encoding, where SNOMED CT is one of the most widely used and comprehensive clinical term ontologies. Applying basic Deep Learning models, however, leads to undesirable results due to the unbalanced nature of the data and the extreme number of classes. We propose a classification procedure that features a multiple-step workflow consisting of label clustering, multi-cluster classification, and clusters-to-labels mapping. For multi-cluster classification, BioBERT is fine-tuned over our custom dataset. The clusters-to-labels mapping is carried out by a one-vs-all classifier (SVC) applied to every single cluster. We also present the steps for automatic dataset generation of textual descriptions annotated with SNOMED CT codes based on public data and linked open data. In order to cope with the problem that our dataset is highly unbalanced, some data augmentation methods are applied. The results from the conducted experiments show high accuracy and reliability of our approach for prediction of SNOMED CT codes relevant to a clinical text."",
}
@",clinical text,natural language process,natural languag,language process,nlp,generat,concept norm,annotat,public data
" ""Developing a Clinical Language Model for {S}wedish: Continued Pretraining of Generic {BERT} with In-Domain Data"","," ""The use of pretrained language models, fine-tuned to perform a specific downstream task, has become widespread in NLP. Using a generic language model in specialized domains may, however, be sub-optimal due to differences in language use and vocabulary. In this paper, it is investigated whether an existing, generic language model for Swedish can be improved for the clinical domain through continued pretraining with clinical text. The generic and domain-specific language models are fine-tuned and evaluated on three representative clinical NLP tasks: (i) identifying protected health information, (ii) assigning ICD-10 diagnosis codes to discharge summaries, and (iii) sentence-level uncertainty prediction. The results show that continued pretraining on in-domain data leads to improved performance on all three downstream tasks, indicating that there is a potential added value of domain-specific language models for clinical NLP."",","{lamproudis-etal-2021-developing,
    title = ""Developing a Clinical Language Model for {S}wedish: Continued Pretraining of Generic {BERT} with In-Domain Data"",
    author = ""Lamproudis, Anastasios  and
      Henriksson, Aron  and
      Dalianis, Hercules"",
    editor = ""Mitkov, Ruslan  and
      Angelova, Galia"",
    booktitle = ""Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)"",
    month = sep,
    year = ""2021"",
    address = ""Held Online"",
    publisher = ""INCOMA Ltd."",
    url = ""https://aclanthology.org/2021.ranlp-1.90"",
    pages = ""790--797"",
    abstract = ""The use of pretrained language models, fine-tuned to perform a specific downstream task, has become widespread in NLP. Using a generic language model in specialized domains may, however, be sub-optimal due to differences in language use and vocabulary. In this paper, it is investigated whether an existing, generic language model for Swedish can be improved for the clinical domain through continued pretraining with clinical text. The generic and domain-specific language models are fine-tuned and evaluated on three representative clinical NLP tasks: (i) identifying protected health information, (ii) assigning ICD-10 diagnosis codes to discharge summaries, and (iii) sentence-level uncertainty prediction. The results show that continued pretraining on in-domain data leads to improved performance on all three downstream tasks, indicating that there is a potential added value of domain-specific language models for clinical NLP."",
}
@",discharge summar,clinical domain,clinical text,natural language process,natural languag,language process,nlp,summar,evalu
" ""{G}e{SERA}: General-domain Summary Evaluation by Relevance Analysis"","," ""We present GeSERA, an open-source improved version of SERA for evaluating automatic extractive and abstractive summaries from the general domain. SERA is based on a search engine that compares candidate and reference summaries (called queries) against an information retrieval document base (called index). SERA was originally designed for the biomedical domain only, where it showed a better correlation with manual methods than the widely used lexical-based ROUGE method. In this paper, we take out SERA from the biomedical domain to the general one by adapting its content-based method to successfully evaluate summaries from the general domain. First, we improve the query reformulation strategy with POS Tags analysis of general-domain corpora. Second, we replace the biomedical index used in SERA with two article collections from AQUAINT-2 and Wikipedia. We conduct experiments with TAC2008, TAC2009, and CNNDM datasets. Results show that, in most cases, GeSERA achieves higher correlations with manual evaluation methods than SERA, while it reduces its gap with ROUGE for general-domain summary evaluation. GeSERA even surpasses ROUGE in two cases of TAC2009. Finally, we conduct extensive experiments and provide a comprehensive study of the impact of human annotators and the index size on summary evaluation with SERA and GeSERA."",","{lopez-espejel-etal-2021-gesera,
    title = ""{G}e{SERA}: General-domain Summary Evaluation by Relevance Analysis"",
    author = {L{\'o}pez Espejel, Jessica  and
      de Chalendar, Ga{\""e}l  and
      Garcia Flores, Jorge  and
      Charnois, Thierry  and
      Meza Ruiz, Ivan Vladimir},
    editor = ""Mitkov, Ruslan  and
      Angelova, Galia"",
    booktitle = ""Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)"",
    month = sep,
    year = ""2021"",
    address = ""Held Online"",
    publisher = ""INCOMA Ltd."",
    url = ""https://aclanthology.org/2021.ranlp-1.98"",
    pages = ""856--867"",
    abstract = ""We present GeSERA, an open-source improved version of SERA for evaluating automatic extractive and abstractive summaries from the general domain. SERA is based on a search engine that compares candidate and reference summaries (called queries) against an information retrieval document base (called index). SERA was originally designed for the biomedical domain only, where it showed a better correlation with manual methods than the widely used lexical-based ROUGE method. In this paper, we take out SERA from the biomedical domain to the general one by adapting its content-based method to successfully evaluate summaries from the general domain. First, we improve the query reformulation strategy with POS Tags analysis of general-domain corpora. Second, we replace the biomedical index used in SERA with two article collections from AQUAINT-2 and Wikipedia. We conduct experiments with TAC2008, TAC2009, and CNNDM datasets. Results show that, in most cases, GeSERA achieves higher correlations with manual evaluation methods than SERA, while it reduces its gap with ROUGE for general-domain summary evaluation. GeSERA even surpasses ROUGE in two cases of TAC2009. Finally, we conduct extensive experiments and provide a comprehensive study of the impact of human annotators and the index size on summary evaluation with SERA and GeSERA."",
}
@",medical domain,natural language process,natural languag,language process,nlp,summar,information retriev,annotat,evalu
" ""Learning Entity-Likeness with Multiple Approximate Matches for Biomedical {NER}"","," ""Biomedical Named Entities are complex, so approximate matching has been used to improve entity coverage. However, the usual approximate matching approach fetches only one matching result, which is often noisy. In this work, we propose a method for biomedical NER that fetches multiple approximate matches for a given phrase to leverage their variations to estimate entity-likeness. The model uses pooling to discard the unnecessary information from the noisy matching results, and learn the entity-likeness of the phrase with multiple approximate matches. Experimental results on three benchmark datasets from the biomedical domain, BC2GM, NCBI-disease, and BC4CHEMD, demonstrate the effectiveness. Our model improves the average by up to +0.21 points compared to a BioBERT-based NER."",","{nguyen-le-etal-2021-learning,
    title = ""Learning Entity-Likeness with Multiple Approximate Matches for Biomedical {NER}"",
    author = ""Nguyen Le, An  and
      Morita, Hajime  and
      Iwakura, Tomoya"",
    editor = ""Mitkov, Ruslan  and
      Angelova, Galia"",
    booktitle = ""Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)"",
    month = sep,
    year = ""2021"",
    address = ""Held Online"",
    publisher = ""INCOMA Ltd."",
    url = ""https://aclanthology.org/2021.ranlp-1.117"",
    pages = ""1040--1049"",
    abstract = ""Biomedical Named Entities are complex, so approximate matching has been used to improve entity coverage. However, the usual approximate matching approach fetches only one matching result, which is often noisy. In this work, we propose a method for biomedical NER that fetches multiple approximate matches for a given phrase to leverage their variations to estimate entity-likeness. The model uses pooling to discard the unnecessary information from the noisy matching results, and learn the entity-likeness of the phrase with multiple approximate matches. Experimental results on three benchmark datasets from the biomedical domain, BC2GM, NCBI-disease, and BC4CHEMD, demonstrate the effectiveness. Our model improves the average by up to +0.21 points compared to a BioBERT-based NER."",
}
@",medical domain,natural language process,natural languag,language process,nlp,benchmark
" ""Comparative Analysis of Fine-tuned Deep Learning Language Models for {ICD}-10 Classification Task for {B}ulgarian Language"","," ""The task of automatic diagnosis encoding into standard medical classifications and ontologies, is of great importance in medicine - both to support the daily tasks of physicians in the preparation and reporting of clinical documentation, and for automatic processing of clinical reports. In this paper we investigate the application and performance of different deep learning transformers for automatic encoding in ICD-10 of clinical texts in Bulgarian. The comparative analysis attempts to find which approach is more efficient to be used for fine-tuning of pretrained BERT family transformer to deal with a specific domain terminology on a rare language as Bulgarian. On the one side are used SlavicBERT and MultiligualBERT, that are pretrained for common vocabulary in Bulgarian, but lack medical terminology. On the other hand in the analysis are used BioBERT, ClinicalBERT, SapBERT, BlueBERT, that are pretrained for medical terminology in English, but lack training for language models in Bulgarian, and more over for vocabulary in Cyrillic. In our research study all BERT models are fine-tuned with additional medical texts in Bulgarian and then applied to the classification task for encoding medical diagnoses in Bulgarian into ICD-10 codes. Big corpora of diagnosis in Bulgarian annotated with ICD-10 codes is used for the classification task. Such an analysis gives a good idea of which of the models would be suitable for tasks of a similar type and domain. The experiments and evaluation results show that both approaches have comparable accuracy."",","{velichkov-etal-2021-comparative,
    title = ""Comparative Analysis of Fine-tuned Deep Learning Language Models for {ICD}-10 Classification Task for {B}ulgarian Language"",
    author = ""Velichkov, Boris  and
      Vassileva, Sylvia  and
      Gerginov, Simeon  and
      Kraychev, Boris  and
      Ivanov, Ivaylo  and
      Ivanov, Philip  and
      Koychev, Ivan  and
      Boytcheva, Svetla"",
    editor = ""Mitkov, Ruslan  and
      Angelova, Galia"",
    booktitle = ""Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)"",
    month = sep,
    year = ""2021"",
    address = ""Held Online"",
    publisher = ""INCOMA Ltd."",
    url = ""https://aclanthology.org/2021.ranlp-1.162"",
    pages = ""1448--1454"",
    abstract = ""The task of automatic diagnosis encoding into standard medical classifications and ontologies, is of great importance in medicine - both to support the daily tasks of physicians in the preparation and reporting of clinical documentation, and for automatic processing of clinical reports. In this paper we investigate the application and performance of different deep learning transformers for automatic encoding in ICD-10 of clinical texts in Bulgarian. The comparative analysis attempts to find which approach is more efficient to be used for fine-tuning of pretrained BERT family transformer to deal with a specific domain terminology on a rare language as Bulgarian. On the one side are used SlavicBERT and MultiligualBERT, that are pretrained for common vocabulary in Bulgarian, but lack medical terminology. On the other hand in the analysis are used BioBERT, ClinicalBERT, SapBERT, BlueBERT, that are pretrained for medical terminology in English, but lack training for language models in Bulgarian, and more over for vocabulary in Cyrillic. In our research study all BERT models are fine-tuned with additional medical texts in Bulgarian and then applied to the classification task for encoding medical diagnoses in Bulgarian into ICD-10 codes. Big corpora of diagnosis in Bulgarian annotated with ICD-10 codes is used for the classification task. Such an analysis gives a good idea of which of the models would be suitable for tasks of a similar type and domain. The experiments and evaluation results show that both approaches have comparable accuracy."",
}
@",clinical text,medical text,natural language process,natural languag,language process,nlp,annotat,evalu
" ""Learning and Evaluating a Differentially Private Pre-trained Language Model"","," ""Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task. While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data. One method to guarantee the privacy of such individuals is to train a differentially-private model, but this usually comes at the expense of model performance. Moreover, it is hard to tell given a privacy parameter $\epsilon$ what was the effect on the trained representation. In this work we aim to guide future practitioners and researchers on how to improve privacy while maintaining good model performance. We demonstrate how to train a differentially-private pre-trained language model (i.e., BERT) with a privacy guarantee of $\epsilon","{hoory-etal-2021-learning,
    title = ""Learning and Evaluating a Differentially Private Pre-trained Language Model"",
    author = ""Hoory, Shlomo  and
      Feder, Amir  and
      Tendler, Avichai  and
      Cohen, Alon  and
      Erell, Sofia  and
      Laish, Itay  and
      Nakhost, Hootan  and
      Stemmer, Uri  and
      Benjamini, Ayelet  and
      Hassidim, Avinatan  and
      Matias, Yossi"",
    editor = ""Feyisetan, Oluwaseyi  and
      Ghanavati, Sepideh  and
      Malmasi, Shervin  and
      Thaine, Patricia"",
    booktitle = ""Proceedings of the Third Workshop on Privacy in Natural Language Processing"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.privatenlp-1.3"",
    doi = ""10.18653/v1/2021.privatenlp-1.3"",
    pages = ""21--29"",
    abstract = ""Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task. While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data. One method to guarantee the privacy of such individuals is to train a differentially-private model, but this usually comes at the expense of model performance. Moreover, it is hard to tell given a privacy parameter $\epsilon$ what was the effect on the trained representation. In this work we aim to guide future practitioners and researchers on how to improve privacy while maintaining good model performance. We demonstrate how to train a differentially-private pre-trained language model (i.e., BERT) with a privacy guarantee of $\epsilon=1$ and with only a small degradation in performance. We experiment on a dataset of clinical notes with a model trained on a target entity extraction task, and compare it to a similar model trained without differential privacy. Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process."",
}
@",clinical not,natural language process,natural languag,language process,nlp,evalu
" ""Applying and Sharing pre-trained {BERT}-models for Named Entity Recognition and Classification in {S}wedish Electronic Patient Records"","," ""To be able to share the valuable information in electronic patient records (EPR) they first need to be de-identified in order to protect the privacy of their subjects. Named entity recognition and classification (NERC) is an important part of this process. In recent years, general-purpose language models pre-trained on large amounts of data, in particular BERT, have achieved state of the art results in NERC, among other NLP tasks. So far, however, no attempts have been made at applying BERT for NERC on Swedish EPR data. This study attempts to fine-tune one Swedish BERT-model and one multilingual BERT-model for NERC on a Swedish EPR corpus. The aim is to assess the applicability of BERT-models for this task as well as to compare the two models in a domain-specific Swedish language task. With the Swedish model, recall of 0.9220 and precision of 0.9226 is achieved. This is an improvement to previous results on the same corpus since the high recall does not sacrifice precision. As the models also perform relatively well when fine-tuned with pseudonymised data, it is concluded that there is good potential in using this method in a shareable de-identification system for Swedish clinical text."",","{grancharova-dalianis-2021-applying,
    title = ""Applying and Sharing pre-trained {BERT}-models for Named Entity Recognition and Classification in {S}wedish Electronic Patient Records"",
    author = ""Grancharova, Mila  and
      Dalianis, Hercules"",
    editor = ""Dobnik, Simon  and
      {\O}vrelid, Lilja"",
    booktitle = ""Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)"",
    month = may # "" 31--2 "" # jun,
    year = ""2021"",
    address = ""Reykjavik, Iceland (Online)"",
    publisher = {Link{\""o}ping University Electronic Press, Sweden},
    url = ""https://aclanthology.org/2021.nodalida-main.23"",
    pages = ""231--239"",
    abstract = ""To be able to share the valuable information in electronic patient records (EPR) they first need to be de-identified in order to protect the privacy of their subjects. Named entity recognition and classification (NERC) is an important part of this process. In recent years, general-purpose language models pre-trained on large amounts of data, in particular BERT, have achieved state of the art results in NERC, among other NLP tasks. So far, however, no attempts have been made at applying BERT for NERC on Swedish EPR data. This study attempts to fine-tune one Swedish BERT-model and one multilingual BERT-model for NERC on a Swedish EPR corpus. The aim is to assess the applicability of BERT-models for this task as well as to compare the two models in a domain-specific Swedish language task. With the Swedish model, recall of 0.9220 and precision of 0.9226 is achieved. This is an improvement to previous results on the same corpus since the high recall does not sacrifice precision. As the models also perform relatively well when fine-tuned with pseudonymised data, it is concluded that there is good potential in using this method in a shareable de-identification system for Swedish clinical text."",
}
@",clinical text,nlp,entity recognit,assess
" ""Towards Automating Medical Scribing : Clinic Visit {D}ialogue2{N}ote Sentence Alignment and Snippet Summarization"","," ""Medical conversations from patient visits are routinely summarized into clinical notes for documentation of clinical care. The automatic creation of clinical note is particularly challenging given that it requires summarization over spoken language and multiple speaker turns; as well, clinical notes include highly technical semi-structured text. In this paper, we describe our corpus creation method and baseline systems for two NLP tasks, clinical dialogue2note sentence alignment and clinical dialogue2note snippet summarization. These two systems, as well as other models created from such a corpus, may be incorporated as parts of an overall end-to-end clinical note generation system."",","{yim-yetisgen-2021-towards,
    title = ""Towards Automating Medical Scribing : Clinic Visit {D}ialogue2{N}ote Sentence Alignment and Snippet Summarization"",
    author = ""Yim, Wen-wai  and
      Yetisgen, Meliha"",
    editor = ""Shivade, Chaitanya  and
      Gangadharaiah, Rashmi  and
      Gella, Spandana  and
      Konam, Sandeep  and
      Yuan, Shaoqing  and
      Zhang, Yi  and
      Bhatia, Parminder  and
      Wallace, Byron"",
    booktitle = ""Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.nlpmc-1.2"",
    doi = ""10.18653/v1/2021.nlpmc-1.2"",
    pages = ""10--20"",
    abstract = ""Medical conversations from patient visits are routinely summarized into clinical notes for documentation of clinical care. The automatic creation of clinical note is particularly challenging given that it requires summarization over spoken language and multiple speaker turns; as well, clinical notes include highly technical semi-structured text. In this paper, we describe our corpus creation method and baseline systems for two NLP tasks, clinical dialogue2note sentence alignment and clinical dialogue2note snippet summarization. These two systems, as well as other models created from such a corpus, may be incorporated as parts of an overall end-to-end clinical note generation system."",
}
@",clinical not,natural language process,natural languag,language process,nlp,generat,summar,challeng
" ""Assertion Detection in Clinical Notes: Medical Language Models to the Rescue?"","," ""In order to provide high-quality care, health professionals must efficiently identify the presence, possibility, or absence of symptoms, treatments and other relevant entities in free-text clinical notes. Such is the task of assertion detection - to identify the assertion class (present, possible, absent) of an entity based on textual cues in unstructured text. We evaluate state-of-the-art medical language models on the task and show that they outperform the baselines in all three classes. As transferability is especially important in the medical domain we further study how the best performing model behaves on unseen data from two other medical datasets. For this purpose we introduce a newly annotated set of 5,000 assertions for the publicly available MIMIC-III dataset. We conclude with an error analysis that reveals situations in which the models still go wrong and points towards future research directions."",","{van-aken-etal-2021-assertion,
    title = ""Assertion Detection in Clinical Notes: Medical Language Models to the Rescue?"",
    author = ""van Aken, Betty  and
      Trajanovska, Ivana  and
      Siu, Amy  and
      Mayrdorfer, Manuel  and
      Budde, Klemens  and
      Loeser, Alexander"",
    editor = ""Shivade, Chaitanya  and
      Gangadharaiah, Rashmi  and
      Gella, Spandana  and
      Konam, Sandeep  and
      Yuan, Shaoqing  and
      Zhang, Yi  and
      Bhatia, Parminder  and
      Wallace, Byron"",
    booktitle = ""Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.nlpmc-1.5"",
    doi = ""10.18653/v1/2021.nlpmc-1.5"",
    pages = ""35--40"",
    abstract = ""In order to provide high-quality care, health professionals must efficiently identify the presence, possibility, or absence of symptoms, treatments and other relevant entities in free-text clinical notes. Such is the task of assertion detection - to identify the assertion class (present, possible, absent) of an entity based on textual cues in unstructured text. We evaluate state-of-the-art medical language models on the task and show that they outperform the baselines in all three classes. As transferability is especially important in the medical domain we further study how the best performing model behaves on unseen data from two other medical datasets. For this purpose we introduce a newly annotated set of 5,000 assertions for the publicly available MIMIC-III dataset. We conclude with an error analysis that reveals situations in which the models still go wrong and points towards future research directions."",
}
@",medical domain,clinical not,natural language process,natural languag,language process,nlp,annotat,evalu
" ""Joint Summarization-Entailment Optimization for Consumer Health Question Understanding"","," ""Understanding the intent of medical questions asked by patients, or Consumer Health Questions, is an essential skill for medical Conversational AI systems. We propose a novel data-augmented and simple joint learning approach combining question summarization and Recognizing Question Entailment (RQE) in the medical domain. Our data augmentation approach enables to use just one dataset for joint learning. We show improvements on both tasks across four biomedical datasets in accuracy (+8{\%}), ROUGE-1 (+2.5{\%}) and human evaluation scores. Human evaluation shows joint learning generates faithful and informative summaries. Finally, we release our code, the two question summarization datasets extracted from a large-scale medical dialogue dataset, as well as our augmented datasets."",","{mrini-etal-2021-joint,
    title = ""Joint Summarization-Entailment Optimization for Consumer Health Question Understanding"",
    author = ""Mrini, Khalil  and
      Dernoncourt, Franck  and
      Chang, Walter  and
      Farcas, Emilia  and
      Nakashole, Ndapa"",
    editor = ""Shivade, Chaitanya  and
      Gangadharaiah, Rashmi  and
      Gella, Spandana  and
      Konam, Sandeep  and
      Yuan, Shaoqing  and
      Zhang, Yi  and
      Bhatia, Parminder  and
      Wallace, Byron"",
    booktitle = ""Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.nlpmc-1.8"",
    doi = ""10.18653/v1/2021.nlpmc-1.8"",
    pages = ""58--65"",
    abstract = ""Understanding the intent of medical questions asked by patients, or Consumer Health Questions, is an essential skill for medical Conversational AI systems. We propose a novel data-augmented and simple joint learning approach combining question summarization and Recognizing Question Entailment (RQE) in the medical domain. Our data augmentation approach enables to use just one dataset for joint learning. We show improvements on both tasks across four biomedical datasets in accuracy (+8{\%}), ROUGE-1 (+2.5{\%}) and human evaluation scores. Human evaluation shows joint learning generates faithful and informative summaries. Finally, we release our code, the two question summarization datasets extracted from a large-scale medical dialogue dataset, as well as our augmented datasets."",
}
@",medical domain,natural language process,natural languag,language process,nlp,generat,summar,evalu
" ""Negation typology and general representation models for cross-lingual zero-shot negation scope resolution in {R}ussian, {F}rench, and {S}panish."","," ""Negation is a linguistic universal that poses difficulties for cognitive and computational processing. Despite many advances in text analytics, negation resolution remains an acute and continuously researched question in Natural Language Processing. Reliable negation parsing affects results in biomedical text mining, sentiment analysis, machine translation, and many other fields. The availability of multilingual pre-trained general representation models makes it possible to experiment with negation detection in languages that lack annotated data. In this work we test the performance of two state-of-the-art contextual representation models, Multilingual BERT and XLM-RoBERTa. We resolve negation scope by conducting zero-shot transfer between English, Spanish, French, and Russian. Our best result amounts to a token-level F1-score of 86.86{\%} between Spanish and Russian. We correlate these results with a linguistic negation typology and lexical capacity of the models."",","{shaitarova-rinaldi-2021-negation,
    title = ""Negation typology and general representation models for cross-lingual zero-shot negation scope resolution in {R}ussian, {F}rench, and {S}panish."",
    author = ""Shaitarova, Anastassia  and
      Rinaldi, Fabio"",
    editor = ""Durmus, Esin  and
      Gupta, Vivek  and
      Liu, Nelson  and
      Peng, Nanyun  and
      Su, Yu"",
    booktitle = ""Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.naacl-srw.3"",
    doi = ""10.18653/v1/2021.naacl-srw.3"",
    pages = ""15--23"",
    abstract = ""Negation is a linguistic universal that poses difficulties for cognitive and computational processing. Despite many advances in text analytics, negation resolution remains an acute and continuously researched question in Natural Language Processing. Reliable negation parsing affects results in biomedical text mining, sentiment analysis, machine translation, and many other fields. The availability of multilingual pre-trained general representation models makes it possible to experiment with negation detection in languages that lack annotated data. In this work we test the performance of two state-of-the-art contextual representation models, Multilingual BERT and XLM-RoBERTa. We resolve negation scope by conducting zero-shot transfer between English, Spanish, French, and Russian. Our best result amounts to a token-level F1-score of 86.86{\%} between Spanish and Russian. We correlate these results with a linguistic negation typology and lexical capacity of the models."",
}
@",medical text,natural language process,natural languag,language process,translat,sentiment,annotat
" ""Modeling Diagnostic Label Correlation for Automatic {ICD} Coding"","," ""Given the clinical notes written in electronic health records (EHRs), it is challenging to predict the diagnostic codes which is formulated as a multi-label classification task. The large set of labels, the hierarchical dependency, and the imbalanced data make this prediction task extremely hard. Most existing work built a binary prediction for each label independently, ignoring the dependencies between labels. To address this problem, we propose a two-stage framework to improve automatic ICD coding by capturing the label correlation. Specifically, we train a label set distribution estimator to rescore the probability of each label set candidate generated by a base predictor. This paper is the first attempt at learning the label set distribution as a reranking module for ICD coding. In the experiments, our proposed framework is able to improve upon best-performing predictors for medical code prediction on the benchmark MIMIC datasets."",","{tsai-etal-2021-modeling,
    title = ""Modeling Diagnostic Label Correlation for Automatic {ICD} Coding"",
    author = ""Tsai, Shang-Chi  and
      Huang, Chao-Wei  and
      Chen, Yun-Nung"",
    editor = ""Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao"",
    booktitle = ""Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.naacl-main.318"",
    doi = ""10.18653/v1/2021.naacl-main.318"",
    pages = ""4043--4052"",
    abstract = ""Given the clinical notes written in electronic health records (EHRs), it is challenging to predict the diagnostic codes which is formulated as a multi-label classification task. The large set of labels, the hierarchical dependency, and the imbalanced data make this prediction task extremely hard. Most existing work built a binary prediction for each label independently, ignoring the dependencies between labels. To address this problem, we propose a two-stage framework to improve automatic ICD coding by capturing the label correlation. Specifically, we train a label set distribution estimator to rescore the probability of each label set candidate generated by a base predictor. This paper is the first attempt at learning the label set distribution as a reranking module for ICD coding. In the experiments, our proposed framework is able to improve upon best-performing predictors for medical code prediction on the benchmark MIMIC datasets."",
}
@",electronic health record,health record,clinical not,generat,challeng,benchmark
" ""Self-Alignment Pretraining for Biomedical Entity Representations"","," ""Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust."",","{liu-etal-2021-self,
    title = ""Self-Alignment Pretraining for Biomedical Entity Representations"",
    author = ""Liu, Fangyu  and
      Shareghi, Ehsan  and
      Meng, Zaiqiao  and
      Basaldella, Marco  and
      Collier, Nigel"",
    editor = ""Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao"",
    booktitle = ""Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.naacl-main.334"",
    doi = ""10.18653/v1/2021.naacl-main.334"",
    pages = ""4228--4238"",
    abstract = ""Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust."",
}
@",medical domain,semant,challeng,benchmark
" ""Leveraging Deep Representations of Radiology Reports in Survival Analysis for Predicting Heart Failure Patient Mortality"","," ""Utilizing clinical texts in survival analysis is difficult because they are largely unstructured. Current automatic extraction models fail to capture textual information comprehensively since their labels are limited in scope. Furthermore, they typically require a large amount of data and high-quality expert annotations for training. In this work, we present a novel method of using BERT-based hidden layer representations of clinical texts as covariates for proportional hazards models to predict patient survival outcomes. We show that hidden layers yield notably more accurate predictions than predefined features, outperforming the previous baseline model by 5.7{\%} on average across C-index and time-dependent AUC. We make our work publicly available at \url{https://github.com/bionlplab/heart_failure_mortality}."",","{lee-etal-2021-leveraging,
    title = ""Leveraging Deep Representations of Radiology Reports in Survival Analysis for Predicting Heart Failure Patient Mortality"",
    author = ""Lee, Hyun Gi  and
      Sholle, Evan  and
      Beecy, Ashley  and
      Al{'}Aref, Subhi  and
      Peng, Yifan"",
    editor = ""Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao"",
    booktitle = ""Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.naacl-main.358"",
    doi = ""10.18653/v1/2021.naacl-main.358"",
    pages = ""4533--4538"",
    abstract = ""Utilizing clinical texts in survival analysis is difficult because they are largely unstructured. Current automatic extraction models fail to capture textual information comprehensively since their labels are limited in scope. Furthermore, they typically require a large amount of data and high-quality expert annotations for training. In this work, we present a novel method of using BERT-based hidden layer representations of clinical texts as covariates for proportional hazards models to predict patient survival outcomes. We show that hidden layers yield notably more accurate predictions than predefined features, outperforming the previous baseline model by 5.7{\%} on average across C-index and time-dependent AUC. We make our work publicly available at \url{https://github.com/bionlplab/heart_failure_mortality}."",
}
@",clinical text,nlp,annotat
" ""Paragraph-level Simplification of Medical Texts"","," ""We consider the problem of learning to simplify medical texts. This is important because most reliable, up-to-date information in biomedicine is dense with jargon and thus practically inaccessible to the lay audience. Furthermore, manual simplification does not scale to the rapidly growing body of biomedical literature, motivating the need for automated approaches. Unfortunately, there are no large-scale resources available for this task. In this work we introduce a new corpus of parallel texts in English comprising technical and lay summaries of all published evidence pertaining to different clinical topics. We then propose a new metric based on likelihood scores from a masked language model pretrained on scientific texts. We show that this automated measure better differentiates between technical and lay summaries than existing heuristics. We introduce and evaluate baseline encoder-decoder Transformer models for simplification and propose a novel augmentation to these in which we explicitly penalize the decoder for producing {``}jargon{''} terms; we find that this yields improvements over baselines in terms of readability."",","{devaraj-etal-2021-paragraph,
    title = ""Paragraph-level Simplification of Medical Texts"",
    author = ""Devaraj, Ashwin  and
      Marshall, Iain  and
      Wallace, Byron  and
      Li, Junyi Jessy"",
    editor = ""Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao"",
    booktitle = ""Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.naacl-main.395"",
    doi = ""10.18653/v1/2021.naacl-main.395"",
    pages = ""4972--4984"",
    abstract = ""We consider the problem of learning to simplify medical texts. This is important because most reliable, up-to-date information in biomedicine is dense with jargon and thus practically inaccessible to the lay audience. Furthermore, manual simplification does not scale to the rapidly growing body of biomedical literature, motivating the need for automated approaches. Unfortunately, there are no large-scale resources available for this task. In this work we introduce a new corpus of parallel texts in English comprising technical and lay summaries of all published evidence pertaining to different clinical topics. We then propose a new metric based on likelihood scores from a masked language model pretrained on scientific texts. We show that this automated measure better differentiates between technical and lay summaries than existing heuristics. We introduce and evaluate baseline encoder-decoder Transformer models for simplification and propose a novel augmentation to these in which we explicitly penalize the decoder for producing {``}jargon{''} terms; we find that this yields improvements over baselines in terms of readability."",
}
@",medical text,summar,evalu
" ""{BBAEG}: Towards {BERT}-based Biomedical Adversarial Example Generation for Text Classification"","," ""Healthcare predictive analytics aids medical decision-making, diagnosis prediction and drug review analysis. Therefore, prediction accuracy is an important criteria which also necessitates robust predictive language models. However, the models using deep learning have been proven vulnerable towards insignificantly perturbed input instances which are less likely to be misclassified by humans. Recent efforts of generating adversaries using rule-based synonyms and BERT-MLMs have been witnessed in general domain, but the ever-increasing biomedical literature poses unique challenges. We propose BBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box attack algorithm for biomedical text classification, leveraging the strengths of both domain-specific synonym replacement for biomedical named entities and BERT-MLM predictions, spelling variation and number replacement. Through automatic and human evaluation on two datasets, we demonstrate that BBAEG performs stronger attack with better language fluency, semantic coherence as compared to prior work."",","{mondal-2021-bbaeg,
    title = ""{BBAEG}: Towards {BERT}-based Biomedical Adversarial Example Generation for Text Classification"",
    author = ""Mondal, Ishani"",
    editor = ""Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao"",
    booktitle = ""Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.naacl-main.423"",
    doi = ""10.18653/v1/2021.naacl-main.423"",
    pages = ""5378--5384"",
    abstract = ""Healthcare predictive analytics aids medical decision-making, diagnosis prediction and drug review analysis. Therefore, prediction accuracy is an important criteria which also necessitates robust predictive language models. However, the models using deep learning have been proven vulnerable towards insignificantly perturbed input instances which are less likely to be misclassified by humans. Recent efforts of generating adversaries using rule-based synonyms and BERT-MLMs have been witnessed in general domain, but the ever-increasing biomedical literature poses unique challenges. We propose BBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box attack algorithm for biomedical text classification, leveraging the strengths of both domain-specific synonym replacement for biomedical named entities and BERT-MLM predictions, spelling variation and number replacement. Through automatic and human evaluation on two datasets, we demonstrate that BBAEG performs stronger attack with better language fluency, semantic coherence as compared to prior work."",
}
@",medical text,generat,semant,challeng,evalu
" ""Discovering Better Model Architectures for Medical Query Understanding"","," ""In developing an online question-answering system for the medical domains, natural language inference (NLI) models play a central role in question matching and intention detection. However, which models are best for our datasets? Manually selecting or tuning a model is time-consuming. Thus we experiment with automatically optimizing the model architectures on the task at hand via neural architecture search (NAS). First, we formulate a novel architecture search space based on the previous NAS literature, supporting cross-sentence attention (cross-attn) modeling. Second, we propose to modify the ENAS method to accelerate and stabilize the search results. We conduct extensive experiments on our two medical NLI tasks. Results show that our system can easily outperform the classical baseline models. We compare different NAS methods and demonstrate our approach provides the best results."",","{zhu-etal-2021-discovering,
    title = ""Discovering Better Model Architectures for Medical Query Understanding"",
    author = ""Zhu, Wei  and
      Ni, Yuan  and
      Wang, Xiaoling  and
      Xie, Guotong"",
    editor = ""Kim, Young-bum  and
      Li, Yunyao  and
      Rambow, Owen"",
    booktitle = ""Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.naacl-industry.29"",
    doi = ""10.18653/v1/2021.naacl-industry.29"",
    pages = ""230--237"",
    abstract = ""In developing an online question-answering system for the medical domains, natural language inference (NLI) models play a central role in question matching and intention detection. However, which models are best for our datasets? Manually selecting or tuning a model is time-consuming. Thus we experiment with automatically optimizing the model architectures on the task at hand via neural architecture search (NAS). First, we formulate a novel architecture search space based on the previous NAS literature, supporting cross-sentence attention (cross-attn) modeling. Second, we propose to modify the ENAS method to accelerate and stabilize the search results. We conduct extensive experiments on our two medical NLI tasks. Results show that our system can easily outperform the classical baseline models. We compare different NAS methods and demonstrate our approach provides the best results."",
}
@",medical domain,natural languag,infer,question-answ
" ""{E}vent{P}lus: A Temporal Event Understanding Pipeline"","," ""We present EventPlus, a temporal event understanding pipeline that integrates various state-of-the-art event understanding components including event trigger and type detection, event argument detection, event duration and temporal relation extraction. Event information, especially event temporal knowledge, is a type of common sense knowledge that helps people understand how stories evolve and provides predictive hints for future events. EventPlus as the first comprehensive temporal event understanding pipeline provides a convenient tool for users to quickly obtain annotations about events and their temporal information for any user-provided document. Furthermore, we show EventPlus can be easily adapted to other domains (e.g., biomedical domain). We make EventPlus publicly available to facilitate event-related information extraction and downstream applications."",","{ma-etal-2021-eventplus,
    title = ""{E}vent{P}lus: A Temporal Event Understanding Pipeline"",
    author = ""Ma, Mingyu Derek  and
      Sun, Jiao  and
      Yang, Mu  and
      Huang, Kung-Hsiang  and
      Wen, Nuan  and
      Singh, Shikhar  and
      Han, Rujun  and
      Peng, Nanyun"",
    editor = ""Sil, Avi  and
      Lin, Xi Victoria"",
    booktitle = ""Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.naacl-demos.7"",
    doi = ""10.18653/v1/2021.naacl-demos.7"",
    pages = ""56--65"",
    abstract = ""We present EventPlus, a temporal event understanding pipeline that integrates various state-of-the-art event understanding components including event trigger and type detection, event argument detection, event duration and temporal relation extraction. Event information, especially event temporal knowledge, is a type of common sense knowledge that helps people understand how stories evolve and provides predictive hints for future events. EventPlus as the first comprehensive temporal event understanding pipeline provides a convenient tool for users to quickly obtain annotations about events and their temporal information for any user-provided document. Furthermore, we show EventPlus can be easily adapted to other domains (e.g., biomedical domain). We make EventPlus publicly available to facilitate event-related information extraction and downstream applications."",
}
@",medical domain,relation extract,annotat
" ""{F}uzzy{BIO}: A Proposal for Fuzzy Representation of Discontinuous Entities"","," ""Discontinuous entities pose a challenge to named entity recognition (NER). These phenomena occur commonly in the biomedical domain. As a solution, expansions of the BIO representation scheme that can handle these entity types are commonly used (i.e. BIOHD). However, the extra tag types make the NER task more difficult to learn. In this paper we propose an alternative; a fuzzy continuous BIO scheme (FuzzyBIO). We focus on the task of Adverse Drug Response extraction and normalization to compare FuzzyBIO to BIOHD. We find that FuzzyBIO improves recall of NER for two of three data sets and results in a higher percentage of correctly identified disjoint and composite entities for all data sets. Using FuzzyBIO also improves end-to-end performance for continuous and composite entities in two of three data sets. Since FuzzyBIO improves performance for some data sets and the conversion from BIOHD to FuzzyBIO is straightforward, we recommend investigating which is more effective for any data set containing discontinuous entities."",","{dirkson-etal-2021-fuzzybio,
    title = ""{F}uzzy{BIO}: A Proposal for Fuzzy Representation of Discontinuous Entities"",
    author = ""Dirkson, Anne  and
      Verberne, Suzan  and
      Kraaij, Wessel"",
    editor = ""Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Lavelli, Alberto  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis"",
    month = apr,
    year = ""2021"",
    address = ""online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.louhi-1.9"",
    pages = ""77--82"",
    abstract = ""Discontinuous entities pose a challenge to named entity recognition (NER). These phenomena occur commonly in the biomedical domain. As a solution, expansions of the BIO representation scheme that can handle these entity types are commonly used (i.e. BIOHD). However, the extra tag types make the NER task more difficult to learn. In this paper we propose an alternative; a fuzzy continuous BIO scheme (FuzzyBIO). We focus on the task of Adverse Drug Response extraction and normalization to compare FuzzyBIO to BIOHD. We find that FuzzyBIO improves recall of NER for two of three data sets and results in a higher percentage of correctly identified disjoint and composite entities for all data sets. Using FuzzyBIO also improves end-to-end performance for continuous and composite entities in two of three data sets. Since FuzzyBIO improves performance for some data sets and the conversion from BIOHD to FuzzyBIO is straightforward, we recommend investigating which is more effective for any data set containing discontinuous entities."",
}
@",medical domain,entity recognit,challeng
" ""New Domain, Major Effort? How Much Data is Necessary to Adapt a Temporal Tagger to the Voice Assistant Domain"","," ""Reliable tagging of Temporal Expressions (TEs, e.g., Book a table at L{'}Osteria for Sunday evening) is a central requirement for Voice Assistants (VAs). However, there is a dearth of resources and systems for the VA domain, since publicly-available temporal taggers are trained only on substantially different domains, such as news and clinical text. Since the cost of annotating large datasets is prohibitive, we investigate the trade-off between in-domain data and performance in DA-Time, a hybrid temporal tagger for the English VA domain which combines a neural architecture for robust TE recognition, with a parser-based TE normalizer. We find that transfer learning goes a long way even with as little as 25 in-domain sentences: DA-Time performs at the state of the art on the news domain, and substantially outperforms it on the VA domain."",","{alam-etal-2021-new,
    title = ""New Domain, Major Effort? How Much Data is Necessary to Adapt a Temporal Tagger to the Voice Assistant Domain"",
    author = ""Alam, Touhidul  and
      Zarcone, Alessandra  and
      Pad{\'o}, Sebastian"",
    editor = ""Zarrie{\ss}, Sina  and
      Bos, Johan  and
      van Noord, Rik  and
      Abzianidze, Lasha"",
    booktitle = ""Proceedings of the 14th International Conference on Computational Semantics (IWCS)"",
    month = jun,
    year = ""2021"",
    address = ""Groningen, The Netherlands (online)"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.iwcs-1.14"",
    pages = ""144--154"",
    abstract = ""Reliable tagging of Temporal Expressions (TEs, e.g., Book a table at L{'}Osteria for Sunday evening) is a central requirement for Voice Assistants (VAs). However, there is a dearth of resources and systems for the VA domain, since publicly-available temporal taggers are trained only on substantially different domains, such as news and clinical text. Since the cost of annotating large datasets is prohibitive, we investigate the trade-off between in-domain data and performance in DA-Time, a hybrid temporal tagger for the English VA domain which combines a neural architecture for robust TE recognition, with a parser-based TE normalizer. We find that transfer learning goes a long way even with as little as 25 in-domain sentences: DA-Time performs at the state of the art on the news domain, and substantially outperforms it on the VA domain."",
}
@",clinical text,semant,annotat
" ""Biomedical Data-to-Text Generation via Fine-Tuning Transformers"","," ""Data-to-text (D2T) generation in the biomedical domain is a promising - yet mostly unexplored - field of research. Here, we apply neural models for D2T generation to a real-world dataset consisting of package leaflets of European medicines. We show that fine-tuned transformers are able to generate realistic, multi-sentence text from data in the biomedical domain, yet have important limitations. We also release a new dataset (BioLeaflets) for benchmarking D2T generation models in the biomedical domain."",","{yermakov-etal-2021-biomedical,
    title = ""Biomedical Data-to-Text Generation via Fine-Tuning Transformers"",
    author = ""Yermakov, Ruslan  and
      Drago, Nicholas  and
      Ziletti, Angelo"",
    editor = ""Belz, Anya  and
      Fan, Angela  and
      Reiter, Ehud  and
      Sripada, Yaji"",
    booktitle = ""Proceedings of the 14th International Conference on Natural Language Generation"",
    month = aug,
    year = ""2021"",
    address = ""Aberdeen, Scotland, UK"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.inlg-1.40"",
    pages = ""364--370"",
    abstract = ""Data-to-text (D2T) generation in the biomedical domain is a promising - yet mostly unexplored - field of research. Here, we apply neural models for D2T generation to a real-world dataset consisting of package leaflets of European medicines. We show that fine-tuned transformers are able to generate realistic, multi-sentence text from data in the biomedical domain, yet have important limitations. We also release a new dataset (BioLeaflets) for benchmarking D2T generation models in the biomedical domain."",
}
@",medical domain,natural languag,generat,benchmark
" ""Knowledge-Guided Paraphrase Identification"","," ""Paraphrase identification (PI), a fundamental task in natural language processing, is to identify whether two sentences express the same or similar meaning, which is a binary classification problem. Recently, BERT-like pre-trained language models have been a popular choice for the frameworks of various PI models, but almost all existing methods consider general domain text. When these approaches are applied to a specific domain, existing models cannot make accurate predictions due to the lack of professional knowledge. In light of this challenge, we propose a novel framework, namely , which can leverage the external unstructured Wikipedia knowledge to accurately identify paraphrases. We propose to mine outline knowledge of concepts related to given sentences from Wikipedia via BM25 model. After retrieving related outline knowledge, makes predictions based on both the semantic information of two sentences and the outline knowledge. Besides, we propose a gating mechanism to aggregate the semantic information-based prediction and the knowledge-based prediction. Extensive experiments are conducted on two public datasets: PARADE (a computer science domain dataset) and clinicalSTS2019 (a biomedical domain dataset). The results show that the proposed outperforms state-of-the-art methods."",","{wang-etal-2021-knowledge-guided,
    title = ""Knowledge-Guided Paraphrase Identification"",
    author = ""Wang, Haoyu  and
      Ma, Fenglong  and
      Wang, Yaqing  and
      Gao, Jing"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2021"",
    month = nov,
    year = ""2021"",
    address = ""Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.findings-emnlp.72"",
    doi = ""10.18653/v1/2021.findings-emnlp.72"",
    pages = ""843--853"",
    abstract = ""Paraphrase identification (PI), a fundamental task in natural language processing, is to identify whether two sentences express the same or similar meaning, which is a binary classification problem. Recently, BERT-like pre-trained language models have been a popular choice for the frameworks of various PI models, but almost all existing methods consider general domain text. When these approaches are applied to a specific domain, existing models cannot make accurate predictions due to the lack of professional knowledge. In light of this challenge, we propose a novel framework, namely , which can leverage the external unstructured Wikipedia knowledge to accurately identify paraphrases. We propose to mine outline knowledge of concepts related to given sentences from Wikipedia via BM25 model. After retrieving related outline knowledge, makes predictions based on both the semantic information of two sentences and the outline knowledge. Besides, we propose a gating mechanism to aggregate the semantic information-based prediction and the knowledge-based prediction. Extensive experiments are conducted on two public datasets: PARADE (a computer science domain dataset) and clinicalSTS2019 (a biomedical domain dataset). The results show that the proposed outperforms state-of-the-art methods."",
}
@",medical domain,natural language process,natural languag,language process,nlp,semant,challeng,public data,public dataset,public dataset
" ""Learning and Evaluating a Differentially Private Pre-trained Language Model"","," ""Contextual language models have led to significantly better results, especially when pre-trained on the same data as the downstream task. While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data. One method to guarantee the privacy of such individuals is to train a differentially-private language model, but this usually comes at the expense of model performance. Also, in the absence of a differentially private vocabulary training, it is not possible to modify the vocabulary to fit the new data, which might further degrade results. In this work we bridge these gaps, and provide guidance to future researchers and practitioners on how to improve privacy while maintaining good model performance. We introduce a novel differentially private word-piece algorithm, which allows training a tailored domain-specific vocabulary while maintaining privacy. We then experiment with entity extraction tasks from clinical notes, and demonstrate how to train a differentially private pre-trained language model (i.e., BERT) with a privacy guarantee of $\epsilon","{hoory-etal-2021-learning-evaluating,
    title = ""Learning and Evaluating a Differentially Private Pre-trained Language Model"",
    author = ""Hoory, Shlomo  and
      Feder, Amir  and
      Tendler, Avichai  and
      Erell, Sofia  and
      Peled-Cohen, Alon  and
      Laish, Itay  and
      Nakhost, Hootan  and
      Stemmer, Uri  and
      Benjamini, Ayelet  and
      Hassidim, Avinatan  and
      Matias, Yossi"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2021"",
    month = nov,
    year = ""2021"",
    address = ""Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.findings-emnlp.102"",
    doi = ""10.18653/v1/2021.findings-emnlp.102"",
    pages = ""1178--1189"",
    abstract = ""Contextual language models have led to significantly better results, especially when pre-trained on the same data as the downstream task. While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data. One method to guarantee the privacy of such individuals is to train a differentially-private language model, but this usually comes at the expense of model performance. Also, in the absence of a differentially private vocabulary training, it is not possible to modify the vocabulary to fit the new data, which might further degrade results. In this work we bridge these gaps, and provide guidance to future researchers and practitioners on how to improve privacy while maintaining good model performance. We introduce a novel differentially private word-piece algorithm, which allows training a tailored domain-specific vocabulary while maintaining privacy. We then experiment with entity extraction tasks from clinical notes, and demonstrate how to train a differentially private pre-trained language model (i.e., BERT) with a privacy guarantee of $\epsilon=1.1$ and with only a small degradation in performance. Finally, as it is hard to tell given a privacy parameter $\epsilon$ what was the effect on the trained representation, we present experiments showing that the trained model does not memorize private information."",
}
@",clinical not,nlp,evalu
" ""{S}ent2{S}pan: Span Detection for {PICO} Extraction in the Biomedical Text without Span Annotations"","," ""The rapid growth in published clinical trials makes it difficult to maintain up-to-date systematic reviews, which require finding all relevant trials. This leads to policy and practice decisions based on out-of-date, incomplete, and biased subsets of available clinical evidence. Extracting and then normalising Population, Intervention, Comparator, and Outcome (PICO) information from clinical trial articles may be an effective way to automatically assign trials to systematic reviews and avoid searching and screening{---}the two most time-consuming systematic review processes. We propose and test a novel approach to PICO span detection. The major difference between our proposed method and previous approaches comes from detecting spans without needing annotated span data and using only crowdsourced sentence-level annotations. Experiments on two datasets show that PICO span detection results achieve much higher results for recall when compared to fully supervised methods with PICO sentence detection at least as good as human annotations. By removing the reliance on expert annotations for span detection, this work could be used in a human-machine pipeline for turning low-quality, crowdsourced, and sentence-level PICO annotations into structured information that can be used to quickly assign trials to relevant systematic reviews."",","{liu-etal-2021-sent2span-span,
    title = ""{S}ent2{S}pan: Span Detection for {PICO} Extraction in the Biomedical Text without Span Annotations"",
    author = ""Liu, Shifeng  and
      Sun, Yifang  and
      Li, Bing  and
      Wang, Wei  and
      Bourgeois, Florence T.  and
      Dunn, Adam G."",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2021"",
    month = nov,
    year = ""2021"",
    address = ""Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.findings-emnlp.147"",
    doi = ""10.18653/v1/2021.findings-emnlp.147"",
    pages = ""1705--1715"",
    abstract = ""The rapid growth in published clinical trials makes it difficult to maintain up-to-date systematic reviews, which require finding all relevant trials. This leads to policy and practice decisions based on out-of-date, incomplete, and biased subsets of available clinical evidence. Extracting and then normalising Population, Intervention, Comparator, and Outcome (PICO) information from clinical trial articles may be an effective way to automatically assign trials to systematic reviews and avoid searching and screening{---}the two most time-consuming systematic review processes. We propose and test a novel approach to PICO span detection. The major difference between our proposed method and previous approaches comes from detecting spans without needing annotated span data and using only crowdsourced sentence-level annotations. Experiments on two datasets show that PICO span detection results achieve much higher results for recall when compared to fully supervised methods with PICO sentence detection at least as good as human annotations. By removing the reliance on expert annotations for span detection, this work could be used in a human-machine pipeline for turning low-quality, crowdsourced, and sentence-level PICO annotations into structured information that can be used to quickly assign trials to relevant systematic reviews."",
}
@",medical text,nlp,annotat
" ""Cross-Domain Data Integration for Named Entity Disambiguation in Biomedical Text"","," ""Named entity disambiguation (NED), which involves mapping textual mentions to structured entities, is particularly challenging in the medical domain due to the presence of rare entities. Existing approaches are limited by the presence of coarse-grained structural resources in biomedical knowledge bases as well as the use of training datasets that provide low coverage over uncommon resources. In this work, we address these issues by proposing a cross-domain data integration method that transfers structural knowledge from a general text knowledge base to the medical domain. We utilize our integration scheme to augment structural resources and generate a large biomedical NED dataset for pretraining. Our pretrained model with injected structural knowledge achieves state-of-the-art performance on two benchmark medical NED datasets: MedMentions and BC5CDR. Furthermore, we improve disambiguation of rare entities by up to 57 accuracy points."",","{varma-etal-2021-cross-domain,
    title = ""Cross-Domain Data Integration for Named Entity Disambiguation in Biomedical Text"",
    author = ""Varma, Maya  and
      Orr, Laurel  and
      Wu, Sen  and
      Leszczynski, Megan  and
      Ling, Xiao  and
      R{\'e}, Christopher"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2021"",
    month = nov,
    year = ""2021"",
    address = ""Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.findings-emnlp.388"",
    doi = ""10.18653/v1/2021.findings-emnlp.388"",
    pages = ""4566--4575"",
    abstract = ""Named entity disambiguation (NED), which involves mapping textual mentions to structured entities, is particularly challenging in the medical domain due to the presence of rare entities. Existing approaches are limited by the presence of coarse-grained structural resources in biomedical knowledge bases as well as the use of training datasets that provide low coverage over uncommon resources. In this work, we address these issues by proposing a cross-domain data integration method that transfers structural knowledge from a general text knowledge base to the medical domain. We utilize our integration scheme to augment structural resources and generate a large biomedical NED dataset for pretraining. Our pretrained model with injected structural knowledge achieves state-of-the-art performance on two benchmark medical NED datasets: MedMentions and BC5CDR. Furthermore, we improve disambiguation of rare entities by up to 57 accuracy points."",
}
@",medical domain,medical text,nlp,generat,challeng,benchmark
" ""{C}o{PHE}: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification"","," ""Large-Scale Multi-Label Text Classification (LMTC) includes tasks with hierarchical label spaces, such as automatic assignment of ICD-9 codes to discharge summaries. Performance of models in prior art is evaluated with standard precision, recall, and F1 measures without regard for the rich hierarchical structure. In this work we argue for hierarchical evaluation of the predictions of neural LMTC models. With the example of the ICD-9 ontology we describe a structural issue in the representation of the structured label space in prior art, and propose an alternative representation based on the depth of the ontology. We propose a set of metrics for hierarchical evaluation using the depth-based representation. We compare the evaluation scores from the proposed metrics with previously used metrics on prior art LMTC models for ICD-9 coding in MIMIC-III. We also propose further avenues of research involving the proposed ontological representation."",","{falis-etal-2021-cophe,
    title = ""{C}o{PHE}: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification"",
    author = ""Falis, Mat{\'u}{\v{s}}  and
      Dong, Hang  and
      Birch, Alexandra  and
      Alex, Beatrice"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.69"",
    doi = ""10.18653/v1/2021.emnlp-main.69"",
    pages = ""907--912"",
    abstract = ""Large-Scale Multi-Label Text Classification (LMTC) includes tasks with hierarchical label spaces, such as automatic assignment of ICD-9 codes to discharge summaries. Performance of models in prior art is evaluated with standard precision, recall, and F1 measures without regard for the rich hierarchical structure. In this work we argue for hierarchical evaluation of the predictions of neural LMTC models. With the example of the ICD-9 ontology we describe a structural issue in the representation of the structured label space in prior art, and propose an alternative representation based on the depth of the ontology. We propose a set of metrics for hierarchical evaluation using the depth-based representation. We compare the evaluation scores from the proposed metrics with previously used metrics on prior art LMTC models for ICD-9 coding in MIMIC-III. We also propose further avenues of research involving the proposed ontological representation."",
}
@",discharge summar,natural language process,natural languag,language process,nlp,summar,evalu
" ""Biomedical Concept Normalization by Leveraging Hypernyms"","," ""Biomedical Concept Normalization (BCN) is widely used in biomedical text processing as a fundamental module. Owing to numerous surface variants of biomedical concepts, BCN still remains challenging and unsolved. In this paper, we exploit biomedical concept hypernyms to facilitate BCN. We propose Biomedical Concept Normalizer with Hypernyms (BCNH), a novel framework that adopts list-wise training to make use of both hypernyms and synonyms, and also employs norm constraint on the representation of hypernym-hyponym entity pairs. The experimental results show that BCNH outperforms the previous state-of-the-art model on the NCBI dataset."",","{yan-etal-2021-biomedical,
    title = ""Biomedical Concept Normalization by Leveraging Hypernyms"",
    author = ""Yan, Cheng  and
      Zhang, Yuanzhe  and
      Liu, Kang  and
      Zhao, Jun  and
      Shi, Yafei  and
      Liu, Shengping"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.284"",
    doi = ""10.18653/v1/2021.emnlp-main.284"",
    pages = ""3512--3517"",
    abstract = ""Biomedical Concept Normalization (BCN) is widely used in biomedical text processing as a fundamental module. Owing to numerous surface variants of biomedical concepts, BCN still remains challenging and unsolved. In this paper, we exploit biomedical concept hypernyms to facilitate BCN. We propose Biomedical Concept Normalizer with Hypernyms (BCNH), a novel framework that adopts list-wise training to make use of both hypernyms and synonyms, and also employs norm constraint on the representation of hypernym-hyponym entity pairs. The experimental results show that BCNH outperforms the previous state-of-the-art model on the NCBI dataset."",
}
@",medical text,natural language process,natural languag,language process,nlp,concept norm,challeng
" ""How to leverage the multimodal {EHR} data for better medical prediction?"","," ""Healthcare is becoming a more and more important research topic recently. With the growing data in the healthcare domain, it offers a great opportunity for deep learning to improve the quality of service and reduce costs. However, the complexity of electronic health records (EHR) data is a challenge for the application of deep learning. Specifically, the data produced in the hospital admissions are monitored by the EHR system, which includes structured data like daily body temperature and unstructured data like free text and laboratory measurements. Although there are some preprocessing frameworks proposed for specific EHR data, the clinical notes that contain significant clinical value are beyond the realm of their consideration. Besides, whether these different data from various views are all beneficial to the medical tasks and how to best utilize these data remain unclear. Therefore, in this paper, we first extract the accompanying clinical notes from EHR and propose a method to integrate these data, we also comprehensively study the different models and the data leverage methods for better medical task prediction performance. The results on two prediction tasks show that our fused model with different data outperforms the state-of-the-art method without clinical notes, which illustrates the importance of our fusion method and the clinical note features."",","{yang-wu-2021-leverage,
    title = ""How to leverage the multimodal {EHR} data for better medical prediction?"",
    author = ""Yang, Bo  and
      Wu, Lijun"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.329"",
    doi = ""10.18653/v1/2021.emnlp-main.329"",
    pages = ""4029--4038"",
    abstract = ""Healthcare is becoming a more and more important research topic recently. With the growing data in the healthcare domain, it offers a great opportunity for deep learning to improve the quality of service and reduce costs. However, the complexity of electronic health records (EHR) data is a challenge for the application of deep learning. Specifically, the data produced in the hospital admissions are monitored by the EHR system, which includes structured data like daily body temperature and unstructured data like free text and laboratory measurements. Although there are some preprocessing frameworks proposed for specific EHR data, the clinical notes that contain significant clinical value are beyond the realm of their consideration. Besides, whether these different data from various views are all beneficial to the medical tasks and how to best utilize these data remain unclear. Therefore, in this paper, we first extract the accompanying clinical notes from EHR and propose a method to integrate these data, we also comprehensively study the different models and the data leverage methods for better medical task prediction performance. The results on two prediction tasks show that our fused model with different data outperforms the state-of-the-art method without clinical notes, which illustrates the importance of our fusion method and the clinical note features."",
}
@",electronic health record,health record,clinical not,natural language process,natural languag,language process,nlp,challeng
" ""Improved Latent Tree Induction with Distant Supervision via Span Constraints"","," ""For over thirty years, researchers have developed and analyzed methods for latent tree induction as an approach for unsupervised syntactic parsing. Nonetheless, modern systems still do not perform well enough compared to their supervised counterparts to have any practical use as structural annotation of text. In this work, we present a technique that uses distant supervision in the form of span constraints (i.e. phrase bracketing) to improve performance in unsupervised constituency parsing. Using a relatively small number of span constraints we can substantially improve the output from DIORA, an already competitive unsupervised parsing system. Compared with full parse tree annotation, span constraints can be acquired with minimal effort, such as with a lexicon derived from Wikipedia, to find exact text matches. Our experiments show span constraints based on entities improves constituency parsing on English WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to any domain where span constraints are easily attainable, and as a case study we demonstrate its effectiveness by parsing biomedical text from the CRAFT dataset."",","{xu-etal-2021-improved,
    title = ""Improved Latent Tree Induction with Distant Supervision via Span Constraints"",
    author = ""Xu, Zhiyang  and
      Drozdov, Andrew  and
      Lee, Jay Yoon  and
      O{'}Gorman, Tim  and
      Rongali, Subendhu  and
      Finkbeiner, Dylan  and
      Suresh, Shilpa  and
      Iyyer, Mohit  and
      McCallum, Andrew"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.395"",
    doi = ""10.18653/v1/2021.emnlp-main.395"",
    pages = ""4818--4831"",
    abstract = ""For over thirty years, researchers have developed and analyzed methods for latent tree induction as an approach for unsupervised syntactic parsing. Nonetheless, modern systems still do not perform well enough compared to their supervised counterparts to have any practical use as structural annotation of text. In this work, we present a technique that uses distant supervision in the form of span constraints (i.e. phrase bracketing) to improve performance in unsupervised constituency parsing. Using a relatively small number of span constraints we can substantially improve the output from DIORA, an already competitive unsupervised parsing system. Compared with full parse tree annotation, span constraints can be acquired with minimal effort, such as with a lexicon derived from Wikipedia, to find exact text matches. Our experiments show span constraints based on entities improves constituency parsing on English WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to any domain where span constraints are easily attainable, and as a case study we demonstrate its effectiveness by parsing biomedical text from the CRAFT dataset."",
}
@",medical text,natural language process,natural languag,language process,nlp,annotat
" ""Incorporating medical knowledge in {BERT} for clinical relation extraction"","," ""In recent years pre-trained language models (PLM) such as BERT have proven to be very effective in diverse NLP tasks such as Information Extraction, Sentiment Analysis and Question Answering. Trained with massive general-domain text, these pre-trained language models capture rich syntactic, semantic and discourse information in the text. However, due to the differences between general and specific domain text (e.g., Wikipedia versus clinic notes), these models may not be ideal for domain-specific tasks (e.g., extracting clinical relations). Furthermore, it may require additional medical knowledge to understand clinical text properly. To solve these issues, in this research, we conduct a comprehensive examination of different techniques to add medical knowledge into a pre-trained BERT model for clinical relation extraction. Our best model outperforms the state-of-the-art systems on the benchmark i2b2/VA 2010 clinical relation extraction dataset."",","{roy-pan-2021-incorporating,
    title = ""Incorporating medical knowledge in {BERT} for clinical relation extraction"",
    author = ""Roy, Arpita  and
      Pan, Shimei"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.435"",
    doi = ""10.18653/v1/2021.emnlp-main.435"",
    pages = ""5357--5366"",
    abstract = ""In recent years pre-trained language models (PLM) such as BERT have proven to be very effective in diverse NLP tasks such as Information Extraction, Sentiment Analysis and Question Answering. Trained with massive general-domain text, these pre-trained language models capture rich syntactic, semantic and discourse information in the text. However, due to the differences between general and specific domain text (e.g., Wikipedia versus clinic notes), these models may not be ideal for domain-specific tasks (e.g., extracting clinical relations). Furthermore, it may require additional medical knowledge to understand clinical text properly. To solve these issues, in this research, we conduct a comprehensive examination of different techniques to add medical knowledge into a pre-trained BERT model for clinical relation extraction. Our best model outperforms the state-of-the-art systems on the benchmark i2b2/VA 2010 clinical relation extraction dataset."",
}
@",clinical text,natural language process,natural languag,language process,nlp,relation extract,sentiment,semant,benchmark
" ""Effective Convolutional Attention Network for Multi-label Clinical Document Classification"","," ""Multi-label document classification (MLDC) problems can be challenging, especially for long documents with a large label set and a long-tail distribution over labels. In this paper, we present an effective convolutional attention network for the MLDC problem with a focus on medical code prediction from clinical documents. Our innovations are three-fold: (1) we utilize a deep convolution-based encoder with the squeeze-and-excitation networks and residual networks to aggregate the information across the document and learn meaningful document representations that cover different ranges of texts; (2) we explore multi-layer and sum-pooling attention to extract the most informative features from these multi-scale representations; (3) we combine binary cross entropy loss and focal loss to improve performance for rare labels. We focus our evaluation study on MIMIC-III, a widely used dataset in the medical domain. Our models outperform prior work on medical coding and achieve new state-of-the-art results on multiple metrics. We also demonstrate the language independent nature of our approach by applying it to two non-English datasets. Our model outperforms prior best model and a multilingual Transformer model by a substantial margin."",","{liu-etal-2021-effective,
    title = ""Effective Convolutional Attention Network for Multi-label Clinical Document Classification"",
    author = ""Liu, Yang  and
      Cheng, Hua  and
      Klopfer, Russell  and
      Gormley, Matthew R.  and
      Schaaf, Thomas"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.481"",
    doi = ""10.18653/v1/2021.emnlp-main.481"",
    pages = ""5941--5953"",
    abstract = ""Multi-label document classification (MLDC) problems can be challenging, especially for long documents with a large label set and a long-tail distribution over labels. In this paper, we present an effective convolutional attention network for the MLDC problem with a focus on medical code prediction from clinical documents. Our innovations are three-fold: (1) we utilize a deep convolution-based encoder with the squeeze-and-excitation networks and residual networks to aggregate the information across the document and learn meaningful document representations that cover different ranges of texts; (2) we explore multi-layer and sum-pooling attention to extract the most informative features from these multi-scale representations; (3) we combine binary cross entropy loss and focal loss to improve performance for rare labels. We focus our evaluation study on MIMIC-III, a widely used dataset in the medical domain. Our models outperform prior work on medical coding and achieve new state-of-the-art results on multiple metrics. We also demonstrate the language independent nature of our approach by applying it to two non-English datasets. Our model outperforms prior best model and a multilingual Transformer model by a substantial margin."",
}
@",medical domain,natural language process,natural languag,language process,nlp,challeng,evalu
" ""{MS}{\^{}}2: Multi-Document Summarization of Medical Studies"","," ""To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MS{\^{}}2 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results, though significant work remains to achieve higher summarization quality. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system{'}s generated summaries. Data and models are available at \url{https://github.com/allenai/ms2}."",","{deyoung-etal-2021-ms,
    title = ""{MS}{\^{}}2: Multi-Document Summarization of Medical Studies"",
    author = ""DeYoung, Jay  and
      Beltagy, Iz  and
      van Zuylen, Madeleine  and
      Kuehl, Bailey  and
      Wang, Lucy Lu"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.594"",
    doi = ""10.18653/v1/2021.emnlp-main.594"",
    pages = ""7494--7513"",
    abstract = ""To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MS{\^{}}2 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results, though significant work remains to achieve higher summarization quality. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system{'}s generated summaries. Data and models are available at \url{https://github.com/allenai/ms2}."",
}
@",medical domain,natural language process,natural languag,language process,nlp,generat,summar,assess
" ""Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting: Phenotype Annotation Use Case"","," ""Contextualised word embeddings is a powerful tool to detect contextual synonyms. However, most of the current state-of-the-art (SOTA) deep learning concept extraction methods remain supervised and underexploit the potential of the context. In this paper, we propose a self-supervised pre-training approach which is able to detect contextual synonyms of concepts being training on the data created by shallow matching. We apply our methodology in the sparse multi-class setting (over 15,000 concepts) to extract phenotype information from electronic health records. We further investigate data augmentation techniques to address the problem of the class sparsity. Our approach achieves a new SOTA for the unsupervised phenotype concept annotation on clinical text on F1 and Recall outperforming the previous SOTA with a gain of up to 4.5 and 4.0 absolute points, respectively. After fine-tuning with as little as 20{\%} of the labelled data, we also outperform BioBERT and ClinicalBERT. The extrinsic evaluation on three ICU benchmarks also shows the benefit of using the phenotypes annotated by our model as features."",","{zhang-etal-2021-self,
    title = ""Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting: Phenotype Annotation Use Case"",
    author = ""Zhang, Jingqing  and
      Bolanos Trujillo, Luis  and
      Li, Tong  and
      Tanwar, Ashwani  and
      Freire, Guilherme  and
      Yang, Xian  and
      Ive, Julia  and
      Gupta, Vibhor  and
      Guo, Yike"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.690"",
    doi = ""10.18653/v1/2021.emnlp-main.690"",
    pages = ""8754--8769"",
    abstract = ""Contextualised word embeddings is a powerful tool to detect contextual synonyms. However, most of the current state-of-the-art (SOTA) deep learning concept extraction methods remain supervised and underexploit the potential of the context. In this paper, we propose a self-supervised pre-training approach which is able to detect contextual synonyms of concepts being training on the data created by shallow matching. We apply our methodology in the sparse multi-class setting (over 15,000 concepts) to extract phenotype information from electronic health records. We further investigate data augmentation techniques to address the problem of the class sparsity. Our approach achieves a new SOTA for the unsupervised phenotype concept annotation on clinical text on F1 and Recall outperforming the previous SOTA with a gain of up to 4.5 and 4.0 absolute points, respectively. After fine-tuning with as little as 20{\%} of the labelled data, we also outperform BioBERT and ClinicalBERT. The extrinsic evaluation on three ICU benchmarks also shows the benefit of using the phenotypes annotated by our model as features."",
}
@",electronic health record,health record,clinical text,natural language process,natural languag,language process,nlp,annotat,benchmark,evalu
" ""{MLEC-QA}: {A} {C}hinese {M}ulti-{C}hoice {B}iomedical {Q}uestion {A}nswering {D}ataset"","," ""Question Answering (QA) has been successfully applied in scenarios of human-computer interaction such as chatbots and search engines. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present MLEC-QA, the largest-scale Chinese multi-choice biomedical QA dataset, collected from the National Medical Licensing Examination in China. The dataset is composed of five subsets with 136,236 biomedical multi-choice questions with extra materials (images or tables) annotated by human experts, and first covers the following biomedical sub-fields: Clinic, Stomatology, Public Health, Traditional Chinese Medicine, and Traditional Chinese Medicine Combined with Western Medicine. We implement eight representative control methods and open-domain QA methods as baselines. Experimental results demonstrate that even the current best model can only achieve accuracies between 40{\%} to 55{\%} on five subsets, especially performing poorly on questions that require sophisticated reasoning ability. We hope the release of the MLEC-QA dataset can serve as a valuable resource for research and evaluation in open-domain QA, and also make advances for biomedical QA systems."",","{li-etal-2021-mlec,
    title = ""{MLEC-QA}: {A} {C}hinese {M}ulti-{C}hoice {B}iomedical {Q}uestion {A}nswering {D}ataset"",
    author = ""Li, Jing  and
      Zhong, Shangping  and
      Chen, Kaizhi"",
    editor = ""Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.698"",
    doi = ""10.18653/v1/2021.emnlp-main.698"",
    pages = ""8862--8874"",
    abstract = ""Question Answering (QA) has been successfully applied in scenarios of human-computer interaction such as chatbots and search engines. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present MLEC-QA, the largest-scale Chinese multi-choice biomedical QA dataset, collected from the National Medical Licensing Examination in China. The dataset is composed of five subsets with 136,236 biomedical multi-choice questions with extra materials (images or tables) annotated by human experts, and first covers the following biomedical sub-fields: Clinic, Stomatology, Public Health, Traditional Chinese Medicine, and Traditional Chinese Medicine Combined with Western Medicine. We implement eight representative control methods and open-domain QA methods as baselines. Experimental results demonstrate that even the current best model can only achieve accuracies between 40{\%} to 55{\%} on five subsets, especially performing poorly on questions that require sophisticated reasoning ability. We hope the release of the MLEC-QA dataset can serve as a valuable resource for research and evaluation in open-domain QA, and also make advances for biomedical QA systems."",
}
@",medical domain,natural language process,natural languag,language process,nlp,annotat,evalu
" ""Open-{D}omain Question-{A}nswering for {COVID}-19 and Other Emergent Domains"","," ""Since late 2019, COVID-19 has quickly emerged as the newest biomedical domain, resulting in a surge of new information. As with other emergent domains, the discussion surrounding the topic has been rapidly changing, leading to the spread of misinformation. This has created the need for a public space for users to ask questions and receive credible, scientific answers. To fulfill this need, we turn to the task of open-domain question-answering, which we can use to efficiently find answers to free-text questions from a large set of documents. In this work, we present such a system for the emergent domain of COVID-19. Despite the small data size available, we are able to successfully train the system to retrieve answers from a large-scale corpus of published COVID-19 scientific papers. Furthermore, we incorporate effective re-ranking and question-answering techniques, such as document diversity and multiple answer spans. Our open-domain question-answering system can further act as a model for the quick development of similar systems that can be adapted and modified for other developing emergent domains."",","{levy-etal-2021-open,
    title = ""Open-{D}omain Question-{A}nswering for {COVID}-19 and Other Emergent Domains"",
    author = ""Levy, Sharon  and
      Mo, Kevin  and
      Xiong, Wenhan  and
      Wang, William Yang"",
    editor = ""Adel, Heike  and
      Shi, Shuming"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-demo.30"",
    doi = ""10.18653/v1/2021.emnlp-demo.30"",
    pages = ""259--266"",
    abstract = ""Since late 2019, COVID-19 has quickly emerged as the newest biomedical domain, resulting in a surge of new information. As with other emergent domains, the discussion surrounding the topic has been rapidly changing, leading to the spread of misinformation. This has created the need for a public space for users to ask questions and receive credible, scientific answers. To fulfill this need, we turn to the task of open-domain question-answering, which we can use to efficiently find answers to free-text questions from a large set of documents. In this work, we present such a system for the emergent domain of COVID-19. Despite the small data size available, we are able to successfully train the system to retrieve answers from a large-scale corpus of published COVID-19 scientific papers. Furthermore, we incorporate effective re-ranking and question-answering techniques, such as document diversity and multiple answer spans. Our open-domain question-answering system can further act as a model for the quick development of similar systems that can be adapted and modified for other developing emergent domains."",
}
@",medical domain,natural language process,natural languag,language process,nlp,question-answ
" ""Clinical Outcome Prediction from Admission Notes using Self-Supervised Knowledge Integration"","," ""Outcome prediction from clinical text can prevent doctors from overlooking possible risks and help hospitals to plan capacities. We simulate patients at admission time, when decision support can be especially valuable, and contribute a novel *admission to discharge* task with four common outcome prediction targets: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay prediction. The ideal system should infer outcomes based on symptoms, pre-conditions and risk factors of a patient. We evaluate the effectiveness of language models to handle this scenario and propose *clinical outcome pre-training* to integrate knowledge about patient outcomes from multiple public sources. We further present a simple method to incorporate ICD code hierarchy into the models. We show that our approach improves performance on the outcome tasks against several baselines. A detailed analysis reveals further strengths of the model, including transferability, but also weaknesses such as handling of vital values and inconsistencies in the underlying data."",","{van-aken-etal-2021-clinical,
    title = ""Clinical Outcome Prediction from Admission Notes using Self-Supervised Knowledge Integration"",
    author = ""van Aken, Betty  and
      Papaioannou, Jens-Michalis  and
      Mayrdorfer, Manuel  and
      Budde, Klemens  and
      Gers, Felix  and
      Loeser, Alexander"",
    editor = ""Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut"",
    booktitle = ""Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume"",
    month = apr,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.eacl-main.75"",
    doi = ""10.18653/v1/2021.eacl-main.75"",
    pages = ""881--893"",
    abstract = ""Outcome prediction from clinical text can prevent doctors from overlooking possible risks and help hospitals to plan capacities. We simulate patients at admission time, when decision support can be especially valuable, and contribute a novel *admission to discharge* task with four common outcome prediction targets: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay prediction. The ideal system should infer outcomes based on symptoms, pre-conditions and risk factors of a patient. We evaluate the effectiveness of language models to handle this scenario and propose *clinical outcome pre-training* to integrate knowledge about patient outcomes from multiple public sources. We further present a simple method to incorporate ICD code hierarchy into the models. We show that our approach improves performance on the outcome tasks against several baselines. A detailed analysis reveals further strengths of the model, including transferability, but also weaknesses such as handling of vital values and inconsistencies in the underlying data."",
}
@",clinical text,infer,evalu
" ""{BERT} Prescriptions to Avoid Unwanted Headaches: A Comparison of Transformer Architectures for Adverse Drug Event Detection"","," ""Pretrained transformer-based models, such as BERT and its variants, have become a common choice to obtain state-of-the-art performances in NLP tasks. In the identification of Adverse Drug Events (ADE) from social media texts, for example, BERT architectures rank first in the leaderboard. However, a systematic comparison between these models has not yet been done. In this paper, we aim at shedding light on the differences between their performance analyzing the results of 12 models, tested on two standard benchmarks. SpanBERT and PubMedBERT emerged as the best models in our evaluation: this result clearly shows that span-based pretraining gives a decisive advantage in the precise recognition of ADEs, and that in-domain language pretraining is particularly useful when the transformer model is trained just on biomedical text from scratch."",","{portelli-etal-2021-bert,
    title = ""{BERT} Prescriptions to Avoid Unwanted Headaches: A Comparison of Transformer Architectures for Adverse Drug Event Detection"",
    author = ""Portelli, Beatrice  and
      Lenzi, Edoardo  and
      Chersoni, Emmanuele  and
      Serra, Giuseppe  and
      Santus, Enrico"",
    editor = ""Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut"",
    booktitle = ""Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume"",
    month = apr,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.eacl-main.149"",
    doi = ""10.18653/v1/2021.eacl-main.149"",
    pages = ""1740--1747"",
    abstract = ""Pretrained transformer-based models, such as BERT and its variants, have become a common choice to obtain state-of-the-art performances in NLP tasks. In the identification of Adverse Drug Events (ADE) from social media texts, for example, BERT architectures rank first in the leaderboard. However, a systematic comparison between these models has not yet been done. In this paper, we aim at shedding light on the differences between their performance analyzing the results of 12 models, tested on two standard benchmarks. SpanBERT and PubMedBERT emerged as the best models in our evaluation: this result clearly shows that span-based pretraining gives a decisive advantage in the precise recognition of ADEs, and that in-domain language pretraining is particularly useful when the transformer model is trained just on biomedical text from scratch."",
}
@",medical text,nlp,benchmark,evalu
" ""Coreference Resolution for the Biomedical Domain: A Survey"","," ""Issues with coreference resolution are one of the most frequently mentioned challenges for information extraction from the biomedical literature. Thus, the biomedical genre has long been the second most researched genre for coreference resolution after the news domain, and the subject of a great deal of research for NLP in general. In recent years this interest has grown enormously leading to the development of a number of substantial datasets, of domain-specific contextual language models, and of several architectures. In this paper we review the state of-the-art of coreference in the biomedical domain with a particular attention on these most recent developments."",","{lu-poesio-2021-coreference,
    title = ""Coreference Resolution for the Biomedical Domain: A Survey"",
    author = ""Lu, Pengcheng  and
      Poesio, Massimo"",
    editor = ""Ogrodniczuk, Maciej  and
      Pradhan, Sameer  and
      Poesio, Massimo  and
      Grishina, Yulia  and
      Ng, Vincent"",
    booktitle = ""Proceedings of the Fourth Workshop on Computational Models of Reference, Anaphora and Coreference"",
    month = nov,
    year = ""2021"",
    address = ""Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.crac-1.2"",
    doi = ""10.18653/v1/2021.crac-1.2"",
    pages = ""12--23"",
    abstract = ""Issues with coreference resolution are one of the most frequently mentioned challenges for information extraction from the biomedical literature. Thus, the biomedical genre has long been the second most researched genre for coreference resolution after the news domain, and the subject of a great deal of research for NLP in general. In recent years this interest has grown enormously leading to the development of a number of substantial datasets, of domain-specific contextual language models, and of several architectures. In this paper we review the state of-the-art of coreference in the biomedical domain with a particular attention on these most recent developments."",
}
@",medical domain,nlp,challeng
" ""{SAFFRON}: tran{S}fer le{A}rning For Food-disease {R}elati{O}n extractio{N}"","," ""The accelerating growth of big data in the biomedical domain, with an endless amount of electronic health records and more than 30 million citations and abstracts in PubMed, introduces the need for automatic structuring of textual biomedical data. In this paper, we develop a method for detecting relations between food and disease entities from raw text. Due to the lack of annotated data on food with respect to health, we explore the feasibility of transfer learning by training BERT-based models on existing datasets annotated for the presence of cause and treat relations among different types of biomedical entities, and using them to recognize the same relations between food and disease entities in a dataset created for the purposes of this study. The best models achieve macro averaged F1 scores of 0.847 and 0.900 for the cause and treat relations, respectively."",","{cenikj-etal-2021-saffron,
    title = ""{SAFFRON}: tran{S}fer le{A}rning For Food-disease {R}elati{O}n extractio{N}"",
    author = ""Cenikj, Gjorgjina  and
      Eftimov, Tome  and
      Korou{\v{s}}i{\'c} Seljak, Barbara"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 20th Workshop on Biomedical Language Processing"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.bionlp-1.4"",
    doi = ""10.18653/v1/2021.bionlp-1.4"",
    pages = ""30--40"",
    abstract = ""The accelerating growth of big data in the biomedical domain, with an endless amount of electronic health records and more than 30 million citations and abstracts in PubMed, introduces the need for automatic structuring of textual biomedical data. In this paper, we develop a method for detecting relations between food and disease entities from raw text. Due to the lack of annotated data on food with respect to health, we explore the feasibility of transfer learning by training BERT-based models on existing datasets annotated for the presence of cause and treat relations among different types of biomedical entities, and using them to recognize the same relations between food and disease entities in a dataset created for the purposes of this study. The best models achieve macro averaged F1 scores of 0.847 and 0.900 for the cause and treat relations, respectively."",
}
@",electronic health record,health record,medical domain,language process,nlp,annotat
" ""Overview of the {MEDIQA} 2021 Shared Task on Summarization in the Medical Domain"","," ""The MEDIQA 2021 shared tasks at the BioNLP 2021 workshop addressed three tasks on summarization for medical text: (i) a question summarization task aimed at exploring new approaches to understanding complex real-world consumer health queries, (ii) a multi-answer summarization task that targeted aggregation of multiple relevant answers to a biomedical question into one concise and relevant answer, and (iii) a radiology report summarization task addressing the development of clinically relevant impressions from radiology report findings. Thirty-five teams participated in these shared tasks with sixteen working notes submitted (fifteen accepted) describing a wide variety of models developed and tested on the shared and external datasets. In this paper, we describe the tasks, the datasets, the models and techniques developed by various teams, the results of the evaluation, and a study of correlations among various summarization evaluation measures. We hope that these shared tasks will bring new research and insights in biomedical text summarization and evaluation."",","{ben-abacha-etal-2021-overview,
    title = ""Overview of the {MEDIQA} 2021 Shared Task on Summarization in the Medical Domain"",
    author = ""Ben Abacha, Asma  and
      Mrabet, Yassine  and
      Zhang, Yuhao  and
      Shivade, Chaitanya  and
      Langlotz, Curtis  and
      Demner-Fushman, Dina"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 20th Workshop on Biomedical Language Processing"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.bionlp-1.8"",
    doi = ""10.18653/v1/2021.bionlp-1.8"",
    pages = ""74--85"",
    abstract = ""The MEDIQA 2021 shared tasks at the BioNLP 2021 workshop addressed three tasks on summarization for medical text: (i) a question summarization task aimed at exploring new approaches to understanding complex real-world consumer health queries, (ii) a multi-answer summarization task that targeted aggregation of multiple relevant answers to a biomedical question into one concise and relevant answer, and (iii) a radiology report summarization task addressing the development of clinically relevant impressions from radiology report findings. Thirty-five teams participated in these shared tasks with sixteen working notes submitted (fifteen accepted) describing a wide variety of models developed and tested on the shared and external datasets. In this paper, we describe the tasks, the datasets, the models and techniques developed by various teams, the results of the evaluation, and a study of correlations among various summarization evaluation measures. We hope that these shared tasks will bring new research and insights in biomedical text summarization and evaluation."",
}
@",medical domain,medical text,language process,nlp,summar,shared task,evalu
" ""Stress Test Evaluation of Biomedical Word Embeddings"","," ""The success of pretrained word embeddings has motivated their use in the biomedical domain, with contextualized embeddings yielding remarkable results in several biomedical NLP tasks. However, there is a lack of research on quantifying their behavior under severe {``}stress{''} scenarios. In this work, we systematically evaluate three language models with adversarial examples {--} automatically constructed tests that allow us to examine how robust the models are. We propose two types of stress scenarios focused on the biomedical named entity recognition (NER) task, one inspired by spelling errors and another based on the use of synonyms for medical terms. Our experiments with three benchmarks show that the performance of the original models decreases considerably, in addition to revealing their weaknesses and strengths. Finally, we show that adversarial training causes the models to improve their robustness and even to exceed the original performance in some cases."",","{araujo-etal-2021-stress,
    title = ""Stress Test Evaluation of Biomedical Word Embeddings"",
    author = ""Araujo, Vladimir  and
      Carvallo, Andr{\'e}s  and
      Aspillaga, Carlos  and
      Thorne, Camilo  and
      Parra, Denis"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 20th Workshop on Biomedical Language Processing"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.bionlp-1.13"",
    doi = ""10.18653/v1/2021.bionlp-1.13"",
    pages = ""119--125"",
    abstract = ""The success of pretrained word embeddings has motivated their use in the biomedical domain, with contextualized embeddings yielding remarkable results in several biomedical NLP tasks. However, there is a lack of research on quantifying their behavior under severe {``}stress{''} scenarios. In this work, we systematically evaluate three language models with adversarial examples {--} automatically constructed tests that allow us to examine how robust the models are. We propose two types of stress scenarios focused on the biomedical named entity recognition (NER) task, one inspired by spelling errors and another based on the use of synonyms for medical terms. Our experiments with three benchmarks show that the performance of the original models decreases considerably, in addition to revealing their weaknesses and strengths. Finally, we show that adversarial training causes the models to improve their robustness and even to exceed the original performance in some cases."",
}
@",medical domain,language process,nlp,entity recognit,benchmark,evalu
" ""{BLAR}: Biomedical Local Acronym Resolver"","," ""NLP has emerged as an essential tool to extract knowledge from the exponentially increasing volumes of biomedical texts. Many NLP tasks, such as named entity recognition and named entity normalization, are especially challenging in the biomedical domain partly because of the prolific use of acronyms. Long names for diseases, bacteria, and chemicals are often replaced by acronyms. We propose Biomedical Local Acronym Resolver (BLAR), a high-performing acronym resolver that leverages state-of-the-art (SOTA) pre-trained language models to accurately resolve local acronyms in biomedical texts. We test BLAR on the Ab3P corpus and achieve state-of-the-art results compared to the current best-performing local acronym resolution algorithms and models."",","{hogan-etal-2021-blar,
    title = ""{BLAR}: Biomedical Local Acronym Resolver"",
    author = ""Hogan, William  and
      Vazquez Baeza, Yoshiki  and
      Katsis, Yannis  and
      Baldwin, Tyler  and
      Kim, Ho-Cheol  and
      Hsu, Chun-Nan"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 20th Workshop on Biomedical Language Processing"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.bionlp-1.14"",
    doi = ""10.18653/v1/2021.bionlp-1.14"",
    pages = ""126--130"",
    abstract = ""NLP has emerged as an essential tool to extract knowledge from the exponentially increasing volumes of biomedical texts. Many NLP tasks, such as named entity recognition and named entity normalization, are especially challenging in the biomedical domain partly because of the prolific use of acronyms. Long names for diseases, bacteria, and chemicals are often replaced by acronyms. We propose Biomedical Local Acronym Resolver (BLAR), a high-performing acronym resolver that leverages state-of-the-art (SOTA) pre-trained language models to accurately resolve local acronyms in biomedical texts. We test BLAR on the Ab3P corpus and achieve state-of-the-art results compared to the current best-performing local acronym resolution algorithms and models."",
}
@",medical domain,medical text,language process,nlp,entity recognit,challeng
" ""Claim Detection in Biomedical {T}witter Posts"","," ""Social media contains unfiltered and unique information, which is potentially of great value, but, in the case of misinformation, can also do great harm. With regards to biomedical topics, false information can be particularly dangerous. Methods of automatic fact-checking and fake news detection address this problem, but have not been applied to the biomedical domain in social media yet. We aim to fill this research gap and annotate a corpus of 1200 tweets for implicit and explicit biomedical claims (the latter also with span annotations for the claim phrase). With this corpus, which we sample to be related to COVID-19, measles, cystic fibrosis, and depression, we develop baseline models which detect tweets that contain a claim automatically. Our analyses reveal that biomedical tweets are densely populated with claims (45 {\%} in a corpus sampled to contain 1200 tweets focused on the domains mentioned above). Baseline classification experiments with embedding-based classifiers and BERT-based transfer learning demonstrate that the detection is challenging, however, shows acceptable performance for the identification of explicit expressions of claims. Implicit claim tweets are more challenging to detect."",","{wuhrl-klinger-2021-claim,
    title = ""Claim Detection in Biomedical {T}witter Posts"",
    author = {W{\""u}hrl, Amelie  and
      Klinger, Roman},
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 20th Workshop on Biomedical Language Processing"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.bionlp-1.15"",
    doi = ""10.18653/v1/2021.bionlp-1.15"",
    pages = ""131--142"",
    abstract = ""Social media contains unfiltered and unique information, which is potentially of great value, but, in the case of misinformation, can also do great harm. With regards to biomedical topics, false information can be particularly dangerous. Methods of automatic fact-checking and fake news detection address this problem, but have not been applied to the biomedical domain in social media yet. We aim to fill this research gap and annotate a corpus of 1200 tweets for implicit and explicit biomedical claims (the latter also with span annotations for the claim phrase). With this corpus, which we sample to be related to COVID-19, measles, cystic fibrosis, and depression, we develop baseline models which detect tweets that contain a claim automatically. Our analyses reveal that biomedical tweets are densely populated with claims (45 {\%} in a corpus sampled to contain 1200 tweets focused on the domains mentioned above). Baseline classification experiments with embedding-based classifiers and BERT-based transfer learning demonstrate that the detection is challenging, however, shows acceptable performance for the identification of explicit expressions of claims. Implicit claim tweets are more challenging to detect."",
}
@",medical domain,language process,nlp,annotat,challeng
" ""{B}io{ELECTRA}:Pretrained Biomedical text Encoder using Discriminators"","," ""Recent advancements in pretraining strategies in NLP have shown a significant improvement in the performance of models on various text mining tasks. We apply {`}replaced token detection{'} pretraining technique proposed by ELECTRA and pretrain a biomedical language model from scratch using biomedical text and vocabulary. We introduce BioELECTRA, a biomedical domain-specific language encoder model that adapts ELECTRA for the Biomedical domain. WE evaluate our model on the BLURB and BLUE biomedical NLP benchmarks. BioELECTRA outperforms the previous models and achieves state of the art (SOTA) on all the 13 datasets in BLURB benchmark and on all the 4 Clinical datasets from BLUE Benchmark across 7 different NLP tasks. BioELECTRA pretrained on PubMed and PMC full text articles performs very well on Clinical datasets as well. BioELECTRA achieves new SOTA 86.34{\%}(1.39{\%} accuracy improvement) on MedNLI and 64{\%} (2.98{\%} accuracy improvement) on PubMedQA dataset."",","{kanakarajan-etal-2021-bioelectra,
    title = ""{B}io{ELECTRA}:Pretrained Biomedical text Encoder using Discriminators"",
    author = ""Kanakarajan, Kamal raj  and
      Kundumani, Bhuvana  and
      Sankarasubbu, Malaikannan"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 20th Workshop on Biomedical Language Processing"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.bionlp-1.16"",
    doi = ""10.18653/v1/2021.bionlp-1.16"",
    pages = ""143--154"",
    abstract = ""Recent advancements in pretraining strategies in NLP have shown a significant improvement in the performance of models on various text mining tasks. We apply {`}replaced token detection{'} pretraining technique proposed by ELECTRA and pretrain a biomedical language model from scratch using biomedical text and vocabulary. We introduce BioELECTRA, a biomedical domain-specific language encoder model that adapts ELECTRA for the Biomedical domain. WE evaluate our model on the BLURB and BLUE biomedical NLP benchmarks. BioELECTRA outperforms the previous models and achieves state of the art (SOTA) on all the 13 datasets in BLURB benchmark and on all the 4 Clinical datasets from BLUE Benchmark across 7 different NLP tasks. BioELECTRA pretrained on PubMed and PMC full text articles performs very well on Clinical datasets as well. BioELECTRA achieves new SOTA 86.34{\%}(1.39{\%} accuracy improvement) on MedNLI and 64{\%} (2.98{\%} accuracy improvement) on PubMedQA dataset."",
}
@",medical domain,medical text,language process,nlp,benchmark,evalu
" ""Improving Biomedical Pretrained Language Models with Knowledge"","," ""Pretrained language models have shown success in many natural language processing tasks. Many works explore to incorporate the knowledge into the language models. In the biomedical domain, experts have taken decades of effort on building large-scale knowledge bases. For example, UMLS contains millions of entities with their synonyms and defines hundreds of relations among entities. Leveraging this knowledge can benefit a variety of downstream tasks such as named entity recognition and relation extraction. To this end, we propose KeBioLM, a biomedical pretrained language model that explicitly leverages knowledge from the UMLS knowledge bases. Specifically, we extract entities from PubMed abstracts and link them to UMLS. We then train a knowledge-aware language model that firstly applies a text-only encoding layer to learn entity representation and then applies a text-entity fusion encoding to aggregate entity representation. In addition, we add two training objectives as entity detection and entity linking. Experiments on the named entity recognition and relation extraction tasks from the BLURB benchmark demonstrate the effectiveness of our approach. Further analysis on a collected probing dataset shows that our model has better ability to model medical knowledge."",","{yuan-etal-2021-improving,
    title = ""Improving Biomedical Pretrained Language Models with Knowledge"",
    author = ""Yuan, Zheng  and
      Liu, Yijia  and
      Tan, Chuanqi  and
      Huang, Songfang  and
      Huang, Fei"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 20th Workshop on Biomedical Language Processing"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.bionlp-1.20"",
    doi = ""10.18653/v1/2021.bionlp-1.20"",
    pages = ""180--190"",
    abstract = ""Pretrained language models have shown success in many natural language processing tasks. Many works explore to incorporate the knowledge into the language models. In the biomedical domain, experts have taken decades of effort on building large-scale knowledge bases. For example, UMLS contains millions of entities with their synonyms and defines hundreds of relations among entities. Leveraging this knowledge can benefit a variety of downstream tasks such as named entity recognition and relation extraction. To this end, we propose KeBioLM, a biomedical pretrained language model that explicitly leverages knowledge from the UMLS knowledge bases. Specifically, we extract entities from PubMed abstracts and link them to UMLS. We then train a knowledge-aware language model that firstly applies a text-only encoding layer to learn entity representation and then applies a text-entity fusion encoding to aggregate entity representation. In addition, we add two training objectives as entity detection and entity linking. Experiments on the named entity recognition and relation extraction tasks from the BLURB benchmark demonstrate the effectiveness of our approach. Further analysis on a collected probing dataset shows that our model has better ability to model medical knowledge."",
}
@",medical domain,natural language process,natural languag,language process,nlp,relation extract,entity recognit,benchmark
" ""{E}ntity{BERT}: Entity-centric Masking Strategy for Model Pretraining for the Clinical Domain"","," ""Transformer-based neural language models have led to breakthroughs for a variety of natural language processing (NLP) tasks. However, most models are pretrained on general domain data. We propose a methodology to produce a model focused on the clinical domain: continued pretraining of a model with a broad representation of biomedical terminology (PubMedBERT) on a clinical corpus along with a novel entity-centric masking strategy to infuse domain knowledge in the learning process. We show that such a model achieves superior results on clinical extraction tasks by comparing our entity-centric masking strategy with classic random masking on three clinical NLP tasks: cross-domain negation detection, document time relation (DocTimeRel) classification, and temporal relation extraction. We also evaluate our models on the PubMedQA dataset to measure the models{'} performance on a non-entity-centric task in the biomedical domain. The language addressed in this work is English."",","{lin-etal-2021-entitybert,
    title = ""{E}ntity{BERT}: Entity-centric Masking Strategy for Model Pretraining for the Clinical Domain"",
    author = ""Lin, Chen  and
      Miller, Timothy  and
      Dligach, Dmitriy  and
      Bethard, Steven  and
      Savova, Guergana"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 20th Workshop on Biomedical Language Processing"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.bionlp-1.21"",
    doi = ""10.18653/v1/2021.bionlp-1.21"",
    pages = ""191--201"",
    abstract = ""Transformer-based neural language models have led to breakthroughs for a variety of natural language processing (NLP) tasks. However, most models are pretrained on general domain data. We propose a methodology to produce a model focused on the clinical domain: continued pretraining of a model with a broad representation of biomedical terminology (PubMedBERT) on a clinical corpus along with a novel entity-centric masking strategy to infuse domain knowledge in the learning process. We show that such a model achieves superior results on clinical extraction tasks by comparing our entity-centric masking strategy with classic random masking on three clinical NLP tasks: cross-domain negation detection, document time relation (DocTimeRel) classification, and temporal relation extraction. We also evaluate our models on the PubMedQA dataset to measure the models{'} performance on a non-entity-centric task in the biomedical domain. The language addressed in this work is English."",
}
@",clinical domain,medical domain,natural language process,natural languag,language process,nlp,relation extract,evalu
" ""Exploring Word Segmentation and Medical Concept Recognition for {C}hinese Medical Texts"","," ""Chinese word segmentation (CWS) and medical concept recognition are two fundamental tasks to process Chinese electronic medical records (EMRs) and play important roles in downstream tasks for understanding Chinese EMRs. One challenge to these tasks is the lack of medical domain datasets with high-quality annotations, especially medical-related tags that reveal the characteristics of Chinese EMRs. In this paper, we collected a Chinese EMR corpus, namely, ACEMR, with human annotations for Chinese word segmentation and EMR-related tags. On the ACEMR corpus, we run well-known models (i.e., BiLSTM, BERT, and ZEN) and existing state-of-the-art systems (e.g., WMSeg and TwASP) for CWS and medical concept recognition. Experimental results demonstrate the necessity of building a dedicated medical dataset and show that models that leverage extra resources achieve the best performance for both tasks, which provides certain guidance for future studies on model selection in the medical domain."",","{liu-etal-2021-exploring,
    title = ""Exploring Word Segmentation and Medical Concept Recognition for {C}hinese Medical Texts"",
    author = ""Liu, Yang  and
      Tian, Yuanhe  and
      Chang, Tsung-Hui  and
      Wu, Song  and
      Wan, Xiang  and
      Song, Yan"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 20th Workshop on Biomedical Language Processing"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.bionlp-1.23"",
    doi = ""10.18653/v1/2021.bionlp-1.23"",
    pages = ""213--220"",
    abstract = ""Chinese word segmentation (CWS) and medical concept recognition are two fundamental tasks to process Chinese electronic medical records (EMRs) and play important roles in downstream tasks for understanding Chinese EMRs. One challenge to these tasks is the lack of medical domain datasets with high-quality annotations, especially medical-related tags that reveal the characteristics of Chinese EMRs. In this paper, we collected a Chinese EMR corpus, namely, ACEMR, with human annotations for Chinese word segmentation and EMR-related tags. On the ACEMR corpus, we run well-known models (i.e., BiLSTM, BERT, and ZEN) and existing state-of-the-art systems (e.g., WMSeg and TwASP) for CWS and medical concept recognition. Experimental results demonstrate the necessity of building a dedicated medical dataset and show that models that leverage extra resources achieve the best performance for both tasks, which provides certain guidance for future studies on model selection in the medical domain."",
}
@",medical record,medical domain,medical text,language process,nlp,annotat,challeng
" ""{B}io{M}-Transformers: Building Large Biomedical Language Models with {BERT}, {ALBERT} and {ELECTRA}"","," ""The impact of design choices on the performance of biomedical language models recently has been a subject for investigation. In this paper, we empirically study biomedical domain adaptation with large transformer models using different design choices. We evaluate the performance of our pretrained models against other existing biomedical language models in the literature. Our results show that we achieve state-of-the-art results on several biomedical domain tasks despite using similar or less computational cost compared to other models in the literature. Our findings highlight the significant effect of design choices on improving the performance of biomedical language models."",","{alrowili-shanker-2021-biom,
    title = ""{B}io{M}-Transformers: Building Large Biomedical Language Models with {BERT}, {ALBERT} and {ELECTRA}"",
    author = ""Alrowili, Sultan  and
      Shanker, Vijay"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 20th Workshop on Biomedical Language Processing"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.bionlp-1.24"",
    doi = ""10.18653/v1/2021.bionlp-1.24"",
    pages = ""221--227"",
    abstract = ""The impact of design choices on the performance of biomedical language models recently has been a subject for investigation. In this paper, we empirically study biomedical domain adaptation with large transformer models using different design choices. We evaluate the performance of our pretrained models against other existing biomedical language models in the literature. Our results show that we achieve state-of-the-art results on several biomedical domain tasks despite using similar or less computational cost compared to other models in the literature. Our findings highlight the significant effect of design choices on improving the performance of biomedical language models."",
}
@",medical domain,language process,nlp,evalu
" ""{UCSD}-Adobe at {MEDIQA} 2021: Transfer Learning and Answer Sentence Selection for Medical Summarization"","," ""In this paper, we describe our approach to question summarization and multi-answer summarization in the context of the 2021 MEDIQA shared task (Ben Abacha et al., 2021). We propose two kinds of transfer learning for the abstractive summarization of medical questions. First, we train on HealthCareMagic, a large question summarization dataset collected from an online healthcare service platform. Second, we leverage the ability of the BART encoder-decoder architecture to model both generation and classification tasks to train on the task of Recognizing Question Entailment (RQE) in the medical domain. We show that both transfer learning methods combined achieve the highest ROUGE scores. Finally, we cast the question-driven extractive summarization of multiple relevant answer documents as an Answer Sentence Selection (AS2) problem. We show how we can preprocess the MEDIQA-AnS dataset such that it can be trained in an AS2 setting. Our AS2 model is able to generate extractive summaries achieving high ROUGE scores."",","{mrini-etal-2021-ucsd,
    title = ""{UCSD}-Adobe at {MEDIQA} 2021: Transfer Learning and Answer Sentence Selection for Medical Summarization"",
    author = ""Mrini, Khalil  and
      Dernoncourt, Franck  and
      Yoon, Seunghyun  and
      Bui, Trung  and
      Chang, Walter  and
      Farcas, Emilia  and
      Nakashole, Ndapa"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 20th Workshop on Biomedical Language Processing"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.bionlp-1.28"",
    doi = ""10.18653/v1/2021.bionlp-1.28"",
    pages = ""257--262"",
    abstract = ""In this paper, we describe our approach to question summarization and multi-answer summarization in the context of the 2021 MEDIQA shared task (Ben Abacha et al., 2021). We propose two kinds of transfer learning for the abstractive summarization of medical questions. First, we train on HealthCareMagic, a large question summarization dataset collected from an online healthcare service platform. Second, we leverage the ability of the BART encoder-decoder architecture to model both generation and classification tasks to train on the task of Recognizing Question Entailment (RQE) in the medical domain. We show that both transfer learning methods combined achieve the highest ROUGE scores. Finally, we cast the question-driven extractive summarization of multiple relevant answer documents as an Answer Sentence Selection (AS2) problem. We show how we can preprocess the MEDIQA-AnS dataset such that it can be trained in an AS2 setting. Our AS2 model is able to generate extractive summaries achieving high ROUGE scores."",
}
@",medical domain,language process,nlp,generat,summar,shared task
" ""{SB}{\_}{NITK} at {MEDIQA} 2021: Leveraging Transfer Learning for Question Summarization in Medical Domain"","," ""Recent strides in the healthcare domain, have resulted in vast quantities of streaming data available for use for building intelligent knowledge-based applications. However, the challenges introduced to the huge volume, velocity of generation, variety and variability of this medical data have to be adequately addressed. In this paper, we describe the model and results for our submission at MEDIQA 2021 Question Summarization shared task. In order to improve the performance of summarization of consumer health questions, our method explores the use of transfer learning to utilize the knowledge of NLP transformers like BART, T5 and PEGASUS. The proposed models utilize the knowledge of pre-trained NLP transformers to achieve improved results when compared to conventional deep learning models such as LSTM, RNN etc. Our team SB{\_}NITK ranked 12th among the total 22 submissions in the official final rankings. Our BART based model achieved a ROUGE-2 F1 score of 0.139."",","{balumuri-etal-2021-sb,
    title = ""{SB}{\_}{NITK} at {MEDIQA} 2021: Leveraging Transfer Learning for Question Summarization in Medical Domain"",
    author = ""Balumuri, Spandana  and
      Bachina, Sony  and
      Kamath S, Sowmya"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 20th Workshop on Biomedical Language Processing"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.bionlp-1.31"",
    doi = ""10.18653/v1/2021.bionlp-1.31"",
    pages = ""273--279"",
    abstract = ""Recent strides in the healthcare domain, have resulted in vast quantities of streaming data available for use for building intelligent knowledge-based applications. However, the challenges introduced to the huge volume, velocity of generation, variety and variability of this medical data have to be adequately addressed. In this paper, we describe the model and results for our submission at MEDIQA 2021 Question Summarization shared task. In order to improve the performance of summarization of consumer health questions, our method explores the use of transfer learning to utilize the knowledge of NLP transformers like BART, T5 and PEGASUS. The proposed models utilize the knowledge of pre-trained NLP transformers to achieve improved results when compared to conventional deep learning models such as LSTM, RNN etc. Our team SB{\_}NITK ranked 12th among the total 22 submissions in the official final rankings. Our BART based model achieved a ROUGE-2 F1 score of 0.139."",
}
@",medical domain,language process,nlp,generat,summar,shared task,challeng
" ""Domain adaptation in practice: Lessons from a real-world information extraction pipeline"","," ""Advances in transfer learning and domain adaptation have raised hopes that once-challenging NLP tasks are ready to be put to use for sophisticated information extraction needs. In this work, we describe an effort to do just that {--} combining state-of-the-art neural methods for negation detection, document time relation extraction, and aspectual link prediction, with the eventual goal of extracting drug timelines from electronic health record text. We train on the THYME colon cancer corpus and test on both the THYME brain cancer corpus and an internal corpus, and show that performance of the combined systems is unacceptable despite good performance of individual systems. Although domain adaptation shows improvements on each individual system, the model selection problem is a barrier to improving overall pipeline performance."",","{miller-etal-2021-domain,
    title = ""Domain adaptation in practice: Lessons from a real-world information extraction pipeline"",
    author = ""Miller, Timothy  and
      Laparra, Egoitz  and
      Bethard, Steven"",
    editor = ""Ben-David, Eyal  and
      Cohen, Shay  and
      McDonald, Ryan  and
      Plank, Barbara  and
      Reichart, Roi  and
      Rotman, Guy  and
      Ziser, Yftah"",
    booktitle = ""Proceedings of the Second Workshop on Domain Adaptation for NLP"",
    month = apr,
    year = ""2021"",
    address = ""Kyiv, Ukraine"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.adaptnlp-1.11"",
    pages = ""105--110"",
    abstract = ""Advances in transfer learning and domain adaptation have raised hopes that once-challenging NLP tasks are ready to be put to use for sophisticated information extraction needs. In this work, we describe an effort to do just that {--} combining state-of-the-art neural methods for negation detection, document time relation extraction, and aspectual link prediction, with the eventual goal of extracting drug timelines from electronic health record text. We train on the THYME colon cancer corpus and test on both the THYME brain cancer corpus and an internal corpus, and show that performance of the combined systems is unacceptable despite good performance of individual systems. Although domain adaptation shows improvements on each individual system, the model selection problem is a barrier to improving overall pipeline performance."",
}
@",electronic health record,health record,nlp,relation extract,challeng
" ""Entity Enhancement for Implicit Discourse Relation Classification in the Biomedical Domain"","," ""Implicit discourse relation classification is a challenging task, in particular when the text domain is different from the standard Penn Discourse Treebank (PDTB; Prasad et al., 2008) training corpus domain (Wall Street Journal in 1990s). We here tackle the task of implicit discourse relation classification on the biomedical domain, for which the Biomedical Discourse Relation Bank (BioDRB; Prasad et al., 2011) is available. We show that entity information can be used to improve discourse relational argument representation. In a first step, we show that explicitly marked instances that are content-wise similar to the target relations can be used to achieve good performance in the cross-domain setting using a simple unsupervised voting pipeline. As a further step, we show that with the linked entity information from the first step, a transformer which is augmented with entity-related information (KBERT; Liu et al., 2020) sets the new state of the art performance on the dataset, outperforming the large pre-trained BioBERT (Lee et al., 2020) model by 2{\%} points."",","{shi-demberg-2021-entity,
    title = ""Entity Enhancement for Implicit Discourse Relation Classification in the Biomedical Domain"",
    author = ""Shi, Wei  and
      Demberg, Vera"",
    editor = ""Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto"",
    booktitle = ""Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)"",
    month = aug,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.acl-short.116"",
    doi = ""10.18653/v1/2021.acl-short.116"",
    pages = ""925--931"",
    abstract = ""Implicit discourse relation classification is a challenging task, in particular when the text domain is different from the standard Penn Discourse Treebank (PDTB; Prasad et al., 2008) training corpus domain (Wall Street Journal in 1990s). We here tackle the task of implicit discourse relation classification on the biomedical domain, for which the Biomedical Discourse Relation Bank (BioDRB; Prasad et al., 2011) is available. We show that entity information can be used to improve discourse relational argument representation. In a first step, we show that explicitly marked instances that are content-wise similar to the target relations can be used to achieve good performance in the cross-domain setting using a simple unsupervised voting pipeline. As a further step, we show that with the linked entity information from the first step, a transformer which is augmented with entity-related information (KBERT; Liu et al., 2020) sets the new state of the art performance on the dataset, outperforming the large pre-trained BioBERT (Lee et al., 2020) model by 2{\%} points."",
}
@",medical domain,natural language process,natural languag,language process,relation classif,challeng
" ""{M}ed{NLI} Is Not Immune: {N}atural Language Inference Artifacts in the Clinical Domain"","," ""Crowdworker-constructed natural language inference (NLI) datasets have been found to contain statistical artifacts associated with the annotation process that allow hypothesis-only classifiers to achieve better-than-random performance (CITATION). We investigate whether MedNLI, a physician-annotated dataset with premises extracted from clinical notes, contains such artifacts (CITATION). We find that entailed hypotheses contain generic versions of specific concepts in the premise, as well as modifiers related to responsiveness, duration, and probability. Neutral hypotheses feature conditions and behaviors that co-occur with, or cause, the condition(s) in the premise. Contradiction hypotheses feature explicit negation of the premise and implicit negation via assertion of good health. Adversarial filtering demonstrates that performance degrades when evaluated on the \textit{difficult} subset. We provide partition information and recommendations for alternative dataset construction strategies for knowledge-intensive domains."",","{herlihy-rudinger-2021-mednli,
    title = ""{M}ed{NLI} Is Not Immune: {N}atural Language Inference Artifacts in the Clinical Domain"",
    author = ""Herlihy, Christine  and
      Rudinger, Rachel"",
    editor = ""Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto"",
    booktitle = ""Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)"",
    month = aug,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.acl-short.129"",
    doi = ""10.18653/v1/2021.acl-short.129"",
    pages = ""1020--1027"",
    abstract = ""Crowdworker-constructed natural language inference (NLI) datasets have been found to contain statistical artifacts associated with the annotation process that allow hypothesis-only classifiers to achieve better-than-random performance (CITATION). We investigate whether MedNLI, a physician-annotated dataset with premises extracted from clinical notes, contains such artifacts (CITATION). We find that entailed hypotheses contain generic versions of specific concepts in the premise, as well as modifiers related to responsiveness, duration, and probability. Neutral hypotheses feature conditions and behaviors that co-occur with, or cause, the condition(s) in the premise. Contradiction hypotheses feature explicit negation of the premise and implicit negation via assertion of good health. Adversarial filtering demonstrates that performance degrades when evaluated on the \textit{difficult} subset. We provide partition information and recommendations for alternative dataset construction strategies for knowledge-intensive domains."",
}
@",clinical domain,clinical not,natural language process,natural languag,language process,infer,annotat,evalu
" ""{CLIP}: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes"","," ""Continuity of care is crucial to ensuring positive health outcomes for patients discharged from an inpatient hospital setting, and improved information sharing can help. To share information, caregivers write discharge notes containing action items to share with patients and their future caregivers, but these action items are easily lost due to the lengthiness of the documents. In this work, we describe our creation of a dataset of clinical action items annotated over MIMIC-III, the largest publicly available dataset of real clinical notes. This dataset, which we call CLIP, is annotated by physicians and covers 718 documents representing 100K sentences. We describe the task of extracting the action items from these documents as multi-aspect extractive summarization, with each aspect representing a type of action to be taken. We evaluate several machine learning models on this task, and show that the best models exploit in-domain language model pre-training on 59K unannotated documents, and incorporate context from neighboring sentences. We also propose an approach to pre-training data selection that allows us to explore the trade-off between size and domain-specificity of pre-training datasets for this task."",","{mullenbach-etal-2021-clip,
    title = ""{CLIP}: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes"",
    author = ""Mullenbach, James  and
      Pruksachatkun, Yada  and
      Adler, Sean  and
      Seale, Jennifer  and
      Swartz, Jordan  and
      McKelvey, Greg  and
      Dai, Hui  and
      Yang, Yi  and
      Sontag, David"",
    editor = ""Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto"",
    booktitle = ""Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"",
    month = aug,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.acl-long.109"",
    doi = ""10.18653/v1/2021.acl-long.109"",
    pages = ""1365--1378"",
    abstract = ""Continuity of care is crucial to ensuring positive health outcomes for patients discharged from an inpatient hospital setting, and improved information sharing can help. To share information, caregivers write discharge notes containing action items to share with patients and their future caregivers, but these action items are easily lost due to the lengthiness of the documents. In this work, we describe our creation of a dataset of clinical action items annotated over MIMIC-III, the largest publicly available dataset of real clinical notes. This dataset, which we call CLIP, is annotated by physicians and covers 718 documents representing 100K sentences. We describe the task of extracting the action items from these documents as multi-aspect extractive summarization, with each aspect representing a type of action to be taken. We evaluate several machine learning models on this task, and show that the best models exploit in-domain language model pre-training on 59K unannotated documents, and incorporate context from neighboring sentences. We also propose an approach to pre-training data selection that allows us to explore the trade-off between size and domain-specificity of pre-training datasets for this task."",
}
@",clinical not,natural language process,natural languag,language process,summar,annotat,evalu,publicly available dataset,publicly available dataset
" ""A Gradually Soft Multi-Task and Data-Augmented Approach to Medical Question Understanding"","," ""Users of medical question answering systems often submit long and detailed questions, making it hard to achieve high recall in answer retrieval. To alleviate this problem, we propose a novel Multi-Task Learning (MTL) method with data augmentation for medical question understanding. We first establish an equivalence between the tasks of question summarization and Recognizing Question Entailment (RQE) using their definitions in the medical domain. Based on this equivalence, we propose a data augmentation algorithm to use just one dataset to optimize for both tasks, with a weighted MTL loss. We introduce gradually soft parameter-sharing: a constraint for decoder parameters to be close, that is gradually loosened as we move to the highest layer. We show through ablation studies that our proposed novelties improve performance. Our method outperforms existing MTL methods across 4 datasets of medical question pairs, in ROUGE scores, RQE accuracy and human evaluation. Finally, we show that our method fares better than single-task learning under 4 low-resource settings."",","{mrini-etal-2021-gradually,
    title = ""A Gradually Soft Multi-Task and Data-Augmented Approach to Medical Question Understanding"",
    author = ""Mrini, Khalil  and
      Dernoncourt, Franck  and
      Yoon, Seunghyun  and
      Bui, Trung  and
      Chang, Walter  and
      Farcas, Emilia  and
      Nakashole, Ndapa"",
    editor = ""Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto"",
    booktitle = ""Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"",
    month = aug,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.acl-long.119"",
    doi = ""10.18653/v1/2021.acl-long.119"",
    pages = ""1505--1515"",
    abstract = ""Users of medical question answering systems often submit long and detailed questions, making it hard to achieve high recall in answer retrieval. To alleviate this problem, we propose a novel Multi-Task Learning (MTL) method with data augmentation for medical question understanding. We first establish an equivalence between the tasks of question summarization and Recognizing Question Entailment (RQE) using their definitions in the medical domain. Based on this equivalence, we propose a data augmentation algorithm to use just one dataset to optimize for both tasks, with a weighted MTL loss. We introduce gradually soft parameter-sharing: a constraint for decoder parameters to be close, that is gradually loosened as we move to the highest layer. We show through ablation studies that our proposed novelties improve performance. Our method outperforms existing MTL methods across 4 datasets of medical question pairs, in ROUGE scores, RQE accuracy and human evaluation. Finally, we show that our method fares better than single-task learning under 4 low-resource settings."",
}
@",medical domain,natural language process,natural languag,language process,summar,evalu
" ""A Neural Transition-based Joint Model for Disease Named Entity Recognition and Normalization"","," ""Disease is one of the fundamental entities in biomedical research. Recognizing such entities from biomedical text and then normalizing them to a standardized disease vocabulary offer a tremendous opportunity for many downstream applications. Previous studies have demonstrated that joint modeling of the two sub-tasks has superior performance than the pipelined counterpart. Although the neural joint model based on multi-task learning framework has achieved state-of-the-art performance, it suffers from the boundary inconsistency problem due to the separate decoding procedures. Moreover, it ignores the rich information (e.g., the text surface form) of each candidate concept in the vocabulary, which is quite essential for entity normalization. In this work, we propose a neural transition-based joint model to alleviate these two issues. We transform the end-to-end disease recognition and normalization task as an action sequence prediction task, which not only jointly learns the model with shared representations of the input, but also jointly searches the output by state transitions in one search space. Moreover, we introduce attention mechanisms to take advantage of the text surface form of each candidate concept for better normalization performance. Experimental results conducted on two publicly available datasets show the effectiveness of the proposed method."",","{ji-etal-2021-neural,
    title = ""A Neural Transition-based Joint Model for Disease Named Entity Recognition and Normalization"",
    author = ""Ji, Zongcheng  and
      Xia, Tian  and
      Han, Mei  and
      Xiao, Jing"",
    editor = ""Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto"",
    booktitle = ""Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"",
    month = aug,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.acl-long.219"",
    doi = ""10.18653/v1/2021.acl-long.219"",
    pages = ""2819--2827"",
    abstract = ""Disease is one of the fundamental entities in biomedical research. Recognizing such entities from biomedical text and then normalizing them to a standardized disease vocabulary offer a tremendous opportunity for many downstream applications. Previous studies have demonstrated that joint modeling of the two sub-tasks has superior performance than the pipelined counterpart. Although the neural joint model based on multi-task learning framework has achieved state-of-the-art performance, it suffers from the boundary inconsistency problem due to the separate decoding procedures. Moreover, it ignores the rich information (e.g., the text surface form) of each candidate concept in the vocabulary, which is quite essential for entity normalization. In this work, we propose a neural transition-based joint model to alleviate these two issues. We transform the end-to-end disease recognition and normalization task as an action sequence prediction task, which not only jointly learns the model with shared representations of the input, but also jointly searches the output by state transitions in one search space. Moreover, we introduce attention mechanisms to take advantage of the text surface form of each candidate concept for better normalization performance. Experimental results conducted on two publicly available datasets show the effectiveness of the proposed method."",
}
@",medical text,natural language process,natural languag,language process,entity recognit,publicly available dataset,publicly available dataset
" ""Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision"","," ""The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a large scale of in-domain relevance training signals, which are not always available in real-world ranking scenarios. To democratize the benefits of Neu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method that generalizes Neu-IR models from label-rich source domains to few-shot target domains. Drawing on source-domain massive relevance supervision, MetaAdaptRank contrastively synthesizes a large number of weak supervision signals for target domains and meta-learns to reweight these synthetic {``}weak{''} data based on their benefits to the target-domain ranking accuracy of Neu-IR models. Experiments on three TREC benchmarks in the web, news, and biomedical domains show that MetaAdaptRank significantly improves the few-shot ranking accuracy of Neu-IR models. Further analyses indicate that MetaAdaptRank thrives from both its contrastive weak data synthesis and meta-reweighted data selection. The code and data of this paper can be obtained from \url{https://github.com/thunlp/MetaAdaptRank}."",","{sun-etal-2021-shot,
    title = ""Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision"",
    author = ""Sun, Si  and
      Qian, Yingzhuo  and
      Liu, Zhenghao  and
      Xiong, Chenyan  and
      Zhang, Kaitao  and
      Bao, Jie  and
      Liu, Zhiyuan  and
      Bennett, Paul"",
    editor = ""Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto"",
    booktitle = ""Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"",
    month = aug,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.acl-long.390"",
    doi = ""10.18653/v1/2021.acl-long.390"",
    pages = ""5030--5043"",
    abstract = ""The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a large scale of in-domain relevance training signals, which are not always available in real-world ranking scenarios. To democratize the benefits of Neu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method that generalizes Neu-IR models from label-rich source domains to few-shot target domains. Drawing on source-domain massive relevance supervision, MetaAdaptRank contrastively synthesizes a large number of weak supervision signals for target domains and meta-learns to reweight these synthetic {``}weak{''} data based on their benefits to the target-domain ranking accuracy of Neu-IR models. Experiments on three TREC benchmarks in the web, news, and biomedical domains show that MetaAdaptRank significantly improves the few-shot ranking accuracy of Neu-IR models. Further analyses indicate that MetaAdaptRank thrives from both its contrastive weak data synthesis and meta-reweighted data selection. The code and data of this paper can be obtained from \url{https://github.com/thunlp/MetaAdaptRank}."",
}
@",medical domain,natural language process,natural languag,language process,nlp,information retriev,benchmark
" ""Cross-modal Memory Networks for Radiology Report Generation"","," ""Medical imaging plays a significant role in clinical practice of medical diagnosis, where the text reports of the images are essential in understanding them and facilitating later treatments. By generating the reports automatically, it is beneficial to help lighten the burden of radiologists and significantly promote clinical automation, which already attracts much attention in applying artificial intelligence to medical domain. Previous studies mainly follow the encoder-decoder paradigm and focus on the aspect of text generation, with few studies considering the importance of cross-modal mappings and explicitly exploit such mappings to facilitate radiology report generation. In this paper, we propose a cross-modal memory networks (CMN) to enhance the encoder-decoder framework for radiology report generation, where a shared memory is designed to record the alignment between images and texts so as to facilitate the interaction and generation across modalities. Experimental results illustrate the effectiveness of our proposed model, where state-of-the-art performance is achieved on two widely used benchmark datasets, i.e., IU X-Ray and MIMIC-CXR. Further analyses also prove that our model is able to better align information from radiology images and texts so as to help generating more accurate reports in terms of clinical indicators."",","{chen-etal-2021-cross-modal,
    title = ""Cross-modal Memory Networks for Radiology Report Generation"",
    author = ""Chen, Zhihong  and
      Shen, Yaling  and
      Song, Yan  and
      Wan, Xiang"",
    editor = ""Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto"",
    booktitle = ""Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"",
    month = aug,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.acl-long.459"",
    doi = ""10.18653/v1/2021.acl-long.459"",
    pages = ""5904--5914"",
    abstract = ""Medical imaging plays a significant role in clinical practice of medical diagnosis, where the text reports of the images are essential in understanding them and facilitating later treatments. By generating the reports automatically, it is beneficial to help lighten the burden of radiologists and significantly promote clinical automation, which already attracts much attention in applying artificial intelligence to medical domain. Previous studies mainly follow the encoder-decoder paradigm and focus on the aspect of text generation, with few studies considering the importance of cross-modal mappings and explicitly exploit such mappings to facilitate radiology report generation. In this paper, we propose a cross-modal memory networks (CMN) to enhance the encoder-decoder framework for radiology report generation, where a shared memory is designed to record the alignment between images and texts so as to facilitate the interaction and generation across modalities. Experimental results illustrate the effectiveness of our proposed model, where state-of-the-art performance is achieved on two widely used benchmark datasets, i.e., IU X-Ray and MIMIC-CXR. Further analyses also prove that our model is able to better align information from radiology images and texts so as to help generating more accurate reports in terms of clinical indicators."",
}
@",medical domain,natural language process,natural languag,language process,generat,benchmark
" ""Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference"","," ""Compared to the general news domain, information extraction (IE) from biomedical text requires much broader domain knowledge. However, many previous IE methods do not utilize any external knowledge during inference. Due to the exponential growth of biomedical publications, models that do not go beyond their fixed set of parameters will likely fall behind. Inspired by how humans look up relevant information to comprehend a scientific text, we present a novel framework that utilizes external knowledge for joint entity and relation extraction named KECI (Knowledge-Enhanced Collective Inference). Given an input text, KECI first constructs an initial span graph representing its initial understanding of the text. It then uses an entity linker to form a knowledge graph containing relevant background knowledge for the the entity mentions in the text. To make the final predictions, KECI fuses the initial span graph and the knowledge graph into a more refined graph using an attention mechanism. KECI takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional networks. Our experimental results show that the framework is highly effective, achieving new state-of-the-art results in two different benchmark datasets: BioRelEx (binding interaction detection) and ADE (adverse drug event extraction). For example, KECI achieves absolute improvements of 4.59{\%} and 4.91{\%} in F1 scores over the state-of-the-art on the BioRelEx entity and relation extraction tasks"",","{lai-etal-2021-joint,
    title = ""Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference"",
    author = ""Lai, Tuan  and
      Ji, Heng  and
      Zhai, ChengXiang  and
      Tran, Quan Hung"",
    editor = ""Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto"",
    booktitle = ""Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"",
    month = aug,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.acl-long.488"",
    doi = ""10.18653/v1/2021.acl-long.488"",
    pages = ""6248--6260"",
    abstract = ""Compared to the general news domain, information extraction (IE) from biomedical text requires much broader domain knowledge. However, many previous IE methods do not utilize any external knowledge during inference. Due to the exponential growth of biomedical publications, models that do not go beyond their fixed set of parameters will likely fall behind. Inspired by how humans look up relevant information to comprehend a scientific text, we present a novel framework that utilizes external knowledge for joint entity and relation extraction named KECI (Knowledge-Enhanced Collective Inference). Given an input text, KECI first constructs an initial span graph representing its initial understanding of the text. It then uses an entity linker to form a knowledge graph containing relevant background knowledge for the the entity mentions in the text. To make the final predictions, KECI fuses the initial span graph and the knowledge graph into a more refined graph using an attention mechanism. KECI takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional networks. Our experimental results show that the framework is highly effective, achieving new state-of-the-art results in two different benchmark datasets: BioRelEx (binding interaction detection) and ADE (adverse drug event extraction). For example, KECI achieves absolute improvements of 4.59{\%} and 4.91{\%} in F1 scores over the state-of-the-art on the BioRelEx entity and relation extraction tasks"",
}
@",medical text,natural language process,natural languag,language process,infer,relation extract,benchmark
" ""Elhuyar submission to the Biomedical Translation Task 2020 on terminology and abstracts translation"","," ""This article describes the systems submitted by Elhuyar to the 2020 Biomedical Translation Shared Task, specifically the systems presented in the subtasks of terminology translation for English-Basque and abstract translation for English-Basque and English-Spanish. In all cases a Transformer architecture was chosen and we studied different strategies to combine open domain data with biomedical domain data for building the training corpora. For the English-Basque pair, given the scarcity of parallel corpora in the biomedical domain, we set out to create domain training data in a synthetic way. The systems presented in the terminology and abstract translation subtasks for the English-Basque language pair ranked first in their respective tasks among four participants, achieving 0.78 accuracy for terminology translation and a BLEU of 0.1279 for the translation of abstracts. In the abstract translation task for the English-Spanish pair our team ranked second (BLEU","{corral-saralegi-2020-elhuyar,
    title = ""Elhuyar submission to the Biomedical Translation Task 2020 on terminology and abstracts translation"",
    author = ""Corral, Ander  and
      Saralegi, Xabier"",
    editor = {Barrault, Lo{\""\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = ""Proceedings of the Fifth Conference on Machine Translation"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.wmt-1.87"",
    pages = ""813--819"",
    abstract = ""This article describes the systems submitted by Elhuyar to the 2020 Biomedical Translation Shared Task, specifically the systems presented in the subtasks of terminology translation for English-Basque and abstract translation for English-Basque and English-Spanish. In all cases a Transformer architecture was chosen and we studied different strategies to combine open domain data with biomedical domain data for building the training corpora. For the English-Basque pair, given the scarcity of parallel corpora in the biomedical domain, we set out to create domain training data in a synthetic way. The systems presented in the terminology and abstract translation subtasks for the English-Basque language pair ranked first in their respective tasks among four participants, achieving 0.78 accuracy for terminology translation and a BLEU of 0.1279 for the translation of abstracts. In the abstract translation task for the English-Spanish pair our team ranked second (BLEU=0.4498) in the case of OK sentences."",
}
@",medical domain,translat,shared task
" ""Ixamed{'}s submission description for {WMT}20 Biomedical shared task: benefits and limitations of using terminologies for domain adaptation"","," ""In this paper we describe the systems developed at Ixa for our participation in WMT20 Biomedical shared task in three language pairs, en-eu, en-es and es-en. When defining our approach, we have put the focus on making an efficient use of corpora recently compiled for training Machine Translation (MT) systems to translate Covid-19 related text, as well as reusing previously compiled corpora and developed systems for biomedical or clinical domain. Regarding the techniques used, we base on the findings from our previous works for translating clinical texts into Basque, making use of clinical terminology for adapting the MT systems to the clinical domain. However, after manually inspecting some of the outputs generated by our systems, for most of the submissions we end up using the system trained only with the basic corpus, since the systems including the clinical terminologies generated outputs shorter in length than the corresponding references. Thus, we present simple baselines for translating abstracts between English and Spanish (en/es); while for translating abstracts and terms from English into Basque (en-eu), we concatenate the best en-es system for each kind of text with our es-eu system. We present automatic evaluation results in terms of BLEU scores, and analyse the effect of including clinical terminology on the average sentence length of the generated outputs. Following the recent recommendations for a responsible use of GPUs for NLP research, we include an estimation of the generated CO2 emissions, based on the power consumed for training the MT systems."",","{soto-etal-2020-ixameds,
    title = ""Ixamed{'}s submission description for {WMT}20 Biomedical shared task: benefits and limitations of using terminologies for domain adaptation"",
    author = ""Soto, Xabier  and
      Perez-de-Vi{\~n}aspre, Olatz  and
      Labaka, Gorka  and
      Oronoz, Maite"",
    editor = {Barrault, Lo{\""\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = ""Proceedings of the Fifth Conference on Machine Translation"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.wmt-1.96"",
    pages = ""875--880"",
    abstract = ""In this paper we describe the systems developed at Ixa for our participation in WMT20 Biomedical shared task in three language pairs, en-eu, en-es and es-en. When defining our approach, we have put the focus on making an efficient use of corpora recently compiled for training Machine Translation (MT) systems to translate Covid-19 related text, as well as reusing previously compiled corpora and developed systems for biomedical or clinical domain. Regarding the techniques used, we base on the findings from our previous works for translating clinical texts into Basque, making use of clinical terminology for adapting the MT systems to the clinical domain. However, after manually inspecting some of the outputs generated by our systems, for most of the submissions we end up using the system trained only with the basic corpus, since the systems including the clinical terminologies generated outputs shorter in length than the corresponding references. Thus, we present simple baselines for translating abstracts between English and Spanish (en/es); while for translating abstracts and terms from English into Basque (en-eu), we concatenate the best en-es system for each kind of text with our es-eu system. We present automatic evaluation results in terms of BLEU scores, and analyse the effect of including clinical terminology on the average sentence length of the generated outputs. Following the recent recommendations for a responsible use of GPUs for NLP research, we include an estimation of the generated CO2 emissions, based on the power consumed for training the MT systems."",
}
@",clinical domain,clinical text,translat,nlp,generat,shared task,evalu
" ""Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"","," ""Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research."",","{chia-etal-2020-red,
    title = ""Red Dragon {AI} at {T}ext{G}raphs 2020 Shared Task : {LIT} : {LSTM}-Interleaved Transformer for Multi-Hop Explanation Ranking"",
    author = ""Chia, Yew Ken  and
      Witteveen, Sam  and
      Andrews, Martin"",
    editor = ""Ustalov, Dmitry  and
      Somasundaran, Swapna  and
      Panchenko, Alexander  and
      Malliaros, Fragkiskos D.  and
      Hulpu{\textcommabelow{s}}, Ioana  and
      Jansen, Peter  and
      Jana, Abhik"",
    booktitle = ""Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs)"",
    month = dec,
    year = ""2020"",
    address = ""Barcelona, Spain (Online)"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.textgraphs-1.14"",
    doi = ""10.18653/v1/2020.textgraphs-1.14"",
    pages = ""115--120"",
    abstract = ""Explainable question answering for science questions is a challenging task that requires multi-hop inference over a large set of fact sentences. To counter the limitations of methods that view each query-document pair in isolation, we propose the LSTM-Interleaved Transformer which incorporates cross-document interactions for improved multi-hop ranking. The LIT architecture can leverage prior ranking positions in the re-ranking setting. Our model is competitive on the current leaderboard for the TextGraphs 2020 shared task, achieving a test-set MAP of 0.5607, and would have gained third place had we submitted before the competition deadline. Our code implementation is made available at [\url{https://github.com/mdda/worldtree_corpus/tree/textgraphs_2020}](\url{https://github.com/mdda/worldtree_corpus/tree/textgraphs_2020})."",
}
@proceedings{tal-2020-traitement-automatique-des,
    title = ""Traitement Automatique des Langues, Volume 61, Num{\'e}ro 3 : Dialogue et syst{\`e}mes de dialogue [Dialogue and dialogue systems]"",
    editor = ""Degand, Liesbeth  and
      Muller, Philippe"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""61"",
    number = ""3"",
    year = ""2020"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2020.tal-3.0"",
}
@article{degand-muller-2020-introduction,
    title = ""Introduction to the Special Issue on Dialogue and Dialogue Systems"",
    author = ""Degand, Liesbeth  and
      Muller, Philippe"",
    editor = ""Degand, Liesbeth  and
      Muller, Philippe"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""61"",
    number = ""3"",
    year = ""2020"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2020.tal-3.1"",
    pages = ""7--15"",
}
@article{pustejovsky-krishnaswamy-2020-situated,
    title = ""Situated Meaning in Multimodal Dialogue: Human-Robot and Human-Computer Interactions"",
    author = ""Pustejovsky, James  and
      Krishnaswamy, Nikhil"",
    editor = ""Degand, Liesbeth  and
      Muller, Philippe"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""61"",
    number = ""3"",
    year = ""2020"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2020.tal-3.2"",
    pages = ""17--41"",
}
@article{maraev-etal-2020-dialogue,
    title = ""Dialogue management with linear logic: the role of metavariables in questions and clarifications"",
    author = ""Maraev, Vladislav  and
      Bernardy, Jean-Philippe  and
      Ginzburg, Jonathan"",
    editor = ""Degand, Liesbeth  and
      Muller, Philippe"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""61"",
    number = ""3"",
    year = ""2020"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2020.tal-3.3"",
    pages = ""43--67"",
}
@article{hallart-etal-2020-comparaison,
    title = ""Comparaison linguistique et neuro-physiologique de conversations humain humain et humain robot [Linguistic and neuro-physiological comparison of human-human and human-robot conversations]"",
    author = ""Hallart, Charlie  and
      Maes, Juliette  and
      Spatola, Nicolas  and
      Pr{\'e}vot, Laurent  and
      Chaminade, Thierry"",
    editor = ""Degand, Liesbeth  and
      Muller, Philippe"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""61"",
    number = ""3"",
    year = ""2020"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2020.tal-3.4"",
    pages = ""69--93"",
    language = ""French"",
}
@proceedings{tal-2020-traitement-automatique,
    title = ""Traitement Automatique des Langues, Volume 61, Num{\'e}ro 2 : TAL et Sant{\'e} [NLP and Health]"",
    editor = ""N{\'e}v{\'e}ol, Aur{\'e}lie  and
      de Bruijn, Berry  and
      Fredouille, Corinne"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""61"",
    number = ""2"",
    year = ""2020"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2020.tal-2.0"",
}
@article{neveol-etal-2020-tal,
    title = ""{TAL} et Sant{\'e} [{NLP} and Health]"",
    author = ""N{\'e}v{\'e}ol, Aur{\'e}lie  and
      de Bruijn, Berry  and
      Fredouille, Corinne"",
    editor = ""N{\'e}v{\'e}ol, Aur{\'e}lie  and
      de Bruijn, Berry  and
      Fredouille, Corinne"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""61"",
    number = ""2"",
    year = ""2020"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2020.tal-2.1"",
    pages = ""7--14"",
    language = ""French"",
}
@article{cardon-grabar-2020-construction,
    title = ""Construction d{'}un corpus parall{\`e}le {\`a} partir de corpus comparables pour la simplification de textes m{\'e}dicaux en fran{\c{c}}ais [Building a parallel corpus from comparable corpora for {F}rench biomedical text simplification]"",
    author = ""Cardon, R{\'e}mi  and
      Grabar, Natalia"",
    editor = ""N{\'e}v{\'e}ol, Aur{\'e}lie  and
      de Bruijn, Berry  and
      Fredouille, Corinne"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""61"",
    number = ""2"",
    year = ""2020"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2020.tal-2.2"",
    pages = ""15--39"",
    language = ""French"",
}
@article{wang-etal-2020-multi-pass,
    title = ""A Multi-pass Sieve for Clinical Concept Normalization"",
    author = ""Wang, Yuxia  and
      Hur, Brian  and
      Verspoor, Karin  and
      Baldwin, Timothy"",
    editor = ""N{\'e}v{\'e}ol, Aur{\'e}lie  and
      de Bruijn, Berry  and
      Fredouille, Corinne"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""61"",
    number = ""2"",
    year = ""2020"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2020.tal-2.3"",
    pages = ""41--65"",
}
@article{martin-etal-2020-detection-de-la,
    title = ""D{\'e}tection de la somnolence dans la voix : nouveaux marqueurs et nouvelles strat{\'e}gies [Sleepiness detection from voice : new features and new strategies]"",
    author = ""Martin, Vincent P.  and
      Rouas, Jean-Luc  and
      Philip, Pierre"",
    editor = ""N{\'e}v{\'e}ol, Aur{\'e}lie  and
      de Bruijn, Berry  and
      Fredouille, Corinne"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""61"",
    number = ""2"",
    year = ""2020"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2020.tal-2.4"",
    pages = ""67--90"",
    language = ""French"",
}
@proceedings{tal-2020-traitement,
    title = ""Traitement Automatique des Langues, Volume 61, Num{\'e}ro 1 : Varia [Varia]"",
    editor = ""Fabre, C{\'e}cile  and
      Morin, Emmanuel  and
      Rosset, Sophie  and
      S{\'e}billot, Pascale"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""61"",
    number = ""1"",
    year = ""2020"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2020.tal-1.0"",
}
@article{vasilescu-etal-2020-alternances,
    title = ""Alternances de voisement et processus de l{\'e}nition et de fortition : une {\'e}tude automatis{\'e}e de grands corpus en cinq langues romanes [Voicing alternations in relation with lenition and fortition phenomena: an automated study of large corpora in five {R}omance languages]"",
    author = ""Vasilescu, Ioana  and
      Wu, Yaru  and
      Jatteau, Ad{\`e}le  and
      Adda-Decker, Martine  and
      Lamel, Lori"",
    editor = ""Fabre, C{\'e}cile  and
      Morin, Emmanuel  and
      Rosset, Sophie  and
      S{\'e}billot, Pascale"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""61"",
    number = ""1"",
    year = ""2020"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2020.tal-1.1"",
    pages = ""13--37"",
    language = ""French"",
}
@proceedings{tacl-2020-transactions,
    title = ""Transactions of the Association for Computational Linguistics, Volume 8"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.0"",
}
@article{pimentel-etal-2020-phonotactic,
    title = ""Phonotactic Complexity and Its Trade-offs"",
    author = ""Pimentel, Tiago  and
      Roark, Brian  and
      Cotterell, Ryan"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.1"",
    doi = ""10.1162/tacl_a_00296"",
    pages = ""1--18"",
    abstract = ""We present methods for calculating a measure of phonotactic complexity{---}bits per phoneme{---} that permits a straightforward cross-linguistic comparison. When given a word, represented as a sequence of phonemic segments such as symbols in the international phonetic alphabet, and a statistical model trained on a sample of word types from the language, we can approximately measure bits per phoneme using the negative log-probability of that word under the model. This simple measure allows us to compare the entropy across languages, giving insight into how complex a language{'}s phonotactics is. Using a collection of 1016 basic concept words across 106 languages, we demonstrate a very strong negative correlation of − 0.74 between bits per phoneme and the average length of words."",
}
@article{wang-etal-2020-amr,
    title = ""{AMR}-To-Text Generation with Graph Transformer"",
    author = ""Wang, Tianming  and
      Wan, Xiaojun  and
      Jin, Hanqi"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.2"",
    doi = ""10.1162/tacl_a_00297"",
    pages = ""19--33"",
    abstract = ""Abstract meaning representation (AMR)-to-text generation is the challenging task of generating natural language texts from AMR graphs, where nodes represent concepts and edges denote relations. The current state-of-the-art methods use graph-to-sequence models; however, they still cannot significantly outperform the previous sequence-to-sequence models or statistical approaches. In this paper, we propose a novel graph-to-sequence model (Graph Transformer) to address this task. The model directly encodes the AMR graphs and learns the node representations. A pairwise interaction function is used for computing the semantic relations between the concepts. Moreover, attention mechanisms are used for aggregating the information from the incoming and outgoing neighbors, which help the model to capture the semantic information effectively. Our model outperforms the state-of-the-art neural approach by 1.5 BLEU points on LDC2015E86 and 4.8 BLEU points on LDC2017T10 and achieves new state-of-the-art performances."",
}
@article{ettinger-2020-bert,
    title = ""What {BERT} Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models"",
    author = ""Ettinger, Allyson"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.3"",
    doi = ""10.1162/tacl_a_00298"",
    pages = ""34--48"",
    abstract = ""Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction{---} and, in particular, it shows clear insensitivity to the contextual impacts of negation."",
}
@article{hisamoto-etal-2020-membership,
    title = ""Membership Inference Attacks on Sequence-to-Sequence Models: {I}s My Data In Your Machine Translation System?"",
    author = ""Hisamoto, Sorami  and
      Post, Matt  and
      Duh, Kevin"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.4"",
    doi = ""10.1162/tacl_a_00299"",
    pages = ""49--63"",
    abstract = ""Data privacy is an important issue for {``}machine learning as a service{''} providers. We focus on the problem of membership inference attacks: Given a data sample and black-box access to a model{'}s API, determine whether the sample existed in the model{'}s training data. Our contribution is an investigation of this problem in the context of sequence-to-sequence models, which are important in applications such as machine translation and video captioning. We define the membership inference problem for sequence generation, provide an open dataset based on state-of-the-art machine translation models, and report initial results on whether these models leak private information against several kinds of membership inference attacks."",
}
@article{joshi-etal-2020-spanbert,
    title = ""{S}pan{BERT}: Improving Pre-training by Representing and Predicting Spans"",
    author = ""Joshi, Mandar  and
      Chen, Danqi  and
      Liu, Yinhan  and
      Weld, Daniel S.  and
      Zettlemoyer, Luke  and
      Levy, Omer"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.5"",
    doi = ""10.1162/tacl_a_00300"",
    pages = ""64--77"",
    abstract = ""We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6{\%} and 88.7{\%} F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6{\%} F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1"",
}
@article{yan-etal-2020-graph,
    title = ""A Graph-based Model for Joint {C}hinese Word Segmentation and Dependency Parsing"",
    author = ""Yan, Hang  and
      Qiu, Xipeng  and
      Huang, Xuanjing"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.6"",
    doi = ""10.1162/tacl_a_00301"",
    pages = ""78--92"",
    abstract = ""Chinese word segmentation and dependency parsing are two fundamental tasks for Chinese natural language processing. The dependency parsing is defined at the word-level. Therefore word segmentation is the precondition of dependency parsing, which makes dependency parsing suffer from error propagation and unable to directly make use of character-level pre-trained language models (such as BERT). In this paper, we propose a graph-based model to integrate Chinese word segmentation and dependency parsing. Different from previous transition-based joint models, our proposed model is more concise, which results in fewer efforts of feature engineering. Our graph-based joint model achieves better performance than previous joint models and state-of-the-art results in both Chinese word segmentation and dependency parsing. Additionally, when BERT is combined, our model can substantially reduce the performance gap of dependency parsing between joint models and gold-segmented word-based models. Our code is publicly available at \url{https://github.com/fastnlp/JointCwsParser}"",
}
@article{guan-etal-2020-knowledge,
    title = ""A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation"",
    author = ""Guan, Jian  and
      Huang, Fei  and
      Zhao, Zhihao  and
      Zhu, Xiaoyan  and
      Huang, Minlie"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.7"",
    doi = ""10.1162/tacl_a_00302"",
    pages = ""93--108"",
    abstract = ""Story generation, namely, generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories. We conjecture that this is because of the difficulty of associating relevant commonsense knowledge, understanding the causal relationships, and planning entities and events with proper temporal order. In this paper, we devise a knowledge-enhanced pretraining model for commonsense story generation. We propose to utilize commonsense knowledge from external knowledge bases to generate reasonable stories. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we use multi-task learning, which combines a discriminative objective to distinguish true and fake stories during fine-tuning. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence."",
}
@article{zhou-etal-2020-improving-candidate,
    title = ""Improving Candidate Generation for Low-resource Cross-lingual Entity Linking"",
    author = ""Zhou, Shuyan  and
      Rijhwani, Shruti  and
      Wieting, John  and
      Carbonell, Jaime  and
      Neubig, Graham"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.8"",
    doi = ""10.1162/tacl_a_00303"",
    pages = ""109--124"",
    abstract = ""Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts. The first step of (X)EL is candidate generation, which retrieves a list of plausible candidate entities from the target-language KB for each mention. Approaches based on resources from Wikipedia have proven successful in the realm of relatively high-resource languages, but these do not extend well to low-resource languages with few, if any, Wikipedia pages. Recently, transfer learning methods have been shown to reduce the demand for resources in the low-resource languages by utilizing resources in closely related languages, but the performance still lags far behind their high-resource counterparts. In this paper, we first assess the problems faced by current entity candidate generation methods for low-resource XEL, then propose three improvements that (1) reduce the disconnect between entity mentions and KB entries, and (2) improve the robustness of the model to low-resource scenarios. The methods are simple, but effective: We experiment with our approach on seven XEL datasets and find that they yield an average gain of 16.9{\%} in Top-30 gold candidate recall, compared with state-of-the-art baselines. Our improved model also yields an average gain of 7.9{\%} in in-KB accuracy of end-to-end XEL.1"",
}
@article{mccoy-etal-2020-syntax,
    title = ""Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks"",
    author = ""McCoy, R. Thomas  and
      Frank, Robert  and
      Linzen, Tal"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.9"",
    doi = ""10.1162/tacl_a_00304"",
    pages = ""125--140"",
    abstract = ""Learners that are exposed to the same training data might generalize differently due to differing inductive biases. In neural network models, inductive biases could in theory arise from any aspect of the model architecture. We investigate which architectural factors affect the generalization behavior of neural sequence-to-sequence models trained on two syntactic tasks, English question formation and English tense reinflection. For both tasks, the training set is consistent with a generalization based on hierarchical structure and a generalization based on linear order. All architectural factors that we investigated qualitatively affected how models generalized, including factors with no clear connection to hierarchical structure. For example, LSTMs and GRUs displayed qualitatively different inductive biases. However, the only factor that consistently contributed a hierarchical bias across tasks was the use of a tree-structured model rather than a model with sequential recurrence, suggesting that human-like syntactic generalization requires architectural syntactic structure."",
}
@article{sun-etal-2020-investigating,
    title = ""Investigating Prior Knowledge for Challenging {C}hinese Machine Reading Comprehension"",
    author = ""Sun, Kai  and
      Yu, Dian  and
      Yu, Dong  and
      Cardie, Claire"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.10"",
    doi = ""10.1162/tacl_a_00305"",
    pages = ""141--155"",
    abstract = ""Machine reading comprehension tasks require a machine reader to answer questions relevant to the given document. In this paper, we present the first free-form multiple-Choice Chinese machine reading Comprehension dataset (C3), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-form questions collected from Chinese-as-a-second-language examinations. We present a comprehensive analysis of the prior knowledge (i.e., linguistic, domain-specific, and general world knowledge) needed for these real-world problems. We implement rule-based and popular neural methods and find that there is still a significant performance gap between the best performing model (68.5{\%}) and human readers (96.0{\%}), especiallyon problems that require prior knowledge. We further study the effects of distractor plausibility and data augmentation based on translated relevant datasets for English on model performance. We expect C3 to present great challenges to existing systems as answering 86.8{\%} of questions requires both knowledge within and beyond the accompanying document, and we hope that C3 can serve as a platform to study how to leverage various kinds of prior knowledge to better understand a given written or orally oriented text. C3 is available at \url{https://dataset.org/c3/}."",
}
@article{hahn-2020-theoretical,
    title = ""Theoretical Limitations of Self-Attention in Neural Sequence Models"",
    author = ""Hahn, Michael"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.11"",
    doi = ""10.1162/tacl_a_00306"",
    pages = ""156--171"",
    abstract = ""Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics."",
}
@article{zhang-etal-2020-target,
    title = ""Target-Guided Structured Attention Network for Target-Dependent Sentiment Analysis"",
    author = ""Zhang, Ji  and
      Chen, Chengyao  and
      Liu, Pengfei  and
      He, Chao  and
      Leung, Cane Wing-Ki"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.12"",
    doi = ""10.1162/tacl_a_00308"",
    pages = ""172--182"",
    abstract = ""Target-dependent sentiment analysis (TDSA) aims to classify the sentiment of a text towards a given target. The major challenge of this task lies in modeling the semantic relatedness between a target and its context sentence. This paper proposes a novel Target-Guided Structured Attention Network (TG-SAN), which captures target-related contexts for TDSA in a fine-to-coarse manner. Given a target and its context sentence, the proposed TG-SAN first identifies multiple semantic segments from the sentence using a target-guided structured attention mechanism. It then fuses the extracted segments based on their relatedness with the target for sentiment classification. We present comprehensive comparative experiments on three benchmarks with three major findings. First, TG-SAN outperforms the state-of-the-art by up to 1.61{\%} and 3.58{\%} in terms of accuracy and Marco-F1, respectively. Second, it shows a strong advantage in determining the sentiment of a target when the context sentence contains multiple semantic segments. Lastly, visualization results show that the attention scores produced by TG-SAN are highly interpretable"",
}
@article{wolfson-etal-2020-break,
    title = ""Break It Down: A Question Understanding Benchmark"",
    author = ""Wolfson, Tomer  and
      Geva, Mor  and
      Gupta, Ankit  and
      Gardner, Matt  and
      Goldberg, Yoav  and
      Deutch, Daniel  and
      Berant, Jonathan"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.13"",
    doi = ""10.1162/tacl_a_00309"",
    pages = ""183--198"",
    abstract = ""Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes the ordered list of steps, expressed through natural language, that are necessary for answering a question. We develop a crowdsourcing pipeline, showing that quality QDMRs can be annotated at scale, and release the Break dataset, containing over 83K pairs of questions and their QDMRs. We demonstrate the utility of QDMR by showing that (a) it can be used to improve open-domain question answering on the HotpotQA dataset, (b) it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use Break to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines."",
}
@article{chen-etal-2020-acoustic,
    title = ""Acoustic-Prosodic and Lexical Cues to Deception and Trust: Deciphering How People Detect Lies"",
    author = ""Chen, Xi (Leslie)  and
      Levitan, Sarah Ita  and
      Levine, Michelle  and
      Mandic, Marko  and
      Hirschberg, Julia"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.14"",
    doi = ""10.1162/tacl_a_00311"",
    pages = ""199--214"",
    abstract = ""Humans rarely perform better than chance at lie detection. To better understand human perception of deception, we created a game framework, LieCatcher, to collect ratings of perceived deception using a large corpus of deceptive and truthful interviews. We analyzed the acoustic-prosodic and linguistic characteristics of language trusted and mistrusted by raters and compared these to characteristics of actual truthful and deceptive language to understand how perception aligns with reality. With this data we built classifiers to automatically distinguish trusted from mistrusted speech, achieving an F1 of 66.1{\%}. We next evaluated whether the strategies raters said they used to discriminate between truthful and deceptive responses were in fact useful. Our results show that, although several prosodic and lexical features were consistently perceived as trustworthy, they were not reliable cues. Also, the strategies that judges reported using in deception detection were not helpful for the task. Our work sheds light on the nature of trusted language and provides insight into the challenging problem of human deception detection."",
}
@article{nishida-nakayama-2020-unsupervised,
    title = ""Unsupervised Discourse Constituency Parsing Using {V}iterbi {EM}"",
    author = ""Nishida, Noriki  and
      Nakayama, Hideki"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.15"",
    doi = ""10.1162/tacl_a_00312"",
    pages = ""215--230"",
    abstract = ""In this paper, we introduce an unsupervised discourse constituency parsing algorithm. We use Viterbi EM with a margin-based criterion to train a span-based discourse parser in an unsupervised manner. We also propose initialization methods for Viterbi training of discourse constituents based on our prior knowledge of text structures. Experimental results demonstrate that our unsupervised parser achieves comparable or even superior performance to fully supervised parsers. We also investigate discourse constituents that are learned by our method."",
}
@article{djokic-etal-2020-decoding,
    title = ""Decoding Brain Activity Associated with Literal and Metaphoric Sentence Comprehension Using Distributional Semantic Models"",
    author = ""Djokic, Vesna G.  and
      Maillard, Jean  and
      Bulat, Luana  and
      Shutova, Ekaterina"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.16"",
    doi = ""10.1162/tacl_a_00307"",
    pages = ""231--246"",
    abstract = ""Recent years have seen a growing interest within the natural language processing (NLP) community in evaluating the ability of semantic models to capture human meaning representation in the brain. Existing research has mainly focused on applying semantic models to decode brain activity patterns associated with the meaning of individual words, and, more recently, this approach has been extended to sentences and larger text fragments. Our work is the first to investigate metaphor processing in the brain in this context. We evaluate a range of semantic models (word embeddings, compositional, and visual models) in their ability to decode brain activity associated with reading of both literal and metaphoric sentences. Our results suggest that compositional models and word embeddings are able to capture differences in the processing of literal and metaphoric sentences, providing support for the idea that the literal meaning is not fully accessible during familiar metaphor comprehension."",
}
@article{settles-etal-2020-machine,
    title = ""Machine Learning{--}Driven Language Assessment"",
    author = ""Settles, Burr  and
      LaFlair, Geoffrey T.  and
      Hagiwara, Masato"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.17"",
    doi = ""10.1162/tacl_a_00310"",
    pages = ""247--263"",
    abstract = ""We describe a method for rapidly creating language proficiency assessments, and provide experimental evidence that such tests can be valid, reliable, and secure. Our approach is the first to use machine learning and natural language processing to induce proficiency scales based on a given standard, and then use linguistic models to estimate item difficulty directly for computer-adaptive testing. This alleviates the need for expensive pilot testing with human subjects. We used these methods to develop an online proficiency exam called the Duolingo English Test, and demonstrate that its scores align significantly with other high-stakes English assessments. Furthermore, our approach produces test scores that are highly reliable, while generating item banks large enough to satisfy security requirements."",
}
@article{rothe-etal-2020-leveraging,
    title = ""Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"",
    author = ""Rothe, Sascha  and
      Narayan, Shashi  and
      Severyn, Aliaksei"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.18"",
    doi = ""10.1162/tacl_a_00313"",
    pages = ""264--280"",
    abstract = ""Unsupervised pre-training of large neural models has recently revolutionized Natural Language Processing. By warm-starting from the publicly released checkpoints, NLP practitioners have pushed the state-of-the-art on multiple benchmarks while saving significant amounts of compute time. So far the focus has been mainly on the Natural Language Understanding tasks. In this paper, we demonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We developed a Transformer-based sequence-to-sequence model that is compatible with publicly available pre-trained BERT, GPT-2, and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model, both encoder and decoder, with these checkpoints. Our models result in new state-of-the-art results on Machine Translation, Text Summarization, Sentence Splitting, and Sentence Fusion."",
}
@article{zhu-etal-2020-crosswoz,
    title = ""{C}ross{WOZ}: A Large-Scale {C}hinese Cross-Domain Task-Oriented Dialogue Dataset"",
    author = ""Zhu, Qi  and
      Huang, Kaili  and
      Zhang, Zheng  and
      Zhu, Xiaoyan  and
      Huang, Minlie"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.19"",
    doi = ""10.1162/tacl_a_00314"",
    pages = ""281--295"",
    abstract = ""To advance multi-domain (cross-domain) dialogue modeling as well as alleviate the shortage of Chinese task-oriented datasets, we propose CrossWOZ, the first large-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It contains 6K dialogue sessions and 102K utterances for 5 domains, including hotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains rich annotation of dialogue states and dialogue acts on both user and system sides. About 60{\%} of the dialogues have cross-domain user goals that favor inter-domain dependency and encourage natural transition across domains in conversation. We also provide a user simulator and several benchmark models for pipelined task-oriented dialogue systems, which will facilitate researchers to compare and evaluate their models on this corpus. The large size and rich annotation of CrossWOZ make it suitable to investigate a variety of tasks in cross-domain dialogue modeling, such as dialogue state tracking, policy learning, user simulation, etc."",
}
@article{lau-etal-2020-furiously,
    title = ""How Furiously Can Colorless Green Ideas Sleep? Sentence Acceptability in Context"",
    author = ""Lau, Jey Han  and
      Armendariz, Carlos  and
      Lappin, Shalom  and
      Purver, Matthew  and
      Shu, Chang"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.20"",
    doi = ""10.1162/tacl_a_00315"",
    pages = ""296--310"",
    abstract = ""We study the influence of context on sentence acceptability. First we compare the acceptability ratings of sentences judged in isolation, with a relevant context, and with an irrelevant context. Our results show that context induces a cognitive load for humans, which compresses the distribution of ratings. Moreover, in relevant contexts we observe a discourse coherence effect that uniformly raises acceptability. Next, we test unidirectional and bidirectional language models in their ability to predict acceptability ratings. The bidirectional models show very promising results, with the best model achieving a new state-of-the-art for unsupervised acceptability prediction. The two sets of experiments provide insights into the cognitive aspects of sentence processing and central issues in the computational modeling of text and discourse."",
}
@article{arora-etal-2020-learning,
    title = ""Learning Lexical Subspaces in a Distributional Vector Space"",
    author = ""Arora, Kushal  and
      Chakraborty, Aishik  and
      Cheung, Jackie C. K."",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.21"",
    doi = ""10.1162/tacl_a_00316"",
    pages = ""311--329"",
    abstract = ""In this paper, we propose LexSub, a novel approach towards unifying lexical and distributional semantics. We inject knowledge about lexical-semantic relations into distributional word embeddings by defining subspaces of the distributional vector space in which a lexical relation should hold. Our framework can handle symmetric attract and repel relations (e.g., synonymy and antonymy, respectively), as well as asymmetric relations (e.g., hypernymy and meronomy). In a suite of intrinsic benchmarks, we show that our model outperforms previous approaches on relatedness tasks and on hypernymy classification and detection, while being competitive on word similarity tasks. It also outperforms previous systems on extrinsic classification tasks that benefit from exploiting lexical relational cues. We perform a series of analyses to understand the behaviors of our model.1Code available at \url{https://github.com/aishikchakraborty/LexSub}."",
}
@article{kumar-etal-2020-syntax,
    title = ""Syntax-Guided Controlled Generation of Paraphrases"",
    author = ""Kumar, Ashutosh  and
      Ahuja, Kabir  and
      Vadapalli, Raghuram  and
      Talukdar, Partha"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.22"",
    doi = ""10.1162/tacl_a_00318"",
    pages = ""329--345"",
    abstract = ""Given a sentence (e.g., {``}I like mangoes{''}) and a constraint (e.g., sentiment flip), the goal of controlled text generation is to produce a sentence that adapts the input sentence to meet the requirements of the constraint (e.g., {``}I hate mangoes{''}). Going beyond such simple constraints, recent work has started exploring the incorporation of complex syntactic-guidance as constraints in the task of controlled paraphrase generation. In these methods, syntactic-guidance is sourced from a separate exemplar sentence. However, this prior work has only utilized limited syntactic information available in the parse tree of the exemplar sentence. We address this limitation in the paper and propose Syntax Guided Controlled Paraphraser (SGCP), an end-to-end framework for syntactic paraphrase generation. We find that Sgcp can generate syntax-conforming sentences while not compromising on relevance. We perform extensive automated and human evaluations over multiple real-world English language datasets to demonstrate the efficacy of Sgcp over state-of-the-art baselines. To drive future research, we have made Sgcp{'}s source code available.1"",
}
@article{yu-etal-2020-better,
    title = ""Better Document-Level Machine Translation with {B}ayes{'} Rule"",
    author = ""Yu, Lei  and
      Sartran, Laurent  and
      Stokowiec, Wojciech  and
      Ling, Wang  and
      Kong, Lingpeng  and
      Blunsom, Phil  and
      Dyer, Chris"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.23"",
    doi = ""10.1162/tacl_a_00319"",
    pages = ""346--360"",
    abstract = ""We show that Bayes{'} rule provides an effective mechanism for creating document translation models that can be learned from only parallel sentences and monolingual documents a compelling benefit because parallel documents are not always available. In our formulation, the posterior probability of a candidate translation is the product of the unconditional (prior) probability of the candidate output document and the {``}reverse translation probability{''} of translating the candidate output back into the source language. Our proposed model uses a powerful autoregressive language model as the prior on target language documents, but it assumes that each sentence is translated independently from the target to the source language. Crucially, at test time, when a source document is observed, the document language model prior induces dependencies between the translations of the source sentences in the posterior. The model{'}s independence assumption not only enables efficient use of available data, but it additionally admits a practical left-to-right beam-search algorithm for carrying out inference. Experiments show that our model benefits from using cross-sentence context in the language model, and it outperforms existing document translation approaches."",
}
@article{azpiazu-pera-2020-hierarchical,
    title = ""Hierarchical Mapping for Crosslingual Word Embedding Alignment"",
    author = ""Azpiazu, Ion Madrazo  and
      Pera, Maria Soledad"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.24"",
    doi = ""10.1162/tacl_a_00320"",
    pages = ""361--376"",
    abstract = ""The alignment of word embedding spaces in different languages into a common crosslingual space has recently been in vogue. Strategies that do so compute pairwise alignments and then map multiple languages to a single pivot language (most often English). These strategies, however, are biased towards the choice of the pivot language, given that language proximity and the linguistic characteristics of the target language can strongly impact the resultant crosslingual space in detriment of topologically distant languages. We present a strategy that eliminates the need for a pivot language by learning the mappings across languages in a hierarchical way. Experiments demonstrate that our strategy significantly improves vocabulary induction scores in all existing benchmarks, as well as in a new non-English{--}centered benchmark we built, which we make publicly available."",
}
@article{warstadt-etal-2020-blimp-benchmark,
    title = ""{BL}i{MP}: The Benchmark of Linguistic Minimal Pairs for {E}nglish"",
    author = ""Warstadt, Alex  and
      Parrish, Alicia  and
      Liu, Haokun  and
      Mohananey, Anhad  and
      Peng, Wei  and
      Wang, Sheng-Fu  and
      Bowman, Samuel R."",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.25"",
    doi = ""10.1162/tacl_a_00321"",
    pages = ""377--392"",
    abstract = ""We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs{---}that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4{\%}. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands."",
}
@article{zhang-duh-2020-reproducible,
    title = ""Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems"",
    author = ""Zhang, Xuan  and
      Duh, Kevin"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.26"",
    doi = ""10.1162/tacl_a_00322"",
    pages = ""393--408"",
    abstract = ""Hyperparameter selection is a crucial part of building neural machine translation (NMT) systems across both academia and industry. Fine-grained adjustments to a model{'}s architecture or training recipe can mean the difference between a positive and negative research result or between a state-of-the-art and underperforming system. While recent literature has proposed methods for automatic hyperparameter optimization (HPO), there has been limited work on applying these methods to neural machine translation (NMT), due in part to the high costs associated with experiments that train large numbers of model variants. To facilitate research in this space, we introduce a lookup-based approach that uses a library of pre-trained models for fast, low cost HPO experimentation. Our contributions include (1) the release of a large collection of trained NMT models covering a wide range of hyperparameters, (2) the proposal of targeted metrics for evaluating HPO methods on NMT, and (3) a reproducible benchmark of several HPO methods against our model library, including novel graph-based and multiobjective methods."",
}
@article{clark-fijalkow-2020-consistent,
    title = ""Consistent Unsupervised Estimators for Anchored {PCFG}s"",
    author = {Clark, Alexander  and
      Fijalkow, Nathana{\""e}l},
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.27"",
    doi = ""10.1162/tacl_a_00323"",
    pages = ""409--422"",
    abstract = ""Learning probabilistic context-free grammars (PCFGs) from strings is a classic problem in computational linguistics since Horning (1969). Here we present an algorithm based on distributional learning that is a consistent estimator for a large class of PCFGs that satisfy certain natural conditions including being anchored (Stratos et al., 2016). We proceed via a reparameterization of (top{--}down) PCFGs that we call a bottom{--}up weighted context-free grammar. We show that if the grammar is anchored and satisfies additional restrictions on its ambiguity, then the parameters can be directly related to distributional properties of the anchoring strings; we show the asymptotic correctness of a naive estimator and present some simulations using synthetic data that show that algorithms based on this approach have good finite sample behavior."",
}
@article{jiang-etal-2020-know,
    title = ""How Can We Know What Language Models Know?"",
    author = ""Jiang, Zhengbao  and
      Xu, Frank F.  and
      Araki, Jun  and
      Neubig, Graham"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.28"",
    doi = ""10.1162/tacl_a_00324"",
    pages = ""423--438"",
    abstract = ""Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as {``}Obama is a {\_}{\_} by profession{''}. These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as {``}Obama worked as a {\_}{\_} {''} may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1{\%} to 39.6{\%}, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at \url{https://github.com/jzbjyb/LPAQA}."",
}
@article{dieng-etal-2020-topic,
    title = ""Topic Modeling in Embedding Spaces"",
    author = ""Dieng, Adji B.  and
      Ruiz, Francisco J. R.  and
      Blei, David M."",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.29"",
    doi = ""10.1162/tacl_a_00325"",
    pages = ""439--453"",
    abstract = ""Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded topic model (etm), a generative model of documents that marries traditional topic models with word embeddings. More specifically, the etm models each word with a categorical distribution whose natural parameter is the inner product between the word{'}s embedding and an embedding of its assigned topic. To fit the etm, we develop an efficient amortized variational inference algorithm. The etm discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation, in terms of both topic quality and predictive performance."",
}
@article{clark-etal-2020-tydi,
    title = ""{T}y{D}i {QA}: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"",
    author = ""Clark, Jonathan H.  and
      Choi, Eunsol  and
      Collins, Michael  and
      Garrette, Dan  and
      Kwiatkowski, Tom  and
      Nikolaev, Vitaly  and
      Palomaki, Jennimaria"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.30"",
    doi = ""10.1162/tacl_a_00317"",
    pages = ""454--470"",
    abstract = ""Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA{---}a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology{---}the set of linguistic features each language expresses{---}such that we expect models performing well on this set to generalize across a large number of the world{'}s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don{'}t know the answer yet, and the data is collected directly in each language without the use of translation."",
}
@article{zhu-etal-2020-neural,
    title = ""A Neural Generative Model for Joint Learning Topics and Topic-Specific Word Embeddings"",
    author = ""Zhu, Lixing  and
      He, Yulan  and
      Zhou, Deyu"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.31"",
    doi = ""10.1162/tacl_a_00326"",
    pages = ""471--485"",
    abstract = ""We propose a novel generative model to explore both local and global context for joint learning topics and topic-specific word embeddings. In particular, we assume that global latent topics are shared across documents, a word is generated by a hidden semantic vector encoding its contextual semantic meaning, and its context words are generated conditional on both the hidden semantic vector and global latent topics. Topics are trained jointly with the word embeddings. The trained model maps words to topic-dependent embeddings, which naturally addresses the issue of word polysemy. Experimental results show that the proposed model outperforms the word-level embedding methods in both word similarity evaluation and word sense disambiguation. Furthermore, the model also extracts more coherent topics compared with existing neural topic models or other models for joint learning of topics and word embeddings. Finally, the model can be easily integrated with existing deep contextualized word embedding learning methods to further improve the performance of downstream tasks such as sentiment classification."",
}
@article{kumar-etal-2020-nurse,
    title = ""Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings"",
    author = ""Kumar, Vaibhav  and
      Bhotia, Tenzin Singhay  and
      Kumar, Vaibhav  and
      Chakraborty, Tanmoy"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.32"",
    doi = ""10.1162/tacl_a_00327"",
    pages = ""486--503"",
    abstract = ""Word embeddings are the standard model for semantic and syntactic representations of words. Unfortunately, these models have been shown to exhibit undesirable word associations resulting from gender, racial, and religious biases. Existing post-processing methods for debiasing word embeddings are unable to mitigate gender bias hidden in the spatial arrangement of word vectors. In this paper, we propose RAN-Debias, a novel gender debiasing methodology that not only eliminates the bias present in a word vector but also alters the spatial distribution of its neighboring vectors, achieving a bias-free setting while maintaining minimal semantic offset. We also propose a new bias evaluation metric, Gender-based Illicit Proximity Estimate (GIPE), which measures the extent of undue proximity in word vectors resulting from the presence of gender-based predilections. Experiments based on a suite of evaluation metrics show that RAN-Debias significantly outperforms the state-of-the-art in reducing proximity bias (GIPE) by at least 42.02{\%}. It also reduces direct bias, adding minimal semantic disturbance, and achieves the best performance in a downstream application task (coreference resolution)."",
}
@article{ben-david-etal-2020-perl,
    title = ""{PERL}: Pivot-based Domain Adaptation for Pre-trained Deep Contextualized Embedding Models"",
    author = ""Ben-David, Eyal  and
      Rabinovitz, Carmel  and
      Reichart, Roi"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.33"",
    doi = ""10.1162/tacl_a_00328"",
    pages = ""504--521"",
    abstract = ""Pivot-based neural representation models have led to significant progress in domain adaptation for NLP. However, previous research following this approach utilize only labeled data from the source domain and unlabeled data from the source and target domains, but neglect to incorporate massive unlabeled corpora that are not necessarily drawn from these domains. To alleviate this, we propose PERL: A representation learning model that extends contextualized word embedding models such as BERT (Devlin et al., 2019) with pivot-based fine-tuning. PERL outperforms strong baselines across 22 sentiment classification domain adaptation setups, improves in-domain model performance, yields effective reduced-size models, and increases model stability.1"",
}
@article{opitz-etal-2020-amr,
    title = ""{AMR} Similarity Metrics from Principles"",
    author = ""Opitz, Juri  and
      Parcalabescu, Letitia  and
      Frank, Anette"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.34"",
    doi = ""10.1162/tacl_a_00329"",
    pages = ""522--538"",
    abstract = ""Different metrics have been proposed to compare Abstract Meaning Representation (AMR) graphs. The canonical Smatch metric (Cai and Knight, 2013) aligns the variables of two graphs and assesses triple matches. The recent SemBleu metric (Song and Gildea, 2019) is based on the machine-translation metric Bleu (Papineni et al., 2002) and increases computational efficiency by ablating the variable-alignment. In this paper, i) we establish criteria that enable researchers to perform a principled assessment of metrics comparing meaning representations like AMR; ii) we undertake a thorough analysis of Smatch and SemBleu where we show that the latter exhibits some undesirable properties. For example, it does not conform to the identity of indiscernibles rule and introduces biases that are hard to control; and iii) we propose a novel metric S2 match that is more benevolent to only very slight meaning deviations and targets the fulfilment of all established criteria. We assess its suitability and show its advantages over Smatch and SemBleu."",
}
@article{fomicheva-etal-2020-unsupervised,
    title = ""Unsupervised Quality Estimation for Neural Machine Translation"",
    author = ""Fomicheva, Marina  and
      Sun, Shuo  and
      Yankovskaya, Lisa  and
      Blain, Fr{\'e}d{\'e}ric  and
      Guzm{\'a}n, Francisco  and
      Fishel, Mark  and
      Aletras, Nikolaos  and
      Chaudhary, Vishrav  and
      Specia, Lucia"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.35"",
    doi = ""10.1162/tacl_a_00330"",
    pages = ""539--555"",
    abstract = ""Quality Estimation (QE) is an important component in making Machine Translation (MT) useful in real-world applications, as it is aimed to inform the user on the quality of the MT output at test time. Existing approaches require large amounts of expert annotated data, computation, and time for training. As an alternative, we devise an unsupervised approach to QE where no training or access to additional resources besides the MT system itself is required. Different from most of the current work that treats the MT system as a black box, we explore useful information that can be extracted from the MT system as a by-product of translation. By utilizing methods for uncertainty quantification, we achieve very good correlation with human judgments of quality, rivaling state-of-the-art supervised QE models. To evaluate our approach we collect the first dataset that enables work on both black-box and glass-box approaches to QE."",
}
@article{andreas-etal-2020-task,
    title = ""Task-Oriented Dialogue as Dataflow Synthesis"",
    author = ""Andreas, Jacob  and
      Bufe, John  and
      Burkett, David  and
      Chen, Charles  and
      Clausman, Josh  and
      Crawford, Jean  and
      Crim, Kate  and
      DeLoach, Jordan  and
      Dorner, Leah  and
      Eisner, Jason  and
      Fang, Hao  and
      Guo, Alan  and
      Hall, David  and
      Hayes, Kristin  and
      Hill, Kellie  and
      Ho, Diana  and
      Iwaszuk, Wendy  and
      Jha, Smriti  and
      Klein, Dan  and
      Krishnamurthy, Jayant  and
      Lanman, Theo  and
      Liang, Percy  and
      Lin, Christopher H.  and
      Lintsbakh, Ilya  and
      McGovern, Andy  and
      Nisnevich, Aleksandr  and
      Pauls, Adam  and
      Petters, Dmitrij  and
      Read, Brent  and
      Roth, Dan  and
      Roy, Subhro  and
      Rusak, Jesse  and
      Short, Beth  and
      Slomin, Div  and
      Snyder, Ben  and
      Striplin, Stephon  and
      Su, Yu  and
      Tellman, Zachary  and
      Thomson, Sam  and
      Vorobev, Andrei  and
      Witoszko, Izabela  and
      Wolfe, Jason  and
      Wray, Abby  and
      Zhang, Yuchen  and
      Zotov, Alexander"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.36"",
    doi = ""10.1162/tacl_a_00333"",
    pages = ""556--571"",
    abstract = ""We describe an approach to task-oriented dialogue in which dialogue state is represented as a dataflow graph. A dialogue agent maps each user utterance to a program that extends this graph. Programs include metacomputation operators for reference and revision that reuse dataflow fragments from previous turns. Our graph-based state enables the expression and manipulation of complex user intents, and explicit metacomputation makes these intents easier for learned models to predict. We introduce a new dataset, SMCalFlow, featuring complex dialogues about events, weather, places, and people. Experiments show that dataflow graphs and metacomputation substantially improve representability and predictability in these natural dialogues. Additional experiments on the MultiWOZ dataset show that our dataflow representation enables an otherwise off-the-shelf sequence-to-sequence model to match the best existing task-specific state tracking model. The SMCalFlow dataset, code for replicating experiments, and a public leaderboard are available at \url{https://www.microsoft.com/en-us/research/project/dataflow-based-dialogue-semantic-machines}."",
}
@article{richardson-sabharwal-2020-qa,
    title = ""What Does My {QA} Model Know? Devising Controlled Probes Using Expert Knowledge"",
    author = ""Richardson, Kyle  and
      Sabharwal, Ashish"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.37"",
    doi = ""10.1162/tacl_a_00331"",
    pages = ""572--588"",
    abstract = ""Open-domain question answering (QA) involves many knowledge and reasoning challenges, but are successful QA models actually learning such knowledge when trained on benchmark QA tasks? We investigate this via several new diagnostic tasks probing whether multiple-choice QA models know definitions and taxonomic reasoning{---}two skills widespread in existing benchmarks and fundamental to more complex reasoning. We introduce a methodology for automatically building probe datasets from expert knowledge sources, allowing for systematic control and a comprehensive evaluation. We include ways to carefully control for artifacts that may arise during this process. Our evaluation confirms that transformer-based multiple-choice QA models are already predisposed to recognize certain types of structural linguistic knowledge. However, it also reveals a more nuanced picture: their performance notably degrades even with a slight increase in the number of {``}hops{''} in the underlying taxonomic hierarchy, and with more challenging distractor candidates. Further, existing models are far from perfect when assessed at the level of clusters of semantically connected probes, such as all hypernym questions about a single concept."",
}
@article{ribeiro-etal-2020-modeling,
    title = ""Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs"",
    author = ""Ribeiro, Leonardo F. R.  and
      Zhang, Yue  and
      Gardent, Claire  and
      Gurevych, Iryna"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.38"",
    doi = ""10.1162/tacl_a_00332"",
    pages = ""589--604"",
    abstract = ""Recent graph-to-text models generate text from graph-based data using either global or local aggregation to learn node representations. Global node encoding allows explicit communication between two distant nodes, thereby neglecting graph topology as all nodes are directly connected. In contrast, local node encoding considers the relations between neighbor nodes capturing the graph structure, but it can fail to capture long-range relations. In this work, we gather both encoding strategies, proposing novel neural models that encode an input graph combining both global and local node contexts, in order to learn better contextualized node embeddings. In our experiments, we demonstrate that our approaches lead to significant improvements on two graph-to-text datasets achieving BLEU scores of 18.01 on the AGENDA dataset, and 63.69 on the WebNLG dataset for seen categories, outperforming state-of-the-art models by 3.7 and 3.1 points, respectively.1"",
}
@article{shibuya-hovy-2020-nested,
    title = ""Nested Named Entity Recognition via Second-best Sequence Learning and Decoding"",
    author = ""Shibuya, Takashi  and
      Hovy, Eduard"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.39"",
    doi = ""10.1162/tacl_a_00334"",
    pages = ""605--620"",
    abstract = ""When an entity name contains other names within it, the identification of all combinations of names can become difficult and expensive. We propose a new method to recognize not only outermost named entities but also inner nested ones. We design an objective function for training a neural model that treats the tag sequence for nested entities as the second best path within the span of their parent entity. In addition, we provide the decoding method for inference that extracts entities iteratively from outermost ones to inner ones in an outside-to-inside way. Our method has no additional hyperparameters to the conditional random field based model widely used for flat named entity recognition tasks. Experiments demonstrate that our method performs better than or at least as well as existing methods capable of handling nested entities, achieving F1-scores of 85.82{\%}, 84.34{\%}, and 77.36{\%} on ACE-2004, ACE-2005, and GENIA datasets, respectively."",
}
@article{tu-etal-2020-empirical,
    title = ""An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models"",
    author = ""Tu, Lifu  and
      Lalwani, Garima  and
      Gella, Spandana  and
      He, He"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.40"",
    doi = ""10.1162/tacl_a_00335"",
    pages = ""621--633"",
    abstract = ""Recent work has shown that pre-trained language models such as BERT improve robustness to spurious correlations in the dataset. Intrigued by these results, we find that the key to their success is generalization from a small amount of counterexamples where the spurious correlations do not hold. When such minority examples are scarce, pre-trained models perform as poorly as models trained from scratch. In the case of extreme minority, we propose to use multi-task learning (MTL) to improve generalization. Our experiments on natural language inference and paraphrase identification show that MTL with the right auxiliary tasks significantly improves performance on challenging examples without hurting the in-distribution performance. Further, we show that the gain from MTL mainly comes from improved generalization from the minority examples. Our results highlight the importance of data diversity for overcoming spurious correlations.1"",
}
@article{lichtarge-etal-2020-data,
    title = ""Data Weighted Training Strategies for Grammatical Error Correction"",
    author = ""Lichtarge, Jared  and
      Alberti, Chris  and
      Kumar, Shankar"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.41"",
    doi = ""10.1162/tacl_a_00336"",
    pages = ""634--646"",
    abstract = ""Recent progress in the task of Grammatical Error Correction (GEC) has been driven by addressing data sparsity, both through new methods for generating large and noisy pretraining data and through the publication of small and higher-quality finetuning data in the BEA-2019 shared task. Building upon recent work in Neural Machine Translation (NMT), we make use of both kinds of data by deriving example-level scores on our large pretraining data based on a smaller, higher-quality dataset. In this work, we perform an empirical study to discover how to best incorporate delta-log-perplexity, a type of example scoring, into a training schedule for GEC. In doing so, we perform experiments that shed light on the function and applicability of delta-log-perplexity. Models trained on scored data achieve state- of-the-art results on common GEC test sets."",
}
@article{zhu-etal-2020-return,
    title = ""The Return of Lexical Dependencies: Neural Lexicalized {PCFG}s"",
    author = ""Zhu, Hao  and
      Bisk, Yonatan  and
      Neubig, Graham"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.42"",
    doi = ""10.1162/tacl_a_00337"",
    pages = ""647--661"",
    abstract = ""In this paper we demonstrate that context free grammar (CFG) based methods for grammar induction benefit from modeling lexical dependencies. This contrasts to the most popular current methods for grammar induction, which focus on discovering either constituents or dependencies. Previous approaches to marry these two disparate syntactic formalisms (e.g., lexicalized PCFGs) have been plagued by sparsity, making them unsuitable for unsupervised grammar induction. However, in this work, we present novel neural models of lexicalized PCFGs that allow us to overcome sparsity problems and effectively induce both constituents and dependencies within a single model. Experiments demonstrate that this unified framework results in stronger results on both representations than achieved when modeling either formalism alone.1"",
}
@article{bartolo-etal-2020-beat,
    title = ""Beat the {AI}: Investigating Adversarial Human Annotation for Reading Comprehension"",
    author = ""Bartolo, Max  and
      Roberts, Alastair  and
      Welbl, Johannes  and
      Riedel, Sebastian  and
      Stenetorp, Pontus"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.43"",
    doi = ""10.1162/tacl_a_00338"",
    pages = ""662--678"",
    abstract = ""Innovations in annotation methodology have been a catalyst for Reading Comprehension (RC) datasets and models. One recent trend to challenge current RC models is to involve a model in the annotation process: Humans create questions adversarially, such that the model fails to answer them correctly. In this work we investigate this annotation methodology and apply it in three different settings, collecting a total of 36,000 samples with progressively stronger models in the annotation loop. This allows us to explore questions such as the reproducibility of the adversarial effect, transfer from data collected with varying model-in-the-loop strengths, and generalization to data collected without a model. We find that training on adversarially collected samples leads to strong generalization to non-adversarially collected datasets, yet with progressive performance deterioration with increasingly stronger models-in-the-loop. Furthermore, we find that stronger models can still learn from datasets collected with substantially weaker models-in-the-loop. When trained on data collected with a BiDAF model in the loop, RoBERTa achieves 39.9F1 on questions that it cannot answer when trained on SQuAD{---}only marginally lower than when trained on data collected using RoBERTa itself (41.0F1)."",
}
@article{ye-etal-2020-sketch,
    title = ""Sketch-Driven Regular Expression Generation from Natural Language and Examples"",
    author = ""Ye, Xi  and
      Chen, Qiaochu  and
      Wang, Xinyu  and
      Dillig, Isil  and
      Durrett, Greg"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.44"",
    doi = ""10.1162/tacl_a_00339"",
    pages = ""679--694"",
    abstract = ""Recent systems for converting natural language descriptions into regular expressions (regexes) have achieved some success, but typically deal with short, formulaic text and can only produce simple regexes. Real-world regexes are complex, hard to describe with brief sentences, and sometimes require examples to fully convey the user{'}s intent. We present a framework for regex synthesis in this setting where both natural language (NL) and examples are available. First, a semantic parser (either grammar-based or neural) maps the natural language description into an intermediate sketch, which is an incomplete regex containing holes to denote missing components. Then a program synthesizer searches over the regex space defined by the sketch and finds a regex that is consistent with the given string examples. Our semantic parser can be trained purely from weak supervision based on correctness of the synthesized regex, or it can leverage heuristically derived sketches. We evaluate on two prior datasets (Kushman and Barzilay 2013; Locascio et al. 2016) and a real-world dataset from Stack Overflow. Our system achieves state-of-the-art performance on the prior datasets and solves 57{\%} of the real-world dataset, which existing neural systems completely fail on.1"",
}
@article{sperber-etal-2020-consistent,
    title = ""Consistent Transcription and Translation of Speech"",
    author = ""Sperber, Matthias  and
      Setiawan, Hendra  and
      Gollan, Christian  and
      Nallasamy, Udhyakumar  and
      Paulik, Matthias"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.45"",
    doi = ""10.1162/tacl_a_00340"",
    pages = ""695--709"",
    abstract = ""The conventional paradigm in speech translation starts with a speech recognition step to generate transcripts, followed by a translation step with the automatic transcripts as input. To address various shortcomings of this paradigm, recent work explores end-to-end trainable direct models that translate without transcribing. However, transcripts can be an indispensable output in practical applications, which often display transcripts alongside the translations to users. We make this common requirement explicit and explore the task of jointly transcribing and translating speech. Although high accuracy of transcript and translation are crucial, even highly accurate systems can suffer from inconsistencies between both outputs that degrade the user experience. We introduce a methodology to evaluate consistency and compare several modeling approaches, including the traditional cascaded approach and end-to-end models. We find that direct models are poorly suited to the joint transcription/translation task, but that end-to-end models that feature a coupled inference procedure are able to achieve strong consistency. We further introduce simple techniques for directly optimizing for consistency, and analyze the resulting trade-offs between consistency, transcription accuracy, and translation accuracy.1"",
}
@article{marie-fujita-2020-synthesizing,
    title = ""Synthesizing Parallel Data of User-Generated Texts with Zero-Shot Neural Machine Translation"",
    author = ""Marie, Benjamin  and
      Fujita, Atsushi"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.46"",
    doi = ""10.1162/tacl_a_00341"",
    pages = ""710--725"",
    abstract = ""Neural machine translation (NMT) systems are usually trained on clean parallel data. They can perform very well for translating clean in-domain texts. However, as demonstrated by previous work, the translation quality significantly worsens when translating noisy texts, such as user-generated texts (UGT) from online social media. Given the lack of parallel data of UGT that can be used to train or adapt NMT systems, we synthesize parallel data of UGT, exploiting monolingual data of UGT through crosslingual language model pre-training and zero-shot NMT systems. This paper presents two different but complementary approaches: One alters given clean parallel data into UGT-like parallel data whereas the other generates translations from monolingual data of UGT. On the MTNT translation tasks, we show that our synthesized parallel data can lead to better NMT systems for UGT while making them more robust in translating texts from various domains and styles."",
}
@article{liu-etal-2020-multilingual-denoising,
    title = ""Multilingual Denoising Pre-training for Neural Machine Translation"",
    author = ""Liu, Yinhan  and
      Gu, Jiatao  and
      Goyal, Naman  and
      Li, Xian  and
      Edunov, Sergey  and
      Ghazvininejad, Marjan  and
      Lewis, Mike  and
      Zettlemoyer, Luke"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.47"",
    doi = ""10.1162/tacl_a_00343"",
    pages = ""726--742"",
    abstract = ""This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART{---}a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.1"",
}
@article{talmor-etal-2020-olmpics,
    title = ""o{LM}pics-On What Language Model Pre-training Captures"",
    author = ""Talmor, Alon  and
      Elazar, Yanai  and
      Goldberg, Yoav  and
      Berant, Jonathan"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.48"",
    doi = ""10.1162/tacl_a_00342"",
    pages = ""743--758"",
    abstract = ""Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models, and objective functions for pre-training."",
}
@article{simpson-etal-2020-interactive,
    title = ""Interactive Text Ranking with {B}ayesian Optimization: A Case Study on Community {QA} and Summarization"",
    author = ""Simpson, Edwin  and
      Gao, Yang  and
      Gurevych, Iryna"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.49"",
    doi = ""10.1162/tacl_a_00344"",
    pages = ""759--775"",
    abstract = ""For many NLP applications, such as question answering and summarization, the goal is to select the best solution from a large space of candidates to meet a particular user{'}s needs. To address the lack of user or task-specific training data, we propose an interactive text ranking approach that actively selects pairs of candidates, from which the user selects the best. Unlike previous strategies, which attempt to learn a ranking across the whole candidate space, our method uses Bayesian optimization to focus the user{'}s labeling effort on high quality candidates and integrate prior knowledge to cope better with small data scenarios. We apply our method to community question answering (cQA) and extractive multidocument summarization, finding that it significantly outperforms existing interactive approaches. We also show that the ranking function learned by our method is an effective reward function for reinforcement learning, which improves the state of the art for interactive summarization."",
}
@article{kuncoro-etal-2020-syntactic,
    title = ""Syntactic Structure Distillation Pretraining for Bidirectional Encoders"",
    author = ""Kuncoro, Adhiguna  and
      Kong, Lingpeng  and
      Fried, Daniel  and
      Yogatama, Dani  and
      Rimell, Laura  and
      Dyer, Chris  and
      Blunsom, Phil"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.50"",
    doi = ""10.1162/tacl_a_00345"",
    pages = ""776--794"",
    abstract = ""Textual representation learners trained on large amounts of data have achieved notable success on downstream tasks; intriguingly, they have also performed well on challenging tests of syntactic competence. Hence, it remains an open question whether scalable learners like BERT can become fully proficient in the syntax of natural language by virtue of data scale alone, or whether they still benefit from more explicit syntactic biases. To answer this question, we introduce a knowledge distillation strategy for injecting syntactic biases into BERT pretraining, by distilling the syntactically informative predictions of a hierarchical{---}albeit harder to scale{---}syntactic language model. Since BERT models masked words in bidirectional context, we propose to distill the approximate marginal distribution over words in context from the syntactic LM. Our approach reduces relative error by 2{--}21{\%} on a diverse set of structured prediction tasks, although we obtain mixed results on the GLUE benchmark. Our findings demonstrate the benefits of syntactic biases, even for representation learners that exploit large amounts of data, and contribute to a better understanding of where syntactic biases are helpful in benchmarks of natural language understanding."",
}
@article{meister-etal-2020-best,
    title = ""Best-First Beam Search"",
    author = ""Meister, Clara  and
      Vieira, Tim  and
      Cotterell, Ryan"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.51"",
    doi = ""10.1162/tacl_a_00346"",
    pages = ""795--809"",
    abstract = ""Decoding for many NLP tasks requires an effective heuristic algorithm for approximating exact search because the problem of searching the full output space is often intractable, or impractical in many settings. The default algorithm for this job is beam search{---}a pruned version of breadth-first search. Quite surprisingly, beam search often returns better results than exact inference due to beneficial search bias for NLP tasks. In this work, we show that the standard implementation of beam search can be made up to 10x faster in practice. Our method assumes that the scoring function is monotonic in the sequence length, which allows us to safely prune hypotheses that cannot be in the final set of hypotheses early on. We devise effective monotonic approximations to popular nonmonontic scoring functions, including length normalization and mutual information decoding. Lastly, we propose a memory-reduced variant of best-first beam search, which has a similar beneficial search bias in terms of downstream performance, but runs in a fraction of the time."",
}
@article{sai-etal-2020-improving,
    title = ""Improving Dialog Evaluation with a Multi-reference Adversarial Dataset and Large Scale Pretraining"",
    author = ""Sai, Ananya B.  and
      Mohankumar, Akash Kumar  and
      Arora, Siddhartha  and
      Khapra, Mitesh M."",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.52"",
    doi = ""10.1162/tacl_a_00347"",
    pages = ""810--827"",
    abstract = ""There is an increasing focus on model-based dialog evaluation metrics such as ADEM, RUBER, and the more recent BERT-based metrics. These models aim to assign a high score to all relevant responses and a low score to all irrelevant responses. Ideally, such models should be trained using multiple relevant and irrelevant responses for any given context. However, no such data is publicly available, and hence existing models are usually trained using a single relevant response and multiple randomly selected responses from other contexts (random negatives). To allow for better training and robust evaluation of model-based metrics, we introduce the DailyDialog++ dataset, consisting of (i) five relevant responses for each context and (ii) five adversarially crafted irrelevant responses for each context. Using this dataset, we first show that even in the presence of multiple correct references, n-gram based metrics and embedding based metrics do not perform well at separating relevant responses from even random negatives. While model-based metrics perform better than n-gram and embedding based metrics on random negatives, their performance drops substantially when evaluated on adversarial examples. To check if large scale pretraining could help, we propose a new BERT-based evaluation metric called DEB, which is pretrained on 727M Reddit conversations and then finetuned on our dataset. DEB significantly outperforms existing models, showing better correlation with human judgments and better performance on random negatives (88.27{\%} accuracy). However, its performance again drops substantially when evaluated on adversarial responses, thereby highlighting that even large-scale pretrained evaluation models are not robust to the adversarial examples in our dataset. The dataset1 and code2 are publicly available."",
}
@article{keung-etal-2020-unsupervised,
    title = ""Unsupervised Bitext Mining and Translation via Self-Trained Contextual Embeddings"",
    author = ""Keung, Phillip  and
      Salazar, Julian  and
      Lu, Yichao  and
      Smith, Noah A."",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.53"",
    doi = ""10.1162/tacl_a_00348"",
    pages = ""828--841"",
    abstract = ""We describe an unsupervised method to create pseudo-parallel corpora for machine translation (MT) from unaligned text. We use multilingual BERT to create source and target sentence embeddings for nearest-neighbor search and adapt the model via self-training. We validate our technique by extracting parallel sentence pairs on the BUCC 2017 bitext mining task and observe up to a 24.5 point increase (absolute) in F1 scores over previous unsupervised methods. We then improve an XLM-based unsupervised neural MT system pre-trained on Wikipedia by supplementing it with pseudo-parallel text mined from the same corpus, boosting unsupervised translation performance by up to 3.5 BLEU on the WMT{'}14 French-English and WMT{'}16 German-English tasks and outperforming the previous state-of-the-art. Finally, we enrich the IWSLT{'}15 English-Vietnamese corpus with pseudo-parallel Wikipedia sentence pairs, yielding a 1.2 BLEU improvement on the low-resource MT task. We demonstrate that unsupervised bitext mining is an effective way of augmenting MT datasets and complements existing techniques like initializing with pre-trained contextual embeddings."",
}
@article{rogers-etal-2020-primer,
    title = ""A Primer in {BERT}ology: What We Know About How {BERT} Works"",
    author = ""Rogers, Anna  and
      Kovaleva, Olga  and
      Rumshisky, Anna"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.54"",
    doi = ""10.1162/tacl_a_00349"",
    pages = ""842--866"",
    abstract = ""Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research."",
}
@proceedings{sustainlp-2020-sustainlp,
    title = ""Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"",
    editor = ""Moosavi, Nafise Sadat  and
      Fan, Angela  and
      Shwartz, Vered  and
      Glava{\v{s}}, Goran  and
      Joty, Shafiq  and
      Wang, Alex  and
      Wolf, Thomas"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.sustainlp-1.0"",
}
@",medical text,natural language process,natural languag,language process,translat,nlp,infer,generat,relation extract,entity recognit,concept norm,summar,sentiment,semant,shared task,annotat,question-answ,challeng,benchmark,evalu,assess,track
" ""Improving Medical {NLI} Using Context-Aware Domain Knowledge"","," ""Domain knowledge is important to understand both the lexical and relational associations of words in natural language text, especially for domain-specific tasks like Natural Language Inference (NLI) in the medical domain, where due to the lack of a large annotated dataset such knowledge cannot be implicitly learned during training. However, because of the linguistic idiosyncrasies of clinical texts (e.g., shorthand jargon), solely relying on domain knowledge from an external knowledge base (e.g., UMLS) can lead to wrong inference predictions as it disregards contextual information and, hence, does not return the most relevant mapping. To remedy this, we devise a knowledge adaptive approach for medical NLI that encodes the premise/hypothesis texts by leveraging supplementary external knowledge, alongside the UMLS, based on the word contexts. By incorporating refined domain knowledge at both the lexical and relational levels through a multi-source attention mechanism, it is able to align the token-level interactions between the premise and hypothesis more effectively. Comprehensive experiments and case study on the recently released MedNLI dataset are conducted to validate the effectiveness of the proposed approach."",","{chowdhury-etal-2020-improving,
    title = ""Improving Medical {NLI} Using Context-Aware Domain Knowledge"",
    author = ""Chowdhury, Shaika  and
      Yu, Philip  and
      Luo, Yuan"",
    editor = ""Gurevych, Iryna  and
      Apidianaki, Marianna  and
      Faruqui, Manaal"",
    booktitle = ""Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics"",
    month = dec,
    year = ""2020"",
    address = ""Barcelona, Spain (Online)"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.starsem-1.1"",
    pages = ""1--11"",
    abstract = ""Domain knowledge is important to understand both the lexical and relational associations of words in natural language text, especially for domain-specific tasks like Natural Language Inference (NLI) in the medical domain, where due to the lack of a large annotated dataset such knowledge cannot be implicitly learned during training. However, because of the linguistic idiosyncrasies of clinical texts (e.g., shorthand jargon), solely relying on domain knowledge from an external knowledge base (e.g., UMLS) can lead to wrong inference predictions as it disregards contextual information and, hence, does not return the most relevant mapping. To remedy this, we devise a knowledge adaptive approach for medical NLI that encodes the premise/hypothesis texts by leveraging supplementary external knowledge, alongside the UMLS, based on the word contexts. By incorporating refined domain knowledge at both the lexical and relational levels through a multi-source attention mechanism, it is able to align the token-level interactions between the premise and hypothesis more effectively. Comprehensive experiments and case study on the recently released MedNLI dataset are conducted to validate the effectiveness of the proposed approach."",
}
@",medical domain,clinical text,natural languag,infer,semant,annotat
" ""On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining"","," ""Neural language representation models such as BERT have recently shown state of the art performance in downstream NLP tasks and bio-medical domain adaptation of BERT (Bio-BERT) has shown same behavior on biomedical text mining tasks. However, due to their large model size and resulting increased computational need, practical application of models such as BERT is challenging making smaller models with comparable performance desirable for real word applications. Recently, a new language transformers based language representation model named ELECTRA is introduced, that makes efficient usage of training data in a generative-discriminative neural model setting that shows performance gains over BERT. These gains are especially impressive for smaller models. Here, we introduce two small ELECTRA based model named Bio-ELECTRA and Bio-ELECTRA++ that are eight times smaller than BERT Base and Bio-BERT and achieves comparable or better performance on biomedical question answering, yes/no question answer classification, question answer candidate ranking and relation extraction tasks. Bio-ELECTRA is pre-trained from scratch on PubMed abstracts using a consumer grade GPU with only 8GB memory. Bio-ELECTRA++ is the further pre-trained version of Bio-ELECTRA trained on a corpus of open access full papers from PubMed Central. While, for biomedical named entity recognition, large BERT Base model outperforms Bio-ELECTRA++, Bio-ELECTRA and ELECTRA-Small++, with hyperparameter tuning Bio-ELECTRA++ achieves results comparable to BERT."",","{ozyurt-2020-effectiveness,
    title = ""On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining"",
    author = ""Ozyurt, Ibrahim Burak"",
    editor = ""Chandrasekaran, Muthu Kumar  and
      de Waard, Anita  and
      Feigenblat, Guy  and
      Freitag, Dayne  and
      Ghosal, Tirthankar  and
      Hovy, Eduard  and
      Knoth, Petr  and
      Konopnicki, David  and
      Mayr, Philipp  and
      Patton, Robert M.  and
      Shmueli-Scheuer, Michal"",
    booktitle = ""Proceedings of the First Workshop on Scholarly Document Processing"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.sdp-1.12"",
    doi = ""10.18653/v1/2020.sdp-1.12"",
    pages = ""104--112"",
    abstract = ""Neural language representation models such as BERT have recently shown state of the art performance in downstream NLP tasks and bio-medical domain adaptation of BERT (Bio-BERT) has shown same behavior on biomedical text mining tasks. However, due to their large model size and resulting increased computational need, practical application of models such as BERT is challenging making smaller models with comparable performance desirable for real word applications. Recently, a new language transformers based language representation model named ELECTRA is introduced, that makes efficient usage of training data in a generative-discriminative neural model setting that shows performance gains over BERT. These gains are especially impressive for smaller models. Here, we introduce two small ELECTRA based model named Bio-ELECTRA and Bio-ELECTRA++ that are eight times smaller than BERT Base and Bio-BERT and achieves comparable or better performance on biomedical question answering, yes/no question answer classification, question answer candidate ranking and relation extraction tasks. Bio-ELECTRA is pre-trained from scratch on PubMed abstracts using a consumer grade GPU with only 8GB memory. Bio-ELECTRA++ is the further pre-trained version of Bio-ELECTRA trained on a corpus of open access full papers from PubMed Central. While, for biomedical named entity recognition, large BERT Base model outperforms Bio-ELECTRA++, Bio-ELECTRA and ELECTRA-Small++, with hyperparameter tuning Bio-ELECTRA++ achieves results comparable to BERT."",
}
@",medical domain,medical text,nlp,generat,relation extract,entity recognit,challeng
" ""{C}omp{L}ex {---} A New Corpus for Lexical Complexity Prediction from {L}ikert {S}cale Data"","," ""Predicting which words are considered hard to understand for a given target population is a vital step in many NLP applications such astext simplification. This task is commonly referred to as Complex Word Identification (CWI). With a few exceptions, previous studieshave approached the task as a binary classification task in which systems predict a complexity value (complex vs. non-complex) fora set of target words in a text. This choice is motivated by the fact that all CWI datasets compiled so far have been annotated using abinary annotation scheme. Our paper addresses this limitation by presenting the first English dataset for continuous lexical complexityprediction. We use a 5-point Likert scale scheme to annotate complex words in texts from three sources/domains: the Bible, Europarl,and biomedical texts. This resulted in a corpus of 9,476 sentences each annotated by around 7 annotators."",","{shardlow-etal-2020-complex,
    title = ""{C}omp{L}ex {---} A New Corpus for Lexical Complexity Prediction from {L}ikert {S}cale Data"",
    author = ""Shardlow, Matthew  and
      Cooper, Michael  and
      Zampieri, Marcos"",
    editor = ""Gala, N{\'u}ria  and
      Wilkens, Rodrigo"",
    booktitle = ""Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI)"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.readi-1.9"",
    pages = ""57--62"",
    abstract = ""Predicting which words are considered hard to understand for a given target population is a vital step in many NLP applications such astext simplification. This task is commonly referred to as Complex Word Identification (CWI). With a few exceptions, previous studieshave approached the task as a binary classification task in which systems predict a complexity value (complex vs. non-complex) fora set of target words in a text. This choice is motivated by the fact that all CWI datasets compiled so far have been annotated using abinary annotation scheme. Our paper addresses this limitation by presenting the first English dataset for continuous lexical complexityprediction. We use a 5-point Likert scale scheme to annotate complex words in texts from three sources/domains: the Bible, Europarl,and biomedical texts. This resulted in a corpus of 9,476 sentences each annotated by around 7 annotators."",
    language = ""English"",
    ISBN = ""979-10-95546-45-0"",
}
@",medical text,nlp,annotat
" ""Robust Prediction of Punctuation and Truecasing for Medical {ASR}"","," ""Automatic speech recognition (ASR) systems in the medical domain that focus on transcribing clinical dictations and doctor-patient conversations often pose many challenges due to the complexity of the domain. ASR output typically undergoes automatic punctuation to enable users to speak naturally, without having to vocalize awkward and explicit punctuation commands, such as {``}period{''}, {``}add comma{''} or {``}exclamation point{''}, while truecasing enhances user readability and improves the performance of downstream NLP tasks. This paper proposes a conditional joint modeling framework for prediction of punctuation and truecasing using pretrained masked language models such as BERT, BioBERT and RoBERTa. We also present techniques for domain and task specific adaptation by fine-tuning masked language models with medical domain data. Finally, we improve the robustness of the model against common errors made in ASR by performing data augmentation. Experiments performed on dictation and conversational style corpora show that our proposed model achieves 5{\%} absolute improvement on ground truth text and 10{\%} improvement on ASR outputs over baseline models under F1 metric."",","{sunkara-etal-2020-robust,
    title = ""Robust Prediction of Punctuation and Truecasing for Medical {ASR}"",
    author = ""Sunkara, Monica  and
      Ronanki, Srikanth  and
      Dixit, Kalpit  and
      Bodapati, Sravan  and
      Kirchhoff, Katrin"",
    editor = ""Bhatia, Parminder  and
      Lin, Steven  and
      Gangadharaiah, Rashmi  and
      Wallace, Byron  and
      Shafran, Izhak  and
      Shivade, Chaitanya  and
      Du, Nan  and
      Diab, Mona"",
    booktitle = ""Proceedings of the First Workshop on Natural Language Processing for Medical Conversations"",
    month = jul,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.nlpmc-1.8"",
    doi = ""10.18653/v1/2020.nlpmc-1.8"",
    pages = ""53--62"",
    abstract = ""Automatic speech recognition (ASR) systems in the medical domain that focus on transcribing clinical dictations and doctor-patient conversations often pose many challenges due to the complexity of the domain. ASR output typically undergoes automatic punctuation to enable users to speak naturally, without having to vocalize awkward and explicit punctuation commands, such as {``}period{''}, {``}add comma{''} or {``}exclamation point{''}, while truecasing enhances user readability and improves the performance of downstream NLP tasks. This paper proposes a conditional joint modeling framework for prediction of punctuation and truecasing using pretrained masked language models such as BERT, BioBERT and RoBERTa. We also present techniques for domain and task specific adaptation by fine-tuning masked language models with medical domain data. Finally, we improve the robustness of the model against common errors made in ASR by performing data augmentation. Experiments performed on dictation and conversational style corpora show that our proposed model achieves 5{\%} absolute improvement on ground truth text and 10{\%} improvement on ASR outputs over baseline models under F1 metric."",
}
@",medical domain,natural language process,natural languag,language process,nlp,challeng
" ""Answering Questions on {COVID}-19 in Real-Time"","," ""The recent outbreak of the novel coronavirus is wreaking havoc on the world and researchers are struggling to effectively combat it. One reason why the fight is difficult is due to the lack of information and knowledge. In this work, we outline our effort to contribute to shrinking this knowledge vacuum by creating covidAsk, a question answering (QA) system that combines biomedical text mining and QA techniques to provide answers to questions in real-time. Our system also leverages information retrieval (IR) approaches to provide entity-level answers that are complementary to QA models. Evaluation of covidAsk is carried out by using a manually created dataset called COVID-19 Questions which is based on information from various sources, including the CDC and the WHO. We hope our system will be able to aid researchers in their search for knowledge and information not only for COVID-19, but for future pandemics as well."",","{lee-etal-2020-answering,
    title = ""Answering Questions on {COVID}-19 in Real-Time"",
    author = ""Lee, Jinhyuk  and
      Yi, Sean S.  and
      Jeong, Minbyul  and
      Sung, Mujeen  and
      Yoon, WonJin  and
      Choi, Yonghwa  and
      Ko, Miyoung  and
      Kang, Jaewoo"",
    editor = ""Verspoor, Karin  and
      Cohen, Kevin Bretonnel  and
      Conway, Michael  and
      de Bruijn, Berry  and
      Dredze, Mark  and
      Mihalcea, Rada  and
      Wallace, Byron"",
    booktitle = ""Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020"",
    month = dec,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.nlpcovid19-2.1"",
    doi = ""10.18653/v1/2020.nlpcovid19-2.1"",
    abstract = ""The recent outbreak of the novel coronavirus is wreaking havoc on the world and researchers are struggling to effectively combat it. One reason why the fight is difficult is due to the lack of information and knowledge. In this work, we outline our effort to contribute to shrinking this knowledge vacuum by creating covidAsk, a question answering (QA) system that combines biomedical text mining and QA techniques to provide answers to questions in real-time. Our system also leverages information retrieval (IR) approaches to provide entity-level answers that are complementary to QA models. Evaluation of covidAsk is carried out by using a manually created dataset called COVID-19 Questions which is based on information from various sources, including the CDC and the WHO. We hope our system will be able to aid researchers in their search for knowledge and information not only for COVID-19, but for future pandemics as well."",
}
@",medical text,nlp,information retriev,evalu
" ""A Multilingual Neural Machine Translation Model for Biomedical Data"","," ""We release a multilingual neural machine translation model, which can be used to translate text in the biomedical domain. The model can translate from 5 languages (French, German, Italian, Korean and Spanish) into English. It is trained with large amounts of generic and biomedical data, using domain tags. Our benchmarks show that it performs near state-of-the-art both on news (generic domain) and biomedical test sets, and that it outperforms the existing publicly released models. We believe that this release will help the large-scale multilingual analysis of the digital content of the COVID-19 crisis and of its effects on society, economy, and healthcare policies. We also release a test set of biomedical text for Korean-English. It consists of 758 sentences from official guidelines and recent papers, all about COVID-19."",","{berard-etal-2020-multilingual,
    title = ""A Multilingual Neural Machine Translation Model for Biomedical Data"",
    author = ""B{\'e}rard, Alexandre  and
      Kim, Zae Myung  and
      Nikoulina, Vassilina  and
      Park, Eunjeong Lucy  and
      Gall{\'e}, Matthias"",
    editor = ""Verspoor, Karin  and
      Cohen, Kevin Bretonnel  and
      Conway, Michael  and
      de Bruijn, Berry  and
      Dredze, Mark  and
      Mihalcea, Rada  and
      Wallace, Byron"",
    booktitle = ""Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020"",
    month = dec,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.nlpcovid19-2.16"",
    doi = ""10.18653/v1/2020.nlpcovid19-2.16"",
    abstract = ""We release a multilingual neural machine translation model, which can be used to translate text in the biomedical domain. The model can translate from 5 languages (French, German, Italian, Korean and Spanish) into English. It is trained with large amounts of generic and biomedical data, using domain tags. Our benchmarks show that it performs near state-of-the-art both on news (generic domain) and biomedical test sets, and that it outperforms the existing publicly released models. We believe that this release will help the large-scale multilingual analysis of the digital content of the COVID-19 crisis and of its effects on society, economy, and healthcare policies. We also release a test set of biomedical text for Korean-English. It consists of 758 sentences from official guidelines and recent papers, all about COVID-19."",
}
@",medical domain,medical text,translat,nlp,benchmark
" ""Building a {N}orwegian Lexical Resource for Medical Entity Recognition"","," ""We present a large Norwegian lexical resource of categorized medical terms. The resource, which merges information from large medical databases, contains over 56,000 entries, including automatically mapped terms from a Norwegian medical dictionary. We describe the methodology behind this automatic dictionary entry mapping based on keywords and suffixes and further present the results of a manual evaluation performed on a subset by a domain expert. The evaluation indicated that ca. 80{\%} of the mappings were correct."",","{pilan-etal-2020-building,
    title = ""Building a {N}orwegian Lexical Resource for Medical Entity Recognition"",
    author = ""Pilan, Ildiko  and
      Brekke, P{\aa}l H.  and
      {\O}vrelid, Lilja"",
    editor = ""Melero, Maite"",
    booktitle = ""Proceedings of the LREC 2020 Workshop on Multilingual Biomedical Text Processing (MultilingualBIO 2020)"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.multilingualbio-1.2"",
    pages = ""9--14"",
    abstract = ""We present a large Norwegian lexical resource of categorized medical terms. The resource, which merges information from large medical databases, contains over 56,000 entries, including automatically mapped terms from a Norwegian medical dictionary. We describe the methodology behind this automatic dictionary entry mapping based on keywords and suffixes and further present the results of a manual evaluation performed on a subset by a domain expert. The evaluation indicated that ca. 80{\%} of the mappings were correct."",
    language = ""English"",
    ISBN = ""979-10-95546-65-8"",
}
@",medical text,entity recognit,evalu
" ""Localising the Clinical Terminology {SNOMED} {CT} by Semi-automated Creation of a {G}erman Interface Vocabulary"","," ""Medical language exhibits great variations regarding users, institutions and language registers. With large parts of clinical documents in free text, NLP is playing a more and more important role in unlocking re-usable and interoperable meaning from medical records. This study describes the architectural principles and the evolution of a German interface vocabulary, combining machine translation with human annotation and rule-based term generation, yielding a resource with 7.7 million raw entries, each of which linked to the reference terminology SNOMED CT, an international standard with about 350 thousand concepts. The purpose is to offer a high coverage of medical jargon in order to optimise terminology grounding of clinical texts by text mining systems. The core resource is a manually curated table of English-to-German word and chunk translations, supported by a set of language generation rules. The work describes a workflow consisting the enrichment and modification of this table with human and machine efforts, manually enriched by grammarspecific tags. Top-down and bottom-up methods for terminology population used in parallel. The final interface terms are produced by a term generator, which creates one-to-many German variants per SNOMED CT English description. Filtering against a large collection of domain terminologies and corpora drastically reduces the size of the vocabulary in favour of more realistic terms or terms that can reasonably be expected to match clinical text passages within a text-mining pipeline. An evaluation was performed by a comparison between the current version of the German interface vocabulary and the English description table of the SNOMED CT International release. An exact term matching was performed with a small parallel corpus constituted by text snippets from different clinical documents. With overall low retrieval parameters (with F-values around 30{\%}), the performance of the German language scenario reaches 80 {--} 90{\%} of the English one. Interestingly, annotations are slightly better with machine-translated (German {--} English) texts, using the International SNOMED CT resource only."",","{schulz-etal-2020-localising,
    title = ""Localising the Clinical Terminology {SNOMED} {CT} by Semi-automated Creation of a {G}erman Interface Vocabulary"",
    author = ""Schulz, Stefan  and
      Hammer, Larissa  and
      Hashemian-Nik, David  and
      Kreuzthaler, Markus"",
    editor = ""Melero, Maite"",
    booktitle = ""Proceedings of the LREC 2020 Workshop on Multilingual Biomedical Text Processing (MultilingualBIO 2020)"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.multilingualbio-1.3"",
    pages = ""15--20"",
    abstract = ""Medical language exhibits great variations regarding users, institutions and language registers. With large parts of clinical documents in free text, NLP is playing a more and more important role in unlocking re-usable and interoperable meaning from medical records. This study describes the architectural principles and the evolution of a German interface vocabulary, combining machine translation with human annotation and rule-based term generation, yielding a resource with 7.7 million raw entries, each of which linked to the reference terminology SNOMED CT, an international standard with about 350 thousand concepts. The purpose is to offer a high coverage of medical jargon in order to optimise terminology grounding of clinical texts by text mining systems. The core resource is a manually curated table of English-to-German word and chunk translations, supported by a set of language generation rules. The work describes a workflow consisting the enrichment and modification of this table with human and machine efforts, manually enriched by grammarspecific tags. Top-down and bottom-up methods for terminology population used in parallel. The final interface terms are produced by a term generator, which creates one-to-many German variants per SNOMED CT English description. Filtering against a large collection of domain terminologies and corpora drastically reduces the size of the vocabulary in favour of more realistic terms or terms that can reasonably be expected to match clinical text passages within a text-mining pipeline. An evaluation was performed by a comparison between the current version of the German interface vocabulary and the English description table of the SNOMED CT International release. An exact term matching was performed with a small parallel corpus constituted by text snippets from different clinical documents. With overall low retrieval parameters (with F-values around 30{\%}), the performance of the German language scenario reaches 80 {--} 90{\%} of the English one. Interestingly, annotations are slightly better with machine-translated (German {--} English) texts, using the International SNOMED CT resource only."",
    language = ""English"",
    ISBN = ""979-10-95546-65-8"",
}
@",medical record,clinical text,medical text,translat,nlp,generat,annotat,evalu
" ""Multilingual enrichment of disease biomedical ontologies"","," ""Translating biomedical ontologies is an important challenge, but doing it manually requires much time and money. We study the possibility to use open-source knowledge bases to translate biomedical ontologies. We focus on two aspects: coverage and quality. We look at the coverage of two biomedical ontologies focusing on diseases with respect to Wikidata for 9 European languages (Czech, Dutch, English, French, German, Italian, Polish, Portuguese and Spanish) for both, plus Arabic, Chinese and Russian for the second. We first use direct links between Wikidata and the studied ontologies and then use second-order links by going through other intermediate ontologies. We then compare the quality of the translations obtained thanks to Wikidata with a commercial machine translation tool, here Google Cloud Translation."",","{bouscarrat-etal-2020-multilingual,
    title = ""Multilingual enrichment of disease biomedical ontologies"",
    author = ""Bouscarrat, L{\'e}o  and
      Bonnefoy, Antoine  and
      Capponi, C{\'e}cile  and
      Ramisch, Carlos"",
    editor = ""Melero, Maite"",
    booktitle = ""Proceedings of the LREC 2020 Workshop on Multilingual Biomedical Text Processing (MultilingualBIO 2020)"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.multilingualbio-1.4"",
    pages = ""21--28"",
    abstract = ""Translating biomedical ontologies is an important challenge, but doing it manually requires much time and money. We study the possibility to use open-source knowledge bases to translate biomedical ontologies. We focus on two aspects: coverage and quality. We look at the coverage of two biomedical ontologies focusing on diseases with respect to Wikidata for 9 European languages (Czech, Dutch, English, French, German, Italian, Polish, Portuguese and Spanish) for both, plus Arabic, Chinese and Russian for the second. We first use direct links between Wikidata and the studied ontologies and then use second-order links by going through other intermediate ontologies. We then compare the quality of the translations obtained thanks to Wikidata with a commercial machine translation tool, here Google Cloud Translation."",
    language = ""English"",
    ISBN = ""979-10-95546-65-8"",
}
@",medical text,translat,challeng
" ""Transfer learning applied to text classification in {S}panish radiological reports"","," ""Pre-trained text encoders have rapidly advanced the state-of-the-art on many Natural Language Processing tasks. This paper presents the use of transfer learning methods applied to the automatic detection of codes in radiological reports in Spanish. Assigning codes to a clinical document is a popular task in NLP and in the biomedical domain. These codes can be of two types: standard classifications (e.g. ICD-10) or specific to each clinic or hospital. In this study we show a system using specific radiology clinic codes. The dataset is composed of 208,167 radiology reports labeled with 89 different codes. The corpus has been evaluated with three methods using the BERT model applied to Spanish: Multilingual BERT, BETO and XLM. The results are interesting obtaining 70{\%} of F1-score with a pre-trained multilingual model."",","{lopez-ubeda-etal-2020-transfer,
    title = ""Transfer learning applied to text classification in {S}panish radiological reports"",
    author = ""L{\'o}pez {\'U}beda, Pilar  and
      D{\'\i}az-Galiano, Manuel Carlos  and
      Urena Lopez, L. Alfonso  and
      Martin, Maite  and
      Mart{\'\i}n-Noguerol, Teodoro  and
      Luna, Antonio"",
    editor = ""Melero, Maite"",
    booktitle = ""Proceedings of the LREC 2020 Workshop on Multilingual Biomedical Text Processing (MultilingualBIO 2020)"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.multilingualbio-1.5"",
    pages = ""29--32"",
    abstract = ""Pre-trained text encoders have rapidly advanced the state-of-the-art on many Natural Language Processing tasks. This paper presents the use of transfer learning methods applied to the automatic detection of codes in radiological reports in Spanish. Assigning codes to a clinical document is a popular task in NLP and in the biomedical domain. These codes can be of two types: standard classifications (e.g. ICD-10) or specific to each clinic or hospital. In this study we show a system using specific radiology clinic codes. The dataset is composed of 208,167 radiology reports labeled with 89 different codes. The corpus has been evaluated with three methods using the BERT model applied to Spanish: Multilingual BERT, BETO and XLM. The results are interesting obtaining 70{\%} of F1-score with a pre-trained multilingual model."",
    language = ""English"",
    ISBN = ""979-10-95546-65-8"",
}
@",medical domain,medical text,natural language process,natural languag,language process,nlp,evalu
" ""Alignment Annotation for Clinic Visit Dialogue to Clinical Note Sentence Language Generation"","," ""For every patient{'}s visit to a clinician, a clinical note is generated documenting their medical conversation, including complaints discussed, treatments, and medical plans. Despite advances in natural language processing, automating clinical note generation from a clinic visit conversation is a largely unexplored area of research. Due to the idiosyncrasies of the task, traditional methods of corpus creation are not effective enough approaches for this problem. In this paper, we present an annotation methodology that is content- and technique- agnostic while associating note sentences to sets of dialogue sentences. The sets can further be grouped with higher order tags to mark sets with related information. This direct linkage from input to output decouples the annotation from specific language understanding or generation strategies. Here we provide data statistics and qualitative analysis describing the unique annotation challenges. Given enough annotated data, such a resource would support multiple modeling methods including information extraction with template language generation, information retrieval type language generation, or sequence to sequence modeling."",","{yim-etal-2020-alignment,
    title = ""Alignment Annotation for Clinic Visit Dialogue to Clinical Note Sentence Language Generation"",
    author = ""Yim, Wen-wai  and
      Yetisgen, Meliha  and
      Huang, Jenny  and
      Grossman, Micah"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Twelfth Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.lrec-1.52"",
    pages = ""413--421"",
    abstract = ""For every patient{'}s visit to a clinician, a clinical note is generated documenting their medical conversation, including complaints discussed, treatments, and medical plans. Despite advances in natural language processing, automating clinical note generation from a clinic visit conversation is a largely unexplored area of research. Due to the idiosyncrasies of the task, traditional methods of corpus creation are not effective enough approaches for this problem. In this paper, we present an annotation methodology that is content- and technique- agnostic while associating note sentences to sets of dialogue sentences. The sets can further be grouped with higher order tags to mark sets with related information. This direct linkage from input to output decouples the annotation from specific language understanding or generation strategies. Here we provide data statistics and qualitative analysis describing the unique annotation challenges. Given enough annotated data, such a resource would support multiple modeling methods including information extraction with template language generation, information retrieval type language generation, or sequence to sequence modeling."",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
@",clinical not,natural language process,natural languag,language process,generat,information retriev,annotat,challeng,evalu
" ""{PATE}: A Corpus of Temporal Expressions for the In-car Voice Assistant Domain"","," ""The recognition and automatic annotation of temporal expressions (e.g. {``}Add an event for tomorrow evening at eight to my calendar{''}) is a key module for AI voice assistants, in order to allow them to interact with apps (for example, a calendar app). However, in the NLP literature, research on temporal expressions has focused mostly on data from the news, from the clinical domain, and from social media. The voice assistant domain is very different than the typical domains that have been the focus of work on temporal expression identification, thus requiring a dedicated data collection. We present a crowdsourcing method for eliciting natural-language commands containing temporal expressions for an AI voice assistant, by using pictures and scenario descriptions. We annotated the elicited commands (480) as well as the commands in the Snips dataset following the TimeML/TIMEX3 annotation guidelines, reaching a total of 1188 annotated commands. The commands can be later used to train the NLU components of an AI voice assistant."",","{zarcone-etal-2020-pate,
    title = ""{PATE}: A Corpus of Temporal Expressions for the In-car Voice Assistant Domain"",
    author = ""Zarcone, Alessandra  and
      Alam, Touhidul  and
      Kolagar, Zahra"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Twelfth Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.lrec-1.66"",
    pages = ""523--530"",
    abstract = ""The recognition and automatic annotation of temporal expressions (e.g. {``}Add an event for tomorrow evening at eight to my calendar{''}) is a key module for AI voice assistants, in order to allow them to interact with apps (for example, a calendar app). However, in the NLP literature, research on temporal expressions has focused mostly on data from the news, from the clinical domain, and from social media. The voice assistant domain is very different than the typical domains that have been the focus of work on temporal expression identification, thus requiring a dedicated data collection. We present a crowdsourcing method for eliciting natural-language commands containing temporal expressions for an AI voice assistant, by using pictures and scenario descriptions. We annotated the elicited commands (480) as well as the commands in the Snips dataset following the TimeML/TIMEX3 annotation guidelines, reaching a total of 1188 annotated commands. The commands can be later used to train the NLU components of an AI voice assistant."",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
@",clinical domain,nlp,annotat,evalu
" ""A Corpus for Detecting High-Context Medical Conditions in Intensive Care Patient Notes Focusing on Frequently Readmitted Patients"","," ""A crucial step within secondary analysis of electronic health records (EHRs) is to identify the patient cohort under investigation. While EHRs contain medical billing codes that aim to represent the conditions and treatments patients may have, much of the information is only present in the patient notes. Therefore, it is critical to develop robust algorithms to infer patients{'} conditions and treatments from their written notes. In this paper, we introduce a dataset for patient phenotyping, a task that is defined as the identification of whether a patient has a given medical condition (also referred to as clinical indication or phenotype) based on their patient note. Nursing Progress Notes and Discharge Summaries from the Intensive Care Unit of a large tertiary care hospital were manually annotated for the presence of several high-context phenotypes relevant to treatment and risk of re-hospitalization. This dataset contains 1102 Discharge Summaries and 1000 Nursing Progress Notes. Each Discharge Summary and Progress Note has been annotated by at least two expert human annotators (one clinical researcher and one resident physician). Annotated phenotypes include treatment non-adherence, chronic pain, advanced/metastatic cancer, as well as 10 other phenotypes. This dataset can be utilized for academic and industrial research in medicine and computer science, particularly within the field of medical natural language processing."",","{moseley-etal-2020-corpus,
    title = ""A Corpus for Detecting High-Context Medical Conditions in Intensive Care Patient Notes Focusing on Frequently Readmitted Patients"",
    author = ""Moseley, Edward T.  and
      Wu, Joy T.  and
      Welt, Jonathan  and
      Foote, John  and
      Tyler, Patrick D.  and
      Grant, David W.  and
      Carlson, Eric T.  and
      Gehrmann, Sebastian  and
      Dernoncourt, Franck  and
      Celi, Leo Anthony"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Twelfth Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.lrec-1.170"",
    pages = ""1362--1367"",
    abstract = ""A crucial step within secondary analysis of electronic health records (EHRs) is to identify the patient cohort under investigation. While EHRs contain medical billing codes that aim to represent the conditions and treatments patients may have, much of the information is only present in the patient notes. Therefore, it is critical to develop robust algorithms to infer patients{'} conditions and treatments from their written notes. In this paper, we introduce a dataset for patient phenotyping, a task that is defined as the identification of whether a patient has a given medical condition (also referred to as clinical indication or phenotype) based on their patient note. Nursing Progress Notes and Discharge Summaries from the Intensive Care Unit of a large tertiary care hospital were manually annotated for the presence of several high-context phenotypes relevant to treatment and risk of re-hospitalization. This dataset contains 1102 Discharge Summaries and 1000 Nursing Progress Notes. Each Discharge Summary and Progress Note has been annotated by at least two expert human annotators (one clinical researcher and one resident physician). Annotated phenotypes include treatment non-adherence, chronic pain, advanced/metastatic cancer, as well as 10 other phenotypes. This dataset can be utilized for academic and industrial research in medicine and computer science, particularly within the field of medical natural language processing."",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
@",electronic health record,health record,discharge summar,natural language process,natural languag,language process,infer,summar,annotat,evalu
" ""The Medical Scribe: Corpus Development and Model Performance Analyses"","," ""There is a growing interest in creating tools to assist in clinical note generation using the audio of provider-patient encounters. Motivated by this goal and with the help of providers and medical scribes, we developed an annotation scheme to extract relevant clinical concepts. We used this annotation scheme to label a corpus of about 6k clinical encounters. This was used to train a state-of-the-art tagging model. We report ontologies, labeling results, model performances, and detailed analyses of the results. Our results show that the entities related to medications can be extracted with a relatively high accuracy of 0.90 F-score, followed by symptoms at 0.72 F-score, and conditions at 0.57 F-score. In our task, we not only identify where the symptoms are mentioned but also map them to canonical forms as they appear in the clinical notes. Of the different types of errors, in about 19-38{\%} of the cases, we find that the model output was correct, and about 17-32{\%} of the errors do not impact the clinical note. Taken together, the models developed in this work are more useful than the F-scores reflect, making it a promising approach for practical applications."",","{shafran-etal-2020-medical,
    title = ""The Medical Scribe: Corpus Development and Model Performance Analyses"",
    author = ""Shafran, Izhak  and
      Du, Nan  and
      Tran, Linh  and
      Perry, Amanda  and
      Keyes, Lauren  and
      Knichel, Mark  and
      Domin, Ashley  and
      Huang, Lei  and
      Chen, Yu-hui  and
      Li, Gang  and
      Wang, Mingqiu  and
      El Shafey, Laurent  and
      Soltau, Hagen  and
      Paul, Justin Stuart"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Twelfth Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.lrec-1.250"",
    pages = ""2036--2044"",
    abstract = ""There is a growing interest in creating tools to assist in clinical note generation using the audio of provider-patient encounters. Motivated by this goal and with the help of providers and medical scribes, we developed an annotation scheme to extract relevant clinical concepts. We used this annotation scheme to label a corpus of about 6k clinical encounters. This was used to train a state-of-the-art tagging model. We report ontologies, labeling results, model performances, and detailed analyses of the results. Our results show that the entities related to medications can be extracted with a relatively high accuracy of 0.90 F-score, followed by symptoms at 0.72 F-score, and conditions at 0.57 F-score. In our task, we not only identify where the symptoms are mentioned but also map them to canonical forms as they appear in the clinical notes. Of the different types of errors, in about 19-38{\%} of the cases, we find that the model output was correct, and about 17-32{\%} of the errors do not impact the clinical note. Taken together, the models developed in this work are more useful than the F-scores reflect, making it a promising approach for practical applications."",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
@",clinical not,generat,annotat,evalu
" ""Sensitive Data Detection and Classification in {S}panish Clinical Text: Experiments with {BERT}"","," ""Massive digital data processing provides a wide range of opportunities and benefits, but at the cost of endangering personal data privacy. Anonymisation consists in removing or replacing sensitive information from data, enabling its exploitation for different purposes while preserving the privacy of individuals. Over the years, a lot of automatic anonymisation systems have been proposed; however, depending on the type of data, the target language or the availability of training documents, the task remains challenging still. The emergence of novel deep-learning models during the last two years has brought large improvements to the state of the art in the field of Natural Language Processing. These advancements have been most noticeably led by BERT, a model proposed by Google in 2018, and the shared language models pre-trained on millions of documents. In this paper, we use a BERT-based sequence labelling model to conduct a series of anonymisation experiments on several clinical datasets in Spanish. We also compare BERT with other algorithms. The experiments show that a simple BERT-based model with general-domain pre-training obtains highly competitive results without any domain specific feature engineering."",","{garcia-pablos-etal-2020-sensitive,
    title = ""Sensitive Data Detection and Classification in {S}panish Clinical Text: Experiments with {BERT}"",
    author = ""Garc{\'\i}a Pablos, Aitor  and
      Perez, Naiara  and
      Cuadros, Montse"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Twelfth Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.lrec-1.552"",
    pages = ""4486--4494"",
    abstract = ""Massive digital data processing provides a wide range of opportunities and benefits, but at the cost of endangering personal data privacy. Anonymisation consists in removing or replacing sensitive information from data, enabling its exploitation for different purposes while preserving the privacy of individuals. Over the years, a lot of automatic anonymisation systems have been proposed; however, depending on the type of data, the target language or the availability of training documents, the task remains challenging still. The emergence of novel deep-learning models during the last two years has brought large improvements to the state of the art in the field of Natural Language Processing. These advancements have been most noticeably led by BERT, a model proposed by Google in 2018, and the shared language models pre-trained on millions of documents. In this paper, we use a BERT-based sequence labelling model to conduct a series of anonymisation experiments on several clinical datasets in Spanish. We also compare BERT with other algorithms. The experiments show that a simple BERT-based model with general-domain pre-training obtains highly competitive results without any domain specific feature engineering."",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
@",clinical text,natural language process,natural languag,language process,sequence label,challeng,evalu
" ""Towards a Versatile Medical-Annotation Guideline Feasible Without Heavy Medical Knowledge: Starting From Critical Lung Diseases"","," ""Applying natural language processing (NLP) to medical and clinical texts can bring important social benefits by mining valuable information from unstructured text. A popular application for that purpose is named entity recognition (NER), but the annotation policies of existing clinical corpora have not been standardized across clinical texts of different types. This paper presents an annotation guideline aimed at covering medical documents of various types such as radiography interpretation reports and medical records. Furthermore, the annotation was designed to avoid burdensome requirements related to medical knowledge, thereby enabling corpus development without medical specialists. To achieve these design features, we specifically focus on critical lung diseases to stabilize linguistic patterns in corpora. After annotating around 1100 electronic medical records following the annotation scheme, we demonstrated its feasibility using an NER task. Results suggest that our guideline is applicable to large-scale clinical NLP projects."",","{yada-etal-2020-towards,
    title = ""Towards a Versatile Medical-Annotation Guideline Feasible Without Heavy Medical Knowledge: Starting From Critical Lung Diseases"",
    author = ""Yada, Shuntaro  and
      Joh, Ayami  and
      Tanaka, Ribeka  and
      Cheng, Fei  and
      Aramaki, Eiji  and
      Kurohashi, Sadao"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Twelfth Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.lrec-1.561"",
    pages = ""4565--4572"",
    abstract = ""Applying natural language processing (NLP) to medical and clinical texts can bring important social benefits by mining valuable information from unstructured text. A popular application for that purpose is named entity recognition (NER), but the annotation policies of existing clinical corpora have not been standardized across clinical texts of different types. This paper presents an annotation guideline aimed at covering medical documents of various types such as radiography interpretation reports and medical records. Furthermore, the annotation was designed to avoid burdensome requirements related to medical knowledge, thereby enabling corpus development without medical specialists. To achieve these design features, we specifically focus on critical lung diseases to stabilize linguistic patterns in corpora. After annotating around 1100 electronic medical records following the annotation scheme, we demonstrated its feasibility using an NER task. Results suggest that our guideline is applicable to large-scale clinical NLP projects."",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
@",medical record,clinical text,natural language process,natural languag,language process,nlp,entity recognit,annotat,evalu
" ""Exploring Transformer Text Generation for Medical Dataset Augmentation"","," ""Natural Language Processing (NLP) can help unlock the vast troves of unstructured data in clinical text and thus improve healthcare research. However, a big barrier to developments in this field is data access due to patient confidentiality which prohibits the sharing of this data, resulting in small, fragmented and sequestered openly available datasets. Since NLP model development requires large quantities of data, we aim to help side-step this roadblock by exploring the usage of Natural Language Generation in augmenting datasets such that they can be used for NLP model development on downstream clinically relevant tasks. We propose a methodology guiding the generation with structured patient information in a sequence-to-sequence manner. We experiment with state-of-the-art Transformer models and demonstrate that our augmented dataset is capable of beating our baselines on a downstream classification task. Finally, we also create a user interface and release the scripts to train generation models to stimulate further research in this area."",","{amin-nejad-etal-2020-exploring,
    title = ""Exploring Transformer Text Generation for Medical Dataset Augmentation"",
    author = ""Amin-Nejad, Ali  and
      Ive, Julia  and
      Velupillai, Sumithra"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Twelfth Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.lrec-1.578"",
    pages = ""4699--4708"",
    abstract = ""Natural Language Processing (NLP) can help unlock the vast troves of unstructured data in clinical text and thus improve healthcare research. However, a big barrier to developments in this field is data access due to patient confidentiality which prohibits the sharing of this data, resulting in small, fragmented and sequestered openly available datasets. Since NLP model development requires large quantities of data, we aim to help side-step this roadblock by exploring the usage of Natural Language Generation in augmenting datasets such that they can be used for NLP model development on downstream clinically relevant tasks. We propose a methodology guiding the generation with structured patient information in a sequence-to-sequence manner. We experiment with state-of-the-art Transformer models and demonstrate that our augmented dataset is capable of beating our baselines on a downstream classification task. Finally, we also create a user interface and release the scripts to train generation models to stimulate further research in this area."",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
@",clinical text,natural language process,natural languag,language process,nlp,generat,evalu
" ""Is Language Modeling Enough? Evaluating Effective Embedding Combinations"","," ""Universal embeddings, such as BERT or ELMo, are useful for a broad set of natural language processing tasks like text classification or sentiment analysis. Moreover, specialized embeddings also exist for tasks like topic modeling or named entity disambiguation. We study if we can complement these universal embeddings with specialized embeddings. We conduct an in-depth evaluation of nine well known natural language understanding tasks with SentEval. Also, we extend SentEval with two additional tasks to the medical domain. We present PubMedSection, a novel topic classification dataset focussed on the biomedical domain. Our comprehensive analysis covers 11 tasks and combinations of six embeddings. We report that combined embeddings outperform state of the art universal embeddings without any embedding fine-tuning. We observe that adding topic model based embeddings helps for most tasks and that differing pre-training tasks encode complementary features. Moreover, we present new state of the art results on the MPQA and SUBJ tasks in SentEval."",","{schneider-etal-2020-language,
    title = ""Is Language Modeling Enough? Evaluating Effective Embedding Combinations"",
    author = ""Schneider, Rudolf  and
      Oberhauser, Tom  and
      Grundmann, Paul  and
      Gers, Felix Alexander  and
      Loeser, Alexander  and
      Staab, Steffen"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Twelfth Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.lrec-1.583"",
    pages = ""4739--4748"",
    abstract = ""Universal embeddings, such as BERT or ELMo, are useful for a broad set of natural language processing tasks like text classification or sentiment analysis. Moreover, specialized embeddings also exist for tasks like topic modeling or named entity disambiguation. We study if we can complement these universal embeddings with specialized embeddings. We conduct an in-depth evaluation of nine well known natural language understanding tasks with SentEval. Also, we extend SentEval with two additional tasks to the medical domain. We present PubMedSection, a novel topic classification dataset focussed on the biomedical domain. Our comprehensive analysis covers 11 tasks and combinations of six embeddings. We report that combined embeddings outperform state of the art universal embeddings without any embedding fine-tuning. We observe that adding topic model based embeddings helps for most tasks and that differing pre-training tasks encode complementary features. Moreover, we present new state of the art results on the MPQA and SUBJ tasks in SentEval."",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
@",medical domain,natural language process,natural languag,language process,sentiment,evalu
" ""Dataset and Enhanced Model for Eligibility Criteria-to-{SQL} Semantic Parsing"","," ""Clinical trials often require that patients meet eligibility criteria (e.g., have specific conditions) to ensure the safety and the effectiveness of studies. However, retrieving eligible patients for a trial from the electronic health record (EHR) database remains a challenging task for clinicians since it requires not only medical knowledge about eligibility criteria, but also an adequate understanding of structured query language (SQL). In this paper, we introduce a new dataset that includes the first-of-its-kind eligibility-criteria corpus and the corresponding queries for criteria-to-sql (Criteria2SQL), a task translating the eligibility criteria to executable SQL queries. Compared to existing datasets, the queries in the dataset here are derived from the eligibility criteria of clinical trials and include \textit{Order-sensitive, Counting-based, and Boolean-type} cases which are not seen before. In addition to the dataset, we propose a novel neural semantic parser as a strong baseline model. Extensive experiments show that the proposed parser outperforms existing state-of-the-art general-purpose text-to-sql models while highlighting the challenges presented by the new dataset. The uniqueness and the diversity of the dataset leave a lot of research opportunities for future improvement."",","{yu-etal-2020-dataset,
    title = ""Dataset and Enhanced Model for Eligibility Criteria-to-{SQL} Semantic Parsing"",
    author = ""Yu, Xiaojing  and
      Chen, Tianlong  and
      Yu, Zhengjie  and
      Li, Huiyu  and
      Yang, Yang  and
      Jiang, Xiaoqian  and
      Jiang, Anxiao"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Twelfth Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.lrec-1.714"",
    pages = ""5829--5837"",
    abstract = ""Clinical trials often require that patients meet eligibility criteria (e.g., have specific conditions) to ensure the safety and the effectiveness of studies. However, retrieving eligible patients for a trial from the electronic health record (EHR) database remains a challenging task for clinicians since it requires not only medical knowledge about eligibility criteria, but also an adequate understanding of structured query language (SQL). In this paper, we introduce a new dataset that includes the first-of-its-kind eligibility-criteria corpus and the corresponding queries for criteria-to-sql (Criteria2SQL), a task translating the eligibility criteria to executable SQL queries. Compared to existing datasets, the queries in the dataset here are derived from the eligibility criteria of clinical trials and include \textit{Order-sensitive, Counting-based, and Boolean-type} cases which are not seen before. In addition to the dataset, we propose a novel neural semantic parser as a strong baseline model. Extensive experiments show that the proposed parser outperforms existing state-of-the-art general-purpose text-to-sql models while highlighting the challenges presented by the new dataset. The uniqueness and the diversity of the dataset leave a lot of research opportunities for future improvement."",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
@",electronic health record,health record,translat,semant,challeng,evalu
" ""Infrastructure for Semantic Annotation in the Genomics Domain"","," ""We describe a novel super-infrastructure for biomedical text mining which incorporates an end-to-end pipeline for the collection, annotation, storage, retrieval and analysis of biomedical and life sciences literature, combining NLP and corpus linguistics methods. The infrastructure permits extreme-scale research on the open access PubMed Central archive. It combines an updatable Gene Ontology Semantic Tagger (GOST) for entity identification and semantic markup in the literature, with a NLP pipeline scheduler (Buster) to collect and process the corpus, and a bespoke columnar corpus database (LexiDB) for indexing. The corpus database is distributed to permit fast indexing, and provides a simple web front-end with corpus linguistics methods for sub-corpus comparison and retrieval. GOST is also connected as a service in the Language Application (LAPPS) Grid, in which context it is interoperable with other NLP tools and data in the Grid and can be combined with them in more complex workflows. In a literature based discovery setting, we have created an annotated corpus of 9,776 papers with 5,481,543 words."",","{el-haj-etal-2020-infrastructure,
    title = ""Infrastructure for Semantic Annotation in the Genomics Domain"",
    author = ""El-Haj, Mahmoud  and
      Rutherford, Nathan  and
      Coole, Matthew  and
      Ezeani, Ignatius  and
      Prentice, Sheryl  and
      Ide, Nancy  and
      Knight, Jo  and
      Piao, Scott  and
      Mariani, John  and
      Rayson, Paul  and
      Suderman, Keith"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Twelfth Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.lrec-1.855"",
    pages = ""6921--6929"",
    abstract = ""We describe a novel super-infrastructure for biomedical text mining which incorporates an end-to-end pipeline for the collection, annotation, storage, retrieval and analysis of biomedical and life sciences literature, combining NLP and corpus linguistics methods. The infrastructure permits extreme-scale research on the open access PubMed Central archive. It combines an updatable Gene Ontology Semantic Tagger (GOST) for entity identification and semantic markup in the literature, with a NLP pipeline scheduler (Buster) to collect and process the corpus, and a bespoke columnar corpus database (LexiDB) for indexing. The corpus database is distributed to permit fast indexing, and provides a simple web front-end with corpus linguistics methods for sub-corpus comparison and retrieval. GOST is also connected as a service in the Language Application (LAPPS) Grid, in which context it is interoperable with other NLP tools and data in the Grid and can be combined with them in more complex workflows. In a literature based discovery setting, we have created an annotated corpus of 9,776 papers with 5,481,543 words."",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
@",medical text,nlp,semant,annotat,evalu
" ""{H}itzal{M}ed: Anonymisation of Clinical Text in {S}panish"","," ""HitzalMed is a web-framed tool that performs automatic detection of sensitive information in clinical texts using machine learning algorithms reported to be competitive for the task. Moreover, once sensitive information is detected, different anonymisation techniques are implemented that are configurable by the user {--}for instance, substitution, where sensitive items are replaced by same category text in an effort to generate a new document that looks as natural as the original one. The tool is able to get data from different document formats and outputs downloadable anonymised data. This paper presents the anonymisation and substitution technology and the demonstrator which is publicly available at \url{https://snlt.vicomtech.org/hitzalmed}."",","{lima-lopez-etal-2020-hitzalmed,
    title = ""{H}itzal{M}ed: Anonymisation of Clinical Text in {S}panish"",
    author = ""Lima Lopez, Salvador  and
      Perez, Naiara  and
      Garc{\'\i}a-Sardi{\~n}a, Laura  and
      Cuadros, Montse"",
    editor = ""Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Twelfth Language Resources and Evaluation Conference"",
    month = may,
    year = ""2020"",
    address = ""Marseille, France"",
    publisher = ""European Language Resources Association"",
    url = ""https://aclanthology.org/2020.lrec-1.870"",
    pages = ""7038--7043"",
    abstract = ""HitzalMed is a web-framed tool that performs automatic detection of sensitive information in clinical texts using machine learning algorithms reported to be competitive for the task. Moreover, once sensitive information is detected, different anonymisation techniques are implemented that are configurable by the user {--}for instance, substitution, where sensitive items are replaced by same category text in an effort to generate a new document that looks as natural as the original one. The tool is able to get data from different document formats and outputs downloadable anonymised data. This paper presents the anonymisation and substitution technology and the demonstrator which is publicly available at \url{https://snlt.vicomtech.org/hitzalmed}."",
    language = ""English"",
    ISBN = ""979-10-95546-34-4"",
}
@",clinical text,generat,evalu
" ""Simple Hierarchical Multi-Task Neural End-To-End Entity Linking for Biomedical Text"","," ""Recognising and linking entities is a crucial first step to many tasks in biomedical text analysis, such as relation extraction and target identification. Traditionally, biomedical entity linking methods rely heavily on heuristic rules and predefined, often domain-specific features. The features try to capture the properties of entities and complex multi-step architectures to detect, and subsequently link entity mentions. We propose a significant simplification to the biomedical entity linking setup that does not rely on any heuristic methods. The system performs all the steps of the entity linking task jointly in either single or two stages. We explore the use of hierarchical multi-task learning, using mention recognition and entity typing tasks as auxiliary tasks. We show that hierarchical multi-task models consistently outperform single-task models when trained tasks are homogeneous. We evaluate the performance of our models on the biomedical entity linking benchmarks using MedMentions and BC5CDR datasets. We achieve state-of-theart results on the challenging MedMentions dataset, and comparable results on BC5CDR."",","{wiatrak-iso-sipila-2020-simple,
    title = ""Simple Hierarchical Multi-Task Neural End-To-End Entity Linking for Biomedical Text"",
    author = ""Wiatrak, Maciej  and
      Iso-Sipila, Juha"",
    editor = ""Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Lavelli, Alberto  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.louhi-1.2"",
    doi = ""10.18653/v1/2020.louhi-1.2"",
    pages = ""12--17"",
    abstract = ""Recognising and linking entities is a crucial first step to many tasks in biomedical text analysis, such as relation extraction and target identification. Traditionally, biomedical entity linking methods rely heavily on heuristic rules and predefined, often domain-specific features. The features try to capture the properties of entities and complex multi-step architectures to detect, and subsequently link entity mentions. We propose a significant simplification to the biomedical entity linking setup that does not rely on any heuristic methods. The system performs all the steps of the entity linking task jointly in either single or two stages. We explore the use of hierarchical multi-task learning, using mention recognition and entity typing tasks as auxiliary tasks. We show that hierarchical multi-task models consistently outperform single-task models when trained tasks are homogeneous. We evaluate the performance of our models on the biomedical entity linking benchmarks using MedMentions and BC5CDR datasets. We achieve state-of-theart results on the challenging MedMentions dataset, and comparable results on BC5CDR."",
}
@",medical text,relation extract,challeng,benchmark,evalu
" ""{GGPONC}: A Corpus of {G}erman Medical Text with Rich Metadata Based on Clinical Practice Guidelines"","," ""The lack of publicly accessible text corpora is a major obstacle for progress in natural language processing. For medical applications, unfortunately, all language communities other than English are low-resourced. In this work, we present GGPONC (German Guideline Program in Oncology NLP Corpus), a freely dis tributable German language corpus based on clinical practice guidelines for oncology. This corpus is one of the largest ever built from German medical documents. Unlike clinical documents, clinical guidelines do not contain any patient-related information and can therefore be used without data protection restrictions. Moreover, GGPONC is the first corpus for the German language covering diverse conditions in a large medical subfield and provides a variety of metadata, such as literature references and evidence levels. By applying and evaluating existing medical information extraction pipelines for German text, we are able to draw comparisons for the use of medical language to other corpora, medical and non-medical ones."",","{borchert-etal-2020-ggponc,
    title = ""{GGPONC}: A Corpus of {G}erman Medical Text with Rich Metadata Based on Clinical Practice Guidelines"",
    author = ""Borchert, Florian  and
      Lohr, Christina  and
      Modersohn, Luise  and
      Langer, Thomas  and
      Follmann, Markus  and
      Sachs, Jan Philipp  and
      Hahn, Udo  and
      Schapranow, Matthieu-P."",
    editor = ""Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Lavelli, Alberto  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.louhi-1.5"",
    doi = ""10.18653/v1/2020.louhi-1.5"",
    pages = ""38--48"",
    abstract = ""The lack of publicly accessible text corpora is a major obstacle for progress in natural language processing. For medical applications, unfortunately, all language communities other than English are low-resourced. In this work, we present GGPONC (German Guideline Program in Oncology NLP Corpus), a freely dis tributable German language corpus based on clinical practice guidelines for oncology. This corpus is one of the largest ever built from German medical documents. Unlike clinical documents, clinical guidelines do not contain any patient-related information and can therefore be used without data protection restrictions. Moreover, GGPONC is the first corpus for the German language covering diverse conditions in a large medical subfield and provides a variety of metadata, such as literature references and evidence levels. By applying and evaluating existing medical information extraction pipelines for German text, we are able to draw comparisons for the use of medical language to other corpora, medical and non-medical ones."",
}
@",medical text,natural language process,natural languag,language process,nlp,communiti,evalu
" ""Contextualized {F}rench Language Models for Biomedical Named Entity Recognition"","," ""Named entity recognition (NER) is key for biomedical applications as it allows knowledge discovery in free text data. As entities are semantic phrases, their meaning is conditioned to the context to avoid ambiguity. In this work, we explore contextualized language models for NER in French biomedical text as part of the D{\'e}fi Fouille de Textes challenge. Our best approach achieved an F1 -measure of 66{\%} for symptoms and signs, and pathology categories, being top 1 for subtask 1. For anatomy, dose, exam, mode, moment, substance, treatment, and value categories, it achieved an F1 -measure of 75{\%} (subtask 2). If considered all categories, our model achieved the best result in the challenge, with an F1 -measure of 72{\%}. The use of an ensemble of neural language models proved to be very effective, improving a CRF baseline by up to 28{\%} and a single specialised language model by 4{\%}."",","{copara-etal-2020-contextualized,
    title = ""Contextualized {F}rench Language Models for Biomedical Named Entity Recognition"",
    author = ""Copara, Jenny  and
      Knafou, Julien  and
      Naderi, Nona  and
      Moro, Claudia  and
      Ruch, Patrick  and
      Teodoro, Douglas"",
    editor = ""Cardon, R{\'e}mi  and
      Grabar, Natalia  and
      Grouin, Cyril  and
      Hamon, Thierry"",
    booktitle = ""Actes de la 6e conf{\'e}rence conjointe Journ{\'e}es d'{\'E}tudes sur la Parole (JEP, 33e {\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\'e}dition), Rencontre des {\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\'E}CITAL, 22e {\'e}dition). Atelier D{\'E}fi Fouille de Textes"",
    month = ""6"",
    year = ""2020"",
    address = ""Nancy, France"",
    publisher = ""ATALA et AFCP"",
    url = ""https://aclanthology.org/2020.jeptalnrecital-deft.4"",
    pages = ""36--48"",
    abstract = ""Named entity recognition (NER) is key for biomedical applications as it allows knowledge discovery in free text data. As entities are semantic phrases, their meaning is conditioned to the context to avoid ambiguity. In this work, we explore contextualized language models for NER in French biomedical text as part of the D{\'e}fi Fouille de Textes challenge. Our best approach achieved an F1 -measure of 66{\%} for symptoms and signs, and pathology categories, being top 1 for subtask 1. For anatomy, dose, exam, mode, moment, substance, treatment, and value categories, it achieved an F1 -measure of 75{\%} (subtask 2). If considered all categories, our model achieved the best result in the challenge, with an F1 -measure of 72{\%}. The use of an ensemble of neural language models proved to be very effective, improving a CRF baseline by up to 28{\%} and a single specialised language model by 4{\%}."",
}
@",medical text,entity recognit,semant,challeng
" ""{QA}2{E}xplanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph"","," ""In the era of Big Knowledge Graphs, Question Answering (QA) systems have reached a milestone in their performance and feasibility. However, their applicability, particularly in specific domains such as the biomedical domain, has not gained wide acceptance due to their {``}black box{''} nature, which hinders transparency, fairness, and accountability of QA systems. Therefore, users are unable to understand how and why particular questions have been answered, whereas some others fail. To address this challenge, in this paper, we develop an automatic approach for generating explanations during various stages of a pipeline-based QA system. Our approach is a supervised and automatic approach which considers three classes (i.e., success, no answer, and wrong answer) for annotating the output of involved QA components. Upon our prediction, a template explanation is chosen and integrated into the output of the corresponding component. To measure the effectiveness of the approach, we conducted a user survey as to how non-expert users perceive our generated explanations. The results of our study show a significant increase in the four dimensions of the human factor from the Human-computer interaction community."",","{shekarpour-etal-2020-qa2explanation,
    title = ""{QA}2{E}xplanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph"",
    author = ""Shekarpour, Saeedeh  and
      Nadgeri, Abhishek  and
      Singh, Kuldeep"",
    editor = ""Bogin, Ben  and
      Iyer, Srinivasan  and
      Lin, Victoria  and
      Radev, Dragomir  and
      Suhr, Alane  and
      {Panupong}  and
      Xiong, Caiming  and
      Yin, Pengcheng  and
      Yu, Tao  and
      Zhang, Rui  and
      Zhong, Victor"",
    booktitle = ""Proceedings of the First Workshop on Interactive and Executable Semantic Parsing"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.intexsempar-1.1"",
    doi = ""10.18653/v1/2020.intexsempar-1.1"",
    pages = ""1--11"",
    abstract = ""In the era of Big Knowledge Graphs, Question Answering (QA) systems have reached a milestone in their performance and feasibility. However, their applicability, particularly in specific domains such as the biomedical domain, has not gained wide acceptance due to their {``}black box{''} nature, which hinders transparency, fairness, and accountability of QA systems. Therefore, users are unable to understand how and why particular questions have been answered, whereas some others fail. To address this challenge, in this paper, we develop an automatic approach for generating explanations during various stages of a pipeline-based QA system. Our approach is a supervised and automatic approach which considers three classes (i.e., success, no answer, and wrong answer) for annotating the output of involved QA components. Upon our prediction, a template explanation is chosen and integrated into the output of the corresponding component. To measure the effectiveness of the approach, we conducted a user survey as to how non-expert users perceive our generated explanations. The results of our study show a significant increase in the four dimensions of the human factor from the Human-computer interaction community."",
}
@",medical domain,generat,semant,annotat,challeng,evalu
" ""Generating {A}ccurate {E}lectronic {H}ealth {A}ssessment from {M}edical {G}raph"","," ""One of the fundamental goals of artificial intelligence is to build computer-based expert systems. Inferring clinical diagnoses to generate a clinical assessment during a patient encounter is a crucial step towards building a medical diagnostic system. Previous works were mainly based on either medical domain-specific knowledge, or patients{'} prior diagnoses and clinical encounters. In this paper, we propose a novel model for automated clinical assessment generation (MCAG). MCAG is built on an innovative graph neural network, where rich clinical knowledge is incorporated into an end-to-end corpus-learning system. Our evaluation results against physician generated gold standard show that MCAG significantly improves the BLEU and rouge score compared with competitive baseline models. Further, physicians{'} evaluation showed that MCAG could generate high-quality assessments."",","{yang-yu-2020-generating,
    title = ""Generating {A}ccurate {E}lectronic {H}ealth {A}ssessment from {M}edical {G}raph"",
    author = ""Yang, Zhichao  and
      Yu, Hong"",
    editor = ""Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2020"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.findings-emnlp.336"",
    doi = ""10.18653/v1/2020.findings-emnlp.336"",
    pages = ""3764--3773"",
    abstract = ""One of the fundamental goals of artificial intelligence is to build computer-based expert systems. Inferring clinical diagnoses to generate a clinical assessment during a patient encounter is a crucial step towards building a medical diagnostic system. Previous works were mainly based on either medical domain-specific knowledge, or patients{'} prior diagnoses and clinical encounters. In this paper, we propose a novel model for automated clinical assessment generation (MCAG). MCAG is built on an innovative graph neural network, where rich clinical knowledge is incorporated into an end-to-end corpus-learning system. Our evaluation results against physician generated gold standard show that MCAG significantly improves the BLEU and rouge score compared with competitive baseline models. Further, physicians{'} evaluation showed that MCAG could generate high-quality assessments."",
}
@",medical domain,nlp,infer,generat,evalu,assess
" ""Generating Radiology Reports via Memory-driven Transformer"","," ""Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain. In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer. Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings."",","{chen-etal-2020-generating,
    title = ""Generating Radiology Reports via Memory-driven Transformer"",
    author = ""Chen, Zhihong  and
      Song, Yan  and
      Chang, Tsung-Hui  and
      Wan, Xiang"",
    editor = ""Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang"",
    booktitle = ""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.emnlp-main.112"",
    doi = ""10.18653/v1/2020.emnlp-main.112"",
    pages = ""1439--1449"",
    abstract = ""Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain. In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer. Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings."",
}
@",medical domain,natural language process,natural languag,language process,nlp,generat,evalu
" ""Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using {BERT}"","," ""The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual annotations by experts. In this work, we introduce a BERT-based approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations. We demonstrate superior performance of a biomedically pretrained BERT model first trained on annotations of a rule-based labeler and then finetuned on a small set of expert annotations augmented with automated backtranslation. We find that our final model, CheXbert, is able to outperform the previous best rules-based labeler with statistical significance, setting a new SOTA for report labeling on one of the largest datasets of chest x-rays."",","{smit-etal-2020-combining,
    title = ""Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using {BERT}"",
    author = ""Smit, Akshay  and
      Jain, Saahil  and
      Rajpurkar, Pranav  and
      Pareek, Anuj  and
      Ng, Andrew  and
      Lungren, Matthew"",
    editor = ""Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang"",
    booktitle = ""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.emnlp-main.117"",
    doi = ""10.18653/v1/2020.emnlp-main.117"",
    pages = ""1500--1519"",
    abstract = ""The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual annotations by experts. In this work, we introduce a BERT-based approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations. We demonstrate superior performance of a biomedically pretrained BERT model first trained on annotations of a rule-based labeler and then finetuned on a small set of expert annotations augmented with automated backtranslation. We find that our final model, CheXbert, is able to outperform the previous best rules-based labeler with statistical significance, setting a new SOTA for report labeling on one of the largest datasets of chest x-rays."",
}
@",medical domain,natural language process,natural languag,language process,translat,nlp,annotat
" ""{B}io{M}egatron: Larger Biomedical Domain Language Model"","," ""There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books. Yet, most works do not study the factors affecting each domain language application deeply. Additionally, the study of model size on domain-specific models has been mostly missing. We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer. We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications. We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of question answering, named entity recognition, and relation extraction. Code and checkpoints to reproduce our experiments are available at [github.com/NVIDIA/NeMo]."",","{shin-etal-2020-biomegatron,
    title = ""{B}io{M}egatron: Larger Biomedical Domain Language Model"",
    author = ""Shin, Hoo-Chang  and
      Zhang, Yang  and
      Bakhturina, Evelina  and
      Puri, Raul  and
      Patwary, Mostofa  and
      Shoeybi, Mohammad  and
      Mani, Raghav"",
    editor = ""Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang"",
    booktitle = ""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.emnlp-main.379"",
    doi = ""10.18653/v1/2020.emnlp-main.379"",
    pages = ""4700--4706"",
    abstract = ""There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books. Yet, most works do not study the factors affecting each domain language application deeply. Additionally, the study of model size on domain-specific models has been mostly missing. We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer. We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications. We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of question answering, named entity recognition, and relation extraction. Code and checkpoints to reproduce our experiments are available at [github.com/NVIDIA/NeMo]."",
}
@",medical domain,medical text,natural language process,natural languag,language process,nlp,relation extract,entity recognit,benchmark,evalu
" ""{BENNERD}: A Neural Named Entity Linking System for {COVID}-19"","," ""We present a biomedical entity linking (EL) system BENNERD that detects named enti- ties in text and links them to the unified medical language system (UMLS) knowledge base (KB) entries to facilitate the corona virus disease 2019 (COVID-19) research. BEN- NERD mainly covers biomedical domain, es- pecially new entity types (e.g., coronavirus, vi- ral proteins, immune responses) by address- ing CORD-NER dataset. It includes several NLP tools to process biomedical texts includ- ing tokenization, flat and nested entity recog- nition, and candidate generation and rank- ing for EL that have been pre-trained using the CORD-NER corpus. To the best of our knowledge, this is the first attempt that ad- dresses NER and EL on COVID-19-related entities, such as COVID-19 virus, potential vaccines, and spreading mechanism, that may benefit research on COVID-19. We release an online system to enable real-time entity annotation with linking for end users. We also release the manually annotated test set and CORD-NERD dataset for leveraging EL task. The BENNERD system is available at \url{https://aistairc.github.io/BENNERD/}."",","{sohrab-etal-2020-bennerd,
    title = ""{BENNERD}: A Neural Named Entity Linking System for {COVID}-19"",
    author = ""Sohrab, Mohammad Golam  and
      Duong, Khoa  and
      Miwa, Makoto  and
      Topi{\'c}, Goran  and
      Masami, Ikeda  and
      Hiroya, Takamura"",
    editor = ""Liu, Qun  and
      Schlangen, David"",
    booktitle = ""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"",
    month = oct,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.emnlp-demos.24"",
    doi = ""10.18653/v1/2020.emnlp-demos.24"",
    pages = ""182--188"",
    abstract = ""We present a biomedical entity linking (EL) system BENNERD that detects named enti- ties in text and links them to the unified medical language system (UMLS) knowledge base (KB) entries to facilitate the corona virus disease 2019 (COVID-19) research. BEN- NERD mainly covers biomedical domain, es- pecially new entity types (e.g., coronavirus, vi- ral proteins, immune responses) by address- ing CORD-NER dataset. It includes several NLP tools to process biomedical texts includ- ing tokenization, flat and nested entity recog- nition, and candidate generation and rank- ing for EL that have been pre-trained using the CORD-NER corpus. To the best of our knowledge, this is the first attempt that ad- dresses NER and EL on COVID-19-related entities, such as COVID-19 virus, potential vaccines, and spreading mechanism, that may benefit research on COVID-19. We release an online system to enable real-time entity annotation with linking for end users. We also release the manually annotated test set and CORD-NERD dataset for leveraging EL task. The BENNERD system is available at \url{https://aistairc.github.io/BENNERD/}."",
}
@",medical domain,medical text,natural language process,natural languag,language process,nlp,generat,annotat
" ""Fine-Grained Error Analysis on {E}nglish-to-{J}apanese Machine Translation in the Medical Domain"","," ""We performed a detailed error analysis in domain-specific neural machine translation (NMT) for the English and Japanese language pair with fine-grained manual annotation. Despite its importance for advancing NMT technologies, research on the performance of domain-specific NMT and non-European languages has been limited. In this study, we designed an error typology based on the error types that were typically generated by NMT systems and might cause significant impact in technical translations: {``}Addition,{''} {``}Omission,{''} {``}Mistranslation,{''} {``}Grammar,{''} and {``}Terminology.{''} The error annotation was targeted to the medical domain and was performed by experienced professional translators specialized in medicine under careful quality control. The annotation detected 4,912 errors on 2,480 sentences, and the frequency and distribution of errors were analyzed. We found that the major errors in NMT were {``}Mistranslation{''} and {``}Terminology{''} rather than {``}Addition{''} and {``}Omission,{''} which have been reported as typical problems of NMT. Interestingly, more errors occurred in documents for professionals compared with those for the general public. The results of our annotation work will be published as a parallel corpus with error labels, which are expected to contribute to developing better NMT models, automatic evaluation metrics, and quality estimation models."",","{hayakawa-arase-2020-fine,
    title = ""Fine-Grained Error Analysis on {E}nglish-to-{J}apanese Machine Translation in the Medical Domain"",
    author = ""Hayakawa, Takeshi  and
      Arase, Yuki"",
    editor = ""Martins, Andr{\'e}  and
      Moniz, Helena  and
      Fumega, Sara  and
      Martins, Bruno  and
      Batista, Fernando  and
      Coheur, Luisa  and
      Parra, Carla  and
      Trancoso, Isabel  and
      Turchi, Marco  and
      Bisazza, Arianna  and
      Moorkens, Joss  and
      Guerberof, Ana  and
      Nurminen, Mary  and
      Marg, Lena  and
      Forcada, Mikel L."",
    booktitle = ""Proceedings of the 22nd Annual Conference of the European Association for Machine Translation"",
    month = nov,
    year = ""2020"",
    address = ""Lisboa, Portugal"",
    publisher = ""European Association for Machine Translation"",
    url = ""https://aclanthology.org/2020.eamt-1.17"",
    pages = ""155--164"",
    abstract = ""We performed a detailed error analysis in domain-specific neural machine translation (NMT) for the English and Japanese language pair with fine-grained manual annotation. Despite its importance for advancing NMT technologies, research on the performance of domain-specific NMT and non-European languages has been limited. In this study, we designed an error typology based on the error types that were typically generated by NMT systems and might cause significant impact in technical translations: {``}Addition,{''} {``}Omission,{''} {``}Mistranslation,{''} {``}Grammar,{''} and {``}Terminology.{''} The error annotation was targeted to the medical domain and was performed by experienced professional translators specialized in medicine under careful quality control. The annotation detected 4,912 errors on 2,480 sentences, and the frequency and distribution of errors were analyzed. We found that the major errors in NMT were {``}Mistranslation{''} and {``}Terminology{''} rather than {``}Addition{''} and {``}Omission,{''} which have been reported as typical problems of NMT. Interestingly, more errors occurred in documents for professionals compared with those for the general public. The results of our annotation work will be published as a parallel corpus with error labels, which are expected to contribute to developing better NMT models, automatic evaluation metrics, and quality estimation models."",
}
@",medical domain,translat,generat,annotat,evalu
" ""Learning to ground medical text in a 3{D} human atlas"","," ""In this paper, we develop a method for grounding medical text into a physically meaningful and interpretable space corresponding to a human atlas. We build on text embedding architectures such as Bert and introduce a loss function that allows us to reason about the semantic and spatial relatedness of medical texts by learning a projection of the embedding into a 3D space representing the human body. We quantitatively and qualitatively demonstrate that our proposed method learns a context sensitive and spatially aware mapping, in both the inter-organ and intra-organ sense, using a large scale medical text dataset from the {``}Large-scale online biomedical semantic indexing{''} track of the 2020 BioASQ challenge. We extend our approach to a self-supervised setting, and find it to be competitive with a classification based method, and a fully supervised variant of approach."",","{grujicic-etal-2020-learning,
    title = ""Learning to ground medical text in a 3{D} human atlas"",
    author = ""Grujicic, Dusan  and
      Radevski, Gorjan  and
      Tuytelaars, Tinne  and
      Blaschko, Matthew"",
    editor = ""Fern{\'a}ndez, Raquel  and
      Linzen, Tal"",
    booktitle = ""Proceedings of the 24th Conference on Computational Natural Language Learning"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.conll-1.23"",
    doi = ""10.18653/v1/2020.conll-1.23"",
    pages = ""302--312"",
    abstract = ""In this paper, we develop a method for grounding medical text into a physically meaningful and interpretable space corresponding to a human atlas. We build on text embedding architectures such as Bert and introduce a loss function that allows us to reason about the semantic and spatial relatedness of medical texts by learning a projection of the embedding into a 3D space representing the human body. We quantitatively and qualitatively demonstrate that our proposed method learns a context sensitive and spatially aware mapping, in both the inter-organ and intra-organ sense, using a large scale medical text dataset from the {``}Large-scale online biomedical semantic indexing{''} track of the 2020 BioASQ challenge. We extend our approach to a self-supervised setting, and find it to be competitive with a classification based method, and a fully supervised variant of approach."",
}
@",medical text,natural languag,semant,challeng,track
" ""Named Entity Recognition for {C}hinese biomedical patents"","," ""There is a large body of work on Biomedical Entity Recognition (Bio-NER) for English but there have only been a few attempts addressing NER for Chinese biomedical texts. Because of the growing amount of Chinese biomedical discoveries being patented, and lack of NER models for patent data, we train and evaluate NER models for the analysis of Chinese biomedical patent data, based on BERT. By doing so, we show the value and potential of this domain-specific NER task. For the evaluation of our methods we built our own Chinese biomedical patents NER dataset, and our optimized model achieved an F1 score of 0.54{\mbox{$\pm$}}0.15. Further biomedical analysis indicates that our solution can help detecting meaningful biomedical entities and novel gene-gene interactions, with limited labeled data, training time and computing power."",","{hu-verberne-2020-named,
    title = ""Named Entity Recognition for {C}hinese biomedical patents"",
    author = ""Hu, Yuting  and
      Verberne, Suzan"",
    editor = ""Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing"",
    booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",
    month = dec,
    year = ""2020"",
    address = ""Barcelona, Spain (Online)"",
    publisher = ""International Committee on Computational Linguistics"",
    url = ""https://aclanthology.org/2020.coling-main.54"",
    doi = ""10.18653/v1/2020.coling-main.54"",
    pages = ""627--637"",
    abstract = ""There is a large body of work on Biomedical Entity Recognition (Bio-NER) for English but there have only been a few attempts addressing NER for Chinese biomedical texts. Because of the growing amount of Chinese biomedical discoveries being patented, and lack of NER models for patent data, we train and evaluate NER models for the analysis of Chinese biomedical patent data, based on BERT. By doing so, we show the value and potential of this domain-specific NER task. For the evaluation of our methods we built our own Chinese biomedical patents NER dataset, and our optimized model achieved an F1 score of 0.54{\mbox{$\pm$}}0.15. Further biomedical analysis indicates that our solution can help detecting meaningful biomedical entities and novel gene-gene interactions, with limited labeled data, training time and computing power."",
}
@",medical text,entity recognit,evalu
" ""{B}io{M}ed{BERT}: A Pre-trained Biomedical Language Model for {QA} and {IR}"","," ""The SARS-CoV-2 (COVID-19) pandemic spotlighted the importance of moving quickly with biomedical research. However, as the number of biomedical research papers continue to increase, the task of finding relevant articles to answer pressing questions has become significant. In this work, we propose a textual data mining tool that supports literature search to accelerate the work of researchers in the biomedical domain. We achieve this by building a neural-based deep contextual understanding model for Question-Answering (QA) and Information Retrieval (IR) tasks. We also leverage the new BREATHE dataset which is one of the largest available datasets of biomedical research literature, containing abstracts and full-text articles from ten different biomedical literature sources on which we pre-train our BioMedBERT model. Our work achieves state-of-the-art results on the QA fine-tuning task on BioASQ 5b, 6b and 7b datasets. In addition, we observe superior relevant results when BioMedBERT embeddings are used with Elasticsearch for the Information Retrieval task on the intelligently formulated BioASQ dataset. We believe our diverse dataset and our unique model architecture are what led us to achieve the state-of-the-art results for QA and IR tasks."",","{chakraborty-etal-2020-biomedbert,
    title = ""{B}io{M}ed{BERT}: A Pre-trained Biomedical Language Model for {QA} and {IR}"",
    author = ""Chakraborty, Souradip  and
      Bisong, Ekaba  and
      Bhatt, Shweta  and
      Wagner, Thomas  and
      Elliott, Riley  and
      Mosconi, Francesco"",
    editor = ""Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing"",
    booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",
    month = dec,
    year = ""2020"",
    address = ""Barcelona, Spain (Online)"",
    publisher = ""International Committee on Computational Linguistics"",
    url = ""https://aclanthology.org/2020.coling-main.59"",
    doi = ""10.18653/v1/2020.coling-main.59"",
    pages = ""669--679"",
    abstract = ""The SARS-CoV-2 (COVID-19) pandemic spotlighted the importance of moving quickly with biomedical research. However, as the number of biomedical research papers continue to increase, the task of finding relevant articles to answer pressing questions has become significant. In this work, we propose a textual data mining tool that supports literature search to accelerate the work of researchers in the biomedical domain. We achieve this by building a neural-based deep contextual understanding model for Question-Answering (QA) and Information Retrieval (IR) tasks. We also leverage the new BREATHE dataset which is one of the largest available datasets of biomedical research literature, containing abstracts and full-text articles from ten different biomedical literature sources on which we pre-train our BioMedBERT model. Our work achieves state-of-the-art results on the QA fine-tuning task on BioASQ 5b, 6b and 7b datasets. In addition, we observe superior relevant results when BioMedBERT embeddings are used with Elasticsearch for the Information Retrieval task on the intelligently formulated BioASQ dataset. We believe our diverse dataset and our unique model architecture are what led us to achieve the state-of-the-art results for QA and IR tasks."",
}
@",medical domain,information retriev,question-answ
" ""{F}rench Biomedical Text Simplification: When Small and Precise Helps"","," ""We present experiments on biomedical text simplification in French. We use two kinds of corpora {--} parallel sentences extracted from existing health comparable corpora in French and WikiLarge corpus translated from English to French {--} and a lexicon that associates medical terms with paraphrases. Then, we train neural models on these parallel corpora using different ratios of general and specialized sentences. We evaluate the results with BLEU, SARI and Kandel scores. The results point out that little specialized data helps significantly the simplification."",","{cardon-grabar-2020-french,
    title = ""{F}rench Biomedical Text Simplification: When Small and Precise Helps"",
    author = ""Cardon, R{\'e}mi  and
      Grabar, Natalia"",
    editor = ""Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing"",
    booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",
    month = dec,
    year = ""2020"",
    address = ""Barcelona, Spain (Online)"",
    publisher = ""International Committee on Computational Linguistics"",
    url = ""https://aclanthology.org/2020.coling-main.62"",
    doi = ""10.18653/v1/2020.coling-main.62"",
    pages = ""710--716"",
    abstract = ""We present experiments on biomedical text simplification in French. We use two kinds of corpora {--} parallel sentences extracted from existing health comparable corpora in French and WikiLarge corpus translated from English to French {--} and a lexicon that associates medical terms with paraphrases. Then, we train neural models on these parallel corpora using different ratios of general and specialized sentences. We evaluate the results with BLEU, SARI and Kandel scores. The results point out that little specialized data helps significantly the simplification."",
}
@",medical text,translat,evalu
" ""Medical Knowledge-enriched Textual Entailment Framework"","," ""One of the cardinal tasks in achieving robust medical question answering systems is textual entailment. The existing approaches make use of an ensemble of pre-trained language models or data augmentation, often to clock higher numbers on the validation metrics. However, two major shortcomings impede higher success in identifying entailment: (1) understanding the focus/intent of the question and (2) ability to utilize the real-world background knowledge to capture the con-text beyond the sentence. In this paper, we present a novel Medical Knowledge-Enriched Textual Entailment framework that allows the model to acquire a semantic and global representation of the input medical text with the help of a relevant domain-specific knowledge graph. We evaluate our framework on the benchmark MEDIQA-RQE dataset and manifest that the use of knowledge-enriched dual-encoding mechanism help in achieving an absolute improvement of 8.27{\%} over SOTA language models."",","{yadav-etal-2020-medical,
    title = ""Medical Knowledge-enriched Textual Entailment Framework"",
    author = ""Yadav, Shweta  and
      Pallagani, Vishal  and
      Sheth, Amit"",
    editor = ""Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing"",
    booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",
    month = dec,
    year = ""2020"",
    address = ""Barcelona, Spain (Online)"",
    publisher = ""International Committee on Computational Linguistics"",
    url = ""https://aclanthology.org/2020.coling-main.161"",
    doi = ""10.18653/v1/2020.coling-main.161"",
    pages = ""1795--1801"",
    abstract = ""One of the cardinal tasks in achieving robust medical question answering systems is textual entailment. The existing approaches make use of an ensemble of pre-trained language models or data augmentation, often to clock higher numbers on the validation metrics. However, two major shortcomings impede higher success in identifying entailment: (1) understanding the focus/intent of the question and (2) ability to utilize the real-world background knowledge to capture the con-text beyond the sentence. In this paper, we present a novel Medical Knowledge-Enriched Textual Entailment framework that allows the model to acquire a semantic and global representation of the input medical text with the help of a relevant domain-specific knowledge graph. We evaluate our framework on the benchmark MEDIQA-RQE dataset and manifest that the use of knowledge-enriched dual-encoding mechanism help in achieving an absolute improvement of 8.27{\%} over SOTA language models."",
}
@",medical text,semant,benchmark,evalu
" ""{T}rain{X} {--} Named Entity Linking with Active Sampling and Bi-Encoders"","," ""We demonstrate TrainX, a system for Named Entity Linking for medical experts. It combines state-of-the-art entity recognition and linking architectures, such as Flair and fine-tuned Bi-Encoders based on BERT, with an easy-to-use interface for healthcare professionals. We support medical experts in annotating training data by using active sampling strategies to forward informative samples to the annotator. We demonstrate that our model is capable of linking against large knowledge bases, such as UMLS (3.6 million entities), and supporting zero-shot cases, where the linker has never seen the entity before. Those zero-shot capabilities help to mitigate the problem of rare and expensive training data that is a common issue in the medical domain."",","{oberhauser-etal-2020-trainx,
    title = ""{T}rain{X} {--} Named Entity Linking with Active Sampling and Bi-Encoders"",
    author = {Oberhauser, Tom  and
      Bischoff, Tim  and
      Brendel, Karl  and
      Menke, Maluna  and
      Klatt, Tobias  and
      Siu, Amy  and
      Gers, Felix Alexander  and
      L{\""o}ser, Alexander},
    editor = ""Ptaszynski, Michal  and
      Ziolko, Bartosz"",
    booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics: System Demonstrations"",
    month = dec,
    year = ""2020"",
    address = ""Barcelona, Spain (Online)"",
    publisher = ""International Committee on Computational Linguistics (ICCL)"",
    url = ""https://aclanthology.org/2020.coling-demos.12"",
    doi = ""10.18653/v1/2020.coling-demos.12"",
    pages = ""64--69"",
    abstract = ""We demonstrate TrainX, a system for Named Entity Linking for medical experts. It combines state-of-the-art entity recognition and linking architectures, such as Flair and fine-tuned Bi-Encoders based on BERT, with an easy-to-use interface for healthcare professionals. We support medical experts in annotating training data by using active sampling strategies to forward informative samples to the annotator. We demonstrate that our model is capable of linking against large knowledge bases, such as UMLS (3.6 million entities), and supporting zero-shot cases, where the linker has never seen the entity before. Those zero-shot capabilities help to mitigate the problem of rare and expensive training data that is a common issue in the medical domain."",
}
@",medical domain,entity recognit,annotat
" ""Information Extraction from {S}wedish Medical Prescriptions with Sig-Transformer Encoder"","," ""Relying on large pretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) for encoding and adding a simple prediction layer has led to impressive performance in many clinical natural language processing (NLP) tasks. In this work, we present a novel extension to the Transformer architecture, by incorporating signature transform with the self-attention model. This architecture is added between embedding and prediction layers. Experiments on a new Swedish prescription data show the proposed architecture to be superior in two of the three information extraction tasks, comparing to baseline models. Finally, we evaluate two different embedding approaches between applying Multilingual BERT and translating the Swedish text to English then encode with a BERT model pretrained on clinical notes."",","{pougue-biyong-etal-2020-information,
    title = ""Information Extraction from {S}wedish Medical Prescriptions with Sig-Transformer Encoder"",
    author = ""Pougu{\'e} Biyong, John  and
      Wang, Bo  and
      Lyons, Terry  and
      Nevado-Holgado, Alejo"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 3rd Clinical Natural Language Processing Workshop"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.clinicalnlp-1.5"",
    doi = ""10.18653/v1/2020.clinicalnlp-1.5"",
    pages = ""41--54"",
    abstract = ""Relying on large pretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) for encoding and adding a simple prediction layer has led to impressive performance in many clinical natural language processing (NLP) tasks. In this work, we present a novel extension to the Transformer architecture, by incorporating signature transform with the self-attention model. This architecture is added between embedding and prediction layers. Experiments on a new Swedish prescription data show the proposed architecture to be superior in two of the three information extraction tasks, comparing to baseline models. Finally, we evaluate two different embedding approaches between applying Multilingual BERT and translating the Swedish text to English then encode with a BERT model pretrained on clinical notes."",
}
@",clinical not,natural language process,natural languag,language process,translat,nlp,evalu
" ""Evaluation of Transfer Learning for Adverse Drug Event ({ADE}) and Medication Entity Extraction"","," ""We evaluate several biomedical contextual embeddings (based on BERT, ELMo, and Flair) for the detection of medication entities such as Drugs and Adverse Drug Events (ADE) from Electronic Health Records (EHR) using the 2018 ADE and Medication Extraction (Track 2) n2c2 data-set. We identify best practices for transfer learning, such as language-model fine-tuning and scalar mix. Our transfer learning models achieve strong performance in the overall task (F1","{narayanan-etal-2020-evaluation,
    title = ""Evaluation of Transfer Learning for Adverse Drug Event ({ADE}) and Medication Entity Extraction"",
    author = ""Narayanan, Sankaran  and
      Mannam, Kaivalya  and
      Rajan, Sreeranga P  and
      Rangan, P Venkat"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 3rd Clinical Natural Language Processing Workshop"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.clinicalnlp-1.6"",
    doi = ""10.18653/v1/2020.clinicalnlp-1.6"",
    pages = ""55--64"",
    abstract = ""We evaluate several biomedical contextual embeddings (based on BERT, ELMo, and Flair) for the detection of medication entities such as Drugs and Adverse Drug Events (ADE) from Electronic Health Records (EHR) using the 2018 ADE and Medication Extraction (Track 2) n2c2 data-set. We identify best practices for transfer learning, such as language-model fine-tuning and scalar mix. Our transfer learning models achieve strong performance in the overall task (F1=92.91{\%}) as well as in ADE identification (F1=53.08{\%}). Flair-based embeddings out-perform in the identification of context-dependent entities such as ADE. BERT-based embeddings out-perform in recognizing clinical terminology such as Drug and Form entities. ELMo-based embeddings deliver competitive performance in all entities. We develop a sentence-augmentation method for enhanced ADE identification benefiting BERT-based and ELMo-based models by up to 3.13{\%} in F1 gains. Finally, we show that a simple ensemble of these models out-paces most current methods in ADE extraction (F1=55.77{\%})."",
}
@",electronic health record,health record,natural language process,natural languag,language process,nlp,evalu,track
" ""{B}io{BERT}pt - A {P}ortuguese Neural Language Model for Clinical Named Entity Recognition"","," ""With the growing number of electronic health record data, clinical NLP tasks have become increasingly relevant to unlock valuable information from unstructured clinical text. Although the performance of downstream NLP tasks, such as named-entity recognition (NER), in English corpus has recently improved by contextualised language models, less research is available for clinical texts in low resource languages. Our goal is to assess a deep contextual embedding model for Portuguese, so called BioBERTpt, to support clinical and biomedical NER. We transfer learned information encoded in a multilingual-BERT model to a corpora of clinical narratives and biomedical-scientific papers in Brazilian Portuguese. To evaluate the performance of BioBERTpt, we ran NER experiments on two annotated corpora containing clinical narratives and compared the results with existing BERT models. Our in-domain model outperformed the baseline model in F1-score by 2.72{\%}, achieving higher performance in 11 out of 13 assessed entities. We demonstrate that enriching contextual embedding models with domain literature can play an important role in improving performance for specific NLP tasks. The transfer learning process enhanced the Portuguese biomedical NER model by reducing the necessity of labeled data and the demand for retraining a whole new model."",","{schneider-etal-2020-biobertpt,
    title = ""{B}io{BERT}pt - A {P}ortuguese Neural Language Model for Clinical Named Entity Recognition"",
    author = ""Schneider, Elisa Terumi Rubel  and
      de Souza, Jo{\~a}o Vitor Andrioli  and
      Knafou, Julien  and
      Oliveira, Lucas Emanuel Silva e  and
      Copara, Jenny  and
      Gumiel, Yohan Bonescki  and
      Oliveira, Lucas Ferro Antunes de  and
      Paraiso, Emerson Cabrera  and
      Teodoro, Douglas  and
      Barra, Cl{\'a}udia Maria Cabral Moro"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 3rd Clinical Natural Language Processing Workshop"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.clinicalnlp-1.7"",
    doi = ""10.18653/v1/2020.clinicalnlp-1.7"",
    pages = ""65--72"",
    abstract = ""With the growing number of electronic health record data, clinical NLP tasks have become increasingly relevant to unlock valuable information from unstructured clinical text. Although the performance of downstream NLP tasks, such as named-entity recognition (NER), in English corpus has recently improved by contextualised language models, less research is available for clinical texts in low resource languages. Our goal is to assess a deep contextual embedding model for Portuguese, so called BioBERTpt, to support clinical and biomedical NER. We transfer learned information encoded in a multilingual-BERT model to a corpora of clinical narratives and biomedical-scientific papers in Brazilian Portuguese. To evaluate the performance of BioBERTpt, we ran NER experiments on two annotated corpora containing clinical narratives and compared the results with existing BERT models. Our in-domain model outperformed the baseline model in F1-score by 2.72{\%}, achieving higher performance in 11 out of 13 assessed entities. We demonstrate that enriching contextual embedding models with domain literature can play an important role in improving performance for specific NLP tasks. The transfer learning process enhanced the Portuguese biomedical NER model by reducing the necessity of labeled data and the demand for retraining a whole new model."",
}
@",electronic health record,health record,clinical narr,clinical text,natural language process,natural languag,language process,nlp,entity recognit,annotat,evalu,assess
" ""Clinical {XLN}et: Modeling Sequential Clinical Notes and Predicting Prolonged Mechanical Ventilation"","," ""Clinical notes contain rich information, which is relatively unexploited in predictive modeling compared to structured data. In this work, we developed a new clinical text representation Clinical XLNet that leverages the temporal information of the sequence of the notes. We evaluated our models on prolonged mechanical ventilation prediction problem and our experiments demonstrated that Clinical XLNet outperforms the best baselines consistently. The models and scripts are made publicly available."",","{huang-etal-2020-clinical,
    title = ""Clinical {XLN}et: Modeling Sequential Clinical Notes and Predicting Prolonged Mechanical Ventilation"",
    author = ""Huang, Kexin  and
      Singh, Abhishek  and
      Chen, Sitong  and
      Moseley, Edward  and
      Deng, Chih-Ying  and
      George, Naomi  and
      Lindvall, Charolotta"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 3rd Clinical Natural Language Processing Workshop"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.clinicalnlp-1.11"",
    doi = ""10.18653/v1/2020.clinicalnlp-1.11"",
    pages = ""94--100"",
    abstract = ""Clinical notes contain rich information, which is relatively unexploited in predictive modeling compared to structured data. In this work, we developed a new clinical text representation Clinical XLNet that leverages the temporal information of the sequence of the notes. We evaluated our models on prolonged mechanical ventilation prediction problem and our experiments demonstrated that Clinical XLNet outperforms the best baselines consistently. The models and scripts are made publicly available."",
}
@",clinical not,clinical text,natural language process,natural languag,language process,nlp,evalu
" ""Automatic recognition of abdominal lymph nodes from clinical text"","," ""Lymph node status plays a pivotal role in the treatment of cancer. The extraction of lymph nodes from radiology text reports enables large-scale training of lymph node detection on MRI. In this work, we first propose an ontology of 41 types of abdominal lymph nodes with a hierarchical relationship. We then introduce an end-to-end approach based on the combination of rules and transformer-based methods to detect these abdominal lymph node mentions and classify their types from the MRI radiology reports. We demonstrate the superior performance of a model fine-tuned on MRI reports using BlueBERT, called MriBERT. We find that MriBERT outperforms the rule-based labeler (0.957 vs 0.644 in micro weighted F1-score) as well as other BERT-based variations (0.913 - 0.928). We make the code and MriBERT publicly available at \url{https://github.com/ncbi-nlp/bluebert}, with the hope that this method can facilitate the development of medical report annotators to produce labels from scratch at scale."",","{peng-etal-2020-automatic,
    title = ""Automatic recognition of abdominal lymph nodes from clinical text"",
    author = ""Peng, Yifan  and
      Lee, Sungwon  and
      Elton, Daniel C.  and
      Shen, Thomas  and
      Tang, Yu-xing  and
      Chen, Qingyu  and
      Wang, Shuai  and
      Zhu, Yingying  and
      Summers, Ronald  and
      Lu, Zhiyong"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 3rd Clinical Natural Language Processing Workshop"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.clinicalnlp-1.12"",
    doi = ""10.18653/v1/2020.clinicalnlp-1.12"",
    pages = ""101--110"",
    abstract = ""Lymph node status plays a pivotal role in the treatment of cancer. The extraction of lymph nodes from radiology text reports enables large-scale training of lymph node detection on MRI. In this work, we first propose an ontology of 41 types of abdominal lymph nodes with a hierarchical relationship. We then introduce an end-to-end approach based on the combination of rules and transformer-based methods to detect these abdominal lymph node mentions and classify their types from the MRI radiology reports. We demonstrate the superior performance of a model fine-tuned on MRI reports using BlueBERT, called MriBERT. We find that MriBERT outperforms the rule-based labeler (0.957 vs 0.644 in micro weighted F1-score) as well as other BERT-based variations (0.913 - 0.928). We make the code and MriBERT publicly available at \url{https://github.com/ncbi-nlp/bluebert}, with the hope that this method can facilitate the development of medical report annotators to produce labels from scratch at scale."",
}
@",clinical text,natural language process,natural languag,language process,nlp,annotat
" ""Relative and Incomplete Time Expression Anchoring for Clinical Text"","," ""Extracting and modeling temporal information in clinical text is an important element for developing timelines and disease trajectories. Time information in written text varies in preciseness and explicitness, posing challenges for NLP approaches that aim to accurately anchor temporal information on a timeline. Relative and incomplete time expressions (RI-Timexes) are expressions that require additional information for their temporal anchor to be resolved, but few studies have addressed this challenge specifically. In this study, we aimed to reproduce and verify a classification approach for identifying anchor dates and relations in clinical text, and propose a novel relation classification approach for this task."",","{dupuis-etal-2020-relative,
    title = ""Relative and Incomplete Time Expression Anchoring for Clinical Text"",
    author = ""Dupuis, Louise  and
      Bergou, Nicol  and
      Tissot, Hegler  and
      Velupillai, Sumithra"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 3rd Clinical Natural Language Processing Workshop"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.clinicalnlp-1.14"",
    doi = ""10.18653/v1/2020.clinicalnlp-1.14"",
    pages = ""117--129"",
    abstract = ""Extracting and modeling temporal information in clinical text is an important element for developing timelines and disease trajectories. Time information in written text varies in preciseness and explicitness, posing challenges for NLP approaches that aim to accurately anchor temporal information on a timeline. Relative and incomplete time expressions (RI-Timexes) are expressions that require additional information for their temporal anchor to be resolved, but few studies have addressed this challenge specifically. In this study, we aimed to reproduce and verify a classification approach for identifying anchor dates and relations in clinical text, and propose a novel relation classification approach for this task."",
}
@",clinical text,natural language process,natural languag,language process,nlp,relation classif,challeng
" ""{M}e{DAL}: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining"","," ""One of the biggest challenges that prohibit the use of many current NLP methods in clinical settings is the availability of public datasets. In this work, we present MeDAL, a large medical text dataset curated for abbreviation disambiguation, designed for natural language understanding pre-training in the medical domain. We pre-trained several models of common architectures on this dataset and empirically showed that such pre-training leads to improved performance and convergence speed when fine-tuning on downstream medical tasks."",","{wen-etal-2020-medal,
    title = ""{M}e{DAL}: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining"",
    author = ""Wen, Zhi  and
      Lu, Xing Han  and
      Reddy, Siva"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 3rd Clinical Natural Language Processing Workshop"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.clinicalnlp-1.15"",
    doi = ""10.18653/v1/2020.clinicalnlp-1.15"",
    pages = ""130--135"",
    abstract = ""One of the biggest challenges that prohibit the use of many current NLP methods in clinical settings is the availability of public datasets. In this work, we present MeDAL, a large medical text dataset curated for abbreviation disambiguation, designed for natural language understanding pre-training in the medical domain. We pre-trained several models of common architectures on this dataset and empirically showed that such pre-training leads to improved performance and convergence speed when fine-tuning on downstream medical tasks."",
}
@",medical domain,medical text,natural language process,natural languag,language process,nlp,challeng,public data,public dataset,public dataset
" ""Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art"","," ""A large array of pretrained models are available to the biomedical NLP (BioNLP) community. Finding the best model for a particular task can be difficult and time-consuming. For many applications in the biomedical and clinical domains, it is crucial that models can be built quickly and are highly accurate. We present a large-scale study across 18 established biomedical and clinical NLP tasks to determine which of several popular open-source biomedical and clinical NLP models work well in different settings. Furthermore, we apply recent advances in pretraining to train new biomedical language models, and carefully investigate the effect of various design choices on downstream performance. Our best models perform well in all of our benchmarks, and set new State-of-the-Art in 9 tasks. We release these models in the hope that they can help the community to speed up and increase the accuracy of BioNLP and text mining applications."",","{lewis-etal-2020-pretrained,
    title = ""Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art"",
    author = ""Lewis, Patrick  and
      Ott, Myle  and
      Du, Jingfei  and
      Stoyanov, Veselin"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 3rd Clinical Natural Language Processing Workshop"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.clinicalnlp-1.17"",
    doi = ""10.18653/v1/2020.clinicalnlp-1.17"",
    pages = ""146--157"",
    abstract = ""A large array of pretrained models are available to the biomedical NLP (BioNLP) community. Finding the best model for a particular task can be difficult and time-consuming. For many applications in the biomedical and clinical domains, it is crucial that models can be built quickly and are highly accurate. We present a large-scale study across 18 established biomedical and clinical NLP tasks to determine which of several popular open-source biomedical and clinical NLP models work well in different settings. Furthermore, we apply recent advances in pretraining to train new biomedical language models, and carefully investigate the effect of various design choices on downstream performance. Our best models perform well in all of our benchmarks, and set new State-of-the-Art in 9 tasks. We release these models in the hope that they can help the community to speed up and increase the accuracy of BioNLP and text mining applications."",
}
@",clinical domain,natural language process,natural languag,language process,nlp,benchmark
" ""Assessment of {D}istil{BERT} performance on Named Entity Recognition task for the detection of Protected Health Information and medical concepts"","," ""Bidirectional Encoder Representations from Transformers (BERT) models achieve state-of-the-art performance on a number of Natural Language Processing tasks. However, their model size on disk often exceeds 1 GB and the process of fine-tuning them and using them to run inference consumes significant hardware resources and runtime. This makes them hard to deploy to production environments. This paper fine-tunes DistilBERT, a lightweight deep learning model, on medical text for the named entity recognition task of Protected Health Information (PHI) and medical concepts. This work provides a full assessment of the performance of DistilBERT in comparison with BERT models that were pre-trained on medical text. For Named Entity Recognition task of PHI, DistilBERT achieved almost the same results as medical versions of BERT in terms of F1 score at almost half the runtime and consuming approximately half the disk space. On the other hand, for the detection of medical concepts, DistilBERT{'}s F1 score was lower by 4 points on average than medical BERT variants."",","{abadeer-2020-assessment,
    title = ""Assessment of {D}istil{BERT} performance on Named Entity Recognition task for the detection of Protected Health Information and medical concepts"",
    author = ""Abadeer, Macarious"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 3rd Clinical Natural Language Processing Workshop"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.clinicalnlp-1.18"",
    doi = ""10.18653/v1/2020.clinicalnlp-1.18"",
    pages = ""158--167"",
    abstract = ""Bidirectional Encoder Representations from Transformers (BERT) models achieve state-of-the-art performance on a number of Natural Language Processing tasks. However, their model size on disk often exceeds 1 GB and the process of fine-tuning them and using them to run inference consumes significant hardware resources and runtime. This makes them hard to deploy to production environments. This paper fine-tunes DistilBERT, a lightweight deep learning model, on medical text for the named entity recognition task of Protected Health Information (PHI) and medical concepts. This work provides a full assessment of the performance of DistilBERT in comparison with BERT models that were pre-trained on medical text. For Named Entity Recognition task of PHI, DistilBERT achieved almost the same results as medical versions of BERT in terms of F1 score at almost half the runtime and consuming approximately half the disk space. On the other hand, for the detection of medical concepts, DistilBERT{'}s F1 score was lower by 4 points on average than medical BERT variants."",
}
@",medical text,natural language process,natural languag,language process,nlp,infer,entity recognit,assess
" ""Extracting Relations between Radiotherapy Treatment Details"","," ""We present work on extraction of radiotherapy treatment information from the clinical narrative in the electronic medical records. Radiotherapy is a central component of the treatment of most solid cancers. Its details are described in non-standardized fashions using jargon not found in other medical specialties, complicating the already difficult task of manual data extraction. We examine the performance of several state-of-the-art neural methods for relation extraction of radiotherapy treatment details, with a goal of automating detailed information extraction. The neural systems perform at 0.82-0.88 macro-average F1, which approximates or in some cases exceeds the inter-annotator agreement. To the best of our knowledge, this is the first effort to develop models for radiotherapy relation extraction and one of the few efforts for relation extraction to describe cancer treatment in general."",","{bitterman-etal-2020-extracting,
    title = ""Extracting Relations between Radiotherapy Treatment Details"",
    author = ""Bitterman, Danielle  and
      Miller, Timothy  and
      Harris, David  and
      Lin, Chen  and
      Finan, Sean  and
      Warner, Jeremy  and
      Mak, Raymond  and
      Savova, Guergana"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 3rd Clinical Natural Language Processing Workshop"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.clinicalnlp-1.21"",
    doi = ""10.18653/v1/2020.clinicalnlp-1.21"",
    pages = ""194--200"",
    abstract = ""We present work on extraction of radiotherapy treatment information from the clinical narrative in the electronic medical records. Radiotherapy is a central component of the treatment of most solid cancers. Its details are described in non-standardized fashions using jargon not found in other medical specialties, complicating the already difficult task of manual data extraction. We examine the performance of several state-of-the-art neural methods for relation extraction of radiotherapy treatment details, with a goal of automating detailed information extraction. The neural systems perform at 0.82-0.88 macro-average F1, which approximates or in some cases exceeds the inter-annotator agreement. To the best of our knowledge, this is the first effort to develop models for radiotherapy relation extraction and one of the few efforts for relation extraction to describe cancer treatment in general."",
}
@",medical record,clinical narr,natural language process,natural languag,language process,nlp,relation extract,annotat
" ""{PHICON}: Improving Generalization of Clinical Text De-identification Models via Data Augmentation"","," ""De-identification is the task of identifying protected health information (PHI) in the clinical text. Existing neural de-identification models often fail to generalize to a new dataset. We propose a simple yet effective data augmentation method PHICON to alleviate the generalization issue. PHICON consists of PHI augmentation and Context augmentation, which creates augmented training corpora by replacing PHI entities with named-entities sampled from external sources, and by changing background context with synonym replacement or random word insertion, respectively. Experimental results on the i2b2 2006 and 2014 de-identification challenge datasets show that PHICON can help three selected de-identification models boost F1-score (by at most 8.6{\%}) on cross-dataset test setting. We also discuss how much augmentation to use and how each augmentation method influences the performance."",","{yue-zhou-2020-phicon,
    title = ""{PHICON}: Improving Generalization of Clinical Text De-identification Models via Data Augmentation"",
    author = ""Yue, Xiang  and
      Zhou, Shuang"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 3rd Clinical Natural Language Processing Workshop"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.clinicalnlp-1.23"",
    doi = ""10.18653/v1/2020.clinicalnlp-1.23"",
    pages = ""209--214"",
    abstract = ""De-identification is the task of identifying protected health information (PHI) in the clinical text. Existing neural de-identification models often fail to generalize to a new dataset. We propose a simple yet effective data augmentation method PHICON to alleviate the generalization issue. PHICON consists of PHI augmentation and Context augmentation, which creates augmented training corpora by replacing PHI entities with named-entities sampled from external sources, and by changing background context with synonym replacement or random word insertion, respectively. Experimental results on the i2b2 2006 and 2014 de-identification challenge datasets show that PHICON can help three selected de-identification models boost F1-score (by at most 8.6{\%}) on cross-dataset test setting. We also discuss how much augmentation to use and how each augmentation method influences the performance."",
}
@",clinical text,natural language process,natural languag,language process,nlp,challeng
" ""Where{'}s the Question? A Multi-channel Deep Convolutional Neural Network for Question Identification in Textual Data"","," ""In most clinical practice settings, there is no rigorous reviewing of the clinical documentation, resulting in inaccurate information captured in the patient medical records. The gold standard in clinical data capturing is achieved via {``}expert-review{''}, where clinicians can have a dialogue with a domain expert (reviewers) and ask them questions about data entry rules. Automatically identifying {``}real questions{''} in these dialogues could uncover ambiguities or common problems in data capturing in a given clinical setting. In this study, we proposed a novel multi-channel deep convolutional neural network architecture, namely Quest-CNN, for the purpose of separating real questions that expect an answer (information or help) about an issue from sentences that are not questions, as well as from questions referring to an issue mentioned in a nearby sentence (e.g., can you clarify this?), which we will refer as {``}c-questions{''}. We conducted a comprehensive performance comparison analysis of the proposed multi-channel deep convolutional neural network against other deep neural networks. Furthermore, we evaluated the performance of traditional rule-based and learning-based methods for detecting question sentences. The proposed Quest-CNN achieved the best F1 score both on a dataset of data entry-review dialogue in a dialysis care setting, and on a general domain dataset."",","{michalopoulos-etal-2020-wheres,
    title = ""Where{'}s the Question? A Multi-channel Deep Convolutional Neural Network for Question Identification in Textual Data"",
    author = ""Michalopoulos, George  and
      Chen, Helen  and
      Wong, Alexander"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 3rd Clinical Natural Language Processing Workshop"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.clinicalnlp-1.24"",
    doi = ""10.18653/v1/2020.clinicalnlp-1.24"",
    pages = ""215--226"",
    abstract = ""In most clinical practice settings, there is no rigorous reviewing of the clinical documentation, resulting in inaccurate information captured in the patient medical records. The gold standard in clinical data capturing is achieved via {``}expert-review{''}, where clinicians can have a dialogue with a domain expert (reviewers) and ask them questions about data entry rules. Automatically identifying {``}real questions{''} in these dialogues could uncover ambiguities or common problems in data capturing in a given clinical setting. In this study, we proposed a novel multi-channel deep convolutional neural network architecture, namely Quest-CNN, for the purpose of separating real questions that expect an answer (information or help) about an issue from sentences that are not questions, as well as from questions referring to an issue mentioned in a nearby sentence (e.g., can you clarify this?), which we will refer as {``}c-questions{''}. We conducted a comprehensive performance comparison analysis of the proposed multi-channel deep convolutional neural network against other deep neural networks. Furthermore, we evaluated the performance of traditional rule-based and learning-based methods for detecting question sentences. The proposed Quest-CNN achieved the best F1 score both on a dataset of data entry-review dialogue in a dialysis care setting, and on a general domain dataset."",
}
@",medical record,natural language process,natural languag,language process,nlp,evalu
" ""An Ensemble Approach for Automatic Structuring of Radiology Reports"","," ""Automatic structuring of electronic medical records is of high demand for clinical workflow solutions to facilitate extraction, storage, and querying of patient care information. However, developing a scalable solution is extremely challenging, specifically for radiology reports, as most healthcare institutes use either no template or department/institute specific templates. Moreover, radiologists{'} reporting style varies from one to another as sentences are written in a telegraphic format and do not follow general English grammar rules. In this work, we present an ensemble method that consolidates the predictions of three models, capturing various attributes of textual information for automatic labeling of sentences with section labels. These three models are: 1) Focus Sentence model, capturing context of the target sentence; 2) Surrounding Context model, capturing the neighboring context of the target sentence; and finally, 3) Formatting/Layout model, aimed at learning report formatting cues. We utilize Bi-directional LSTMs, followed by sentence encoders, to acquire the context. Furthermore, we define several features that incorporate the structure of reports. We compare our proposed approach against multiple baselines and state-of-the-art approaches on a proprietary dataset as well as 100 manually annotated radiology notes from the MIMIC-III dataset, which we are making publicly available. Our proposed approach significantly outperforms other approaches by achieving 97.1{\%} accuracy."",","{pourreza-shahri-etal-2020-ensemble,
    title = ""An Ensemble Approach for Automatic Structuring of Radiology Reports"",
    author = ""Pourreza Shahri, Morteza  and
      Tahmasebi, Amir  and
      Ye, Bingyang  and
      Zhu, Henghui  and
      Aslam, Javed  and
      Ferris, Timothy"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 3rd Clinical Natural Language Processing Workshop"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.clinicalnlp-1.28"",
    doi = ""10.18653/v1/2020.clinicalnlp-1.28"",
    pages = ""249--258"",
    abstract = ""Automatic structuring of electronic medical records is of high demand for clinical workflow solutions to facilitate extraction, storage, and querying of patient care information. However, developing a scalable solution is extremely challenging, specifically for radiology reports, as most healthcare institutes use either no template or department/institute specific templates. Moreover, radiologists{'} reporting style varies from one to another as sentences are written in a telegraphic format and do not follow general English grammar rules. In this work, we present an ensemble method that consolidates the predictions of three models, capturing various attributes of textual information for automatic labeling of sentences with section labels. These three models are: 1) Focus Sentence model, capturing context of the target sentence; 2) Surrounding Context model, capturing the neighboring context of the target sentence; and finally, 3) Formatting/Layout model, aimed at learning report formatting cues. We utilize Bi-directional LSTMs, followed by sentence encoders, to acquire the context. Furthermore, we define several features that incorporate the structure of reports. We compare our proposed approach against multiple baselines and state-of-the-art approaches on a proprietary dataset as well as 100 manually annotated radiology notes from the MIMIC-III dataset, which we are making publicly available. Our proposed approach significantly outperforms other approaches by achieving 97.1{\%} accuracy."",
}
@",medical record,natural language process,natural languag,language process,nlp,annotat,challeng
" ""The {C}hilean Waiting List Corpus: a new resource for clinical Named Entity Recognition in {S}panish"","," ""In this work we describe the Waiting List Corpus consisting of de-identified referrals for several specialty consultations from the waiting list in Chilean public hospitals. A subset of 900 referrals was manually annotated with 9,029 entities, 385 attributes, and 284 pairs of relations with clinical relevance. A trained medical doctor annotated these referrals, and then together with other three researchers, consolidated each of the annotations. The annotated corpus has nested entities, with 32.2{\%} of entities embedded in other entities. We use this annotated corpus to obtain preliminary results for Named Entity Recognition (NER). The best results were achieved by using a biLSTM-CRF architecture using word embeddings trained over Spanish Wikipedia together with clinical embeddings computed by the group. NER models applied to this corpus can leverage statistics of diseases and pending procedures within this waiting list. This work constitutes the first annotated corpus using clinical narratives from Chile, and one of the few for the Spanish language. The annotated corpus, the clinical word embeddings, and the annotation guidelines are freely released to the research community."",","{baez-etal-2020-chilean,
    title = ""The {C}hilean Waiting List Corpus: a new resource for clinical Named Entity Recognition in {S}panish"",
    author = ""B{\'a}ez, Pablo  and
      Villena, Fabi{\'a}n  and
      Rojas, Mat{\'\i}as  and
      Dur{\'a}n, Manuel  and
      Dunstan, Jocelyn"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 3rd Clinical Natural Language Processing Workshop"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.clinicalnlp-1.32"",
    doi = ""10.18653/v1/2020.clinicalnlp-1.32"",
    pages = ""291--300"",
    abstract = ""In this work we describe the Waiting List Corpus consisting of de-identified referrals for several specialty consultations from the waiting list in Chilean public hospitals. A subset of 900 referrals was manually annotated with 9,029 entities, 385 attributes, and 284 pairs of relations with clinical relevance. A trained medical doctor annotated these referrals, and then together with other three researchers, consolidated each of the annotations. The annotated corpus has nested entities, with 32.2{\%} of entities embedded in other entities. We use this annotated corpus to obtain preliminary results for Named Entity Recognition (NER). The best results were achieved by using a biLSTM-CRF architecture using word embeddings trained over Spanish Wikipedia together with clinical embeddings computed by the group. NER models applied to this corpus can leverage statistics of diseases and pending procedures within this waiting list. This work constitutes the first annotated corpus using clinical narratives from Chile, and one of the few for the Spanish language. The annotated corpus, the clinical word embeddings, and the annotation guidelines are freely released to the research community."",
}
@",clinical narr,natural language process,natural languag,language process,nlp,entity recognit,annotat
" ""Experimental Evaluation and Development of a Silver-Standard for the {MIMIC}-{III} Clinical Coding Dataset"","," ""Clinical coding is currently a labour-intensive, error-prone, but a critical administrative process whereby hospital patient episodes are manually assigned codes by qualified staff from large, standardised taxonomic hierarchies of codes. Automating clinical coding has a long history in NLP research and has recently seen novel developments setting new benchmark results. A popular dataset used in this task is MIMIC-III, a large database of clinical free text notes and their associated codes amongst other data. We argue for the reconsideration of the validity MIMIC-III{'}s assigned codes, as MIMIC-III has not undergone secondary validation. This work presents an open-source, reproducible experimental methodology for assessing the validity of EHR discharge summaries. We exemplify the methodology with MIMIC-III discharge summaries and show the most frequently assigned codes in MIMIC-III are undercoded up to 35{\%}."",","{searle-etal-2020-experimental,
    title = ""Experimental Evaluation and Development of a Silver-Standard for the {MIMIC}-{III} Clinical Coding Dataset"",
    author = ""Searle, Thomas  and
      Ibrahim, Zina  and
      Dobson, Richard"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing"",
    month = jul,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.bionlp-1.8"",
    doi = ""10.18653/v1/2020.bionlp-1.8"",
    pages = ""76--85"",
    abstract = ""Clinical coding is currently a labour-intensive, error-prone, but a critical administrative process whereby hospital patient episodes are manually assigned codes by qualified staff from large, standardised taxonomic hierarchies of codes. Automating clinical coding has a long history in NLP research and has recently seen novel developments setting new benchmark results. A popular dataset used in this task is MIMIC-III, a large database of clinical free text notes and their associated codes amongst other data. We argue for the reconsideration of the validity MIMIC-III{'}s assigned codes, as MIMIC-III has not undergone secondary validation. This work presents an open-source, reproducible experimental methodology for assessing the validity of EHR discharge summaries. We exemplify the methodology with MIMIC-III discharge summaries and show the most frequently assigned codes in MIMIC-III are undercoded up to 35{\%}."",
}
@",discharge summar,language process,nlp,summar,benchmark,evalu,assess
" ""Comparative Analysis of Text Classification Approaches in Electronic Health Records"","," ""Text classification tasks which aim at harvesting and/or organizing information from electronic health records are pivotal to support clinical and translational research. However these present specific challenges compared to other classification tasks, notably due to the particular nature of the medical lexicon and language used in clinical records. Recent advances in embedding methods have shown promising results for several clinical tasks, yet there is no exhaustive comparison of such approaches with other commonly used word representations and classification models. In this work, we analyse the impact of various word representations, text pre-processing and classification algorithms on the performance of four different text classification tasks. The results show that traditional approaches, when tailored to the specific language and structure of the text inherent to the classification task, can achieve or exceed the performance of more recent ones based on contextual embeddings such as BERT."",","{mascio-etal-2020-comparative,
    title = ""Comparative Analysis of Text Classification Approaches in Electronic Health Records"",
    author = ""Mascio, Aurelie  and
      Kraljevic, Zeljko  and
      Bean, Daniel  and
      Dobson, Richard  and
      Stewart, Robert  and
      Bendayan, Rebecca  and
      Roberts, Angus"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing"",
    month = jul,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.bionlp-1.9"",
    doi = ""10.18653/v1/2020.bionlp-1.9"",
    pages = ""86--94"",
    abstract = ""Text classification tasks which aim at harvesting and/or organizing information from electronic health records are pivotal to support clinical and translational research. However these present specific challenges compared to other classification tasks, notably due to the particular nature of the medical lexicon and language used in clinical records. Recent advances in embedding methods have shown promising results for several clinical tasks, yet there is no exhaustive comparison of such approaches with other commonly used word representations and classification models. In this work, we analyse the impact of various word representations, text pre-processing and classification algorithms on the performance of four different text classification tasks. The results show that traditional approaches, when tailored to the specific language and structure of the text inherent to the classification task, can achieve or exceed the performance of more recent ones based on contextual embeddings such as BERT."",
}
@",electronic health record,health record,clinical record,language process,translat,nlp,challeng
" ""Evaluating the Utility of Model Configurations and Data Augmentation on Clinical Semantic Textual Similarity"","," ""In this paper, we apply pre-trained language models to the Semantic Textual Similarity (STS) task, with a specific focus on the clinical domain. In low-resource setting of clinical STS, these large models tend to be impractical and prone to overfitting. Building on BERT, we study the impact of a number of model design choices, namely different fine-tuning and pooling strategies. We observe that the impact of domain-specific fine-tuning on clinical STS is much less than that in the general domain, likely due to the concept richness of the domain. Based on this, we propose two data augmentation techniques. Experimental results on N2C2-STS 1 demonstrate substantial improvements, validating the utility of the proposed methods."",","{wang-etal-2020-evaluating,
    title = ""Evaluating the Utility of Model Configurations and Data Augmentation on Clinical Semantic Textual Similarity"",
    author = ""Wang, Yuxia  and
      Liu, Fei  and
      Verspoor, Karin  and
      Baldwin, Timothy"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing"",
    month = jul,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.bionlp-1.11"",
    doi = ""10.18653/v1/2020.bionlp-1.11"",
    pages = ""105--111"",
    abstract = ""In this paper, we apply pre-trained language models to the Semantic Textual Similarity (STS) task, with a specific focus on the clinical domain. In low-resource setting of clinical STS, these large models tend to be impractical and prone to overfitting. Building on BERT, we study the impact of a number of model design choices, namely different fine-tuning and pooling strategies. We observe that the impact of domain-specific fine-tuning on clinical STS is much less than that in the general domain, likely due to the concept richness of the domain. Based on this, we propose two data augmentation techniques. Experimental results on N2C2-STS 1 demonstrate substantial improvements, validating the utility of the proposed methods."",
}
@",clinical domain,language process,nlp,semant,evalu
" ""Domain Adaptation and Instance Selection for Disease Syndrome Classification over Veterinary Clinical Notes"","," ""Identifying the reasons for antibiotic administration in veterinary records is a critical component of understanding antimicrobial usage patterns. This informs antimicrobial stewardship programs designed to fight antimicrobial resistance, a major health crisis affecting both humans and animals in which veterinarians have an important role to play. We propose a document classification approach to determine the reason for administration of a given drug, with particular focus on domain adaptation from one drug to another, and instance selection to minimize annotation effort."",","{hur-etal-2020-domain,
    title = ""Domain Adaptation and Instance Selection for Disease Syndrome Classification over Veterinary Clinical Notes"",
    author = ""Hur, Brian  and
      Baldwin, Timothy  and
      Verspoor, Karin  and
      Hardefeldt, Laura  and
      Gilkerson, James"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing"",
    month = jul,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.bionlp-1.17"",
    doi = ""10.18653/v1/2020.bionlp-1.17"",
    pages = ""156--166"",
    abstract = ""Identifying the reasons for antibiotic administration in veterinary records is a critical component of understanding antimicrobial usage patterns. This informs antimicrobial stewardship programs designed to fight antimicrobial resistance, a major health crisis affecting both humans and animals in which veterinarians have an important role to play. We propose a document classification approach to determine the reason for administration of a given drug, with particular focus on domain adaptation from one drug to another, and instance selection to minimize annotation effort."",
}
@",clinical not,language process,nlp,annotat
" ""Benchmark and Best Practices for Biomedical Knowledge Graph Embeddings"","," ""Much of biomedical and healthcare data is encoded in discrete, symbolic form such as text and medical codes. There is a wealth of expert-curated biomedical domain knowledge stored in knowledge bases and ontologies, but the lack of reliable methods for learning knowledge representation has limited their usefulness in machine learning applications. While text-based representation learning has significantly improved in recent years through advances in natural language processing, attempts to learn biomedical concept embeddings so far have been lacking. A recent family of models called knowledge graph embeddings have shown promising results on general domain knowledge graphs, and we explore their capabilities in the biomedical domain. We train several state-of-the-art knowledge graph embedding models on the SNOMED-CT knowledge graph, provide a benchmark with comparison to existing methods and in-depth discussion on best practices, and make a case for the importance of leveraging the multi-relational nature of knowledge graphs for learning biomedical knowledge representation. The embeddings, code, and materials will be made available to the community."",","{chang-etal-2020-benchmark,
    title = ""Benchmark and Best Practices for Biomedical Knowledge Graph Embeddings"",
    author = ""Chang, David  and
      Bala{\v{z}}evi{\'c}, Ivana  and
      Allen, Carl  and
      Chawla, Daniel  and
      Brandt, Cynthia  and
      Taylor, Andrew"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing"",
    month = jul,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.bionlp-1.18"",
    doi = ""10.18653/v1/2020.bionlp-1.18"",
    pages = ""167--176"",
    abstract = ""Much of biomedical and healthcare data is encoded in discrete, symbolic form such as text and medical codes. There is a wealth of expert-curated biomedical domain knowledge stored in knowledge bases and ontologies, but the lack of reliable methods for learning knowledge representation has limited their usefulness in machine learning applications. While text-based representation learning has significantly improved in recent years through advances in natural language processing, attempts to learn biomedical concept embeddings so far have been lacking. A recent family of models called knowledge graph embeddings have shown promising results on general domain knowledge graphs, and we explore their capabilities in the biomedical domain. We train several state-of-the-art knowledge graph embedding models on the SNOMED-CT knowledge graph, provide a benchmark with comparison to existing methods and in-depth discussion on best practices, and make a case for the importance of leveraging the multi-relational nature of knowledge graphs for learning biomedical knowledge representation. The embeddings, code, and materials will be made available to the community."",
}
@",medical domain,natural language process,natural languag,language process,nlp,benchmark
" ""A Data-driven Approach for Noise Reduction in Distantly Supervised Biomedical Relation Extraction"","," ""Fact triples are a common form of structured knowledge used within the biomedical domain. As the amount of unstructured scientific texts continues to grow, manual annotation of these texts for the task of relation extraction becomes increasingly expensive. Distant supervision offers a viable approach to combat this by quickly producing large amounts of labeled, but considerably noisy, data. We aim to reduce such noise by extending an entity-enriched relation classification BERT model to the problem of multiple instance learning, and defining a simple data encoding scheme that significantly reduces noise, reaching state-of-the-art performance for distantly-supervised biomedical relation extraction. Our approach further encodes knowledge about the direction of relation triples, allowing for increased focus on relation learning by reducing noise and alleviating the need for joint learning with knowledge graph completion."",","{amin-etal-2020-data,
    title = ""A Data-driven Approach for Noise Reduction in Distantly Supervised Biomedical Relation Extraction"",
    author = ""Amin, Saadullah  and
      Dunfield, Katherine Ann  and
      Vechkaeva, Anna  and
      Neumann, Guenter"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing"",
    month = jul,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.bionlp-1.20"",
    doi = ""10.18653/v1/2020.bionlp-1.20"",
    pages = ""187--194"",
    abstract = ""Fact triples are a common form of structured knowledge used within the biomedical domain. As the amount of unstructured scientific texts continues to grow, manual annotation of these texts for the task of relation extraction becomes increasingly expensive. Distant supervision offers a viable approach to combat this by quickly producing large amounts of labeled, but considerably noisy, data. We aim to reduce such noise by extending an entity-enriched relation classification BERT model to the problem of multiple instance learning, and defining a simple data encoding scheme that significantly reduces noise, reaching state-of-the-art performance for distantly-supervised biomedical relation extraction. Our approach further encodes knowledge about the direction of relation triples, allowing for increased focus on relation learning by reducing noise and alleviating the need for joint learning with knowledge graph completion."",
}
@",medical domain,language process,nlp,relation extract,relation classif,annotat
" ""Global Locality in Biomedical Relation and Event Extraction"","," ""Due to the exponential growth of biomedical literature, event and relation extraction are important tasks in biomedical text mining. Most work only focus on relation extraction, and detect a single entity pair mention on a short span of text, which is not ideal due to long sentences that appear in biomedical contexts. We propose an approach to both relation and event extraction, for simultaneously predicting relationships between all mention pairs in a text. We also perform an empirical study to discuss different network setups for this purpose. The best performing model includes a set of multi-head attentions and convolutions, an adaptation of the transformer architecture, which offers self-attention the ability to strengthen dependencies among related elements, and models the interaction between features extracted by multiple attention heads. Experiment results demonstrate that our approach outperforms the state of the art on a set of benchmark biomedical corpora including BioNLP 2009, 2011, 2013 and BioCreative 2017 shared tasks."",","{shafieibavani-etal-2020-global,
    title = ""Global Locality in Biomedical Relation and Event Extraction"",
    author = ""ShafieiBavani, Elaheh  and
      Jimeno Yepes, Antonio  and
      Zhong, Xu  and
      Martinez Iraola, David"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing"",
    month = jul,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.bionlp-1.21"",
    doi = ""10.18653/v1/2020.bionlp-1.21"",
    pages = ""195--204"",
    abstract = ""Due to the exponential growth of biomedical literature, event and relation extraction are important tasks in biomedical text mining. Most work only focus on relation extraction, and detect a single entity pair mention on a short span of text, which is not ideal due to long sentences that appear in biomedical contexts. We propose an approach to both relation and event extraction, for simultaneously predicting relationships between all mention pairs in a text. We also perform an empirical study to discuss different network setups for this purpose. The best performing model includes a set of multi-head attentions and convolutions, an adaptation of the transformer architecture, which offers self-attention the ability to strengthen dependencies among related elements, and models the interaction between features extracted by multiple attention heads. Experiment results demonstrate that our approach outperforms the state of the art on a set of benchmark biomedical corpora including BioNLP 2009, 2011, 2013 and BioCreative 2017 shared tasks."",
}
@",medical text,language process,nlp,relation extract,shared task,benchmark
" ""Visual Question Generation from Radiology Images"","," ""Visual Question Generation (VQG), the task of generating a question based on image contents, is an increasingly important area that combines natural language processing and computer vision. Although there are some recent works that have attempted to generate questions from images in the open domain, the task of VQG in the medical domain has not been explored so far. In this paper, we introduce an approach to generation of visual questions about radiology images called VQGR, i.e. an algorithm that is able to ask a question when shown an image. VQGR first generates new training data from the existing examples, based on contextual word embeddings and image augmentation techniques. It then uses the variational auto-encoders model to encode images into a latent space and decode natural language questions. Experimental automatic evaluations performed on the VQA-RAD dataset of clinical visual questions show that VQGR achieves good performances compared with the baseline system. The source code is available at \url{https://github.com/sarrouti/vqgr}."",","{sarrouti-etal-2020-visual,
    title = ""Visual Question Generation from Radiology Images"",
    author = ""Sarrouti, Mourad  and
      Ben Abacha, Asma  and
      Demner-Fushman, Dina"",
    editor = ""Wang, Xin  and
      Thomason, Jesse  and
      Hu, Ronghang  and
      Chen, Xinlei  and
      Anderson, Peter  and
      Wu, Qi  and
      Celikyilmaz, Asli  and
      Baldridge, Jason  and
      Wang, William Yang"",
    booktitle = ""Proceedings of the First Workshop on Advances in Language and Vision Research"",
    month = jul,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.alvr-1.3"",
    doi = ""10.18653/v1/2020.alvr-1.3"",
    pages = ""12--18"",
    abstract = ""Visual Question Generation (VQG), the task of generating a question based on image contents, is an increasingly important area that combines natural language processing and computer vision. Although there are some recent works that have attempted to generate questions from images in the open domain, the task of VQG in the medical domain has not been explored so far. In this paper, we introduce an approach to generation of visual questions about radiology images called VQGR, i.e. an algorithm that is able to ask a question when shown an image. VQGR first generates new training data from the existing examples, based on contextual word embeddings and image augmentation techniques. It then uses the variational auto-encoders model to encode images into a latent space and decode natural language questions. Experimental automatic evaluations performed on the VQA-RAD dataset of clinical visual questions show that VQGR achieves good performances compared with the baseline system. The source code is available at \url{https://github.com/sarrouti/vqgr}."",
}
@",medical domain,natural language process,natural languag,language process,generat,evalu
" ""Clinical Reading Comprehension: A Thorough Analysis of the emr{QA} Dataset"","," ""Machine reading comprehension has made great progress in recent years owing to large-scale annotated datasets. In the clinical domain, however, creating such datasets is quite difficult due to the domain expertise required for annotation. Recently, Pampari et al. (EMNLP{'}18) tackled this issue by using expert-annotated question templates and existing i2b2 annotations to create emrQA, the first large-scale dataset for question answering (QA) based on clinical notes. In this paper, we provide an in-depth analysis of this dataset and the clinical reading comprehension (CliniRC) task. From our qualitative analysis, we find that (i) emrQA answers are often incomplete, and (ii) emrQA questions are often answerable without using domain knowledge. From our quantitative experiments, surprising results include that (iii) using a small sampled subset (5{\%}-20{\%}), we can obtain roughly equal performance compared to the model trained on the entire dataset, (iv) this performance is close to human expert{'}s performance, and (v) BERT models do not beat the best performing base model. Following our analysis of the emrQA, we further explore two desired aspects of CliniRC systems: the ability to utilize clinical domain knowledge and to generalize to unseen questions and contexts. We argue that both should be considered when creating future datasets."",","{yue-etal-2020-clinical,
    title = ""Clinical Reading Comprehension: A Thorough Analysis of the emr{QA} Dataset"",
    author = ""Yue, Xiang  and
      Jimenez Gutierrez, Bernal  and
      Sun, Huan"",
    editor = ""Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel"",
    booktitle = ""Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"",
    month = jul,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.acl-main.410"",
    doi = ""10.18653/v1/2020.acl-main.410"",
    pages = ""4474--4486"",
    abstract = ""Machine reading comprehension has made great progress in recent years owing to large-scale annotated datasets. In the clinical domain, however, creating such datasets is quite difficult due to the domain expertise required for annotation. Recently, Pampari et al. (EMNLP{'}18) tackled this issue by using expert-annotated question templates and existing i2b2 annotations to create emrQA, the first large-scale dataset for question answering (QA) based on clinical notes. In this paper, we provide an in-depth analysis of this dataset and the clinical reading comprehension (CliniRC) task. From our qualitative analysis, we find that (i) emrQA answers are often incomplete, and (ii) emrQA questions are often answerable without using domain knowledge. From our quantitative experiments, surprising results include that (iii) using a small sampled subset (5{\%}-20{\%}), we can obtain roughly equal performance compared to the model trained on the entire dataset, (iv) this performance is close to human expert{'}s performance, and (v) BERT models do not beat the best performing base model. Following our analysis of the emrQA, we further explore two desired aspects of CliniRC systems: the ability to utilize clinical domain knowledge and to generalize to unseen questions and contexts. We argue that both should be considered when creating future datasets."",
}
@",clinical domain,clinical not,nlp,annotat
" ""Closing the Gap: Joint De-Identification and Concept Extraction in the Clinical Domain"","," ""Exploiting natural language processing in the clinical domain requires de-identification, i.e., anonymization of personal information in texts. However, current research considers de-identification and downstream tasks, such as concept extraction, only in isolation and does not study the effects of de-identification on other tasks. In this paper, we close this gap by reporting concept extraction performance on automatically anonymized data and investigating joint models for de-identification and concept extraction. In particular, we propose a stacked model with restricted access to privacy sensitive information and a multitask model. We set the new state of the art on benchmark datasets in English (96.1{\%} F1 for de-identification and 88.9{\%} F1 for concept extraction) and Spanish (91.4{\%} F1 for concept extraction)."",","{lange-etal-2020-closing,
    title = ""Closing the Gap: Joint De-Identification and Concept Extraction in the Clinical Domain"",
    author = {Lange, Lukas  and
      Adel, Heike  and
      Str{\""o}tgen, Jannik},
    editor = ""Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel"",
    booktitle = ""Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"",
    month = jul,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.acl-main.621"",
    doi = ""10.18653/v1/2020.acl-main.621"",
    pages = ""6945--6952"",
    abstract = ""Exploiting natural language processing in the clinical domain requires de-identification, i.e., anonymization of personal information in texts. However, current research considers de-identification and downstream tasks, such as concept extraction, only in isolation and does not study the effects of de-identification on other tasks. In this paper, we close this gap by reporting concept extraction performance on automatically anonymized data and investigating joint models for de-identification and concept extraction. In particular, we propose a stacked model with restricted access to privacy sensitive information and a multitask model. We set the new state of the art on benchmark datasets in English (96.1{\%} F1 for de-identification and 88.9{\%} F1 for concept extraction) and Spanish (91.4{\%} F1 for concept extraction)."",
}
@",clinical domain,natural language process,natural languag,language process,benchmark
" ""{BENTO}: A Visual Platform for Building Clinical {NLP} Pipelines Based on {C}oda{L}ab"","," ""CodaLab is an open-source web-based platform for collaborative computational research. Although CodaLab has gained popularity in the research community, its interface has limited support for creating reusable tools that can be easily applied to new datasets and composed into pipelines. In clinical domain, natural language processing (NLP) on medical notes generally involves multiple steps, like tokenization, named entity recognition, etc. Since these steps require different tools which are usually scattered in different publications, it is not easy for researchers to use them to process their own datasets. In this paper, we present BENTO, a workflow management platform with a graphic user interface (GUI) that is built on top of CodaLab, to facilitate the process of building clinical NLP pipelines. BENTO comes with a number of clinical NLP tools that have been pre-trained using medical notes and expert annotations and can be readily used for various clinical NLP tasks. It also allows researchers and developers to create their custom tools (e.g., pre-trained NLP models) and use them in a controlled and reproducible way. In addition, the GUI interface enables researchers with limited computer background to compose tools into NLP pipelines and then apply the pipelines on their own datasets in a {``}what you see is what you get{''} (WYSIWYG) way. Although BENTO is designed for clinical NLP applications, the underlying architecture is flexible to be tailored to any other domains."",","{jin-etal-2020-bento,
    title = ""{BENTO}: A Visual Platform for Building Clinical {NLP} Pipelines Based on {C}oda{L}ab"",
    author = ""Jin, Yonghao  and
      Li, Fei  and
      Yu, Hong"",
    editor = ""Celikyilmaz, Asli  and
      Wen, Tsung-Hsien"",
    booktitle = ""Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations"",
    month = jul,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.acl-demos.13"",
    doi = ""10.18653/v1/2020.acl-demos.13"",
    pages = ""95--100"",
    abstract = ""CodaLab is an open-source web-based platform for collaborative computational research. Although CodaLab has gained popularity in the research community, its interface has limited support for creating reusable tools that can be easily applied to new datasets and composed into pipelines. In clinical domain, natural language processing (NLP) on medical notes generally involves multiple steps, like tokenization, named entity recognition, etc. Since these steps require different tools which are usually scattered in different publications, it is not easy for researchers to use them to process their own datasets. In this paper, we present BENTO, a workflow management platform with a graphic user interface (GUI) that is built on top of CodaLab, to facilitate the process of building clinical NLP pipelines. BENTO comes with a number of clinical NLP tools that have been pre-trained using medical notes and expert annotations and can be readily used for various clinical NLP tasks. It also allows researchers and developers to create their custom tools (e.g., pre-trained NLP models) and use them in a controlled and reproducible way. In addition, the GUI interface enables researchers with limited computer background to compose tools into NLP pipelines and then apply the pipelines on their own datasets in a {``}what you see is what you get{''} (WYSIWYG) way. Although BENTO is designed for clinical NLP applications, the underlying architecture is flexible to be tailored to any other domains."",
}
@",clinical domain,natural language process,natural languag,language process,nlp,entity recognit,annotat
" ""Learning from the Experience of Doctors: Automated Diagnosis of Appendicitis Based on Clinical Notes"","," ""The objective of this work is to develop an automated diagnosis system that is able to predict the probability of appendicitis given a free-text emergency department (ED) note and additional structured information (e.g., lab test results). Our clinical corpus consists of about 180,000 ED notes based on ten years of patient visits to the Accident and Emergency (A{\&}E) Department of the National University Hospital (NUH), Singapore. We propose a novel neural network approach that learns to diagnose acute appendicitis based on doctors{'} free-text ED notes without any feature engineering. On a test set of 2,000 ED notes with equal number of appendicitis (positive) and non-appendicitis (negative) diagnosis and in which all the negative ED notes only consist of abdominal-related diagnosis, our model is able to achieve a promising F{\_}0.5-score of 0.895 while ED doctors achieve F{\_}0.5-score of 0.900. Visualization shows that our model is able to learn important features, signs, and symptoms of patients from unstructured free-text ED notes, which will help doctors to make better diagnosis."",","{yuwono-etal-2019-learning,
    title = ""Learning from the Experience of Doctors: Automated Diagnosis of Appendicitis Based on Clinical Notes"",
    author = ""Yuwono, Steven Kester  and
      Ng, Hwee Tou  and
      Ngiam, Kee Yuan"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5002"",
    doi = ""10.18653/v1/W19-5002"",
    pages = ""11--19"",
    abstract = ""The objective of this work is to develop an automated diagnosis system that is able to predict the probability of appendicitis given a free-text emergency department (ED) note and additional structured information (e.g., lab test results). Our clinical corpus consists of about 180,000 ED notes based on ten years of patient visits to the Accident and Emergency (A{\&}E) Department of the National University Hospital (NUH), Singapore. We propose a novel neural network approach that learns to diagnose acute appendicitis based on doctors{'} free-text ED notes without any feature engineering. On a test set of 2,000 ED notes with equal number of appendicitis (positive) and non-appendicitis (negative) diagnosis and in which all the negative ED notes only consist of abdominal-related diagnosis, our model is able to achieve a promising F{\_}0.5-score of 0.895 while ED doctors achieve F{\_}0.5-score of 0.900. Visualization shows that our model is able to learn important features, signs, and symptoms of patients from unstructured free-text ED notes, which will help doctors to make better diagnosis."",
}
@",clinical not,nlp,shared task
" ""A Paraphrase Generation System for {EHR} Question Answering"","," ""This paper proposes a dataset and method for automatically generating paraphrases for clinical questions relating to patient-specific information in electronic health records (EHRs). Crowdsourcing is used to collect 10,578 unique questions across 946 semantically distinct paraphrase clusters. This corpus is then used with a deep learning-based question paraphrasing method utilizing variational autoencoder and LSTM encoder/decoder. The ultimate use of such a method is to improve the performance of automatic question answering methods for EHRs."",","{soni-roberts-2019-paraphrase,
    title = ""A Paraphrase Generation System for {EHR} Question Answering"",
    author = ""Soni, Sarvesh  and
      Roberts, Kirk"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5003"",
    doi = ""10.18653/v1/W19-5003"",
    pages = ""20--29"",
    abstract = ""This paper proposes a dataset and method for automatically generating paraphrases for clinical questions relating to patient-specific information in electronic health records (EHRs). Crowdsourcing is used to collect 10,578 unique questions across 946 semantically distinct paraphrase clusters. This corpus is then used with a deep learning-based question paraphrasing method utilizing variational autoencoder and LSTM encoder/decoder. The ultimate use of such a method is to improve the performance of automatic question answering methods for EHRs."",
}
@",electronic health record,health record,nlp,generat,semant,shared task
" ""{RE}flex: Flexible Framework for Relation Extraction in Multiple Domains"","," ""Systematic comparison of methods for relation extraction (RE) is difficult because many experiments in the field are not described precisely enough to be completely reproducible and many papers fail to report ablation studies that would highlight the relative contributions of their various combined techniques. In this work, we build a unifying framework for RE, applying this on three highly used datasets (from the general, biomedical and clinical domains) with the ability to be extendable to new datasets. By performing a systematic exploration of modeling, pre-processing and training methodologies, we find that choices of preprocessing are a large contributor performance and that omission of such information can further hinder fair comparison. Other insights from our exploration allow us to provide recommendations for future research in this area."",","{chauhan-etal-2019-reflex,
    title = ""{RE}flex: Flexible Framework for Relation Extraction in Multiple Domains"",
    author = ""Chauhan, Geeticka  and
      McDermott, Matthew B.A.  and
      Szolovits, Peter"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5004"",
    doi = ""10.18653/v1/W19-5004"",
    pages = ""30--47"",
    abstract = ""Systematic comparison of methods for relation extraction (RE) is difficult because many experiments in the field are not described precisely enough to be completely reproducible and many papers fail to report ablation studies that would highlight the relative contributions of their various combined techniques. In this work, we build a unifying framework for RE, applying this on three highly used datasets (from the general, biomedical and clinical domains) with the ability to be extendable to new datasets. By performing a systematic exploration of modeling, pre-processing and training methodologies, we find that choices of preprocessing are a large contributor performance and that omission of such information can further hinder fair comparison. Other insights from our exploration allow us to provide recommendations for future research in this area."",
}
@",clinical domain,nlp,relation extract,shared task
" ""Analysing Representations of Memory Impairment in a Clinical Notes Classification Model"","," ""Despite recent advances in the application of deep neural networks to various kinds of medical data, extracting information from unstructured textual sources remains a challenging task. The challenges of training and interpreting document classification models are amplified when dealing with small and highly technical datasets, as are common in the clinical domain. Using a dataset of de-identified clinical letters gathered at a memory clinic, we construct several recurrent neural network models for letter classification, and evaluate them on their ability to build meaningful representations of the documents and predict patients{'} diagnoses. Additionally, we probe sentence embedding models in order to build a human-interpretable representation of the neural network{'}s features, using a simple and intuitive technique based on perturbative approaches to sentence importance. In addition to showing which sentences in a document are most informative about the patient{'}s condition, this method reveals the types of sentences that lead the model to make incorrect diagnoses. Furthermore, we identify clusters of sentences in the embedding space that correlate strongly with importance scores for each clinical diagnosis class."",","{ormerod-etal-2019-analysing,
    title = ""Analysing Representations of Memory Impairment in a Clinical Notes Classification Model"",
    author = ""Ormerod, Mark  and
      Mart{\'\i}nez-del-Rinc{\'o}n, Jes{\'u}s  and
      Robertson, Neil  and
      McGuinness, Bernadette  and
      Devereux, Barry"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5005"",
    doi = ""10.18653/v1/W19-5005"",
    pages = ""48--57"",
    abstract = ""Despite recent advances in the application of deep neural networks to various kinds of medical data, extracting information from unstructured textual sources remains a challenging task. The challenges of training and interpreting document classification models are amplified when dealing with small and highly technical datasets, as are common in the clinical domain. Using a dataset of de-identified clinical letters gathered at a memory clinic, we construct several recurrent neural network models for letter classification, and evaluate them on their ability to build meaningful representations of the documents and predict patients{'} diagnoses. Additionally, we probe sentence embedding models in order to build a human-interpretable representation of the neural network{'}s features, using a simple and intuitive technique based on perturbative approaches to sentence importance. In addition to showing which sentences in a document are most informative about the patient{'}s condition, this method reveals the types of sentences that lead the model to make incorrect diagnoses. Furthermore, we identify clusters of sentences in the embedding space that correlate strongly with importance scores for each clinical diagnosis class."",
}
@",clinical domain,clinical not,nlp,shared task,challeng,evalu
" ""Transfer Learning in Biomedical Natural Language Processing: An Evaluation of {BERT} and {ELM}o on Ten Benchmarking Datasets"","," ""Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at \url{https://github.com/ncbi-nlp/BLUE_Benchmark}."",","{peng-etal-2019-transfer,
    title = ""Transfer Learning in Biomedical Natural Language Processing: An Evaluation of {BERT} and {ELM}o on Ten Benchmarking Datasets"",
    author = ""Peng, Yifan  and
      Yan, Shankai  and
      Lu, Zhiyong"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5006"",
    doi = ""10.18653/v1/W19-5006"",
    pages = ""58--65"",
    abstract = ""Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at \url{https://github.com/ncbi-nlp/BLUE_Benchmark}."",
}
@",clinical not,clinical text,natural language process,natural languag,language process,nlp,shared task,benchmark,evalu
" ""Combining Structured and Free-text Electronic Medical Record Data for Real-time Clinical Decision Support"","," ""The goal of this work is to utilize Electronic Medical Record (EMR) data for real-time Clinical Decision Support (CDS). We present a deep learning approach to combining in real time available diagnosis codes (ICD codes) and free-text notes: Patient Context Vectors. Patient Context Vectors are created by averaging ICD code embeddings, and by predicting the same from free-text notes via a Convolutional Neural Network. The Patient Context Vectors were then simply appended to available structured data (vital signs and lab results) to build prediction models for a specific condition. Experiments on predicting ARDS, a rare and complex condition, demonstrate the utility of Patient Context Vectors as a means of summarizing the patient history and overall condition, and improve significantly the prediction model results."",","{apostolova-etal-2019-combining,
    title = ""Combining Structured and Free-text Electronic Medical Record Data for Real-time Clinical Decision Support"",
    author = ""Apostolova, Emilia  and
      Wang, Tony  and
      Tschampel, Tim  and
      Koutroulis, Ioannis  and
      Velez, Tom"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5007"",
    doi = ""10.18653/v1/W19-5007"",
    pages = ""66--70"",
    abstract = ""The goal of this work is to utilize Electronic Medical Record (EMR) data for real-time Clinical Decision Support (CDS). We present a deep learning approach to combining in real time available diagnosis codes (ICD codes) and free-text notes: Patient Context Vectors. Patient Context Vectors are created by averaging ICD code embeddings, and by predicting the same from free-text notes via a Convolutional Neural Network. The Patient Context Vectors were then simply appended to available structured data (vital signs and lab results) to build prediction models for a specific condition. Experiments on predicting ARDS, a rare and complex condition, demonstrate the utility of Patient Context Vectors as a means of summarizing the patient history and overall condition, and improve significantly the prediction model results."",
}
@",medical record,nlp,summar,shared task
" ""Annotating Temporal Information in Clinical Notes for Timeline Reconstruction: Towards the Definition of Calendar Expressions"","," ""To automatically analyse complex trajectory information enclosed in clinical text (e.g. timing of symptoms, duration of treatment), it is important to understand the related temporal aspects, anchoring each event on an absolute point in time. In the clinical domain, few temporally annotated corpora are currently available. Moreover, underlying annotation schemas - which mainly rely on the TimeML standard - are not necessarily easily applicable for applications such as patient timeline reconstruction. In this work, we investigated how temporal information is documented in clinical text by annotating a corpus of medical reports with time expressions (TIMEXes), based on TimeML. The developed corpus is available to the NLP community. Starting from our annotations, we analysed the suitability of the TimeML TIMEX schema for capturing timeline information, identifying challenges and possible solutions. As a result, we propose a novel annotation schema that could be useful for timeline reconstruction: CALendar EXpression (CALEX)."",","{viani-etal-2019-annotating,
    title = ""Annotating Temporal Information in Clinical Notes for Timeline Reconstruction: Towards the Definition of Calendar Expressions"",
    author = ""Viani, Natalia  and
      Tissot, Hegler  and
      Bernardino, Ariane  and
      Velupillai, Sumithra"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5021"",
    doi = ""10.18653/v1/W19-5021"",
    pages = ""201--210"",
    abstract = ""To automatically analyse complex trajectory information enclosed in clinical text (e.g. timing of symptoms, duration of treatment), it is important to understand the related temporal aspects, anchoring each event on an absolute point in time. In the clinical domain, few temporally annotated corpora are currently available. Moreover, underlying annotation schemas - which mainly rely on the TimeML standard - are not necessarily easily applicable for applications such as patient timeline reconstruction. In this work, we investigated how temporal information is documented in clinical text by annotating a corpus of medical reports with time expressions (TIMEXes), based on TimeML. The developed corpus is available to the NLP community. Starting from our annotations, we analysed the suitability of the TimeML TIMEX schema for capturing timeline information, identifying challenges and possible solutions. As a result, we propose a novel annotation schema that could be useful for timeline reconstruction: CALendar EXpression (CALEX)."",
}
@",clinical domain,clinical not,clinical text,nlp,shared task,annotat,challeng
" ""Leveraging Sublanguage Features for the Semantic Categorization of Clinical Terms"","," ""The automatic processing of clinical documents, such as Electronic Health Records (EHRs), could benefit substantially from the enrichment of medical terminologies with terms encountered in clinical practice. To integrate such terms into existing knowledge sources, they must be linked to corresponding concepts. We present a method for the semantic categorization of clinical terms based on their surface form. We find that features based on sublanguage properties can provide valuable cues for the classification of term variants."",","{gron-etal-2019-leveraging,
    title = ""Leveraging Sublanguage Features for the Semantic Categorization of Clinical Terms"",
    author = {Gr{\""o}n, Leonie  and
      Bertels, Ann  and
      Heylen, Kris},
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5022"",
    doi = ""10.18653/v1/W19-5022"",
    pages = ""211--216"",
    abstract = ""The automatic processing of clinical documents, such as Electronic Health Records (EHRs), could benefit substantially from the enrichment of medical terminologies with terms encountered in clinical practice. To integrate such terms into existing knowledge sources, they must be linked to corresponding concepts. We present a method for the semantic categorization of clinical terms based on their surface form. We find that features based on sublanguage properties can provide valuable cues for the classification of term variants."",
}
@",electronic health record,health record,nlp,semant,shared task
" ""Enhancing {PIO} Element Detection in Medical Text Using Contextualized Embedding"","," ""In this paper, we presented an improved methodology to extract PIO elements, from abstracts of medical papers, that reduces ambiguity. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. In addition, we investigated a contextualized embedding, BioBERT, trained on medical corpora. It has been found that using the BioBERT embedding improved the classification accuracy, outperforming the BERT-based model. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context. Furthermore, to enhance the accuracy of the model, we have investigated an ensemble method based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to learn a linear combination of the predicted probabilities for the 3 classes with the TF-IDF score and the QIEF that optimizes the classification. The results indicate that these text features were good features to consider in order to boost the deeply contextualized classification model. We compared the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the highest score in terms of AUC when we combine the base learners. The present work resulted in the creation of a PIO element dataset, PICONET, and a classification tool. These constitute and important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated."",","{mezaoui-etal-2019-enhancing,
    title = ""Enhancing {PIO} Element Detection in Medical Text Using Contextualized Embedding"",
    author = ""Mezaoui, Hichem  and
      Gunasekara, Isuru  and
      Gontcharov, Aleksandr"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5023"",
    doi = ""10.18653/v1/W19-5023"",
    pages = ""217--222"",
    abstract = ""In this paper, we presented an improved methodology to extract PIO elements, from abstracts of medical papers, that reduces ambiguity. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. In addition, we investigated a contextualized embedding, BioBERT, trained on medical corpora. It has been found that using the BioBERT embedding improved the classification accuracy, outperforming the BERT-based model. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context. Furthermore, to enhance the accuracy of the model, we have investigated an ensemble method based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to learn a linear combination of the predicted probabilities for the 3 classes with the TF-IDF score and the QIEF that optimizes the classification. The results indicate that these text features were good features to consider in order to boost the deeply contextualized classification model. We compared the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the highest score in terms of AUC when we combine the base learners. The present work resulted in the creation of a PIO element dataset, PICONET, and a classification tool. These constitute and important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated."",
}
@",medical text,nlp,shared task
" ""Contributions to Clinical Named Entity Recognition in {P}ortuguese"","," ""Having in mind that different languages might present different challenges, this paper presents the following contributions to the area of Information Extraction from clinical text, targeting the Portuguese language: a collection of 281 clinical texts in this language, with manually-annotated named entities; word embeddings trained in a larger collection of similar texts; results of using BiLSTM-CRF neural networks for named entity recognition on the annotated collection, including a comparison of using in-domain or out-of-domain word embeddings in this task. Although learned with much less data, performance is higher when using in-domain embeddings. When tested in 20 independent clinical texts, this model achieved better results than a model using larger out-of-domain embeddings."",","{lopes-etal-2019-contributions,
    title = ""Contributions to Clinical Named Entity Recognition in {P}ortuguese"",
    author = ""Lopes, F{\'a}bio  and
      Teixeira, C{\'e}sar  and
      Gon{\c{c}}alo Oliveira, Hugo"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5024"",
    doi = ""10.18653/v1/W19-5024"",
    pages = ""223--233"",
    abstract = ""Having in mind that different languages might present different challenges, this paper presents the following contributions to the area of Information Extraction from clinical text, targeting the Portuguese language: a collection of 281 clinical texts in this language, with manually-annotated named entities; word embeddings trained in a larger collection of similar texts; results of using BiLSTM-CRF neural networks for named entity recognition on the annotated collection, including a comparison of using in-domain or out-of-domain word embeddings in this task. Although learned with much less data, performance is higher when using in-domain embeddings. When tested in 20 independent clinical texts, this model achieved better results than a model using larger out-of-domain embeddings."",
}
@",clinical text,nlp,entity recognit,shared task,annotat,challeng
" ""Is artificial data useful for biomedical Natural Language Processing algorithms?"","," ""A major obstacle to the development of Natural Language Processing (NLP) methods in the biomedical domain is data accessibility. This problem can be addressed by generating medical data artificially. Most previous studies have focused on the generation of short clinical text, and evaluation of the data utility has been limited. We propose a generic methodology to guide the generation of clinical text with key phrases. We use the artificial data as additional training data in two key biomedical NLP tasks: text classification and temporal relation extraction. We show that artificially generated training data used in conjunction with real training data can lead to performance boosts for data-greedy neural network algorithms. We also demonstrate the usefulness of the generated data for NLP setups where it fully replaces real training data."",","{wang-etal-2019-artificial,
    title = ""Is artificial data useful for biomedical Natural Language Processing algorithms?"",
    author = ""Wang, Zixu  and
      Ive, Julia  and
      Velupillai, Sumithra  and
      Specia, Lucia"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5026"",
    doi = ""10.18653/v1/W19-5026"",
    pages = ""240--249"",
    abstract = ""A major obstacle to the development of Natural Language Processing (NLP) methods in the biomedical domain is data accessibility. This problem can be addressed by generating medical data artificially. Most previous studies have focused on the generation of short clinical text, and evaluation of the data utility has been limited. We propose a generic methodology to guide the generation of clinical text with key phrases. We use the artificial data as additional training data in two key biomedical NLP tasks: text classification and temporal relation extraction. We show that artificially generated training data used in conjunction with real training data can lead to performance boosts for data-greedy neural network algorithms. We also demonstrate the usefulness of the generated data for NLP setups where it fully replaces real training data."",
}
@",medical domain,clinical text,natural language process,natural languag,language process,nlp,generat,relation extract,shared task,evalu
" ""{C}hi{M}ed: A {C}hinese Medical Corpus for Question Answering"","," ""Question answering (QA) is a challenging task in natural language processing (NLP), especially when it is applied to specific domains. While models trained in the general domain can be adapted to a new target domain, their performance often degrades significantly due to domain mismatch. Alternatively, one can require a large amount of domain-specific QA data, but such data are rare, especially for the medical domain. In this study, we first collect a large-scale Chinese medical QA corpus called ChiMed; second we annotate a small fraction of the corpus to check the quality of the answers; third, we extract two datasets from the corpus and use them for the relevancy prediction task and the adoption prediction task. Several benchmark models are applied to the datasets, producing good results for both tasks."",","{tian-etal-2019-chimed,
    title = ""{C}hi{M}ed: A {C}hinese Medical Corpus for Question Answering"",
    author = ""Tian, Yuanhe  and
      Ma, Weicheng  and
      Xia, Fei  and
      Song, Yan"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5027"",
    doi = ""10.18653/v1/W19-5027"",
    pages = ""250--260"",
    abstract = ""Question answering (QA) is a challenging task in natural language processing (NLP), especially when it is applied to specific domains. While models trained in the general domain can be adapted to a new target domain, their performance often degrades significantly due to domain mismatch. Alternatively, one can require a large amount of domain-specific QA data, but such data are rare, especially for the medical domain. In this study, we first collect a large-scale Chinese medical QA corpus called ChiMed; second we annotate a small fraction of the corpus to check the quality of the answers; third, we extract two datasets from the corpus and use them for the relevancy prediction task and the adoption prediction task. Several benchmark models are applied to the datasets, producing good results for both tasks."",
}
@",medical domain,natural language process,natural languag,language process,nlp,shared task,annotat,challeng,benchmark
" ""Clinical Concept Extraction for Document-Level Coding"","," ""The text of clinical notes can be a valuable source of patient information and clinical assessments. Historically, the primary approach for exploiting clinical notes has been information extraction: linking spans of text to concepts in a detailed domain ontology. However, recent work has demonstrated the potential of supervised machine learning to extract document-level codes directly from the raw text of clinical notes. We propose to bridge the gap between the two approaches with two novel syntheses: (1) treating extracted concepts as features, which are used to supplement or replace the text of the note; (2) treating extracted concepts as labels, which are used to learn a better representation of the text. Unfortunately, the resulting concepts do not yield performance gains on the document-level clinical coding task. We explore possible explanations and future research directions."",","{wiegreffe-etal-2019-clinical,
    title = ""Clinical Concept Extraction for Document-Level Coding"",
    author = ""Wiegreffe, Sarah  and
      Choi, Edward  and
      Yan, Sherry  and
      Sun, Jimeng  and
      Eisenstein, Jacob"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5028"",
    doi = ""10.18653/v1/W19-5028"",
    pages = ""261--272"",
    abstract = ""The text of clinical notes can be a valuable source of patient information and clinical assessments. Historically, the primary approach for exploiting clinical notes has been information extraction: linking spans of text to concepts in a detailed domain ontology. However, recent work has demonstrated the potential of supervised machine learning to extract document-level codes directly from the raw text of clinical notes. We propose to bridge the gap between the two approaches with two novel syntheses: (1) treating extracted concepts as features, which are used to supplement or replace the text of the note; (2) treating extracted concepts as labels, which are used to learn a better representation of the text. Unfortunately, the resulting concepts do not yield performance gains on the document-level clinical coding task. We explore possible explanations and future research directions."",
}
@",clinical not,nlp,shared task,assess
" ""Clinical Case Reports for {NLP}"","," ""Textual data are useful for accessing expert information. Yet, since the texts are representative of distinct language uses, it is necessary to build specific corpora in order to be able to design suitable NLP tools. In some domains, such as medical domain, it may be complicated to access the representative textual data and their semantic annotations, while there exists a real need for providing efficient tools and methods. Our paper presents a corpus of clinical cases written in French, and their semantic annotations. Thus, we manually annotated a set of 717 files into four general categories (age, gender, outcome, and origin) for a total number of 2,835 annotations. The values of age, gender, and outcome are normalized. A subset with 70 files has been additionally manually annotated into 27 categories for a total number of 5,198 annotations."",","{grouin-etal-2019-clinical,
    title = ""Clinical Case Reports for {NLP}"",
    author = ""Grouin, Cyril  and
      Grabar, Natalia  and
      Claveau, Vincent  and
      Hamon, Thierry"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5029"",
    doi = ""10.18653/v1/W19-5029"",
    pages = ""273--282"",
    abstract = ""Textual data are useful for accessing expert information. Yet, since the texts are representative of distinct language uses, it is necessary to build specific corpora in order to be able to design suitable NLP tools. In some domains, such as medical domain, it may be complicated to access the representative textual data and their semantic annotations, while there exists a real need for providing efficient tools and methods. Our paper presents a corpus of clinical cases written in French, and their semantic annotations. Thus, we manually annotated a set of 717 files into four general categories (age, gender, outcome, and origin) for a total number of 2,835 annotations. The values of age, gender, and outcome are normalized. A subset with 70 files has been additionally manually annotated into 27 categories for a total number of 5,198 annotations."",
}
@",medical domain,nlp,semant,shared task,annotat
" ""Two-stage Federated Phenotyping and Patient Representation Learning"","," ""A large percentage of medical information is in unstructured text format in electronic medical record systems. Manual extraction of information from clinical notes is extremely time consuming. Natural language processing has been widely used in recent years for automatic information extraction from medical texts. However, algorithms trained on data from a single healthcare provider are not generalizable and error-prone due to the heterogeneity and uniqueness of medical documents. We develop a two-stage federated natural language processing method that enables utilization of clinical notes from different hospitals or clinics without moving the data, and demonstrate its performance using obesity and comorbities phenotyping as medical task. This approach not only improves the quality of a specific clinical task but also facilitates knowledge progression in the whole healthcare system, which is an essential part of learning health system. To the best of our knowledge, this is the first application of federated machine learning in clinical NLP."",","{liu-etal-2019-two,
    title = ""Two-stage Federated Phenotyping and Patient Representation Learning"",
    author = ""Liu, Dianbo  and
      Dligach, Dmitriy  and
      Miller, Timothy"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5030"",
    doi = ""10.18653/v1/W19-5030"",
    pages = ""283--291"",
    abstract = ""A large percentage of medical information is in unstructured text format in electronic medical record systems. Manual extraction of information from clinical notes is extremely time consuming. Natural language processing has been widely used in recent years for automatic information extraction from medical texts. However, algorithms trained on data from a single healthcare provider are not generalizable and error-prone due to the heterogeneity and uniqueness of medical documents. We develop a two-stage federated natural language processing method that enables utilization of clinical notes from different hospitals or clinics without moving the data, and demonstrate its performance using obesity and comorbities phenotyping as medical task. This approach not only improves the quality of a specific clinical task but also facilitates knowledge progression in the whole healthcare system, which is an essential part of learning health system. To the best of our knowledge, this is the first application of federated machine learning in clinical NLP."",
}
@",medical record,clinical not,medical text,natural language process,natural languag,language process,nlp,shared task
" ""Simplification-induced transformations: typology and some characteristics"","," ""The purpose of automatic text simplification is to transform technical or difficult to understand texts into a more friendly version. The semantics must be preserved during this transformation. Automatic text simplification can be done at different levels (lexical, syntactic, semantic, stylistic...) and relies on the corresponding knowledge and resources (lexicon, rules...). Our objective is to propose methods and material for the creation of transformation rules from a small set of parallel sentences differentiated by their technicity. We also propose a typology of transformations and quantify them. We work with French-language data related to the medical domain, although we assume that the method can be exploited on texts in any language and from any domain."",","{koptient-etal-2019-simplification,
    title = ""Simplification-induced transformations: typology and some characteristics"",
    author = {Koptient, Ana{\""\i}s  and
      Cardon, R{\'e}mi  and
      Grabar, Natalia},
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5033"",
    doi = ""10.18653/v1/W19-5033"",
    pages = ""309--318"",
    abstract = ""The purpose of automatic text simplification is to transform technical or difficult to understand texts into a more friendly version. The semantics must be preserved during this transformation. Automatic text simplification can be done at different levels (lexical, syntactic, semantic, stylistic...) and relies on the corresponding knowledge and resources (lexicon, rules...). Our objective is to propose methods and material for the creation of transformation rules from a small set of parallel sentences differentiated by their technicity. We also propose a typology of transformations and quantify them. We work with French-language data related to the medical domain, although we assume that the method can be exploited on texts in any language and from any domain."",
}
@",medical domain,nlp,semant,shared task
" ""{S}cispa{C}y: Fast and Robust Models for Biomedical Natural Language Processing"","," ""Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at \url{https://allenai.github.io/scispacy/}."",","{neumann-etal-2019-scispacy,
    title = ""{S}cispa{C}y: Fast and Robust Models for Biomedical Natural Language Processing"",
    author = ""Neumann, Mark  and
      King, Daniel  and
      Beltagy, Iz  and
      Ammar, Waleed"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5034"",
    doi = ""10.18653/v1/W19-5034"",
    pages = ""319--327"",
    abstract = ""Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at \url{https://allenai.github.io/scispacy/}."",
}
@",clinical text,natural language process,natural languag,language process,nlp,shared task
" ""Overview of the {MEDIQA} 2019 Shared Task on Textual Inference, Question Entailment and Question Answering"","," ""This paper presents the MEDIQA 2019 shared task organized at the ACL-BioNLP workshop. The shared task is motivated by a need to develop relevant methods, techniques and gold standards for inference and entailment in the medical domain, and their application to improve domain specific information retrieval and question answering systems. MEDIQA 2019 includes three tasks: Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and Question Answering (QA) in the medical domain. 72 teams participated in the challenge, achieving an accuracy of 98{\%} in the NLI task, 74.9{\%} in the RQE task, and 78.3{\%} in the QA task. In this paper, we describe the tasks, the datasets, and the participants{'} approaches and results. We hope that this shared task will attract further research efforts in textual inference, question entailment, and question answering in the medical domain."",","{ben-abacha-etal-2019-overview,
    title = ""Overview of the {MEDIQA} 2019 Shared Task on Textual Inference, Question Entailment and Question Answering"",
    author = ""Ben Abacha, Asma  and
      Shivade, Chaitanya  and
      Demner-Fushman, Dina"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5039"",
    doi = ""10.18653/v1/W19-5039"",
    pages = ""370--379"",
    abstract = ""This paper presents the MEDIQA 2019 shared task organized at the ACL-BioNLP workshop. The shared task is motivated by a need to develop relevant methods, techniques and gold standards for inference and entailment in the medical domain, and their application to improve domain specific information retrieval and question answering systems. MEDIQA 2019 includes three tasks: Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and Question Answering (QA) in the medical domain. 72 teams participated in the challenge, achieving an accuracy of 98{\%} in the NLI task, 74.9{\%} in the RQE task, and 78.3{\%} in the QA task. In this paper, we describe the tasks, the datasets, and the participants{'} approaches and results. We hope that this shared task will attract further research efforts in textual inference, question entailment, and question answering in the medical domain."",
}
@",medical domain,natural languag,nlp,infer,information retriev,shared task,challeng
" ""Pentagon at {MEDIQA} 2019: Multi-task Learning for Filtering and Re-ranking Answers using Language Inference and Question Entailment"","," ""Parallel deep learning architectures like fine-tuned BERT and MT-DNN, have quickly become the state of the art, bypassing previous deep and shallow learning methods by a large margin. More recently, pre-trained models from large related datasets have been able to perform well on many downstream tasks by just fine-tuning on domain-specific datasets (similar to transfer learning). However, using powerful models on non-trivial tasks, such as ranking and large document classification, still remains a challenge due to input size limitations of parallel architecture and extremely small datasets (insufficient for fine-tuning). In this work, we introduce an end-to-end system, trained in a multi-task setting, to filter and re-rank answers in the medical domain. We use task-specific pre-trained models as deep feature extractors. Our model achieves the highest Spearman{'}s Rho and Mean Reciprocal Rank of 0.338 and 0.9622 respectively, on the ACL-BioNLP workshop MediQA Question Answering shared-task."",","{pugaliya-etal-2019-pentagon,
    title = ""Pentagon at {MEDIQA} 2019: Multi-task Learning for Filtering and Re-ranking Answers using Language Inference and Question Entailment"",
    author = ""Pugaliya, Hemant  and
      Saxena, Karan  and
      Garg, Shefali  and
      Shalini, Sheetal  and
      Gupta, Prashant  and
      Nyberg, Eric  and
      Mitamura, Teruko"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5041"",
    doi = ""10.18653/v1/W19-5041"",
    pages = ""389--398"",
    abstract = ""Parallel deep learning architectures like fine-tuned BERT and MT-DNN, have quickly become the state of the art, bypassing previous deep and shallow learning methods by a large margin. More recently, pre-trained models from large related datasets have been able to perform well on many downstream tasks by just fine-tuning on domain-specific datasets (similar to transfer learning). However, using powerful models on non-trivial tasks, such as ranking and large document classification, still remains a challenge due to input size limitations of parallel architecture and extremely small datasets (insufficient for fine-tuning). In this work, we introduce an end-to-end system, trained in a multi-task setting, to filter and re-rank answers in the medical domain. We use task-specific pre-trained models as deep feature extractors. Our model achieves the highest Spearman{'}s Rho and Mean Reciprocal Rank of 0.338 and 0.9622 respectively, on the ACL-BioNLP workshop MediQA Question Answering shared-task."",
}
@",medical domain,nlp,infer,shared task,shared-task,challeng
" ""{D}ouble{T}ransfer at {MEDIQA} 2019: Multi-Source Transfer Learning for Natural Language Understanding in the Medical Domain"","," ""This paper describes our competing system to enter the MEDIQA-2019 competition. We use a multi-source transfer learning approach to transfer the knowledge from MT-DNN and SciBERT to natural language understanding tasks in the medical domain. For transfer learning fine-tuning, we use multi-task learning on NLI, RQE and QA tasks on general and medical domains to improve performance. The proposed methods are proved effective for natural language understanding in the medical domain, and we rank the first place on the QA task."",","{xu-etal-2019-doubletransfer,
    title = ""{D}ouble{T}ransfer at {MEDIQA} 2019: Multi-Source Transfer Learning for Natural Language Understanding in the Medical Domain"",
    author = ""Xu, Yichong  and
      Liu, Xiaodong  and
      Li, Chunyuan  and
      Poon, Hoifung  and
      Gao, Jianfeng"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5042"",
    doi = ""10.18653/v1/W19-5042"",
    pages = ""399--405"",
    abstract = ""This paper describes our competing system to enter the MEDIQA-2019 competition. We use a multi-source transfer learning approach to transfer the knowledge from MT-DNN and SciBERT to natural language understanding tasks in the medical domain. For transfer learning fine-tuning, we use multi-task learning on NLI, RQE and QA tasks on general and medical domains to improve performance. The proposed methods are proved effective for natural language understanding in the medical domain, and we rank the first place on the QA task."",
}
@",medical domain,natural languag,nlp,shared task
" ""Surf at {MEDIQA} 2019: Improving Performance of Natural Language Inference in the Clinical Domain by Adopting Pre-trained Language Model"","," ""While deep learning techniques have shown promising results in many natural language processing (NLP) tasks, it has not been widely applied to the clinical domain. The lack of large datasets and the pervasive use of domain-specific language (i.e. abbreviations and acronyms) in the clinical domain causes slower progress in NLP tasks than that of the general NLP tasks. To fill this gap, we employ word/subword-level based models that adopt large-scale data-driven methods such as pre-trained language models and transfer learning in analyzing text for the clinical domain. Empirical results demonstrate the superiority of the proposed methods by achieving 90.6{\%} accuracy in medical domain natural language inference task. Furthermore, we inspect the independent strengths of the proposed approaches in quantitative and qualitative manners. This analysis will help researchers to select necessary components in building models for the medical domain."",","{nam-etal-2019-surf,
    title = ""Surf at {MEDIQA} 2019: Improving Performance of Natural Language Inference in the Clinical Domain by Adopting Pre-trained Language Model"",
    author = ""Nam, Jiin  and
      Yoon, Seunghyun  and
      Jung, Kyomin"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5043"",
    doi = ""10.18653/v1/W19-5043"",
    pages = ""406--414"",
    abstract = ""While deep learning techniques have shown promising results in many natural language processing (NLP) tasks, it has not been widely applied to the clinical domain. The lack of large datasets and the pervasive use of domain-specific language (i.e. abbreviations and acronyms) in the clinical domain causes slower progress in NLP tasks than that of the general NLP tasks. To fill this gap, we employ word/subword-level based models that adopt large-scale data-driven methods such as pre-trained language models and transfer learning in analyzing text for the clinical domain. Empirical results demonstrate the superiority of the proposed methods by achieving 90.6{\%} accuracy in medical domain natural language inference task. Furthermore, we inspect the independent strengths of the proposed approaches in quantitative and qualitative manners. This analysis will help researchers to select necessary components in building models for the medical domain."",
}
@",clinical domain,medical domain,natural language process,natural languag,language process,nlp,infer,shared task
" ""{DUT}-{BIM} at {MEDIQA} 2019: Utilizing Transformer Network and Medical Domain-Specific Contextualized Representations for Question Answering"","," ""In medical domain, given a medical question, it is difficult to manually select the most relevant information from a large number of search results. BioNLP 2019 proposes Question Answering (QA) task, which encourages the use of text mining technology to automatically judge whether a search result is an answer to the medical question. The main challenge of QA task is how to mine the semantic relation between question and answer. We propose BioBERT Transformer model to tackle this challenge, which applies Transformers to extract semantic relation between different words in questions and answers. Furthermore, BioBERT is utilized to encode medical domain-specific contextualized word representations. Our method has reached the accuracy of 76.24{\%} and spearman of 17.12{\%} on the BioNLP 2019 QA task."",","{zhou-etal-2019-dut-bim,
    title = ""{DUT}-{BIM} at {MEDIQA} 2019: Utilizing Transformer Network and Medical Domain-Specific Contextualized Representations for Question Answering"",
    author = ""Zhou, Huiwei  and
      Lei, Bizun  and
      Liu, Zhe  and
      Liu, Zhuang"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5047"",
    doi = ""10.18653/v1/W19-5047"",
    pages = ""446--452"",
    abstract = ""In medical domain, given a medical question, it is difficult to manually select the most relevant information from a large number of search results. BioNLP 2019 proposes Question Answering (QA) task, which encourages the use of text mining technology to automatically judge whether a search result is an answer to the medical question. The main challenge of QA task is how to mine the semantic relation between question and answer. We propose BioBERT Transformer model to tackle this challenge, which applies Transformers to extract semantic relation between different words in questions and answers. Furthermore, BioBERT is utilized to encode medical domain-specific contextualized word representations. Our method has reached the accuracy of 76.24{\%} and spearman of 17.12{\%} on the BioNLP 2019 QA task."",
}
@",medical domain,nlp,semant,shared task,challeng
" ""{D}r.{Q}uad at {MEDIQA} 2019: Towards Textual Inference and Question Entailment using contextualized representations"","," ""This paper presents the submissions by TeamDr.Quad to the ACL-BioNLP 2019 shared task on Textual Inference and Question Entailment in the Medical Domain. Our system is based on the prior work Liu et al. (2019) which uses a multi-task objective function for textual entailment. In this work, we explore different strategies for generalizing state-of-the-art language understanding models to the specialized medical domain. Our results on the shared task demonstrate that incorporating domain knowledge through data augmentation is a powerful strategy for addressing challenges posed specialized domains such as medicine."",","{bannihatti-kumar-etal-2019-dr,
    title = ""{D}r.{Q}uad at {MEDIQA} 2019: Towards Textual Inference and Question Entailment using contextualized representations"",
    author = ""Bannihatti Kumar, Vinayshekhar  and
      Srinivasan, Ashwin  and
      Chaudhary, Aditi  and
      Route, James  and
      Mitamura, Teruko  and
      Nyberg, Eric"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5048"",
    doi = ""10.18653/v1/W19-5048"",
    pages = ""453--461"",
    abstract = ""This paper presents the submissions by TeamDr.Quad to the ACL-BioNLP 2019 shared task on Textual Inference and Question Entailment in the Medical Domain. Our system is based on the prior work Liu et al. (2019) which uses a multi-task objective function for textual entailment. In this work, we explore different strategies for generalizing state-of-the-art language understanding models to the specialized medical domain. Our results on the shared task demonstrate that incorporating domain knowledge through data augmentation is a powerful strategy for addressing challenges posed specialized domains such as medicine."",
}
@",medical domain,nlp,infer,shared task,challeng
" ""Sieg at {MEDIQA} 2019: Multi-task Neural Ensemble for Biomedical Inference and Entailment"","," ""This paper presents a multi-task learning approach to natural language inference (NLI) and question entailment (RQE) in the biomedical domain. Recognizing textual inference relations and question similarity can address the issue of answering new consumer health questions by mapping them to Frequently Asked Questions on reputed websites like the NIH. We show that leveraging information from parallel tasks across domains along with medical knowledge integration allows our model to learn better biomedical feature representations. Our final models for the NLI and RQE tasks achieve the 4th and 2nd rank on the shared-task leaderboard respectively."",","{bhaskar-etal-2019-sieg,
    title = ""Sieg at {MEDIQA} 2019: Multi-task Neural Ensemble for Biomedical Inference and Entailment"",
    author = ""Bhaskar, Sai Abishek  and
      Rungta, Rashi  and
      Route, James  and
      Nyberg, Eric  and
      Mitamura, Teruko"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5049"",
    doi = ""10.18653/v1/W19-5049"",
    pages = ""462--470"",
    abstract = ""This paper presents a multi-task learning approach to natural language inference (NLI) and question entailment (RQE) in the biomedical domain. Recognizing textual inference relations and question similarity can address the issue of answering new consumer health questions by mapping them to Frequently Asked Questions on reputed websites like the NIH. We show that leveraging information from parallel tasks across domains along with medical knowledge integration allows our model to learn better biomedical feature representations. Our final models for the NLI and RQE tasks achieve the 4th and 2nd rank on the shared-task leaderboard respectively."",
}
@",medical domain,natural languag,nlp,infer,shared task,shared-task
" ""{ANU}-{CSIRO} at {MEDIQA} 2019: Question Answering Using Deep Contextual Knowledge"","," ""We report on our system for textual inference and question entailment in the medical domain for the ACL BioNLP 2019 Shared Task, MEDIQA. Textual inference is the task of finding the semantic relationships between pairs of text. Question entailment involves identifying pairs of questions which have similar semantic content. To improve upon medical natural language inference and question entailment approaches to further medical question answering, we propose a system that incorporates open-domain and biomedical domain approaches to improve semantic understanding and ambiguity resolution. Our models achieve 80{\%} accuracy on medical natural language inference (6.5{\%} absolute improvement over the original baseline), 48.9{\%} accuracy on recognising medical question entailment, 0.248 Spearman{'}s rho for question answering ranking and 68.6{\%} accuracy for question answering classification."",","{nguyen-etal-2019-anu,
    title = ""{ANU}-{CSIRO} at {MEDIQA} 2019: Question Answering Using Deep Contextual Knowledge"",
    author = ""Nguyen, Vincent  and
      Karimi, Sarvnaz  and
      Xing, Zhenchang"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5051"",
    doi = ""10.18653/v1/W19-5051"",
    pages = ""478--487"",
    abstract = ""We report on our system for textual inference and question entailment in the medical domain for the ACL BioNLP 2019 Shared Task, MEDIQA. Textual inference is the task of finding the semantic relationships between pairs of text. Question entailment involves identifying pairs of questions which have similar semantic content. To improve upon medical natural language inference and question entailment approaches to further medical question answering, we propose a system that incorporates open-domain and biomedical domain approaches to improve semantic understanding and ambiguity resolution. Our models achieve 80{\%} accuracy on medical natural language inference (6.5{\%} absolute improvement over the original baseline), 48.9{\%} accuracy on recognising medical question entailment, 0.248 Spearman{'}s rho for question answering ranking and 68.6{\%} accuracy for question answering classification."",
}
@",medical domain,natural languag,nlp,infer,semant,shared task
" ""{MSIT}{\_}{SRIB} at {MEDIQA} 2019: Knowledge Directed Multi-task Framework for Natural Language Inference in Clinical Domain."","," ""In this paper, we present Biomedical Multi-Task Deep Neural Network (Bio-MTDNN) on the NLI task of MediQA 2019 challenge. Bio-MTDNN utilizes {``}transfer learning{''} based paradigm where not only the source and target domains are different but also the source and target tasks are varied, although related. Further, Bio-MTDNN integrates knowledge from external sources such as clinical databases (UMLS) enhancing its performance on the clinical domain. Our proposed method outperformed the official baseline and other prior models (such as ESIM and Infersent on dev set) by a considerable margin as evident from our experimental results."",","{chopra-etal-2019-msit,
    title = ""{MSIT}{\_}{SRIB} at {MEDIQA} 2019: Knowledge Directed Multi-task Framework for Natural Language Inference in Clinical Domain."",
    author = ""Chopra, Sahil  and
      Gupta, Ankita  and
      Kaushik, Anupama"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5052"",
    doi = ""10.18653/v1/W19-5052"",
    pages = ""488--492"",
    abstract = ""In this paper, we present Biomedical Multi-Task Deep Neural Network (Bio-MTDNN) on the NLI task of MediQA 2019 challenge. Bio-MTDNN utilizes {``}transfer learning{''} based paradigm where not only the source and target domains are different but also the source and target tasks are varied, although related. Further, Bio-MTDNN integrates knowledge from external sources such as clinical databases (UMLS) enhancing its performance on the clinical domain. Our proposed method outperformed the official baseline and other prior models (such as ESIM and Infersent on dev set) by a considerable margin as evident from our experimental results."",
}
@",clinical domain,natural languag,nlp,infer,shared task,challeng
" ""{UU}{\_}{TAILS} at {MEDIQA} 2019: Learning Textual Entailment in the Medical Domain"","," ""This article describes the participation of the UU{\_}TAILS team in the 2019 MEDIQA challenge intended to improve domain-specific models in medical and clinical NLP. The challenge consists of 3 tasks: medical language inference (NLI), recognizing textual entailment (RQE) and question answering (QA). Our team participated in tasks 1 and 2 and our best runs achieved a performance accuracy of 0.852 and 0.584 respectively for the test sets. The models proposed for task 1 relied on BERT embeddings and different ensemble techniques. For the RQE task, we trained a traditional multilayer perceptron network based on embeddings generated by the universal sentence encoder."",","{tawfik-spruit-2019-uu,
    title = ""{UU}{\_}{TAILS} at {MEDIQA} 2019: Learning Textual Entailment in the Medical Domain"",
    author = ""Tawfik, Noha  and
      Spruit, Marco"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5053"",
    doi = ""10.18653/v1/W19-5053"",
    pages = ""493--499"",
    abstract = ""This article describes the participation of the UU{\_}TAILS team in the 2019 MEDIQA challenge intended to improve domain-specific models in medical and clinical NLP. The challenge consists of 3 tasks: medical language inference (NLI), recognizing textual entailment (RQE) and question answering (QA). Our team participated in tasks 1 and 2 and our best runs achieved a performance accuracy of 0.852 and 0.584 respectively for the test sets. The models proposed for task 1 relied on BERT embeddings and different ensemble techniques. For the RQE task, we trained a traditional multilayer perceptron network based on embeddings generated by the universal sentence encoder."",
}
@",medical domain,nlp,infer,generat,shared task,challeng
" ""Saama Research at {MEDIQA} 2019: Pre-trained {B}io{BERT} with Attention Visualisation for Medical Natural Language Inference"","," ""Natural Language inference is the task of identifying relation between two sentences as entailment, contradiction or neutrality. MedNLI is a biomedical flavour of NLI for clinical domain. This paper explores the use of Bidirectional Encoder Representation from Transformer (BERT) for solving MedNLI. The proposed model, BERT pre-trained on PMC, PubMed and fine-tuned on MIMICIII v1.4, achieves state of the art results on MedNLI (83.45{\%}) and an accuracy of 78.5{\%} in MEDIQA challenge. The authors present an analysis of the attention patterns that emerged as a result of training BERT on MedNLI using a visualization tool, bertviz."",","{kanakarajan-etal-2019-saama,
    title = ""Saama Research at {MEDIQA} 2019: Pre-trained {B}io{BERT} with Attention Visualisation for Medical Natural Language Inference"",
    author = ""Kanakarajan, Kamal raj  and
      Ramamoorthy, Suriyadeepan  and
      Archana, Vaidheeswaran  and
      Chatterjee, Soham  and
      Sankarasubbu, Malaikannan"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5055"",
    doi = ""10.18653/v1/W19-5055"",
    pages = ""510--516"",
    abstract = ""Natural Language inference is the task of identifying relation between two sentences as entailment, contradiction or neutrality. MedNLI is a biomedical flavour of NLI for clinical domain. This paper explores the use of Bidirectional Encoder Representation from Transformer (BERT) for solving MedNLI. The proposed model, BERT pre-trained on PMC, PubMed and fine-tuned on MIMICIII v1.4, achieves state of the art results on MedNLI (83.45{\%}) and an accuracy of 78.5{\%} in MEDIQA challenge. The authors present an analysis of the attention patterns that emerged as a result of training BERT on MedNLI using a visualization tool, bertviz."",
}
@",clinical domain,natural languag,nlp,infer,shared task,challeng
" ""{NCUEE} at {MEDIQA} 2019: Medical Text Inference Using Ensemble {BERT}-{B}i{LSTM}-Attention Model"","," ""This study describes the model design of the NCUEE system for the MEDIQA challenge at the ACL-BioNLP 2019 workshop. We use the BERT (Bidirectional Encoder Representations from Transformers) as the word embedding method to integrate the BiLSTM (Bidirectional Long Short-Term Memory) network with an attention mechanism for medical text inferences. A total of 42 teams participated in natural language inference task at MEDIQA 2019. Our best accuracy score of 0.84 ranked the top-third among all submissions in the leaderboard."",","{lee-etal-2019-ncuee,
    title = ""{NCUEE} at {MEDIQA} 2019: Medical Text Inference Using Ensemble {BERT}-{B}i{LSTM}-Attention Model"",
    author = ""Lee, Lung-Hao  and
      Lu, Yi  and
      Chen, Po-Han  and
      Lee, Po-Lei  and
      Shyu, Kuo-Kai"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5058"",
    doi = ""10.18653/v1/W19-5058"",
    pages = ""528--532"",
    abstract = ""This study describes the model design of the NCUEE system for the MEDIQA challenge at the ACL-BioNLP 2019 workshop. We use the BERT (Bidirectional Encoder Representations from Transformers) as the word embedding method to integrate the BiLSTM (Bidirectional Long Short-Term Memory) network with an attention mechanism for medical text inferences. A total of 42 teams participated in natural language inference task at MEDIQA 2019. Our best accuracy score of 0.84 ranked the top-third among all submissions in the leaderboard."",
}
@",medical text,natural languag,nlp,infer,shared task,challeng
" ""A Benchmark Corpus of {E}nglish Misspellings and a Minimally-supervised Model for Spelling Correction"","," ""Spelling correction has attracted a lot of attention in the NLP community. However, models have been usually evaluated on artificiallycreated or proprietary corpora. A publiclyavailable corpus of authentic misspellings, annotated in context, is still lacking. To address this, we present and release an annotated data set of 6,121 spelling errors in context, based on a corpus of essays written by English language learners. We also develop a minimallysupervised context-aware approach to spelling correction. It achieves strong results on our data: 88.12{\%} accuracy. This approach can also train with a minimal amount of annotated data (performance reduced by less than 1{\%}). Furthermore, this approach allows easy portability to new domains. We evaluate our model on data from a medical domain and demonstrate that it rivals the performance of a model trained and tuned on in-domain data."",","{flor-etal-2019-benchmark,
    title = ""A Benchmark Corpus of {E}nglish Misspellings and a Minimally-supervised Model for Spelling Correction"",
    author = ""Flor, Michael  and
      Fried, Michael  and
      Rozovskaya, Alla"",
    editor = ""Yannakoudakis, Helen  and
      Kochmar, Ekaterina  and
      Leacock, Claudia  and
      Madnani, Nitin  and
      Pil{\'a}n, Ildik{\'o}  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-4407"",
    doi = ""10.18653/v1/W19-4407"",
    pages = ""76--86"",
    abstract = ""Spelling correction has attracted a lot of attention in the NLP community. However, models have been usually evaluated on artificiallycreated or proprietary corpora. A publiclyavailable corpus of authentic misspellings, annotated in context, is still lacking. To address this, we present and release an annotated data set of 6,121 spelling errors in context, based on a corpus of essays written by English language learners. We also develop a minimallysupervised context-aware approach to spelling correction. It achieves strong results on our data: 88.12{\%} accuracy. This approach can also train with a minimal amount of annotated data (performance reduced by less than 1{\%}). Furthermore, this approach allows easy portability to new domains. We evaluate our model on data from a medical domain and demonstrate that it rivals the performance of a model trained and tuned on in-domain data."",
}
@",medical domain,nlp,annotat,benchmark,evalu
" ""Extracting Kinship from Obituary to Enhance Electronic Health Records for Genetic Research"","," ""Claims database and electronic health records database do not usually capture kinship or family relationship information, which is imperative for genetic research. We identify online obituaries as a new data source and propose a special named entity recognition and relation extraction solution to extract names and kinships from online obituaries. Built on 1,809 annotated obituaries and a novel tagging scheme, our joint neural model achieved macro-averaged precision, recall and F measure of 72.69{\%}, 78.54{\%} and 74.93{\%}, and micro-averaged precision, recall and F measure of 95.74{\%}, 98.25{\%} and 96.98{\%} using 57 kinships with 10 or more examples in a 10-fold cross-validation experiment. The model performance improved dramatically when trained with 34 kinships with 50 or more examples. Leveraging additional information such as age, death date, birth date and residence mentioned by obituaries, we foresee a promising future of supplementing EHR databases with comprehensive and accurate kinship information for genetic research."",","{he-etal-2019-extracting,
    title = ""Extracting Kinship from Obituary to Enhance Electronic Health Records for Genetic Research"",
    author = ""He, Kai  and
      Wu, Jialun  and
      Ma, Xiaoyong  and
      Zhang, Chong  and
      Huang, Ming  and
      Li, Chen  and
      Yao, Lixia"",
    editor = ""Weissenbacher, Davy  and
      Gonzalez-Hernandez, Graciela"",
    booktitle = ""Proceedings of the Fourth Social Media Mining for Health Applications ({\#}SMM4H) Workshop {\&} Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-3201"",
    doi = ""10.18653/v1/W19-3201"",
    pages = ""1--10"",
    abstract = ""Claims database and electronic health records database do not usually capture kinship or family relationship information, which is imperative for genetic research. We identify online obituaries as a new data source and propose a special named entity recognition and relation extraction solution to extract names and kinships from online obituaries. Built on 1,809 annotated obituaries and a novel tagging scheme, our joint neural model achieved macro-averaged precision, recall and F measure of 72.69{\%}, 78.54{\%} and 74.93{\%}, and micro-averaged precision, recall and F measure of 95.74{\%}, 98.25{\%} and 96.98{\%} using 57 kinships with 10 or more examples in a 10-fold cross-validation experiment. The model performance improved dramatically when trained with 34 kinships with 50 or more examples. Leveraging additional information such as age, death date, birth date and residence mentioned by obituaries, we foresee a promising future of supplementing EHR databases with comprehensive and accurate kinship information for genetic research."",
}
@",electronic health record,health record,relation extract,entity recognit,shared task,annotat
" ""Lexical Normalization of User-Generated Medical Text"","," ""In the medical domain, user-generated social media text is increasingly used as a valuable complementary knowledge source to scientific medical literature. The extraction of this knowledge is complicated by colloquial language use and misspellings. Yet, lexical normalization of such data has not been addressed properly. This paper presents an unsupervised, data-driven spelling correction module for medical social media. Our method outperforms state-of-the-art spelling correction and can detect mistakes with an F0.5 of 0.888. Additionally, we present a novel corpus for spelling mistake detection and correction on a medical patient forum."",","{dirkson-etal-2019-lexical,
    title = ""Lexical Normalization of User-Generated Medical Text"",
    author = ""Dirkson, Anne  and
      Verberne, Suzan  and
      Kraaij, Wessel"",
    editor = ""Weissenbacher, Davy  and
      Gonzalez-Hernandez, Graciela"",
    booktitle = ""Proceedings of the Fourth Social Media Mining for Health Applications ({\#}SMM4H) Workshop {\&} Shared Task"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-3202"",
    doi = ""10.18653/v1/W19-3202"",
    pages = ""11--20"",
    abstract = ""In the medical domain, user-generated social media text is increasingly used as a valuable complementary knowledge source to scientific medical literature. The extraction of this knowledge is complicated by colloquial language use and misspellings. Yet, lexical normalization of such data has not been addressed properly. This paper presents an unsupervised, data-driven spelling correction module for medical social media. Our method outperforms state-of-the-art spelling correction and can detect mistakes with an F0.5 of 0.888. Additionally, we present a novel corpus for spelling mistake detection and correction on a medical patient forum."",
}
@",medical domain,medical text,generat,shared task
" ""Effective Feature Representation for Clinical Text Concept Extraction"","," ""Crucial information about the practice of healthcare is recorded only in free-form text, which creates an enormous opportunity for high-impact NLP. However, annotated healthcare datasets tend to be small and expensive to obtain, which raises the question of how to make maximally efficient uses of the available data. To this end, we develop an LSTM-CRF model for combining unsupervised word representations and hand-built feature representations derived from publicly available healthcare ontologies. We show that this combined model yields superior performance on five datasets of diverse kinds of healthcare text (clinical, social, scientific, commercial). Each involves the labeling of complex, multi-word spans that pick out different healthcare concepts. We also introduce a new labeled dataset for identifying the treatment relations between drugs and diseases."",","{tao-etal-2019-effective,
    title = ""Effective Feature Representation for Clinical Text Concept Extraction"",
    author = ""Tao, Yifeng  and
      Godefroy, Bruno  and
      Genthial, Guillaume  and
      Potts, Christopher"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 2nd Clinical Natural Language Processing Workshop"",
    month = jun,
    year = ""2019"",
    address = ""Minneapolis, Minnesota, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-1901"",
    doi = ""10.18653/v1/W19-1901"",
    pages = ""1--14"",
    abstract = ""Crucial information about the practice of healthcare is recorded only in free-form text, which creates an enormous opportunity for high-impact NLP. However, annotated healthcare datasets tend to be small and expensive to obtain, which raises the question of how to make maximally efficient uses of the available data. To this end, we develop an LSTM-CRF model for combining unsupervised word representations and hand-built feature representations derived from publicly available healthcare ontologies. We show that this combined model yields superior performance on five datasets of diverse kinds of healthcare text (clinical, social, scientific, commercial). Each involves the labeling of complex, multi-word spans that pick out different healthcare concepts. We also introduce a new labeled dataset for identifying the treatment relations between drugs and diseases."",
}
@",clinical text,natural language process,natural languag,language process,nlp,annotat
" ""Towards Automatic Generation of Shareable Synthetic Clinical Notes Using Neural Language Models"","," ""Large-scale clinical data is invaluable to driving many computational scientific advances today. However, understandable concerns regarding patient privacy hinder the open dissemination of such data and give rise to suboptimal siloed research. De-identification methods attempt to address these concerns but were shown to be susceptible to adversarial attacks. In this work, we focus on the vast amounts of unstructured natural language data stored in clinical notes and propose to automatically generate synthetic clinical notes that are more amenable to sharing using generative models trained on real de-identified records. To evaluate the merit of such notes, we measure both their privacy preservation properties as well as utility in training clinical NLP models. Experiments using neural language models yield notes whose utility is close to that of the real ones in some clinical NLP tasks, yet leave ample room for future improvements."",","{melamud-shivade-2019-towards,
    title = ""Towards Automatic Generation of Shareable Synthetic Clinical Notes Using Neural Language Models"",
    author = ""Melamud, Oren  and
      Shivade, Chaitanya"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 2nd Clinical Natural Language Processing Workshop"",
    month = jun,
    year = ""2019"",
    address = ""Minneapolis, Minnesota, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-1905"",
    doi = ""10.18653/v1/W19-1905"",
    pages = ""35--45"",
    abstract = ""Large-scale clinical data is invaluable to driving many computational scientific advances today. However, understandable concerns regarding patient privacy hinder the open dissemination of such data and give rise to suboptimal siloed research. De-identification methods attempt to address these concerns but were shown to be susceptible to adversarial attacks. In this work, we focus on the vast amounts of unstructured natural language data stored in clinical notes and propose to automatically generate synthetic clinical notes that are more amenable to sharing using generative models trained on real de-identified records. To evaluate the merit of such notes, we measure both their privacy preservation properties as well as utility in training clinical NLP models. Experiments using neural language models yield notes whose utility is close to that of the real ones in some clinical NLP tasks, yet leave ample room for future improvements."",
}
@",clinical not,natural language process,natural languag,language process,nlp,generat,evalu
" ""A General-Purpose Annotation Model for Knowledge Discovery: Case Study in {S}panish Clinical Text"","," ""Knowledge discovery from text in natural language is a task usually aided by the manual construction of annotated corpora. Specifically in the clinical domain, several annotation models are used depending on the characteristics of the task to solve (e.g., named entity recognition, relation extraction, etc.). However, few general-purpose annotation models exist, that can support a broad range of knowledge extraction tasks. This paper presents an annotation model designed to capture a large portion of the semantics of natural language text. The structure of the annotation model is presented, with examples of annotated sentences and a brief description of each semantic role and relation defined. This research focuses on an application to clinical texts in the Spanish language. Nevertheless, the presented annotation model is extensible to other domains and languages. An example of annotated sentences, guidelines, and suitable configuration files for an annotation tool are also provided for the research community."",","{piad-morffis-etal-2019-general,
    title = ""A General-Purpose Annotation Model for Knowledge Discovery: Case Study in {S}panish Clinical Text"",
    author = ""Piad-Morffis, Alejandro  and
      Guit{\'e}rrez, Yoan  and
      Estevez-Velarde, Suilan  and
      Mu{\~n}oz, Rafael"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 2nd Clinical Natural Language Processing Workshop"",
    month = jun,
    year = ""2019"",
    address = ""Minneapolis, Minnesota, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-1910"",
    doi = ""10.18653/v1/W19-1910"",
    pages = ""79--88"",
    abstract = ""Knowledge discovery from text in natural language is a task usually aided by the manual construction of annotated corpora. Specifically in the clinical domain, several annotation models are used depending on the characteristics of the task to solve (e.g., named entity recognition, relation extraction, etc.). However, few general-purpose annotation models exist, that can support a broad range of knowledge extraction tasks. This paper presents an annotation model designed to capture a large portion of the semantics of natural language text. The structure of the annotation model is presented, with examples of annotated sentences and a brief description of each semantic role and relation defined. This research focuses on an application to clinical texts in the Spanish language. Nevertheless, the presented annotation model is extensible to other domains and languages. An example of annotated sentences, guidelines, and suitable configuration files for an annotation tool are also provided for the research community."",
}
@",clinical domain,clinical text,natural language process,natural languag,language process,relation extract,entity recognit,semant,annotat
" ""Medical Entity Linking using Triplet Network"","," ""Entity linking (or Normalization) is an essential task in text mining that maps the entity mentions in the medical text to standard entities in a given Knowledge Base (KB). This task is of great importance in the medical domain. It can also be used for merging different medical and clinical ontologies. In this paper, we center around the problem of disease linking or normalization. This task is executed in two phases: candidate generation and candidate scoring. In this paper, we present an approach to rank the candidate Knowledge Base entries based on their similarity with disease mention. We make use of the Triplet Network for candidate ranking. While the existing methods have used carefully generated sieves and external resources for candidate generation, we introduce a robust and portable candidate generation scheme that does not make use of the hand-crafted rules. Experimental results on the standard benchmark NCBI disease dataset demonstrate that our system outperforms the prior methods by a significant margin."",","{mondal-etal-2019-medical,
    title = ""Medical Entity Linking using Triplet Network"",
    author = ""Mondal, Ishani  and
      Purkayastha, Sukannya  and
      Sarkar, Sudeshna  and
      Goyal, Pawan  and
      Pillai, Jitesh  and
      Bhattacharyya, Amitava  and
      Gattu, Mahanandeeshwar"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 2nd Clinical Natural Language Processing Workshop"",
    month = jun,
    year = ""2019"",
    address = ""Minneapolis, Minnesota, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-1912"",
    doi = ""10.18653/v1/W19-1912"",
    pages = ""95--100"",
    abstract = ""Entity linking (or Normalization) is an essential task in text mining that maps the entity mentions in the medical text to standard entities in a given Knowledge Base (KB). This task is of great importance in the medical domain. It can also be used for merging different medical and clinical ontologies. In this paper, we center around the problem of disease linking or normalization. This task is executed in two phases: candidate generation and candidate scoring. In this paper, we present an approach to rank the candidate Knowledge Base entries based on their similarity with disease mention. We make use of the Triplet Network for candidate ranking. While the existing methods have used carefully generated sieves and external resources for candidate generation, we introduce a robust and portable candidate generation scheme that does not make use of the hand-crafted rules. Experimental results on the standard benchmark NCBI disease dataset demonstrate that our system outperforms the prior methods by a significant margin."",
}
@",medical domain,medical text,natural language process,natural languag,language process,generat,benchmark
" ""Annotating and Characterizing Clinical Sentences with Explicit Why-{QA} Cues"","," ""Many clinical information needs can be stated as why-questions. The answers to them represent important clinical reasoning and justification. Clinical notes are a rich source for such why-question answering (why-QA). However, there are few dedicated corpora, and little is known about the characteristics of clinical why-QA narratives. To address this gap, the study performed manual annotation of 277 sentences containing explicit why-QA cues and summarized their quantitative and qualitative properties. The contributions are: 1) sharing a seed corpus that can be used for various QA-related training purposes, 2) adding to our knowledge about the diversity and distribution of clinical why-QA contents."",","{fan-2019-annotating,
    title = ""Annotating and Characterizing Clinical Sentences with Explicit Why-{QA} Cues"",
    author = ""Fan, Jungwei"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 2nd Clinical Natural Language Processing Workshop"",
    month = jun,
    year = ""2019"",
    address = ""Minneapolis, Minnesota, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-1913"",
    doi = ""10.18653/v1/W19-1913"",
    pages = ""101--106"",
    abstract = ""Many clinical information needs can be stated as why-questions. The answers to them represent important clinical reasoning and justification. Clinical notes are a rich source for such why-question answering (why-QA). However, there are few dedicated corpora, and little is known about the characteristics of clinical why-QA narratives. To address this gap, the study performed manual annotation of 277 sentences containing explicit why-QA cues and summarized their quantitative and qualitative properties. The contributions are: 1) sharing a seed corpus that can be used for various QA-related training purposes, 2) adding to our knowledge about the diversity and distribution of clinical why-QA contents."",
}
@",clinical not,natural language process,natural languag,language process,summar,annotat
" ""Distinguishing Clinical Sentiment: The Importance of Domain Adaptation in Psychiatric Patient Health Records"","," ""Recently natural language processing (NLP) tools have been developed to identify and extract salient risk indicators in electronic health records (EHRs). Sentiment analysis, although widely used in non-medical areas for improving decision making, has been studied minimally in the clinical setting. In this study, we undertook, to our knowledge, the first domain adaptation of sentiment analysis to psychiatric EHRs by defining psychiatric clinical sentiment, performing an annotation project, and evaluating multiple sentence-level sentiment machine learning (ML) models. Results indicate that off-the-shelf sentiment analysis tools fail in identifying clinically positive or negative polarity, and that the definition of clinical sentiment that we provide is learnable with relatively small amounts of training data. This project is an initial step towards further refining sentiment analysis methods for clinical use. Our long-term objective is to incorporate the results of this project as part of a machine learning model that predicts inpatient readmission risk. We hope that this work will initiate a discussion concerning domain adaptation of sentiment analysis to the clinical setting."",","{holderness-etal-2019-distinguishing,
    title = ""Distinguishing Clinical Sentiment: The Importance of Domain Adaptation in Psychiatric Patient Health Records"",
    author = ""Holderness, Eben  and
      Cawkwell, Philip  and
      Bolton, Kirsten  and
      Pustejovsky, James  and
      Hall, Mei-Hua"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the 2nd Clinical Natural Language Processing Workshop"",
    month = jun,
    year = ""2019"",
    address = ""Minneapolis, Minnesota, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-1915"",
    doi = ""10.18653/v1/W19-1915"",
    pages = ""117--123"",
    abstract = ""Recently natural language processing (NLP) tools have been developed to identify and extract salient risk indicators in electronic health records (EHRs). Sentiment analysis, although widely used in non-medical areas for improving decision making, has been studied minimally in the clinical setting. In this study, we undertook, to our knowledge, the first domain adaptation of sentiment analysis to psychiatric EHRs by defining psychiatric clinical sentiment, performing an annotation project, and evaluating multiple sentence-level sentiment machine learning (ML) models. Results indicate that off-the-shelf sentiment analysis tools fail in identifying clinically positive or negative polarity, and that the definition of clinical sentiment that we provide is learnable with relatively small amounts of training data. This project is an initial step towards further refining sentiment analysis methods for clinical use. Our long-term objective is to incorporate the results of this project as part of a machine learning model that predicts inpatient readmission risk. We hope that this work will initiate a discussion concerning domain adaptation of sentiment analysis to the clinical setting."",
}
@",electronic health record,health record,natural language process,natural languag,language process,nlp,sentiment,annotat,evalu
" ""Parallel Sentence Retrieval From Comparable Corpora for Biomedical Text Simplification"","," ""Parallel sentences provide semantically similar information which can vary on a given dimension, such as language or register. Parallel sentences with register variation (like expert and non-expert documents) can be exploited for the automatic text simplification. The aim of automatic text simplification is to better access and understand a given information. In the biomedical field, simplification may permit patients to understand medical and health texts. Yet, there is currently no such available resources. We propose to exploit comparable corpora which are distinguished by their registers (specialized and simplified versions) to detect and align parallel sentences. These corpora are in French and are related to the biomedical area. Manually created reference data show 0.76 inter-annotator agreement. Our purpose is to state whether a given pair of specialized and simplified sentences is parallel and can be aligned or not. We treat this task as binary classification (alignment/non-alignment). We perform experiments with a controlled ratio of imbalance and on the highly unbalanced real data. Our results show that the method we present here can be used to automatically generate a corpus of parallel sentences from our comparable corpus."",","{cardon-grabar-2019-parallel,
    title = ""Parallel Sentence Retrieval From Comparable Corpora for Biomedical Text Simplification"",
    author = ""Cardon, R{\'e}mi  and
      Grabar, Natalia"",
    editor = ""Mitkov, Ruslan  and
      Angelova, Galia"",
    booktitle = ""Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)"",
    month = sep,
    year = ""2019"",
    address = ""Varna, Bulgaria"",
    publisher = ""INCOMA Ltd."",
    url = ""https://aclanthology.org/R19-1020"",
    doi = ""10.26615/978-954-452-056-4_020"",
    pages = ""168--177"",
    abstract = ""Parallel sentences provide semantically similar information which can vary on a given dimension, such as language or register. Parallel sentences with register variation (like expert and non-expert documents) can be exploited for the automatic text simplification. The aim of automatic text simplification is to better access and understand a given information. In the biomedical field, simplification may permit patients to understand medical and health texts. Yet, there is currently no such available resources. We propose to exploit comparable corpora which are distinguished by their registers (specialized and simplified versions) to detect and align parallel sentences. These corpora are in French and are related to the biomedical area. Manually created reference data show 0.76 inter-annotator agreement. Our purpose is to state whether a given pair of specialized and simplified sentences is parallel and can be aligned or not. We treat this task as binary classification (alignment/non-alignment). We perform experiments with a controlled ratio of imbalance and on the highly unbalanced real data. Our results show that the method we present here can be used to automatically generate a corpus of parallel sentences from our comparable corpus."",
}
@",medical text,natural language process,natural languag,language process,nlp,generat,semant,annotat
" ""Toponym Detection in the Bio-Medical Domain: A Hybrid Approach with Deep Learning"","," ""This paper compares how different machine learning classifiers can be used together with simple string matching and named entity recognition to detect locations in texts. We compare five different state-of-the-art machine learning classifiers in order to predict whether a sentence contains a location or not. Following this classification task, we use a string matching algorithm with a gazetteer to identify the exact index of a toponym within the sentence. We evaluate different approaches in terms of machine learning classifiers, text pre-processing and location extraction on the SemEval-2019 Task 12 dataset, compiled for toponym resolution in the bio-medical domain. Finally, we compare the results with our system that was previously submitted to the SemEval-2019 task evaluation."",","{plum-etal-2019-toponym,
    title = ""Toponym Detection in the Bio-Medical Domain: A Hybrid Approach with Deep Learning"",
    author = ""Plum, Alistair  and
      Ranasinghe, Tharindu  and
      Orasan, Constantin"",
    editor = ""Mitkov, Ruslan  and
      Angelova, Galia"",
    booktitle = ""Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)"",
    month = sep,
    year = ""2019"",
    address = ""Varna, Bulgaria"",
    publisher = ""INCOMA Ltd."",
    url = ""https://aclanthology.org/R19-1106"",
    doi = ""10.26615/978-954-452-056-4_106"",
    pages = ""912--921"",
    abstract = ""This paper compares how different machine learning classifiers can be used together with simple string matching and named entity recognition to detect locations in texts. We compare five different state-of-the-art machine learning classifiers in order to predict whether a sentence contains a location or not. Following this classification task, we use a string matching algorithm with a gazetteer to identify the exact index of a toponym within the sentence. We evaluate different approaches in terms of machine learning classifiers, text pre-processing and location extraction on the SemEval-2019 Task 12 dataset, compiled for toponym resolution in the bio-medical domain. Finally, we compare the results with our system that was previously submitted to the SemEval-2019 task evaluation."",
}
@",medical domain,natural language process,natural languag,language process,nlp,entity recognit,evalu
" ""Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts"","," ""Many natural language processing tasks require discriminating the particular meaning of a word in context, but building corpora for developing sense-aware models can be a challenge. We present a large resource of example usages for words having a particular meaning, called Paraphrase-Sense-Tagged Sentences (PSTS). Built on the premise that a word{'}s paraphrases instantiate its fine-grained meanings (i.e., bug has different meanings corresponding to its paraphrases fly and microbe) the resource contains up to 10,000 sentences for each of 3 million target-paraphrase pairs where the target word takes on the meaning of the paraphrase. We describe an automatic method based on bilingual pivoting used to enumerate sentences for PSTS, and present two models for ranking PSTS sentences based on their quality. Finally, we demonstrate the utility of PSTS by using it to build a dataset for the task of hypernym prediction in context. Training a model on this automatically generated dataset produces accuracy that is competitive with a model trained on smaller datasets crafted with some manual effort."",","{thorsteinsson-etal-2019-wide,
    title = ""A Wide-Coverage Context-Free Grammar for {I}celandic and an Accompanying Parsing System"",
    author = ""{\TH}orsteinsson, Vilhj{\'a}lmur  and
      {\'O}lad{\'o}ttir, Hulda  and
      Loftsson, Hrafn"",
    editor = ""Mitkov, Ruslan  and
      Angelova, Galia"",
    booktitle = ""Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)"",
    month = sep,
    year = ""2019"",
    address = ""Varna, Bulgaria"",
    publisher = ""INCOMA Ltd."",
    url = ""https://aclanthology.org/R19-1160"",
    doi = ""10.26615/978-954-452-056-4_160"",
    pages = ""1397--1404"",
    abstract = ""We present an open-source, wide-coverage context-free grammar (CFG) for Icelandic, and an accompanying parsing system. The grammar has over 5,600 nonterminals, 4,600 terminals and 19,000 productions in fully expanded form, with feature agreement constraints for case, gender, number and person. The parsing system consists of an enhanced Earley-based parser and a mechanism to select best-scoring parse trees from shared packed parse forests. Our parsing system is able to parse about 90{\%} of all sentences in articles published on the main Icelandic news websites. Preliminary evaluation with evalb shows an F-measure of 70.72{\%} on parsed sentences. Our system demonstrates that parsing a morphologically rich language using a wide-coverage CFG can be practical."",
}
@proceedings{tacl-2019-transactions,
    title = ""Transactions of the Association for Computational Linguistics, Volume 7"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1000"",
}
@article{rozovskaya-roth-2019-grammar,
    title = ""Grammar Error Correction in Morphologically Rich Languages: The Case of {R}ussian"",
    author = ""Rozovskaya, Alla  and
      Roth, Dan"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1001"",
    doi = ""10.1162/tacl_a_00251"",
    pages = ""1--17"",
    abstract = ""Until now, most of the research in grammar error correction focused on English, and the problem has hardly been explored for other languages. We address the task of correcting writing mistakes in morphologically rich languages, with a focus on Russian. We present a corrected and error-tagged corpus of Russian learner writing and develop models that make use of existing state-of-the-art methods that have been well studied for English. Although impressive results have recently been achieved for grammar error correction of non-native English writing, these results are limited to domains where plentiful training data are available. Because annotation is extremely costly, these approaches are not suitable for the majority of domains and languages. We thus focus on methods that use {``}minimal supervision{''}; that is, those that do not rely on large amounts of annotated training data, and show how existing minimal-supervision approaches extend to a highly inflectional language such as Russian. The results demonstrate that these methods are particularly useful for correcting mistakes in grammatical phenomena that involve rich morphology."",
}
@article{song-etal-2019-semantic,
    title = ""Semantic Neural Machine Translation Using {AMR}"",
    author = ""Song, Linfeng  and
      Gildea, Daniel  and
      Zhang, Yue  and
      Wang, Zhiguo  and
      Su, Jinsong"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1002"",
    doi = ""10.1162/tacl_a_00252"",
    pages = ""19--31"",
    abstract = ""It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-to-German dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based sequence-to-sequence neural translation model."",
}
@article{more-etal-2019-joint,
    title = ""Joint Transition-Based Models for Morpho-Syntactic Parsing: Parsing Strategies for {MRL}s and a Case Study from {M}odern {H}ebrew"",
    author = ""More, Amir  and
      Seker, Amit  and
      Basmova, Victoria  and
      Tsarfaty, Reut"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1003"",
    doi = ""10.1162/tacl_a_00253"",
    pages = ""33--48"",
    abstract = ""In standard NLP pipelines, morphological analysis and disambiguation (MA{\&}D) precedes syntactic and semantic downstream tasks. However, for languages with complex and ambiguous word-internal structure, known as morphologically rich languages (MRLs), it has been hypothesized that syntactic context may be crucial for accurate MA{\&}D, and vice versa. In this work we empirically confirm this hypothesis for Modern Hebrew, an MRL with complex morphology and severe word-level ambiguity, in a novel transition-based framework. Specifically, we propose a joint morphosyntactic transition-based framework which formally unifies two distinct transition systems, morphological and syntactic, into a single transition-based system with joint training and joint inference. We empirically show that MA{\&}D results obtained in the joint settings outperform MA{\&}D results obtained by the respective standalone components, and that end-to-end parsing results obtained by our joint system present a new state of the art for Hebrew dependency parsing."",
}
@article{belinkov-glass-2019-analysis,
    title = ""Analysis Methods in Neural Language Processing: A Survey"",
    author = ""Belinkov, Yonatan  and
      Glass, James"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1004"",
    doi = ""10.1162/tacl_a_00254"",
    pages = ""49--72"",
    abstract = ""The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work."",
}
@article{coavoux-etal-2019-unlexicalized,
    title = ""Unlexicalized Transition-based Discontinuous Constituency Parsing"",
    author = ""Coavoux, Maximin  and
      Crabb{\'e}, Beno{\^\i}t  and
      Cohen, Shay B."",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1005"",
    doi = ""10.1162/tacl_a_00255"",
    pages = ""73--89"",
    abstract = ""Lexicalized parsing models are based on the assumptions that (i) constituents are organized around a lexical head and (ii) bilexical statistics are crucial to solve ambiguities. In this paper, we introduce an unlexicalized transition-based parser for discontinuous constituency structures, based on a structure-label transition system and a bi-LSTM scoring system. We compare it with lexicalized parsing models in order to address the question of lexicalization in the context of discontinuous constituency parsing. Our experiments show that unlexicalized models systematically achieve higher results than lexicalized models, and provide additional empirical evidence that lexicalization is not necessary to achieve strong parsing results. Our best unlexicalized model sets a new state of the art on English and German discontinuous constituency treebanks. We further provide a per-phenomenon analysis of its errors on discontinuous constituents."",
}
@article{zhou-etal-2019-synchronous,
    title = ""Synchronous Bidirectional Neural Machine Translation"",
    author = ""Zhou, Long  and
      Zhang, Jiajun  and
      Zong, Chengqing"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1006"",
    doi = ""10.1162/tacl_a_00256"",
    pages = ""91--105"",
    abstract = ""Existing approaches to neural machine translation (NMT) generate the target language sequence token-by-token from left to right. However, this kind of unidirectional decoding framework cannot make full use of the target-side future contexts which can be produced in a right-to-left decoding direction, and thus suffers from the issue of unbalanced outputs. In this paper, we introduce a synchronous bidirectional{--}neural machine translation (SB-NMT) that predicts its outputs using left-to-right and right-to-left decoding simultaneously and interactively, in order to leverage both of the history and future information at the same time. Specifically, we first propose a new algorithm that enables synchronous bidirectional decoding in a single model. Then, we present an interactive decoding model in which left-to-right (right-to-left) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by right-to-left (left-to-right) decoding. We extensively evaluate the proposed SB-NMT model on large-scale NIST Chinese{--}English, WMT14 English{--}German, and WMT18 Russian{--}English translation tasks. Experimental results demonstrate that our model achieves significant improvements over the strong Transformer model by 3.92, 1.49, and 1.04 BLEU points, respectively, and obtains the state-of-the-art performance on Chinese{--}English and English{--}German translation tasks."",
}
@article{jawanpuria-etal-2019-learning,
    title = ""Learning Multilingual Word Embeddings in Latent Metric Space: A Geometric Approach"",
    author = ""Jawanpuria, Pratik  and
      Balgovind, Arjun  and
      Kunchukuttan, Anoop  and
      Mishra, Bamdev"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1007"",
    doi = ""10.1162/tacl_a_00257"",
    pages = ""107--120"",
    abstract = ""We propose a novel geometric approach for learning bilingual mappings given monolingual embeddings and a bilingual dictionary. Our approach decouples the source-to-target language transformation into (a) language-specific rotations on the original embeddings to align them in a common, latent space, and (b) a language-independent similarity metric in this common space to better model the similarity between the embeddings. Overall, we pose the bilingual mapping problem as a classification problem on smooth Riemannian manifolds. Empirically, our approach outperforms previous approaches on the bilingual lexicon induction and cross-lingual word similarity tasks. We next generalize our framework to represent multiple languages in a common latent space. Language-specific rotations for all the languages and a common similarity metric in the latent space are learned jointly from bilingual dictionaries for multiple language pairs. We illustrate the effectiveness of joint learning for multiple languages in an indirect word translation setting."",
}
@article{dangovski-etal-2019-rotational,
    title = ""Rotational Unit of Memory: A Novel Representation Unit for {RNN}s with Scalable Applications"",
    author = ""Dangovski, Rumen  and
      Jing, Li  and
      Nakov, Preslav  and
      Tatalovi{\'c}, Mi{\'c}o  and
      Solja{\v{c}}i{\'c}, Marin"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1008"",
    doi = ""10.1162/tacl_a_00258"",
    pages = ""121--138"",
    abstract = ""Stacking long short-term memory (LSTM) cells or gated recurrent units (GRUs) as part of a recurrent neural network (RNN) has become a standard approach to solving a number of tasks ranging from language modeling to text summarization. Although LSTMs and GRUs were designed to model long-range dependencies more accurately than conventional RNNs, they nevertheless have problems copying or recalling information from the long distant past. Here, we derive a phase-coded representation of the memory state, Rotational Unit of Memory (RUM), that unifies the concepts of unitary learning and associative memory. We show experimentally that RNNs based on RUMs can solve basic sequential tasks such as memory copying and memory recall much better than LSTMs/GRUs. We further demonstrate that by replacing LSTM/GRU with RUM units we can apply neural networks to real-world problems such as language modeling and text summarization, yielding results comparable to the state of the art."",
}
@article{pappas-henderson-2019-gile,
    title = ""{GILE}: A Generalized Input-Label Embedding for Text Classification"",
    author = ""Pappas, Nikolaos  and
      Henderson, James"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1009"",
    doi = ""10.1162/tacl_a_00259"",
    pages = ""139--155"",
    abstract = ""Neural text classification models typically treat output labels as categorical variables that lack description and semantics. This forces their parametrization to be dependent on the label set size, and, hence, they are unable to scale to large label sets and generalize to unseen ones. Existing joint input-label text models overcome these issues by exploiting label descriptions, but they are unable to capture complex label relationships, have rigid parametrization, and their gains on unseen labels happen often at the expense of weak performance on the labels seen during training. In this paper, we propose a new input-label model that generalizes over previous such models, addresses their limitations, and does not compromise performance on seen labels. The model consists of a joint nonlinear input-label embedding with controllable capacity and a joint-space-dependent classification unit that is trained with cross-entropy loss to optimize classification performance. We evaluate models on full-resource and low- or zero-resource text classification of multilingual news and biomedical text with a large label set. Our model outperforms monolingual and multilingual models that do not leverage label semantics and previous joint input-label space models in both scenarios."",
}
@article{chandlee-jardine-2019-autosegmental,
    title = ""Autosegmental Input Strictly Local Functions"",
    author = ""Chandlee, Jane  and
      Jardine, Adam"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1010"",
    doi = ""10.1162/tacl_a_00260"",
    pages = ""157--168"",
    abstract = ""Autosegmental representations (ARs; Goldsmith, 1976) are claimed to enable local analyses of otherwise non-local phenomena Odden (1994). Focusing on the domain of tone, we investigate this ability of ARs using a computationally well-defined notion of locality extended from Chandlee (2014). The result is a more nuanced understanding of the way in which ARs interact with phonological locality."",
}
@article{arnold-etal-2019-sector,
    title = ""{SECTOR}: A Neural Model for Coherent Topic Segmentation and Classification"",
    author = {Arnold, Sebastian  and
      Schneider, Rudolf  and
      Cudr{\'e}-Mauroux, Philippe  and
      Gers, Felix A.  and
      L{\""o}ser, Alexander},
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1011"",
    doi = ""10.1162/tacl_a_00261"",
    pages = ""169--184"",
    abstract = ""When searching for information, a human reader first glances over a document, spots relevant sections, and then focuses on a few sentences for resolving her intention. However, the high variance of document structure complicates the identification of the salient topic of a given section at a glance. To tackle this challenge, we present SECTOR, a model to support machine reading systems by segmenting documents into coherent sections and assigning topic labels to each section. Our deep neural network architecture learns a latent topic embedding over the course of a document. This can be leveraged to classify local topics from plain text and segment a document at topic shifts. In addition, we contribute WikiSection, a publicly available data set with 242k labeled sections in English and German from two distinct domains: diseases and cities. From our extensive evaluation of 20 architectures, we report a highest score of 71.6{\%} F1 for the segmentation and classification of 30 topics from the English city domain, scored by our SECTOR long short-term memory model with Bloom filter embeddings and bidirectional segmentation. This is a significant improvement of 29.5 points F1 over state-of-the-art CNN classifiers with baseline segmentation."",
}
@article{saha-etal-2019-complex,
    title = ""Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs"",
    author = ""Saha, Amrita  and
      Ansari, Ghulam Ahmed  and
      Laddha, Abhishek  and
      Sankaranarayanan, Karthik  and
      Chakrabarti, Soumen"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1012"",
    doi = ""10.1162/tacl_a_00262"",
    pages = ""185--200"",
    abstract = ""Recent years have seen increasingly complex question-answering on knowledge bases (KBQA) involving logical, quantitative, and comparative reasoning over KB subgraphs. Neural Program Induction (NPI) is a pragmatic approach toward modularizing the reasoning process by translating a complex natural language query into a multi-step executable program. While NPI has been commonly trained with the {`}{`}gold{'}{'} program or its sketch, for realistic KBQA applications such gold programs are expensive to obtain. There, practically only natural language queries and the corresponding answers can be provided for training. The resulting combinatorial explosion in program space, along with extremely sparse rewards, makes NPI for KBQA ambitious and challenging. We present Complex Imperative Program Induction from Terminal Rewards (CIPITR), an advanced neural programmer that mitigates reward sparsity with auxiliary rewards, and restricts the program space to semantically correct programs using high-level constraints, KB schema, and inferred answer type. CIPITR solves complex KBQA considerably more accurately than key-value memory networks and neural symbolic machines (NSM). For moderately complex queries requiring 2- to 5-step programs, CIPITR scores at least 3{\mbox{$\times$}} higher F1 than the competing systems. On one of the hardest class of programs (comparative reasoning) with 5{--}10 steps, CIPITR outperforms NSM by a factor of 89 and memory networks by 9 times."",
}
@article{kim-etal-2019-categorical,
    title = ""Categorical Metadata Representation for Customized Text Classification"",
    author = ""Kim, Jihyeok  and
      Amplayo, Reinald Kim  and
      Lee, Kyungjae  and
      Sung, Sua  and
      Seo, Minji  and
      Hwang, Seung-won"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1013"",
    doi = ""10.1162/tacl_a_00263"",
    pages = ""201--215"",
    abstract = ""The performance of text classification has improved tremendously using intelligently engineered neural-based models, especially those injecting categorical metadata as additional information, e.g., using user/product information for sentiment classification. This information has been used to modify parts of the model (e.g., word embeddings, attention mechanisms) such that results can be customized according to the metadata. We observe that current representation methods for categorical metadata, which are devised for human consumption, are not as effective as claimed in popular classification methods, outperformed even by simple concatenation of categorical features in the final layer of the sentence encoder. We conjecture that categorical features are harder to represent for machine use, as available context only indirectly describes the category, and even such context is often scarce (for tail category). To this end, we propose using basis vectors to effectively incorporate categorical metadata on various parts of a neural-based model. This additionally decreases the number of parameters dramatically, especially when the number of categorical features is large. Extensive experiments on various data sets with different properties are performed and show that through our method, we can represent categorical metadata more effectively to customize parts of the model, including unexplored ones, and increase the performance of the model greatly."",
}
@article{sun-etal-2019-dream,
    title = ""{DREAM}: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension"",
    author = ""Sun, Kai  and
      Yu, Dian  and
      Chen, Jianshu  and
      Yu, Dong  and
      Choi, Yejin  and
      Cardie, Claire"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1014"",
    doi = ""10.1162/tacl_a_00264"",
    pages = ""217--231"",
    abstract = ""We present DREAM, the first dialogue-based multiple-choice reading comprehension data set. Collected from English as a Foreign Language examinations designed by human experts to evaluate the comprehension level of Chinese learners of English, our data set contains 10,197 multiple-choice questions for 6,444 dialogues. In contrast to existing reading comprehension data sets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding. DREAM is likely to present significant challenges for existing reading comprehension systems: 84{\%} of answers are non-extractive, 85{\%} of questions require reasoning beyond a single sentence, and 34{\%} of questions also involve commonsense knowledge. We apply several popular neural reading comprehension models that primarily exploit surface information within the text and find them to, at best, just barely outperform a rule-based approach. We next investigate the effects of incorporating dialogue structure and different kinds of general world knowledge into both rule-based and (neural and non-neural) machine learning-based reading comprehension models. Experimental results on the DREAM data set show the effectiveness of dialogue structure and general world knowledge. DREAM is available at \url{https://dataset.org/dream/}."",
}
@article{jehl-etal-2019-learning,
    title = ""Learning Neural Sequence-to-Sequence Models from Weak Feedback with Bipolar Ramp Loss"",
    author = ""Jehl, Laura  and
      Lawrence, Carolin  and
      Riezler, Stefan"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1015"",
    doi = ""10.1162/tacl_a_00265"",
    pages = ""233--248"",
    abstract = ""In many machine learning scenarios, supervision by gold labels is not available and conse quently neural models cannot be trained directly by maximum likelihood estimation. In a weak supervision scenario, metric-augmented objectives can be employed to assign feedback to model outputs, which can be used to extract a supervision signal for training. We present several objectives for two separate weakly supervised tasks, machine translation and semantic parsing. We show that objectives should actively discourage negative outputs in addition to promoting a surrogate gold structure. This notion of bipolarity is naturally present in ramp loss objectives, which we adapt to neural models. We show that bipolar ramp loss objectives outperform other non-bipolar ramp loss objectives and minimum risk training on both weakly supervised tasks, as well as on a supervised machine translation task. Additionally, we introduce a novel token-level ramp loss objective, which is able to outperform even the best sequence-level ramp loss on both weakly supervised tasks."",
}
@article{reddy-etal-2019-coqa,
    title = ""{C}o{QA}: A Conversational Question Answering Challenge"",
    author = ""Reddy, Siva  and
      Chen, Danqi  and
      Manning, Christopher D."",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1016"",
    doi = ""10.1162/tacl_a_00266"",
    pages = ""249--266"",
    abstract = ""Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4{\%}, which is 23.4 points behind human performance (88.8{\%}), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at \url{https://stanfordnlp.github.io/coqa}."",
}
@article{zeng-etal-2019-say,
    title = ""What You Say and How You Say it: Joint Modeling of Topics and Discourse in Microblog Conversations"",
    author = ""Zeng, Jichuan  and
      Li, Jing  and
      He, Yulan  and
      Gao, Cuiyun  and
      Lyu, Michael R.  and
      King, Irwin"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1017"",
    doi = ""10.1162/tacl_a_00267"",
    pages = ""267--281"",
    abstract = ""This paper presents an unsupervised framework for jointly modeling topic content and discourse behavior in microblog conversations. Concretely, we propose a neural model to discover word clusters indicating what a conversation concerns (i.e., topics) and those reflecting how participants voice their opinions (i.e., discourse).1 Extensive experiments show that our model can yield both coherent topics and meaningful discourse behavior. Further study shows that our topic and discourse representations can benefit the classification of microblog messages, especially when they are jointly trained with the classifier. Our data sets and code are available at: \url{http://github.com/zengjichuan/Topic_Disc}."",
}
@article{nederhof-2019-calculating,
    title = ""Calculating the Optimal Step in Shift-Reduce Dependency Parsing: From Cubic to Linear Time"",
    author = ""Nederhof, Mark-Jan"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1018"",
    doi = ""10.1162/tacl_a_00268"",
    pages = ""283--296"",
    abstract = ""We present a new cubic-time algorithm to calculate the optimal next step in shift-reduce dependency parsing, relative to ground truth, commonly referred to as dynamic oracle. Unlike existing algorithms, it is applicable if the training corpus contains non-projective structures. We then show that for a projective training corpus, the time complexity can be improved from cubic to linear."",
}
@article{guo-etal-2019-densely,
    title = ""Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning"",
    author = ""Guo, Zhijiang  and
      Zhang, Yan  and
      Teng, Zhiyang  and
      Lu, Wei"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1019"",
    doi = ""10.1162/tacl_a_00269"",
    pages = ""297--312"",
    abstract = ""We focus on graph-to-sequence learning, which can be framed as transducing graph structures to sequences for text generation. To capture structural information associated with graphs, we investigate the problem of encoding graphs using graph convolutional networks (GCNs). Unlike various existing approaches where shallow architectures were used for capturing local structural information only, we introduce a dense connection strategy, proposing a novel Densely Connected Graph Convolutional Network (DCGCN). Such a deep architecture is able to integrate both local and non-local features to learn a better structural representation of a graph. Our model outperforms the state-of-the-art neural models significantly on AMR-to-text generation and syntax-based neural machine translation."",
}
@article{sperber-etal-2019-attention,
    title = ""Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation"",
    author = ""Sperber, Matthias  and
      Neubig, Graham  and
      Niehues, Jan  and
      Waibel, Alex"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1020"",
    doi = ""10.1162/tacl_a_00270"",
    pages = ""313--325"",
    abstract = ""Speech translation has traditionally been approached through cascaded models consisting of a speech recognizer trained on a corpus of transcribed speech, and a machine translation system trained on parallel texts. Several recent works have shown the feasibility of collapsing the cascade into a single, direct model that can be trained in an end-to-end fashion on a corpus of translated speech. However, experiments are inconclusive on whether the cascade or the direct model is stronger, and have only been conducted under the unrealistic assumption that both are trained on equal amounts of data, ignoring other available speech recognition and machine translation corpora. In this paper, we demonstrate that direct speech translation models require more data to perform well than cascaded models, and although they allow including auxiliary data through multi-task training, they are poor at exploiting such data, putting them at a severe disadvantage. As a remedy, we propose the use of end- to-end trainable models with two attention mechanisms, the first establishing source speech to source text alignments, the second modeling source to target text alignment. We show that such models naturally decompose into multi-task{--}trainable recognition and translation tasks and propose an attention-passing technique that alleviates error propagation issues in a previous formulation of a model with two attention stages. Our proposed model outperforms all examined baselines and is able to exploit auxiliary training data much more effectively than direct attentional models."",
}
@article{cotterell-etal-2019-complexity,
    title = ""On the Complexity and Typology of Inflectional Morphological Systems"",
    author = ""Cotterell, Ryan  and
      Kirov, Christo  and
      Hulden, Mans  and
      Eisner, Jason"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1021"",
    doi = ""10.1162/tacl_a_00271"",
    pages = ""327--342"",
    abstract = ""We quantify the linguistic complexity of different languages{'} morphological systems. We verify that there is a statistically significant empirical trade-off between paradigm size and irregularity: A language{'}s inflectional paradigms may be either large in size or highly irregular, but never both. We define a new measure of paradigm irregularity based on the conditional entropy of the surface realization of a paradigm{---} how hard it is to jointly predict all the word forms in a paradigm from the lemma. We estimate irregularity by training a predictive model. Our measurements are taken on large morphological paradigms from 36 typologically diverse languages."",
}
@article{cai-lapata-2019-syntax,
    title = ""Syntax-aware Semantic Role Labeling without Parsing"",
    author = ""Cai, Rui  and
      Lapata, Mirella"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1022"",
    doi = ""10.1162/tacl_a_00272"",
    pages = ""343--356"",
    abstract = ""In this paper we focus on learning dependency aware representations for semantic role labeling without recourse to an external parser. The backbone of our model is an LSTM-based semantic role labeler jointly trained with two auxiliary tasks: predicting the dependency label of a word and whether there exists an arc linking it to the predicate. The auxiliary tasks provide syntactic information that is specific to semantic role labeling and are learned from training data (dependency annotations) without relying on existing dependency parsers, which can be noisy (e.g., on out-of-domain data or infrequent constructions). Experimental results on the CoNLL-2009 benchmark dataset show that our model outperforms the state of the art in English, and consistently improves performance in other languages, including Chinese, German, and Spanish."",
}
@article{li-etal-2019-generative,
    title = ""A Generative Model for Punctuation in Dependency Trees"",
    author = ""Li, Xiang Lisa  and
      Wang, Dingquan  and
      Eisner, Jason"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1023"",
    doi = ""10.1162/tacl_a_00273"",
    pages = ""357--373"",
    abstract = ""Treebanks traditionally treat punctuation marks as ordinary words, but linguists have suggested that a tree{'}s {``}true{''} punctuation marks are not observed (Nunberg, 1990). These latent {``}underlying{''} marks serve to delimit or separate constituents in the syntax tree. When the tree{'}s yield is rendered as a written sentence, a string rewriting mechanism transduces the underlying marks into {``}surface{''} marks, which are part of the observed (surface) string but should not be regarded as part of the tree. We formalize this idea in a generative model of punctuation that admits efficient dynamic programming. We train it without observing the underlying marks, by locally maximizing the incomplete data likelihood (similarly to the EM algorithm). When we use the trained model to reconstruct the tree{'}s underlying punctuation, the results appear plausible across 5 languages, and in particular are consistent with Nunberg{'}s analysis of English. We show that our generative model can be used to beat baselines on punctuation restoration. Also, our reconstruction of a sentence{'}s underlying punctuation lets us appropriately render the surface punctuation (via our trained underlying-to-surface mechanism) when we syntactically transform the sentence."",
}
@article{rajendran-etal-2019-learning,
    title = ""Learning End-to-End Goal-Oriented Dialog with Maximal User Task Success and Minimal Human Agent Use"",
    author = ""Rajendran, Janarthanan  and
      Ganhotra, Jatin  and
      Polymenakos, Lazaros C."",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1024"",
    doi = ""10.1162/tacl_a_00274"",
    pages = ""375--386"",
    abstract = ""Neural end-to-end goal-oriented dialog systems showed promise to reduce the workload of human agents for customer service, as well as reduce wait time for users. However, their inability to handle new user behavior at deployment has limited their usage in real world. In this work, we propose an end-to-end trainable method for neural goal-oriented dialog systems that handles new user behaviors at deployment by transferring the dialog to a human agent intelligently. The proposed method has three goals: 1) maximize user{'}s task success by transferring to human agents, 2) minimize the load on the human agents by transferring to them only when it is essential, and 3) learn online from the human agent{'}s responses to reduce human agents{'} load further. We evaluate our proposed method on a modified-bAbI dialog task, which simulates the scenario of new user behaviors occurring at test time. Experimental results show that our proposed method is effective in achieving the desired goals."",
}
@article{wallace-etal-2019-trick,
    title = ""Trick Me If You Can: Human-in-the-Loop Generation of Adversarial Examples for Question Answering"",
    author = ""Wallace, Eric  and
      Rodriguez, Pedro  and
      Feng, Shi  and
      Yamada, Ikuya  and
      Boyd-Graber, Jordan"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1029"",
    doi = ""10.1162/tacl_a_00279"",
    pages = ""387--401"",
    abstract = ""Adversarial evaluation stress-tests a model{'}s understanding of natural language. Because past approaches expose superficial patterns, the resulting adversarial examples are limited in complexity and diversity. We propose human- in-the-loop adversarial generation, where human authors are guided to break models. We aid the authors with interpretations of model predictions through an interactive user interface. We apply this generation framework to a question answering task called Quizbowl, where trivia enthusiasts craft adversarial questions. The resulting questions are validated via live human{--}computer matches: Although the questions appear ordinary to humans, they systematically stump neural and information retrieval models. The adversarial questions cover diverse phenomena from multi-hop reasoning to entity type distractors, exposing open challenges in robust question answering."",
}
@article{shwartz-dagan-2019-still,
    title = ""Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition"",
    author = ""Shwartz, Vered  and
      Dagan, Ido"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1027"",
    doi = ""10.1162/tacl_a_00277"",
    pages = ""403--419"",
    abstract = ""Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical composition can shift the meanings of the constituent words and introduce implicit information. We tested a broad range of textual representations for their capacity to address these issues. We found that, as expected, contextualized word representations perform better than static word embeddings, more so on detecting meaning shift than in recovering implicit information, in which their performance is still far from that of humans. Our evaluation suite, consisting of six tasks related to lexical composition effects, can serve future research aiming to improve representations."",
}
@article{azpiazu-pera-2019-multiattentive,
    title = ""Multiattentive Recurrent Neural Network Architecture for Multilingual Readability Assessment"",
    author = ""Azpiazu, Ion Madrazo  and
      Pera, Maria Soledad"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1028"",
    doi = ""10.1162/tacl_a_00278"",
    pages = ""421--436"",
    abstract = ""We present a multiattentive recurrent neural network architecture for automatic multilingual readability assessment. This architecture considers raw words as its main input, but internally captures text structure and informs its word attention process using other syntax- and morphology-related datapoints, known to be of great importance to readability. This is achieved by a multiattentive strategy that allows the neural network to focus on specific parts of a text for predicting its reading level. We conducted an exhaustive evaluation using data sets targeting multiple languages and prediction task types, to compare the proposed model with traditional, state-of-the-art, and other neural network strategies."",
}
@article{dima-etal-2019-word,
    title = ""No Word is an {I}sland{---}{A} Transformation Weighting Model for Semantic Composition"",
    author = {Dima, Corina  and
      de Kok, Dani{\""e}l  and
      Witte, Neele  and
      Hinrichs, Erhard},
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1025"",
    doi = ""10.1162/tacl_a_00275"",
    pages = ""437--451"",
    abstract = ""Composition models of distributional semantics are used to construct phrase representations from the representations of their words. Composition models are typically situated on two ends of a spectrum. They either have a small number of parameters but compose all phrases in the same way, or they perform word-specific compositions at the cost of a far larger number of parameters. In this paper we propose transformation weighting (TransWeight), a composition model that consistently outperforms existing models on nominal compounds, adjective-noun phrases, and adverb-adjective phrases in English, German, and Dutch. TransWeight drastically reduces the number of parameters needed compared with the best model in the literature by composing similar words in the same way."",
}
@article{kwiatkowski-etal-2019-natural,
    title = ""Natural Questions: A Benchmark for Question Answering Research"",
    author = ""Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1026"",
    doi = ""10.1162/tacl_a_00276"",
    pages = ""452--466"",
    abstract = ""We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature."",
}
@article{hahn-baroni-2019-tabula,
    title = ""Tabula Nearly Rasa: Probing the Linguistic Knowledge of Character-level Neural Language Models Trained on Unsegmented Text"",
    author = ""Hahn, Michael  and
      Baroni, Marco"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1033"",
    doi = ""10.1162/tacl_a_00283"",
    pages = ""467--484"",
    abstract = ""Recurrent neural networks (RNNs) have reached striking performance in many natural language processing tasks. This has renewed interest in whether these generic sequence processing devices are inducing genuine linguistic knowledge. Nearly all current analytical studies, however, initialize the RNNs with a vocabulary of known words, and feed them tokenized input during training. We present a multi-lingual study of the linguistic knowledge encoded in RNNs trained as character-level language models, on input data with word boundaries removed. These networks face a tougher and more cognitively realistic task, having to discover any useful linguistic unit from scratch based on input statistics. The results show that our {``}near tabula rasa{''} RNNs are mostly able to solve morphological, syntactic and semantic tasks that intuitively presuppose word-level knowledge, and indeed they learned, to some extent, to track word boundaries. Our study opens the door to speculations about the necessity of an explicit, rigid word lexicon in language learning and usage."",
}
@article{banerjee-khapra-2019-graph,
    title = ""Graph Convolutional Network with Sequential Attention for Goal-Oriented Dialogue Systems"",
    author = ""Banerjee, Suman  and
      Khapra, Mitesh M."",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1034"",
    doi = ""10.1162/tacl_a_00284"",
    pages = ""485--500"",
    abstract = ""Domain-specific goal-oriented dialogue systems typically require modeling three types of inputs, namely, (i) the knowledge-base associated with the domain, (ii) the history of the conversation, which is a sequence of utterances, and (iii) the current utterance for which the response needs to be generated. While modeling these inputs, current state-of-the-art models such as Mem2Seq typically ignore the rich structure inherent in the knowledge graph and the sentences in the conversation context. Inspired by the recent success of structure-aware Graph Convolutional Networks (GCNs) for various NLP tasks such as machine translation, semantic role labeling, and document dating, we propose a memory-augmented GCN for goal-oriented dialogues. Our model exploits (i) the entity relation graph in a knowledge-base and (ii) the dependency graph associated with an utterance to compute richer representations for words and entities. Further, we take cognizance of the fact that in certain situations, such as when the conversation is in a code-mixed language, dependency parsers may not be available. We show that in such situations we could use the global word co-occurrence graph to enrich the representations of utterances. We experiment with four datasets: (i) the modified DSTC2 dataset, (ii) recently released code-mixed versions of DSTC2 dataset in four languages, (iii) Wizard-of-Oz style CAM676 dataset, and (iv) Wizard-of-Oz style MultiWOZ dataset. On all four datasets our method outperforms existing methods, on a wide range of evaluation metrics."",
}
@article{govindarajan-etal-2019-decomposing,
    title = ""Decomposing Generalization: Models of Generic, Habitual, and Episodic Statements"",
    author = ""Govindarajan, Venkata  and
      Van Durme, Benjamin  and
      White, Aaron Steven"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1035"",
    doi = ""10.1162/tacl_a_00285"",
    pages = ""501--517"",
    abstract = ""We present a novel semantic framework for modeling linguistic expressions of generalization{---} generic, habitual, and episodic statements{---}as combinations of simple, real-valued referential properties of predicates and their arguments. We use this framework to construct a dataset covering the entirety of the Universal Dependencies English Web Treebank. We use this dataset to probe the efficacy of type-level and token-level information{---}including hand-engineered features and static (GloVe) and contextual (ELMo) word embeddings{---}for predicting expressions of generalization."",
}
@article{elazar-goldberg-2019-wheres,
    title = ""Where{'}s My Head? {D}efinition, Data Set, and Models for Numeric Fused-Head Identification and Resolution"",
    author = ""Elazar, Yanai  and
      Goldberg, Yoav"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1030"",
    doi = ""10.1162/tacl_a_00280"",
    pages = ""519--535"",
    abstract = ""We provide the first computational treatment of fused-heads constructions (FHs), focusing on the numeric fused-heads (NFHs). FHs constructions are noun phrases in which the head noun is missing and is said to be {``}fused{''} with its dependent modifier. This missing information is implicit and is important for sentence understanding. The missing references are easily filled in by humans but pose a challenge for computational models. We formulate the handling of FHs as a two stages process: Identification of the FH construction and resolution of the missing head. We explore the NFH phenomena in large corpora of English text and create (1) a data set and a highly accurate method for NFH identification; (2) a 10k examples (1 M tokens) crowd-sourced data set of NFH resolution; and (3) a neural baseline for the NFH resolution task. We release our code and data set, to foster further research into this challenging problem."",
}
@article{luu-etal-2019-measuring,
    title = ""Measuring Online Debaters{'} Persuasive Skill from Text over Time"",
    author = ""Luu, Kelvin  and
      Tan, Chenhao  and
      Smith, Noah A."",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1031"",
    doi = ""10.1162/tacl_a_00281"",
    pages = ""537--550"",
    abstract = ""Online debates allow people to express their persuasive abilities and provide exciting opportunities for understanding persuasion. Prior studies have focused on studying persuasion in debate content, but without accounting for each debater{'}s history or exploring the progression of a debater{'}s persuasive ability. We study debater skill by modeling how participants progress over time in a collection of debates from Debate.org. We build on a widely used model of skill in two-player games and augment it with linguistic features of a debater{'}s content. We show that online debaters{'} skill levels do tend to improve over time. Incorporating linguistic profiles leads to more robust skill estimation than winning records alone. Notably, we find that an interaction feature combining uncertainty cues (hedging) with terms strongly associated with either side of a particular debate (fightin{'} words) is more predictive than either feature on its own, indicating the importance of fine- grained linguistic features."",
}
@article{napoles-etal-2019-enabling,
    title = ""Enabling Robust Grammatical Error Correction in New Domains: Data Sets, Metrics, and Analyses"",
    author = ""Napoles, Courtney  and
      N{\u{a}}dejde, Maria  and
      Tetreault, Joel"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1032"",
    doi = ""10.1162/tacl_a_00282"",
    pages = ""551--566"",
    abstract = ""Until now, grammatical error correction (GEC) has been primarily evaluated on text written by non-native English speakers, with a focus on student essays. This paper enables GEC development on text written by native speakers by providing a new data set and metric. We present a multiple-reference test corpus for GEC that includes 4,000 sentences in two new domains (formal and informal writing by native English speakers) and 2,000 sentences from a diverse set of non-native student writing. We also collect human judgments of several GEC systems on this new test set and perform a meta-evaluation, assessing how reliable automatic metrics are across these domains. We find that commonly used GEC metrics have inconsistent performance across domains, and therefore we propose a new ensemble metric that is robust on all three domains of text."",
}
@article{akyurek-etal-2019-morphological,
    title = ""Morphological Analysis Using a Sequence Decoder"",
    author = {Aky{\""u}rek, Ekin  and
      Dayan{\i}k, Erenay  and
      Yuret, Deniz},
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1036"",
    doi = ""10.1162/tacl_a_00286"",
    pages = ""567--579"",
    abstract = ""We introduce Morse, a recurrent encoder-decoder model that produces morphological analyses of each word in a sentence. The encoder turns the relevant information about the word and its context into a fixed size vector representation and the decoder generates the sequence of characters for the lemma followed by a sequence of individual morphological features. We show that generating morphological features individually rather than as a combined tag allows the model to handle rare or unseen tags and to outperform whole-tag models. In addition, generating morphological features as a sequence rather than, for example, an unordered set allows our model to produce an arbitrary number of features that represent multiple inflectional groups in morphologically complex languages. We obtain state-of-the-art results in nine languages of different morphological complexity under low-resource, high-resource, and transfer learning settings. We also introduce TrMor2018, a new high-accuracy Turkish morphology data set. Our Morse implementation and the TrMor2018 data set are available online to support future research.1See \url{https://github.com/ai-ku/Morse.jl} for a Morse implementation in Julia/Knet (Yuret, 2016) and \url{https://github.com/ai-ku/TrMor2018} for the new Turkish data set."",
}
@article{xu-lapata-2019-weakly,
    title = ""Weakly Supervised Domain Detection"",
    author = ""Xu, Yumo  and
      Lapata, Mirella"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1037"",
    doi = ""10.1162/tacl_a_00287"",
    pages = ""581--596"",
    abstract = ""In this paper we introduce domain detection as a new natural language processing task. We argue that the ability to detect textual segments that are domain-heavy (i.e., sentences or phrases that are representative of and provide evidence for a given domain) could enhance the robustness and portability of various text classification applications. We propose an encoder-detector framework for domain detection and bootstrap classifiers with multiple instance learning. The model is hierarchically organized and suited to multilabel classification. We demonstrate that despite learning with minimal supervision, our model can be applied to text spans of different granularities, languages, and genres. We also showcase the potential of domain detection for text summarization."",
}
@article{artetxe-schwenk-2019-massively,
    title = ""Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond"",
    author = ""Artetxe, Mikel  and
      Schwenk, Holger"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1038"",
    doi = ""10.1162/tacl_a_00288"",
    pages = ""597--610"",
    abstract = ""We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared byte-pair encoding vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our experiments in cross-lingual natural language inference (XNLI data set), cross-lingual document classification (MLDoc data set), and parallel corpus mining (BUCC data set) show the effectiveness of our approach. We also introduce a new test set of aligned sentences in 112 languages, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low- resource languages. Our implementation, the pre-trained encoder, and the multilingual test set are available at \url{https://github.com/facebookresearch/LASER}."",
}
@article{li-etal-2019-efficient,
    title = ""Efficient Contextual Representation Learning With Continuous Outputs"",
    author = ""Li, Liunian Harold  and
      Chen, Patrick H.  and
      Hsieh, Cho-Jui  and
      Chang, Kai-Wei"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1039"",
    doi = ""10.1162/tacl_a_00289"",
    pages = ""611--624"",
    abstract = ""Contextual representation models have achieved great success in improving various downstream natural language processing tasks. However, these language-model-based encoders are difficult to train due to their large parameter size and high computational complexity. By carefully examining the training procedure, we observe that the softmax layer, which predicts a distribution of the target word, often induces significant overhead, especially when the vocabulary size is large. Therefore, we revisit the design of the output layer and consider directly predicting the pre-trained embedding of the target word for a given context. When applied to ELMo, the proposed approach achieves a 4-fold speedup and eliminates 80{\%} trainable parameters while achieving competitive performance on downstream tasks. Further analysis shows that the approach maintains the speed advantage under various settings, even when the sentence encoder is scaled up."",
}
@article{warstadt-etal-2019-neural,
    title = ""Neural Network Acceptability Judgments"",
    author = ""Warstadt, Alex  and
      Singh, Amanpreet  and
      Bowman, Samuel R."",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1040"",
    doi = ""10.1162/tacl_a_00290"",
    pages = ""625--641"",
    abstract = ""This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et al.{'}s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions."",
}
@article{doitch-etal-2019-perturbation,
    title = ""Perturbation Based Learning for Structured {NLP} Tasks with Application to Dependency Parsing"",
    author = ""Doitch, Amichay  and
      Yazdi, Ram  and
      Hazan, Tamir  and
      Reichart, Roi"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1041"",
    doi = ""10.1162/tacl_a_00291"",
    pages = ""643--659"",
    abstract = ""The best solution of structured prediction models in NLP is often inaccurate because of limited expressive power of the model or to non-exact parameter estimation. One way to mitigate this problem is sampling candidate solutions from the model{'}s solution space, reasoning that effective exploration of this space should yield high-quality solutions. Unfortunately, sampling is often computationally hard and many works hence back-off to sub-optimal strategies, such as extraction of the best scoring solutions of the model, which are not as diverse as sampled solutions. In this paper we propose a perturbation-based approach where sampling from a probabilistic model is computationally efficient. We present a learning algorithm for the variance of the perturbations, and empirically demonstrate its importance. Moreover, while finding the argmax in our model is intractable, we propose an efficient and effective approximation. We apply our framework to cross-lingual dependency parsing across 72 corpora from 42 languages and to lightly supervised dependency parsing across 13 corpora from 12 languages, and demonstrate strong results in terms of both the quality of the entire solution list and of the final solution.1"",
}
@article{gu-etal-2019-insertion,
    title = ""Insertion-based Decoding with Automatically Inferred Generation Order"",
    author = ""Gu, Jiatao  and
      Liu, Qi  and
      Cho, Kyunghyun"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1042"",
    doi = ""10.1162/tacl_a_00292"",
    pages = ""661--676"",
    abstract = ""Conventional neural autoregressive decoding commonly assumes a fixed left-to-right generation order, which may be sub-optimal. In this work, we propose a novel decoding algorithm{---} InDIGO{---}which supports flexible sequence generation in arbitrary orders through insertion operations. We extend Transformer, a state-of-the-art sequence generation model, to efficiently implement the proposed approach, enabling it to be trained with either a pre-defined generation order or adaptive orders obtained from beam-search. Experiments on four real-world tasks, including word order recovery, machine translation, image caption, and code generation, demonstrate that our algorithm can generate sequences following arbitrary orders, while achieving competitive or even better performance compared with the conventional left-to-right generation. The generated sequences show that InDIGO adopts adaptive generation orders based on input information."",
}
@article{pavlick-kwiatkowski-2019-inherent,
    title = ""Inherent Disagreements in Human Textual Inferences"",
    author = ""Pavlick, Ellie  and
      Kwiatkowski, Tom"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1043"",
    doi = ""10.1162/tacl_a_00293"",
    pages = ""677--694"",
    abstract = ""We analyze human{'}s disagreements about the validity of natural language inferences. We show that, very often, disagreements are not dismissible as annotation {``}noise{''}, but rather persist as we collect more ratings and as we vary the amount of context provided to raters. We further show that the type of uncertainty captured by current state-of-the-art models for natural language inference is not reflective of the type of uncertainty present in human disagreements. We discuss implications of our results in relation to the recognizing textual entailment (RTE)/natural language inference (NLI) task. We argue for a refined evaluation objective that requires models to explicitly capture the full distribution of plausible human judgments."",
}
@article{rotman-reichart-2019-deep,
    title = ""Deep Contextualized Self-training for Low Resource Dependency Parsing"",
    author = ""Rotman, Guy  and
      Reichart, Roi"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1044"",
    doi = ""10.1162/tacl_a_00294"",
    pages = ""695--713"",
    abstract = ""Neural dependency parsing has proven very effective, achieving state-of-the-art results on numerous domains and languages. Unfortunately, it requires large amounts of labeled data, which is costly and laborious to create. In this paper we propose a self-training algorithm that alleviates this annotation bottleneck by training a parser on its own output. Our Deep Contextualized Self-training (DCST) algorithm utilizes representation models trained on sequence labeling tasks that are derived from the parser{'}s output when applied to unlabeled data, and integrates these models with the base parser through a gating mechanism. We conduct experiments across multiple languages, both in low resource in-domain and in cross-domain setups, and demonstrate that DCST substantially outperforms traditional self-training as well as recent semi-supervised training methods.1"",
}
@article{cocos-callison-burch-2019-paraphrase,
    title = ""Paraphrase-Sense-Tagged Sentences"",
    author = ""Cocos, Anne  and
      Callison-Burch, Chris"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""7"",
    year = ""2019"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q19-1045"",
    doi = ""10.1162/tacl_a_00295"",
    pages = ""714--728"",
    abstract = ""Many natural language processing tasks require discriminating the particular meaning of a word in context, but building corpora for developing sense-aware models can be a challenge. We present a large resource of example usages for words having a particular meaning, called Paraphrase-Sense-Tagged Sentences (PSTS). Built on the premise that a word{'}s paraphrases instantiate its fine-grained meanings (i.e., bug has different meanings corresponding to its paraphrases fly and microbe) the resource contains up to 10,000 sentences for each of 3 million target-paraphrase pairs where the target word takes on the meaning of the paraphrase. We describe an automatic method based on bilingual pivoting used to enumerate sentences for PSTS, and present two models for ranking PSTS sentences based on their quality. Finally, we demonstrate the utility of PSTS by using it to build a dataset for the task of hypernym prediction in context. Training a model on this automatically generated dataset produces accuracy that is competitive with a model trained on smaller datasets crafted with some manual effort."",
}
@proceedings{acl-2019-association-linguistics-tutorial,
    title = ""Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts"",
    editor = ""Nakov, Preslav  and
      Palmer, Alexis"",
    month = jul,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/P19-4000"",
}
@",medical text,natural language process,natural languag,language process,translat,nlp,infer,generat,sequence label,summar,information retriev,sentiment,semant,annotat,question-answ,challeng,benchmark,evalu,assess,track
" ""Automatic Generation of High Quality {CCG}banks for Parser Domain Adaptation"","," ""We propose a new domain adaptation method for Combinatory Categorial Grammar (CCG) parsing, based on the idea of automatic generation of CCG corpora exploiting cheaper resources of dependency trees. Our solution is conceptually simple, and not relying on a specific parser architecture, making it applicable to the current best-performing parsers. We conduct extensive parsing experiments with detailed discussion; on top of existing benchmark datasets on (1) biomedical texts and (2) question sentences, we create experimental datasets of (3) speech conversation and (4) math problems. When applied to the proposed method, an off-the-shelf CCG parser shows significant performance gains, improving from 90.7{\%} to 96.6{\%} on speech conversation, and from 88.5{\%} to 96.8{\%} on math problems."",","{yoshikawa-etal-2019-automatic,
    title = ""Automatic Generation of High Quality {CCG}banks for Parser Domain Adaptation"",
    author = ""Yoshikawa, Masashi  and
      Noji, Hiroshi  and
      Mineshima, Koji  and
      Bekki, Daisuke"",
    editor = ""Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s"",
    booktitle = ""Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"",
    month = jul,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/P19-1013"",
    doi = ""10.18653/v1/P19-1013"",
    pages = ""129--139"",
    abstract = ""We propose a new domain adaptation method for Combinatory Categorial Grammar (CCG) parsing, based on the idea of automatic generation of CCG corpora exploiting cheaper resources of dependency trees. Our solution is conceptually simple, and not relying on a specific parser architecture, making it applicable to the current best-performing parsers. We conduct extensive parsing experiments with detailed discussion; on top of existing benchmark datasets on (1) biomedical texts and (2) question sentences, we create experimental datasets of (3) speech conversation and (4) math problems. When applied to the proposed method, an off-the-shelf CCG parser shows significant performance gains, improving from 90.7{\%} to 96.6{\%} on speech conversation, and from 88.5{\%} to 96.8{\%} on math problems."",
}
@",medical text,generat,benchmark
" ""Joint Entity Extraction and Assertion Detection for Clinical Text"","," ""Negative medical findings are prevalent in clinical reports, yet discriminating them from positive findings remains a challenging task for in-formation extraction. Most of the existing systems treat this task as a pipeline of two separate tasks, i.e., named entity recognition (NER)and rule-based negation detection. We consider this as a multi-task problem and present a novel end-to-end neural model to jointly extract entities and negations. We extend a standard hierarchical encoder-decoder NER model and first adopt a shared encoder followed by separate decoders for the two tasks. This architecture performs considerably better than the previous rule-based and machine learning-based systems. To overcome the problem of increased parameter size especially for low-resource settings, we propose the Conditional Softmax Shared Decoder architecture which achieves state-of-art results for NER and negation detection on the 2010 i2b2/VA challenge dataset and a proprietary de-identified clinical dataset."",","{bhatia-etal-2019-joint,
    title = ""Joint Entity Extraction and Assertion Detection for Clinical Text"",
    author = ""Bhatia, Parminder  and
      Celikkaya, Busra  and
      Khalilia, Mohammed"",
    editor = ""Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s"",
    booktitle = ""Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"",
    month = jul,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/P19-1091"",
    doi = ""10.18653/v1/P19-1091"",
    pages = ""954--959"",
    abstract = ""Negative medical findings are prevalent in clinical reports, yet discriminating them from positive findings remains a challenging task for in-formation extraction. Most of the existing systems treat this task as a pipeline of two separate tasks, i.e., named entity recognition (NER)and rule-based negation detection. We consider this as a multi-task problem and present a novel end-to-end neural model to jointly extract entities and negations. We extend a standard hierarchical encoder-decoder NER model and first adopt a shared encoder followed by separate decoders for the two tasks. This architecture performs considerably better than the previous rule-based and machine learning-based systems. To overcome the problem of increased parameter size especially for low-resource settings, we propose the Conditional Softmax Shared Decoder architecture which achieves state-of-art results for NER and negation detection on the 2010 i2b2/VA challenge dataset and a proprietary de-identified clinical dataset."",
}
@",clinical text,entity recognit,challeng
" ""Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations)"","," ""Rapid growth in adoption of electronic health records (EHRs) has led to an unprecedented expansion in the availability of large longitudinal datasets. Large initiatives such as the Electronic Medical Records and Genomics (eMERGE) Network, the Patient-Centered Outcomes Research Network (PCORNet), and the Observational Health Data Science and Informatics (OHDSI) consortium, have been established and have reported successful applications of secondary use of EHRs in clinical research and practice. In these applications, natural language processing (NLP) technologies have played a crucial role as much of detailed patient information in EHRs is embedded in narrative clinical documents. Meanwhile, a number of clinical NLP systems, such as MedLEE, MetaMap/MetaMap Lite, cTAKES, and MedTagger have been developed and utilized to extract useful information from diverse types of clinical text, such as clinical notes, radiology reports, and pathology reports. Success stories in applying these tools have been reported widely. Despite the demonstrated success of NLP in the clinical domain, methodologies and tools developed for the clinical NLP are still underknown and underutilized by students and experts in the general NLP domain, mainly due to the limited exposure to EHR data. Through this tutorial, we would like to introduce NLP methodologies and tools developed in the clinical domain, and showcase the real-world NLP applications in clinical research and practice at Mayo Clinic (the No. 1 national hospital ranked by the U.S. News {\&} World Report) and the University of Minnesota (the No. 41 best global universities ranked by the U.S. News {\&} World Report). We will review NLP techniques in solving clinical problems and facilitating clinical research, the state-of-the art clinical NLP tools, and share collaboration experience with clinicians, as well as publicly available EHR data and medical resources, and finally conclude the tutorial with vast opportunities and challenges of clinical NLP. The tutorial will provide an overview of clinical backgrounds, and does not presume knowledge in medicine or health care. The goal of this tutorial is to encourage NLP researchers in the general domain (as opposed to the specialized clinical domain) to contribute to this burgeoning area. In this tutorial, we will first present an overview of clinical NLP. We will then dive into two subareas of clinical NLP in clinical research, including big data infrastructure for large-scale clinical NLP and advances of NLP in clinical research, and two subareas in clinical practice, including clinical information extraction and patient cohort retrieval using EHRs. Around 70{\%} of the tutorial will review clinical problems, cutting-edge methodologies, and real-world clinical NLP tools while another 30{\%} introduce use cases at Mayo Clinic and the University of Minnesota. Finally, we will conclude the tutorial with challenges and opportunities in this rapidly developing domain."",","{wang-etal-2019-applications,
    title = ""Applications of Natural Language Processing in Clinical Research and Practice"",
    author = ""Wang, Yanshan  and
      Tafti, Ahmad  and
      Sohn, Sunghwan  and
      Zhang, Rui"",
    editor = ""Sarkar, Anoop  and
      Strube, Michael"",
    booktitle = ""Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Tutorials"",
    month = jun,
    year = ""2019"",
    address = ""Minneapolis, Minnesota"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/N19-5006"",
    doi = ""10.18653/v1/N19-5006"",
    pages = ""22--25"",
    abstract = ""Rapid growth in adoption of electronic health records (EHRs) has led to an unprecedented expansion in the availability of large longitudinal datasets. Large initiatives such as the Electronic Medical Records and Genomics (eMERGE) Network, the Patient-Centered Outcomes Research Network (PCORNet), and the Observational Health Data Science and Informatics (OHDSI) consortium, have been established and have reported successful applications of secondary use of EHRs in clinical research and practice. In these applications, natural language processing (NLP) technologies have played a crucial role as much of detailed patient information in EHRs is embedded in narrative clinical documents. Meanwhile, a number of clinical NLP systems, such as MedLEE, MetaMap/MetaMap Lite, cTAKES, and MedTagger have been developed and utilized to extract useful information from diverse types of clinical text, such as clinical notes, radiology reports, and pathology reports. Success stories in applying these tools have been reported widely. Despite the demonstrated success of NLP in the clinical domain, methodologies and tools developed for the clinical NLP are still underknown and underutilized by students and experts in the general NLP domain, mainly due to the limited exposure to EHR data. Through this tutorial, we would like to introduce NLP methodologies and tools developed in the clinical domain, and showcase the real-world NLP applications in clinical research and practice at Mayo Clinic (the No. 1 national hospital ranked by the U.S. News {\&} World Report) and the University of Minnesota (the No. 41 best global universities ranked by the U.S. News {\&} World Report). We will review NLP techniques in solving clinical problems and facilitating clinical research, the state-of-the art clinical NLP tools, and share collaboration experience with clinicians, as well as publicly available EHR data and medical resources, and finally conclude the tutorial with vast opportunities and challenges of clinical NLP. The tutorial will provide an overview of clinical backgrounds, and does not presume knowledge in medicine or health care. The goal of this tutorial is to encourage NLP researchers in the general domain (as opposed to the specialized clinical domain) to contribute to this burgeoning area. In this tutorial, we will first present an overview of clinical NLP. We will then dive into two subareas of clinical NLP in clinical research, including big data infrastructure for large-scale clinical NLP and advances of NLP in clinical research, and two subareas in clinical practice, including clinical information extraction and patient cohort retrieval using EHRs. Around 70{\%} of the tutorial will review clinical problems, cutting-edge methodologies, and real-world clinical NLP tools while another 30{\%} introduce use cases at Mayo Clinic and the University of Minnesota. Finally, we will conclude the tutorial with challenges and opportunities in this rapidly developing domain."",
}
@proceedings{naacl-2019-2019-north-american-chapter,
    title = ""Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations)"",
    editor = ""Ammar, Waleed  and
      Louis, Annie  and
      Mostafazadeh, Nasrin"",
    month = jun,
    year = ""2019"",
    address = ""Minneapolis, Minnesota"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/N19-4000"",
}
@",electronic health record,health record,medical record,clinical domain,clinical not,clinical text,natural language process,natural languag,language process,nlp,challeng
" ""Audio De-identification - a New Entity Recognition Task"","," ""Named Entity Recognition (NER) has been mostly studied in the context of written text. Specifically, NER is an important step in de-identification (de-ID) of medical records, many of which are recorded conversations between a patient and a doctor. In such recordings, audio spans with personal information should be redacted, similar to the redaction of sensitive character spans in de-ID for written text. The application of NER in the context of audio de-identification has yet to be fully investigated. To this end, we define the task of audio de-ID, in which audio spans with entity mentions should be detected. We then present our pipeline for this task, which involves Automatic Speech Recognition (ASR), NER on the transcript text, and text-to-audio alignment. Finally, we introduce a novel metric for audio de-ID and a new evaluation benchmark consisting of a large labeled segment of the Switchboard and Fisher audio datasets and detail our pipeline{'}s results on it."",","{cohn-etal-2019-audio,
    title = ""Audio De-identification - a New Entity Recognition Task"",
    author = ""Cohn, Ido  and
      Laish, Itay  and
      Beryozkin, Genady  and
      Li, Gang  and
      Shafran, Izhak  and
      Szpektor, Idan  and
      Hartman, Tzvika  and
      Hassidim, Avinatan  and
      Matias, Yossi"",
    editor = ""Loukina, Anastassia  and
      Morales, Michelle  and
      Kumar, Rohit"",
    booktitle = ""Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)"",
    month = jun,
    year = ""2019"",
    address = ""Minneapolis, Minnesota"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/N19-2025"",
    doi = ""10.18653/v1/N19-2025"",
    pages = ""197--204"",
    abstract = ""Named Entity Recognition (NER) has been mostly studied in the context of written text. Specifically, NER is an important step in de-identification (de-ID) of medical records, many of which are recorded conversations between a patient and a doctor. In such recordings, audio spans with personal information should be redacted, similar to the redaction of sensitive character spans in de-ID for written text. The application of NER in the context of audio de-identification has yet to be fully investigated. To this end, we define the task of audio de-ID, in which audio spans with entity mentions should be detected. We then present our pipeline for this task, which involves Automatic Speech Recognition (ASR), NER on the transcript text, and text-to-audio alignment. Finally, we introduce a novel metric for audio de-ID and a new evaluation benchmark consisting of a large labeled segment of the Switchboard and Fisher audio datasets and detail our pipeline{'}s results on it."",
}
@",medical record,entity recognit,benchmark,evalu
" ""Biomedical Event Extraction based on Knowledge-driven Tree-{LSTM}"","," ""Event extraction for the biomedical domain is more challenging than that in the general news domain since it requires broader acquisition of domain-specific knowledge and deeper understanding of complex contexts. To better encode contextual information and external background knowledge, we propose a novel knowledge base (KB)-driven tree-structured long short-term memory networks (Tree-LSTM) framework, incorporating two new types of features: (1) dependency structures to capture wide contexts; (2) entity properties (types and category descriptions) from external ontologies via entity linking. We evaluate our approach on the BioNLP shared task with Genia dataset and achieve a new state-of-the-art result. In addition, both quantitative and qualitative studies demonstrate the advancement of the Tree-LSTM and the external knowledge representation for biomedical event extraction."",","{li-etal-2019-biomedical,
    title = ""Biomedical Event Extraction based on Knowledge-driven Tree-{LSTM}"",
    author = ""Li, Diya  and
      Huang, Lifu  and
      Ji, Heng  and
      Han, Jiawei"",
    editor = ""Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar"",
    booktitle = ""Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"",
    month = jun,
    year = ""2019"",
    address = ""Minneapolis, Minnesota"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/N19-1145"",
    doi = ""10.18653/v1/N19-1145"",
    pages = ""1421--1430"",
    abstract = ""Event extraction for the biomedical domain is more challenging than that in the general news domain since it requires broader acquisition of domain-specific knowledge and deeper understanding of complex contexts. To better encode contextual information and external background knowledge, we propose a novel knowledge base (KB)-driven tree-structured long short-term memory networks (Tree-LSTM) framework, incorporating two new types of features: (1) dependency structures to capture wide contexts; (2) entity properties (types and category descriptions) from external ontologies via entity linking. We evaluate our approach on the BioNLP shared task with Genia dataset and achieve a new state-of-the-art result. In addition, both quantitative and qualitative studies demonstrate the advancement of the Tree-LSTM and the external knowledge representation for biomedical event extraction."",
}
@",medical domain,nlp,shared task,challeng,evalu
" ""{NLP} Whack-A-Mole: {C}hallenges in Cross-Domain Temporal Expression Extraction"","," ""Incorporating domain knowledge is vital in building successful natural language processing (NLP) applications. Many times, cross-domain application of a tool results in poor performance as the tool does not account for domain-specific attributes. The clinical domain is challenging in this aspect due to specialized medical terms and nomenclature, shorthand notation, fragmented text, and a variety of writing styles used by different medical units. Temporal resolution is an NLP task that, in general, is domain-agnostic because temporal information is represented using a limited lexicon. However, domain-specific aspects of temporal resolution are present in clinical texts. Here we explore parsing issues that arose when running our system, a tool built on Newswire text, on clinical notes in the THYME corpus. Many parsing issues were straightforward to correct; however, a few code changes resulted in a cascading series of parsing errors that had to be resolved before an improvement in performance was observed, revealing the complexity temporal resolution and rule-based parsing. Our system now outperforms current state-of-the-art systems on the THYME corpus with little change in its performance on Newswire texts."",","{olex-etal-2019-nlp,
    title = ""{NLP} Whack-A-Mole: {C}hallenges in Cross-Domain Temporal Expression Extraction"",
    author = ""Olex, Amy  and
      Maffey, Luke  and
      McInnes, Bridget"",
    editor = ""Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar"",
    booktitle = ""Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"",
    month = jun,
    year = ""2019"",
    address = ""Minneapolis, Minnesota"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/N19-1369"",
    doi = ""10.18653/v1/N19-1369"",
    pages = ""3682--3692"",
    abstract = ""Incorporating domain knowledge is vital in building successful natural language processing (NLP) applications. Many times, cross-domain application of a tool results in poor performance as the tool does not account for domain-specific attributes. The clinical domain is challenging in this aspect due to specialized medical terms and nomenclature, shorthand notation, fragmented text, and a variety of writing styles used by different medical units. Temporal resolution is an NLP task that, in general, is domain-agnostic because temporal information is represented using a limited lexicon. However, domain-specific aspects of temporal resolution are present in clinical texts. Here we explore parsing issues that arose when running our system, a tool built on Newswire text, on clinical notes in the THYME corpus. Many parsing issues were straightforward to correct; however, a few code changes resulted in a cascading series of parsing errors that had to be resolved before an improvement in performance was observed, revealing the complexity temporal resolution and rule-based parsing. Our system now outperforms current state-of-the-art systems on the THYME corpus with little change in its performance on Newswire texts."",
}
@",clinical domain,clinical not,clinical text,natural language process,natural languag,language process,nlp,challeng
" ""Learning a Unified Named Entity Tagger from Multiple Partially Annotated Corpora for Efficient Adaptation"","," ""Named entity recognition (NER) identifies typed entity mentions in raw text. While the task is well-established, there is no universally used tagset: often, datasets are annotated for use in downstream applications and accordingly only cover a small set of entity types relevant to a particular task. For instance, in the biomedical domain, one corpus might annotate genes, another chemicals, and another diseases{---}despite the texts in each corpus containing references to all three types of entities. In this paper, we propose a deep structured model to integrate these {``}partially annotated{''} datasets to jointly identify all entity types appearing in the training corpora. By leveraging multiple datasets, the model can learn robust input representations; by building a joint structured model, it avoids potential conflicts caused by combining several models{'} predictions at test time. Experiments show that the proposed model significantly outperforms strong multi-task learning baselines when training on multiple, partially annotated datasets and testing on datasets that contain tags from more than one of the training corpora."",","{huang-etal-2019-learning,
    title = ""Learning a Unified Named Entity Tagger from Multiple Partially Annotated Corpora for Efficient Adaptation"",
    author = ""Huang, Xiao  and
      Dong, Li  and
      Boschee, Elizabeth  and
      Peng, Nanyun"",
    editor = ""Bansal, Mohit  and
      Villavicencio, Aline"",
    booktitle = ""Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/K19-1048"",
    doi = ""10.18653/v1/K19-1048"",
    pages = ""515--527"",
    abstract = ""Named entity recognition (NER) identifies typed entity mentions in raw text. While the task is well-established, there is no universally used tagset: often, datasets are annotated for use in downstream applications and accordingly only cover a small set of entity types relevant to a particular task. For instance, in the biomedical domain, one corpus might annotate genes, another chemicals, and another diseases{---}despite the texts in each corpus containing references to all three types of entities. In this paper, we propose a deep structured model to integrate these {``}partially annotated{''} datasets to jointly identify all entity types appearing in the training corpora. By leveraging multiple datasets, the model can learn robust input representations; by building a joint structured model, it avoids potential conflicts caused by combining several models{'} predictions at test time. Experiments show that the proposed model significantly outperforms strong multi-task learning baselines when training on multiple, partially annotated datasets and testing on datasets that contain tags from more than one of the training corpora."",
}
@",medical domain,natural languag,entity recognit,annotat
" ""Coherence-based Modeling of Clinical Concepts Inferred from Heterogeneous Clinical Notes for {ICU} Patient Risk Stratification"","," ""In hospitals, critical care patients are often susceptible to various complications that adversely affect their morbidity and mortality. Digitized patient data from Electronic Health Records (EHRs) can be utilized to facilitate risk stratification accurately and provide prioritized care. Existing clinical decision support systems are heavily reliant on the structured nature of the EHRs. However, the valuable patient-specific data contained in unstructured clinical notes are often manually transcribed into EHRs. The prolific use of extensive medical jargon, heterogeneity, sparsity, rawness, inconsistent abbreviations, and complex structure of the clinical notes poses significant challenges, and also results in a loss of information during the manual conversion process. In this work, we employ two coherence-based topic modeling approaches to model the free-text in the unstructured clinical nursing notes and capture its semantic textual features with the emphasis on human interpretability. Furthermore, we present FarSight, a long-term aggregation mechanism intended to detect the onset of disease with the earliest recorded symptoms and infections. We utilize the predictive capabilities of deep neural models for the clinical task of risk stratification through ICD-9 code group prediction. Our experimental validation on MIMIC-III (v1.4) database underlined the efficacy of FarSight with coherence-based topic modeling, in extracting discriminative clinical features from the unstructured nursing notes. The proposed approach achieved a superior predictive performance when benchmarked against the structured EHR data based state-of-the-art model, with an improvement of 11.50{\%} in AUPRC and 1.16{\%} in AUROC."",","{gangavarapu-etal-2019-coherence,
    title = ""Coherence-based Modeling of Clinical Concepts Inferred from Heterogeneous Clinical Notes for {ICU} Patient Risk Stratification"",
    author = ""Gangavarapu, Tushaar  and
      Krishnan, Gokul S  and
      Kamath S, Sowmya"",
    editor = ""Bansal, Mohit  and
      Villavicencio, Aline"",
    booktitle = ""Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/K19-1095"",
    doi = ""10.18653/v1/K19-1095"",
    pages = ""1012--1022"",
    abstract = ""In hospitals, critical care patients are often susceptible to various complications that adversely affect their morbidity and mortality. Digitized patient data from Electronic Health Records (EHRs) can be utilized to facilitate risk stratification accurately and provide prioritized care. Existing clinical decision support systems are heavily reliant on the structured nature of the EHRs. However, the valuable patient-specific data contained in unstructured clinical notes are often manually transcribed into EHRs. The prolific use of extensive medical jargon, heterogeneity, sparsity, rawness, inconsistent abbreviations, and complex structure of the clinical notes poses significant challenges, and also results in a loss of information during the manual conversion process. In this work, we employ two coherence-based topic modeling approaches to model the free-text in the unstructured clinical nursing notes and capture its semantic textual features with the emphasis on human interpretability. Furthermore, we present FarSight, a long-term aggregation mechanism intended to detect the onset of disease with the earliest recorded symptoms and infections. We utilize the predictive capabilities of deep neural models for the clinical task of risk stratification through ICD-9 code group prediction. Our experimental validation on MIMIC-III (v1.4) database underlined the efficacy of FarSight with coherence-based topic modeling, in extracting discriminative clinical features from the unstructured nursing notes. The proposed approach achieved a superior predictive performance when benchmarked against the structured EHR data based state-of-the-art model, with an improvement of 11.50{\%} in AUPRC and 1.16{\%} in AUROC."",
}
@",electronic health record,health record,clinical not,natural languag,infer,semant,challeng,benchmark
" ""On the Effectiveness of the Pooling Methods for Biomedical Relation Extraction with Deep Learning"","," ""Deep learning models have achieved state-of-the-art performances on many relation extraction datasets. A common element in these deep learning models involves the pooling mechanisms where a sequence of hidden vectors is aggregated to generate a single representation vector, serving as the features to perform prediction for RE. Unfortunately, the models in the literature tend to employ different strategies to perform pooling for RE, leading to the challenge to determine the best pooling mechanism for this problem, especially in the biomedical domain. In order to answer this question, in this work, we conduct a comprehensive study to evaluate the effectiveness of different pooling mechanisms for the deep learning models in biomedical RE. The experimental results suggest that dependency-based pooling is the best pooling strategy for RE in the biomedical domain, yielding the state-of-the-art performance on two benchmark datasets for this problem."",","{nguyen-etal-2019-effectiveness,
    title = ""On the Effectiveness of the Pooling Methods for Biomedical Relation Extraction with Deep Learning"",
    author = ""Nguyen, Tuan Ngo  and
      Dernoncourt, Franck  and
      Nguyen, Thien Huu"",
    editor = ""Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Lavelli, Alberto  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-6203"",
    doi = ""10.18653/v1/D19-6203"",
    pages = ""18--27"",
    abstract = ""Deep learning models have achieved state-of-the-art performances on many relation extraction datasets. A common element in these deep learning models involves the pooling mechanisms where a sequence of hidden vectors is aggregated to generate a single representation vector, serving as the features to perform prediction for RE. Unfortunately, the models in the literature tend to employ different strategies to perform pooling for RE, leading to the challenge to determine the best pooling mechanism for this problem, especially in the biomedical domain. In order to answer this question, in this work, we conduct a comprehensive study to evaluate the effectiveness of different pooling mechanisms for the deep learning models in biomedical RE. The experimental results suggest that dependency-based pooling is the best pooling strategy for RE in the biomedical domain, yielding the state-of-the-art performance on two benchmark datasets for this problem."",
}
@",medical domain,generat,relation extract,challeng,benchmark,evalu
" ""Leveraging Hierarchical Category Knowledge for Data-Imbalanced Multi-Label Diagnostic Text Understanding"","," ""Clinical notes are essential medical documents to record each patient{'}s symptoms. Each record is typically annotated with medical diagnostic codes, which means diagnosis and treatment. This paper focuses on predicting diagnostic codes given the descriptive present illness in electronic health records by leveraging domain knowledge. We investigate various losses in a convolutional model to utilize hierarchical category knowledge of diagnostic codes in order to allow the model to share semantics across different labels under the same category. The proposed model not only considers the external domain knowledge but also addresses the issue about data imbalance. The MIMIC3 benchmark experiments show that the proposed methods can effectively utilize category knowledge and provide informative cues to improve the performance in terms of the top-ranked diagnostic codes which is better than the prior state-of-the-art. The investigation and discussion express the potential of integrating the domain knowledge in the current machine learning based models and guiding future research directions."",","{tsai-etal-2019-leveraging,
    title = ""Leveraging Hierarchical Category Knowledge for Data-Imbalanced Multi-Label Diagnostic Text Understanding"",
    author = ""Tsai, Shang-Chi  and
      Chang, Ting-Yun  and
      Chen, Yun-Nung"",
    editor = ""Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Lavelli, Alberto  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-6206"",
    doi = ""10.18653/v1/D19-6206"",
    pages = ""39--43"",
    abstract = ""Clinical notes are essential medical documents to record each patient{'}s symptoms. Each record is typically annotated with medical diagnostic codes, which means diagnosis and treatment. This paper focuses on predicting diagnostic codes given the descriptive present illness in electronic health records by leveraging domain knowledge. We investigate various losses in a convolutional model to utilize hierarchical category knowledge of diagnostic codes in order to allow the model to share semantics across different labels under the same category. The proposed model not only considers the external domain knowledge but also addresses the issue about data imbalance. The MIMIC3 benchmark experiments show that the proposed methods can effectively utilize category knowledge and provide informative cues to improve the performance in terms of the top-ranked diagnostic codes which is better than the prior state-of-the-art. The investigation and discussion express the potential of integrating the domain knowledge in the current machine learning based models and guiding future research directions."",
}
@",electronic health record,health record,clinical not,semant,annotat,benchmark
" ""Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction"","," ""Predicting which patients are more likely to be readmitted to a hospital within 30 days after discharge is a valuable piece of information in clinical decision-making. Building a successful readmission risk classifier based on the content of Electronic Health Records (EHRs) has proved, however, to be a challenging task. Previously explored features include mainly structured information, such as sociodemographic data, comorbidity codes and physiological variables. In this paper we assess incorporating additional clinically interpretable NLP-based features such as topic extraction and clinical sentiment analysis to predict early readmission risk in psychiatry patients."",","{alvarez-mellado-etal-2019-assessing,
    title = ""Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction"",
    author = ""Alvarez-Mellado, Elena  and
      Holderness, Eben  and
      Miller, Nicholas  and
      Dhang, Fyonn  and
      Cawkwell, Philip  and
      Bolton, Kirsten  and
      Pustejovsky, James  and
      Hall, Mei-Hua"",
    editor = ""Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Lavelli, Alberto  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-6211"",
    doi = ""10.18653/v1/D19-6211"",
    pages = ""81--86"",
    abstract = ""Predicting which patients are more likely to be readmitted to a hospital within 30 days after discharge is a valuable piece of information in clinical decision-making. Building a successful readmission risk classifier based on the content of Electronic Health Records (EHRs) has proved, however, to be a challenging task. Previously explored features include mainly structured information, such as sociodemographic data, comorbidity codes and physiological variables. In this paper we assess incorporating additional clinically interpretable NLP-based features such as topic extraction and clinical sentiment analysis to predict early readmission risk in psychiatry patients."",
}
@",electronic health record,health record,nlp,sentiment,challeng,assess
" ""Towards Understanding of Medical Randomized Controlled Trials by Conclusion Generation"","," ""Randomized controlled trials (RCTs) represent the paramount evidence of clinical medicine. Using machines to interpret the massive amount of RCTs has the potential of aiding clinical decision-making. We propose a RCT conclusion generation task from the PubMed 200k RCT sentence classification dataset to examine the effectiveness of sequence-to-sequence models on understanding RCTs. We first build a pointer-generator baseline model for conclusion generation. Then we fine-tune the state-of-the-art GPT-2 language model, which is pre-trained with general domain data, for this new medical domain task. Both automatic and human evaluation show that our GPT-2 fine-tuned models achieve improved quality and correctness in the generated conclusions compared to the baseline pointer-generator model. Further inspection points out the limitations of this current approach and future directions to explore."",","{shieh-etal-2019-towards,
    title = ""Towards Understanding of Medical Randomized Controlled Trials by Conclusion Generation"",
    author = ""Shieh, Alexander Te-Wei  and
      Chuang, Yung-Sung  and
      Su, Shang-Yu  and
      Chen, Yun-Nung"",
    editor = ""Holderness, Eben  and
      Jimeno Yepes, Antonio  and
      Lavelli, Alberto  and
      Minard, Anne-Lyse  and
      Pustejovsky, James  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-6214"",
    doi = ""10.18653/v1/D19-6214"",
    pages = ""108--117"",
    abstract = ""Randomized controlled trials (RCTs) represent the paramount evidence of clinical medicine. Using machines to interpret the massive amount of RCTs has the potential of aiding clinical decision-making. We propose a RCT conclusion generation task from the PubMed 200k RCT sentence classification dataset to examine the effectiveness of sequence-to-sequence models on understanding RCTs. We first build a pointer-generator baseline model for conclusion generation. Then we fine-tune the state-of-the-art GPT-2 language model, which is pre-trained with general domain data, for this new medical domain task. Both automatic and human evaluation show that our GPT-2 fine-tuned models achieve improved quality and correctness in the generated conclusions compared to the baseline pointer-generator model. Further inspection points out the limitations of this current approach and future directions to explore."",
}
@",medical domain,generat,evalu
" ""Deep Bidirectional Transformers for Relation Extraction without Supervision"","," ""We present a novel framework to deal with relation extraction tasks in cases where there is complete lack of supervision, either in the form of gold annotations, or relations from a knowledge base. Our approach leverages syntactic parsing and pre-trained word embeddings to extract few but precise relations, which are then used to annotate a larger corpus, in a manner identical to distant supervision. The resulting data set is employed to fine tune a pre-trained BERT model in order to perform relation extraction. Empirical evaluation on four data sets from the biomedical domain shows that our method significantly outperforms two simple baselines for unsupervised relation extraction and, even if not using any supervision at all, achieves slightly worse results than the state-of-the-art in three out of four data sets. Importantly, we show that it is possible to successfully fine tune a large pretrained language model with noisy data, as opposed to previous works that rely on gold data for fine tuning."",","{papanikolaou-etal-2019-deep,
    title = ""Deep Bidirectional Transformers for Relation Extraction without Supervision"",
    author = ""Papanikolaou, Yannis  and
      Roberts, Ian  and
      Pierleoni, Andrea"",
    editor = ""Cherry, Colin  and
      Durrett, Greg  and
      Foster, George  and
      Haffari, Reza  and
      Khadivi, Shahram  and
      Peng, Nanyun  and
      Ren, Xiang  and
      Swayamdipta, Swabha"",
    booktitle = ""Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-6108"",
    doi = ""10.18653/v1/D19-6108"",
    pages = ""67--75"",
    abstract = ""We present a novel framework to deal with relation extraction tasks in cases where there is complete lack of supervision, either in the form of gold annotations, or relations from a knowledge base. Our approach leverages syntactic parsing and pre-trained word embeddings to extract few but precise relations, which are then used to annotate a larger corpus, in a manner identical to distant supervision. The resulting data set is employed to fine tune a pre-trained BERT model in order to perform relation extraction. Empirical evaluation on four data sets from the biomedical domain shows that our method significantly outperforms two simple baselines for unsupervised relation extraction and, even if not using any supervision at all, achieves slightly worse results than the state-of-the-art in three out of four data sets. Importantly, we show that it is possible to successfully fine tune a large pretrained language model with noisy data, as opposed to previous works that rely on gold data for fine tuning."",
}
@",medical domain,nlp,relation extract,annotat,evalu
" ""{P}harma{C}o{NER}: Pharmacological Substances, Compounds and proteins Named Entity Recognition track"","," ""One of the biomedical entity types of relevance for medicine or biosciences are chemical compounds and drugs. The correct detection these entities is critical for other text mining applications building on them, such as adverse drug-reaction detection, medication-related fake news or drug-target extraction. Although a significant effort was made to detect mentions of drugs/chemicals in English texts, so far only very limited attempts were made to recognize them in medical documents in other languages. Taking into account the growing amount of medical publications and clinical records written in Spanish, we have organized the first shared task on detecting drug and chemical entities in Spanish medical documents. Additionally, we included a clinical concept-indexing sub-track asking teams to return SNOMED-CT identifiers related to drugs/chemicals for a collection of documents. For this task, named PharmaCoNER, we generated annotation guidelines together with a corpus of 1,000 manually annotated clinical case studies. A total of 22 teams participated in the sub-track 1, (77 system runs), and 7 teams in the sub-track 2 (19 system runs). Top scoring teams used sophisticated deep learning approaches yielding very competitive results with F-measures above 0.91. These results indicate that there is a real interest in promoting biomedical text mining efforts beyond English. We foresee that the PharmaCoNER annotation guidelines, corpus and participant systems will foster the development of new resources for clinical and biomedical text mining systems of Spanish medical data."",","{gonzalez-agirre-etal-2019-pharmaconer,
    title = ""{P}harma{C}o{NER}: Pharmacological Substances, Compounds and proteins Named Entity Recognition track"",
    author = ""Gonzalez-Agirre, Aitor  and
      Marimon, Montserrat  and
      Intxaurrondo, Ander  and
      Rabal, Obdulia  and
      Villegas, Marta  and
      Krallinger, Martin"",
    editor = ""Jin-Dong, Kim  and
      Claire, N{\'e}dellec  and
      Robert, Bossy  and
      Louise, Del{\'e}ger"",
    booktitle = ""Proceedings of the 5th Workshop on BioNLP Open Shared Tasks"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-5701"",
    doi = ""10.18653/v1/D19-5701"",
    pages = ""1--10"",
    abstract = ""One of the biomedical entity types of relevance for medicine or biosciences are chemical compounds and drugs. The correct detection these entities is critical for other text mining applications building on them, such as adverse drug-reaction detection, medication-related fake news or drug-target extraction. Although a significant effort was made to detect mentions of drugs/chemicals in English texts, so far only very limited attempts were made to recognize them in medical documents in other languages. Taking into account the growing amount of medical publications and clinical records written in Spanish, we have organized the first shared task on detecting drug and chemical entities in Spanish medical documents. Additionally, we included a clinical concept-indexing sub-track asking teams to return SNOMED-CT identifiers related to drugs/chemicals for a collection of documents. For this task, named PharmaCoNER, we generated annotation guidelines together with a corpus of 1,000 manually annotated clinical case studies. A total of 22 teams participated in the sub-track 1, (77 system runs), and 7 teams in the sub-track 2 (19 system runs). Top scoring teams used sophisticated deep learning approaches yielding very competitive results with F-measures above 0.91. These results indicate that there is a real interest in promoting biomedical text mining efforts beyond English. We foresee that the PharmaCoNER annotation guidelines, corpus and participant systems will foster the development of new resources for clinical and biomedical text mining systems of Spanish medical data."",
}
@",clinical record,medical text,nlp,generat,entity recognit,shared task,annotat,track
" ""When Specialization Helps: Using Pooled Contextualized Embeddings to Detect Chemical and Biomedical Entities in {S}panish"","," ""The recognition of pharmacological substances, compounds and proteins is an essential preliminary work for the recognition of relations between chemicals and other biomedically relevant units. In this paper, we describe an approach to Task 1 of the PharmaCoNER Challenge, which involves the recognition of mentions of chemicals and drugs in Spanish medical texts. We train a state-of-the-art BiLSTM-CRF sequence tagger with stacked Pooled Contextualized Embeddings, word and sub-word embeddings using the open-source framework FLAIR. We present a new corpus composed of articles and papers from Spanish health science journals, termed the Spanish Health Corpus, and use it to train domain-specific embeddings which we incorporate in our model training. We achieve a result of 89.76{\%} F1-score using pre-trained embeddings and are able to improve these results to 90.52{\%} F1-score using specialized embeddings."",","{stoeckel-etal-2019-specialization,
    title = ""When Specialization Helps: Using Pooled Contextualized Embeddings to Detect Chemical and Biomedical Entities in {S}panish"",
    author = ""Stoeckel, Manuel  and
      Hemati, Wahed  and
      Mehler, Alexander"",
    editor = ""Jin-Dong, Kim  and
      Claire, N{\'e}dellec  and
      Robert, Bossy  and
      Louise, Del{\'e}ger"",
    booktitle = ""Proceedings of the 5th Workshop on BioNLP Open Shared Tasks"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-5702"",
    doi = ""10.18653/v1/D19-5702"",
    pages = ""11--15"",
    abstract = ""The recognition of pharmacological substances, compounds and proteins is an essential preliminary work for the recognition of relations between chemicals and other biomedically relevant units. In this paper, we describe an approach to Task 1 of the PharmaCoNER Challenge, which involves the recognition of mentions of chemicals and drugs in Spanish medical texts. We train a state-of-the-art BiLSTM-CRF sequence tagger with stacked Pooled Contextualized Embeddings, word and sub-word embeddings using the open-source framework FLAIR. We present a new corpus composed of articles and papers from Spanish health science journals, termed the Spanish Health Corpus, and use it to train domain-specific embeddings which we incorporate in our model training. We achieve a result of 89.76{\%} F1-score using pre-trained embeddings and are able to improve these results to 90.52{\%} F1-score using specialized embeddings."",
}
@",medical text,nlp,shared task,challeng
" ""{I}xa{M}ed at {P}harmaco{NER} Challenge 2019"","," ""The aim of this paper is to present our approach (IxaMed) in the PharmacoNER 2019 task. The task consists of identifying chemical, drug, and gene/protein mentions from clinical case studies written in Spanish. The evaluation of the task is divided in two scenarios: one corresponding to the detection of named entities and one corresponding to the indexation of named entities that have been previously identified. In order to identify named entities we have made use of a Bi-LSTM with a CRF on top in combination with different types of word embeddings. We have achieved our best result (86.81 F-Score) combining pretrained word embeddings of Wikipedia and Electronic Health Records (50M words) with contextual string embeddings of Wikipedia and Electronic Health Records. On the other hand, for the indexation of the named entities we have used the Levenshtein distance obtaining a 85.34 F-Score as our best result."",","{lahuerta-etal-2019-ixamed,
    title = ""{I}xa{M}ed at {P}harmaco{NER} Challenge 2019"",
    author = ""Lahuerta, Xabier  and
      Goenaga, Iakes  and
      Gojenola, Koldo  and
      Atutxa Salazar, Aitziber  and
      Oronoz, Maite"",
    editor = ""Jin-Dong, Kim  and
      Claire, N{\'e}dellec  and
      Robert, Bossy  and
      Louise, Del{\'e}ger"",
    booktitle = ""Proceedings of the 5th Workshop on BioNLP Open Shared Tasks"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-5704"",
    doi = ""10.18653/v1/D19-5704"",
    pages = ""21--25"",
    abstract = ""The aim of this paper is to present our approach (IxaMed) in the PharmacoNER 2019 task. The task consists of identifying chemical, drug, and gene/protein mentions from clinical case studies written in Spanish. The evaluation of the task is divided in two scenarios: one corresponding to the detection of named entities and one corresponding to the indexation of named entities that have been previously identified. In order to identify named entities we have made use of a Bi-LSTM with a CRF on top in combination with different types of word embeddings. We have achieved our best result (86.81 F-Score) combining pretrained word embeddings of Wikipedia and Electronic Health Records (50M words) with contextual string embeddings of Wikipedia and Electronic Health Records. On the other hand, for the indexation of the named entities we have used the Levenshtein distance obtaining a 85.34 F-Score as our best result."",
}
@",electronic health record,health record,nlp,shared task,challeng,evalu
" ""A Deep Learning-Based System for {P}harma{C}o{NER}"","," ""The Biological Text Mining Unit at BSC and CNIO organized the first shared task on chemical {\&} drug mention recognition from Spanish medical texts called PharmaCoNER (Pharmacological Substances, Compounds and proteins and Named Entity Recognition track) in 2019, which includes two tracks: one for NER offset and entity classification (track 1) and the other one for concept indexing (track 2). We developed a pipeline system based on deep learning methods for this shared task, specifically, a subsystem based on BERT (Bidirectional Encoder Representations from Transformers) for NER offset and entity classification and a subsystem based on Bpool (Bi-LSTM with max/mean pooling) for concept indexing. Evaluation conducted on the shared task data showed that our system achieves a micro-average F1-score of 0.9105 on track 1 and a micro-average F1-score of 0.8391 on track 2."",","{xiong-etal-2019-deep,
    title = ""A Deep Learning-Based System for {P}harma{C}o{NER}"",
    author = ""Xiong, Ying  and
      Shen, Yedan  and
      Huang, Yuanhang  and
      Chen, Shuai  and
      Tang, Buzhou  and
      Wang, Xiaolong  and
      Chen, Qingcai  and
      Yan, Jun  and
      Zhou, Yi"",
    editor = ""Jin-Dong, Kim  and
      Claire, N{\'e}dellec  and
      Robert, Bossy  and
      Louise, Del{\'e}ger"",
    booktitle = ""Proceedings of the 5th Workshop on BioNLP Open Shared Tasks"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-5706"",
    doi = ""10.18653/v1/D19-5706"",
    pages = ""33--37"",
    abstract = ""The Biological Text Mining Unit at BSC and CNIO organized the first shared task on chemical {\&} drug mention recognition from Spanish medical texts called PharmaCoNER (Pharmacological Substances, Compounds and proteins and Named Entity Recognition track) in 2019, which includes two tracks: one for NER offset and entity classification (track 1) and the other one for concept indexing (track 2). We developed a pipeline system based on deep learning methods for this shared task, specifically, a subsystem based on BERT (Bidirectional Encoder Representations from Transformers) for NER offset and entity classification and a subsystem based on Bpool (Bi-LSTM with max/mean pooling) for concept indexing. Evaluation conducted on the shared task data showed that our system achieves a micro-average F1-score of 0.9105 on track 1 and a micro-average F1-score of 0.8391 on track 2."",
}
@",medical text,nlp,entity recognit,shared task,evalu,track
" ""Deep neural model with enhanced embeddings for pharmaceutical and chemical entities recognition in {S}panish clinical text"","," ""In this work, we introduce a Deep Learning architecture for pharmaceutical and chemical Named Entity Recognition in Spanish clinical cases texts. We propose a hybrid model approach based on two Bidirectional Long Short-Term Memory (Bi-LSTM) network and Conditional Random Field (CRF) network using character, word, concept and sense embeddings to deal with the extraction of semantic, syntactic and morphological features. The approach was evaluated on the PharmaCoNER Corpus obtaining an F-measure of 85.24{\%} for subtask 1 and 49.36{\%} for subtask2. These results prove that deep learning methods with specific domain embedding representations can outperform the state-of-the-art approaches."",","{rivera-martinez-2019-deep,
    title = ""Deep neural model with enhanced embeddings for pharmaceutical and chemical entities recognition in {S}panish clinical text"",
    author = ""Rivera, Renzo  and
      Mart{\'\i}nez, Paloma"",
    editor = ""Jin-Dong, Kim  and
      Claire, N{\'e}dellec  and
      Robert, Bossy  and
      Louise, Del{\'e}ger"",
    booktitle = ""Proceedings of the 5th Workshop on BioNLP Open Shared Tasks"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-5707"",
    doi = ""10.18653/v1/D19-5707"",
    pages = ""38--46"",
    abstract = ""In this work, we introduce a Deep Learning architecture for pharmaceutical and chemical Named Entity Recognition in Spanish clinical cases texts. We propose a hybrid model approach based on two Bidirectional Long Short-Term Memory (Bi-LSTM) network and Conditional Random Field (CRF) network using character, word, concept and sense embeddings to deal with the extraction of semantic, syntactic and morphological features. The approach was evaluated on the PharmaCoNER Corpus obtaining an F-measure of 85.24{\%} for subtask 1 and 49.36{\%} for subtask2. These results prove that deep learning methods with specific domain embedding representations can outperform the state-of-the-art approaches."",
}
@",clinical text,nlp,entity recognit,semant,shared task,evalu
" ""Transfer Learning in Biomedical Named Entity Recognition: An Evaluation of {BERT} in the {P}harma{C}o{NER} task"","," ""To date, a large amount of biomedical content has been published in non-English texts, especially for clinical documents. Therefore, it is of considerable significance to conduct Natural Language Processing (NLP) research in non-English literature. PharmaCoNER is the first Named Entity Recognition (NER) task to recognize chemical and protein entities from Spanish biomedical texts. Since there have been abundant resources in the NLP field, how to exploit these existing resources to a new task to obtain competitive performance is a meaningful study. Inspired by the success of transfer learning with language models, we introduce the BERT benchmark to facilitate the research of PharmaCoNER task. In this paper, we evaluate two baselines based on Multilingual BERT and BioBERT on the PharmaCoNER corpus. Experimental results show that transferring the knowledge learned from source large-scale datasets to the target domain offers an effective solution for the PharmaCoNER task."",","{sun-yang-2019-transfer,
    title = ""Transfer Learning in Biomedical Named Entity Recognition: An Evaluation of {BERT} in the {P}harma{C}o{NER} task"",
    author = ""Sun, Cong  and
      Yang, Zhihao"",
    editor = ""Jin-Dong, Kim  and
      Claire, N{\'e}dellec  and
      Robert, Bossy  and
      Louise, Del{\'e}ger"",
    booktitle = ""Proceedings of the 5th Workshop on BioNLP Open Shared Tasks"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-5715"",
    doi = ""10.18653/v1/D19-5715"",
    pages = ""100--104"",
    abstract = ""To date, a large amount of biomedical content has been published in non-English texts, especially for clinical documents. Therefore, it is of considerable significance to conduct Natural Language Processing (NLP) research in non-English literature. PharmaCoNER is the first Named Entity Recognition (NER) task to recognize chemical and protein entities from Spanish biomedical texts. Since there have been abundant resources in the NLP field, how to exploit these existing resources to a new task to obtain competitive performance is a meaningful study. Inspired by the success of transfer learning with language models, we introduce the BERT benchmark to facilitate the research of PharmaCoNER task. In this paper, we evaluate two baselines based on Multilingual BERT and BioBERT on the PharmaCoNER corpus. Experimental results show that transferring the knowledge learned from source large-scale datasets to the target domain offers an effective solution for the PharmaCoNER task."",
}
@",medical text,natural language process,natural languag,language process,nlp,entity recognit,shared task,benchmark,evalu
" ""Using Snomed to recognize and index chemical and drug mentions."","," ""In this paper we describe a new named entity extraction system. Our work proposes a system for the identification and annotation of drug names in Spanish biomedical texts based on machine learning and deep learning models. Subsequently, a standardized code using Snomed is assigned to these drugs, for this purpose, Natural Language Processing tools and techniques have been used, and a dictionary of different sources of information has been built. The results are promising, we obtain 78{\%} in F1 score on the first sub-track and in the second task we map with Snomed correctly 72{\%} of the found entities."",","{lopez-ubeda-etal-2019-using,
    title = ""Using Snomed to recognize and index chemical and drug mentions."",
    author = ""L{\'o}pez {\'U}beda, Pilar  and
      D{\'\i}az Galiano, Manuel Carlos  and
      Urena Lopez, L. Alfonso  and
      Martin, Maite"",
    editor = ""Jin-Dong, Kim  and
      Claire, N{\'e}dellec  and
      Robert, Bossy  and
      Louise, Del{\'e}ger"",
    booktitle = ""Proceedings of the 5th Workshop on BioNLP Open Shared Tasks"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-5718"",
    doi = ""10.18653/v1/D19-5718"",
    pages = ""115--120"",
    abstract = ""In this paper we describe a new named entity extraction system. Our work proposes a system for the identification and annotation of drug names in Spanish biomedical texts based on machine learning and deep learning models. Subsequently, a standardized code using Snomed is assigned to these drugs, for this purpose, Natural Language Processing tools and techniques have been used, and a dictionary of different sources of information has been built. The results are promising, we obtain 78{\%} in F1 score on the first sub-track and in the second task we map with Snomed correctly 72{\%} of the found entities."",
}
@",medical text,natural language process,natural languag,language process,nlp,shared task,annotat,track
" ""{BOUN}-{ISIK} Participation: An Unsupervised Approach for the Named Entity Normalization and Relation Extraction of Bacteria Biotopes"","," ""This paper presents our participation to the Bacteria Biotope Task of the BioNLP Shared Task 2019. Our participation includes two systems for the two subtasks of the Bacteria Biotope Task: the normalization of entities (BB-norm) and the identification of the relations between the entities given a biomedical text (BB-rel). For the normalization of entities, we utilized word embeddings and syntactic re-ranking. For the relation extraction task, pre-defined rules are used. Although both approaches are unsupervised, in the sense that they do not need any labeled data, they achieved promising results. Especially, for the BB-norm task, the results have shown that the proposed method performs as good as deep learning based methods, which require labeled data."",","{karadeniz-etal-2019-boun,
    title = ""{BOUN}-{ISIK} Participation: An Unsupervised Approach for the Named Entity Normalization and Relation Extraction of Bacteria Biotopes"",
    author = {Karadeniz, {\.I}lknur  and
      Tuna, {\""O}mer Faruk  and
      {\""O}zg{\""u}r, Arzucan},
    editor = ""Jin-Dong, Kim  and
      Claire, N{\'e}dellec  and
      Robert, Bossy  and
      Louise, Del{\'e}ger"",
    booktitle = ""Proceedings of the 5th Workshop on BioNLP Open Shared Tasks"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-5722"",
    doi = ""10.18653/v1/D19-5722"",
    pages = ""150--157"",
    abstract = ""This paper presents our participation to the Bacteria Biotope Task of the BioNLP Shared Task 2019. Our participation includes two systems for the two subtasks of the Bacteria Biotope Task: the normalization of entities (BB-norm) and the identification of the relations between the entities given a biomedical text (BB-rel). For the normalization of entities, we utilized word embeddings and syntactic re-ranking. For the relation extraction task, pre-defined rules are used. Although both approaches are unsupervised, in the sense that they do not need any labeled data, they achieved promising results. Especially, for the BB-norm task, the results have shown that the proposed method performs as good as deep learning based methods, which require labeled data."",
}
@",medical text,nlp,relation extract,shared task
" ""{CRAFT} Shared Tasks 2019 Overview {---} Integrated Structure, Semantics, and Coreference"","," ""As part of the BioNLP Open Shared Tasks 2019, the CRAFT Shared Tasks 2019 provides a platform to gauge the state of the art for three fundamental language processing tasks {---} dependency parse construction, coreference resolution, and ontology concept identification {---} over full-text biomedical articles. The structural annotation task requires the automatic generation of dependency parses for each sentence of an article given only the article text. The coreference resolution task focuses on linking coreferring base noun phrase mentions into chains using the symmetrical and transitive identity relation. The ontology concept annotation task involves the identification of concept mentions within text using the classes of ten distinct ontologies in the biomedical domain, both unmodified and augmented with extension classes. This paper provides an overview of each task, including descriptions of the data provided to participants and the evaluation metrics used, and discusses participant results relative to baseline performances for each of the three tasks."",","{baumgartner-etal-2019-craft,
    title = ""{CRAFT} Shared Tasks 2019 Overview {---} Integrated Structure, Semantics, and Coreference"",
    author = ""Baumgartner, William  and
      Bada, Michael  and
      Pyysalo, Sampo  and
      Ciosici, Manuel R.  and
      Hailu, Negacy  and
      Pielke-Lombardo, Harrison  and
      Regan, Michael  and
      Hunter, Lawrence"",
    editor = ""Jin-Dong, Kim  and
      Claire, N{\'e}dellec  and
      Robert, Bossy  and
      Louise, Del{\'e}ger"",
    booktitle = ""Proceedings of the 5th Workshop on BioNLP Open Shared Tasks"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-5725"",
    doi = ""10.18653/v1/D19-5725"",
    pages = ""174--184"",
    abstract = ""As part of the BioNLP Open Shared Tasks 2019, the CRAFT Shared Tasks 2019 provides a platform to gauge the state of the art for three fundamental language processing tasks {---} dependency parse construction, coreference resolution, and ontology concept identification {---} over full-text biomedical articles. The structural annotation task requires the automatic generation of dependency parses for each sentence of an article given only the article text. The coreference resolution task focuses on linking coreferring base noun phrase mentions into chains using the symmetrical and transitive identity relation. The ontology concept annotation task involves the identification of concept mentions within text using the classes of ten distinct ontologies in the biomedical domain, both unmodified and augmented with extension classes. This paper provides an overview of each task, including descriptions of the data provided to participants and the evaluation metrics used, and discusses participant results relative to baseline performances for each of the three tasks."",
}
@",medical domain,language process,nlp,generat,semant,shared task,annotat,evalu
" ""Neural Dependency Parsing of Biomedical Text: {T}urku{NLP} entry in the {CRAFT} Structural Annotation Task"","," ""We present the approach taken by the TurkuNLP group in the CRAFT Structural Annotation task, a shared task on dependency parsing. Our approach builds primarily on the Turku neural parser, a native dependency parser that ranked among the best in the recent CoNLL tasks on parsing Universal Dependencies. To adapt the parser to the biomedical domain, we considered and evaluated a number of approaches, including the generation of custom word embeddings, combination with other in-domain resources, and the incorporation of information from named entity recognition. We achieved a labeled attachment score of 89.7{\%}, the best result among task participants."",","{ngo-etal-2019-neural,
    title = ""Neural Dependency Parsing of Biomedical Text: {T}urku{NLP} entry in the {CRAFT} Structural Annotation Task"",
    author = ""Ngo, Thang Minh  and
      Kanerva, Jenna  and
      Ginter, Filip  and
      Pyysalo, Sampo"",
    editor = ""Jin-Dong, Kim  and
      Claire, N{\'e}dellec  and
      Robert, Bossy  and
      Louise, Del{\'e}ger"",
    booktitle = ""Proceedings of the 5th Workshop on BioNLP Open Shared Tasks"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-5728"",
    doi = ""10.18653/v1/D19-5728"",
    pages = ""206--215"",
    abstract = ""We present the approach taken by the TurkuNLP group in the CRAFT Structural Annotation task, a shared task on dependency parsing. Our approach builds primarily on the Turku neural parser, a native dependency parser that ranked among the best in the recent CoNLL tasks on parsing Universal Dependencies. To adapt the parser to the biomedical domain, we considered and evaluated a number of approaches, including the generation of custom word embeddings, combination with other in-domain resources, and the incorporation of information from named entity recognition. We achieved a labeled attachment score of 89.7{\%}, the best result among task participants."",
}
@",medical domain,medical text,nlp,generat,entity recognit,shared task,annotat,evalu
" ""{RD}o{C} Task at {B}io{NLP}-{OST} 2019"","," ""BioNLP Open Shared Tasks (BioNLP-OST) is an international competition organized to facilitate development and sharing of computational tasks of biomedical text mining and solutions to them. For BioNLP-OST 2019, we introduced a new mental health informatics task called {``}RDoC Task{''}, which is composed of two subtasks: information retrieval and sentence extraction through National Institutes of Mental Health{'}s Research Domain Criteria framework. Five and four teams around the world participated in the two tasks, respectively. According to the performance on the two tasks, we observe that there is room for improvement for text mining on brain research and mental illness."",","{anani-etal-2019-rdoc,
    title = ""{RD}o{C} Task at {B}io{NLP}-{OST} 2019"",
    author = ""Anani, Mohammad  and
      Kazi, Nazmul  and
      Kuntz, Matthew  and
      Kahanda, Indika"",
    editor = ""Jin-Dong, Kim  and
      Claire, N{\'e}dellec  and
      Robert, Bossy  and
      Louise, Del{\'e}ger"",
    booktitle = ""Proceedings of the 5th Workshop on BioNLP Open Shared Tasks"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-5729"",
    doi = ""10.18653/v1/D19-5729"",
    pages = ""216--226"",
    abstract = ""BioNLP Open Shared Tasks (BioNLP-OST) is an international competition organized to facilitate development and sharing of computational tasks of biomedical text mining and solutions to them. For BioNLP-OST 2019, we introduced a new mental health informatics task called {``}RDoC Task{''}, which is composed of two subtasks: information retrieval and sentence extraction through National Institutes of Mental Health{'}s Research Domain Criteria framework. Five and four teams around the world participated in the two tasks, respectively. According to the performance on the two tasks, we observe that there is room for improvement for text mining on brain research and mental illness."",
}
@",medical text,nlp,information retriev,shared task
" ""{M}ed{CATT}rainer: A Biomedical Free Text Annotation Interface with Active Learning and Research Use Case Specific Customisation"","," ""An interface for building, improving and customising a given Named Entity Recognition and Linking (NER+L) model for biomedical domain text, and the efficient collation of accurate research use case specific training data and subsequent model training. Screencast demo available here: \url{https://www.youtube.com/watch?v","{searle-etal-2019-medcattrainer,
    title = ""{M}ed{CATT}rainer: A Biomedical Free Text Annotation Interface with Active Learning and Research Use Case Specific Customisation"",
    author = ""Searle, Thomas  and
      Kraljevic, Zeljko  and
      Bendayan, Rebecca  and
      Bean, Daniel  and
      Dobson, Richard"",
    editor = ""Pad{\'o}, Sebastian  and
      Huang, Ruihong"",
    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-3024"",
    doi = ""10.18653/v1/D19-3024"",
    pages = ""139--144"",
    abstract = ""An interface for building, improving and customising a given Named Entity Recognition and Linking (NER+L) model for biomedical domain text, and the efficient collation of accurate research use case specific training data and subsequent model training. Screencast demo available here: \url{https://www.youtube.com/watch?v=lM914DQjvSo}"",
}
@",medical domain,natural language process,natural languag,language process,nlp,entity recognit,annotat
" ""Leveraging Dependency Forest for Neural Medical Relation Extraction"","," ""Medical relation extraction discovers relations between entity mentions in text, such as research articles. For this task, dependency syntax has been recognized as a crucial source of features. Yet in the medical domain, 1-best parse trees suffer from relatively low accuracies, diminishing their usefulness. We investigate a method to alleviate this problem by utilizing dependency forests. Forests contain more than one possible decisions and therefore have higher recall but more noise compared with 1-best outputs. A graph neural network is used to represent the forests, automatically distinguishing the useful syntactic information from parsing noise. Results on two benchmarks show that our method outperforms the standard tree-based methods, giving the state-of-the-art results in the literature."",","{song-etal-2019-leveraging,
    title = ""Leveraging Dependency Forest for Neural Medical Relation Extraction"",
    author = ""Song, Linfeng  and
      Zhang, Yue  and
      Gildea, Daniel  and
      Yu, Mo  and
      Wang, Zhiguo  and
      Su, Jinsong"",
    editor = ""Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun"",
    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-1020"",
    doi = ""10.18653/v1/D19-1020"",
    pages = ""208--218"",
    abstract = ""Medical relation extraction discovers relations between entity mentions in text, such as research articles. For this task, dependency syntax has been recognized as a crucial source of features. Yet in the medical domain, 1-best parse trees suffer from relatively low accuracies, diminishing their usefulness. We investigate a method to alleviate this problem by utilizing dependency forests. Forests contain more than one possible decisions and therefore have higher recall but more noise compared with 1-best outputs. A graph neural network is used to represent the forests, automatically distinguishing the useful syntactic information from parsing noise. Results on two benchmarks show that our method outperforms the standard tree-based methods, giving the state-of-the-art results in the literature."",
}
@",medical domain,natural language process,natural languag,language process,nlp,relation extract,benchmark
" ""Leveraging Medical Literature for Section Prediction in Electronic Health Records"","," ""Electronic Health Records (EHRs) contain both structured content and unstructured (text) content about a patient{'}s medical history. In the unstructured text parts, there are common sections such as Assessment and Plan, Social History, and Medications. These sections help physicians find information easily and can be used by an information retrieval system to return specific information sought by a user. However, it is common that the exact format of sections in a particular EHR does not adhere to known patterns. Therefore, being able to predict sections and headers in EHRs automatically is beneficial to physicians. Prior approaches in EHR section prediction have only used text data from EHRs and have required significant manual annotation. We propose using sections from medical literature (e.g., textbooks, journals, web content) that contain content similar to that found in EHR sections. Our approach uses data from a different kind of source where labels are provided without the need of a time-consuming annotation effort. We use this data to train two models: an RNN and a BERT-based model. We apply the learned models along with source data via transfer learning to predict sections in EHRs. Our results show that medical literature can provide helpful supervision signal for this classification task."",","{rosenthal-etal-2019-leveraging,
    title = ""Leveraging Medical Literature for Section Prediction in Electronic Health Records"",
    author = ""Rosenthal, Sara  and
      Barker, Ken  and
      Liang, Zhicheng"",
    editor = ""Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun"",
    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-1492"",
    doi = ""10.18653/v1/D19-1492"",
    pages = ""4864--4873"",
    abstract = ""Electronic Health Records (EHRs) contain both structured content and unstructured (text) content about a patient{'}s medical history. In the unstructured text parts, there are common sections such as Assessment and Plan, Social History, and Medications. These sections help physicians find information easily and can be used by an information retrieval system to return specific information sought by a user. However, it is common that the exact format of sections in a particular EHR does not adhere to known patterns. Therefore, being able to predict sections and headers in EHRs automatically is beneficial to physicians. Prior approaches in EHR section prediction have only used text data from EHRs and have required significant manual annotation. We propose using sections from medical literature (e.g., textbooks, journals, web content) that contain content similar to that found in EHR sections. Our approach uses data from a different kind of source where labels are provided without the need of a time-consuming annotation effort. We use this data to train two models: an RNN and a BERT-based model. We apply the learned models along with source data via transfer learning to predict sections in EHRs. Our results show that medical literature can provide helpful supervision signal for this classification task."",
}
@",electronic health record,health record,natural language process,natural languag,language process,nlp,information retriev,annotat,assess
" ""Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph"","," ""Symptom diagnosis is a challenging yet profound problem in natural language processing. Most previous research focus on investigating the standard electronic medical records for symptom diagnosis, while the dialogues between doctors and patients that contain more rich information are not well studied. In this paper, we first construct a dialogue symptom diagnosis dataset based on an online medical forum with a large amount of dialogues between patients and doctors. Then, we provide some benchmark models on this dataset to boost the research of dialogue symptom diagnosis. In order to further enhance the performance of symptom diagnosis over dialogues, we propose a global attention mechanism to capture more symptom related information, and build a symptom graph to model the associations between symptoms rather than treating each symptom independently. Experimental results show that both the global attention and symptom graph are effective to boost dialogue symptom diagnosis. In particular, our proposed model achieves the state-of-the-art performance on the constructed dataset."",","{lin-etal-2019-enhancing,
    title = ""Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph"",
    author = ""Lin, Xinzhu  and
      He, Xiahui  and
      Chen, Qin  and
      Tou, Huaixiao  and
      Wei, Zhongyu  and
      Chen, Ting"",
    editor = ""Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun"",
    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-1508"",
    doi = ""10.18653/v1/D19-1508"",
    pages = ""5033--5042"",
    abstract = ""Symptom diagnosis is a challenging yet profound problem in natural language processing. Most previous research focus on investigating the standard electronic medical records for symptom diagnosis, while the dialogues between doctors and patients that contain more rich information are not well studied. In this paper, we first construct a dialogue symptom diagnosis dataset based on an online medical forum with a large amount of dialogues between patients and doctors. Then, we provide some benchmark models on this dataset to boost the research of dialogue symptom diagnosis. In order to further enhance the performance of symptom diagnosis over dialogues, we propose a global attention mechanism to capture more symptom related information, and build a symptom graph to model the associations between symptoms rather than treating each symptom independently. Experimental results show that both the global attention and symptom graph are effective to boost dialogue symptom diagnosis. In particular, our proposed model achieves the state-of-the-art performance on the constructed dataset."",
}
@",medical record,natural language process,natural languag,language process,nlp,challeng,benchmark
" ""Using Clinical Notes with Time Series Data for {ICU} Management"","," ""Monitoring patients in ICU is a challenging and high-cost task. Hence, predicting the condition of patients during their ICU stay can help provide better acute care and plan the hospital{'}s resources. There has been continuous progress in machine learning research for ICU management, and most of this work has focused on using time series signals recorded by ICU instruments. In our work, we show that adding clinical notes as another modality improves the performance of the model for three benchmark tasks: in-hospital mortality prediction, modeling decompensation, and length of stay forecasting that play an important role in ICU management. While the time-series data is measured at regular intervals, doctor notes are charted at irregular times, making it challenging to model them together. We propose a method to model them jointly, achieving considerable improvement across benchmark tasks over baseline time-series model."",","{khadanga-etal-2019-using,
    title = ""Using Clinical Notes with Time Series Data for {ICU} Management"",
    author = ""Khadanga, Swaraj  and
      Aggarwal, Karan  and
      Joty, Shafiq  and
      Srivastava, Jaideep"",
    editor = ""Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun"",
    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D19-1678"",
    doi = ""10.18653/v1/D19-1678"",
    pages = ""6432--6437"",
    abstract = ""Monitoring patients in ICU is a challenging and high-cost task. Hence, predicting the condition of patients during their ICU stay can help provide better acute care and plan the hospital{'}s resources. There has been continuous progress in machine learning research for ICU management, and most of this work has focused on using time series signals recorded by ICU instruments. In our work, we show that adding clinical notes as another modality improves the performance of the model for three benchmark tasks: in-hospital mortality prediction, modeling decompensation, and length of stay forecasting that play an important role in ICU management. While the time-series data is measured at regular intervals, doctor notes are charted at irregular times, making it challenging to model them together. We propose a method to model them jointly, achieving considerable improvement across benchmark tasks over baseline time-series model."",
}
@",clinical not,natural language process,natural languag,language process,nlp,challeng,benchmark
" ""{DRC}o{V}e: An Augmented Word Representation Approach using Distributional and Relational Context"","," ""Word representation using the distributional information of words from a sizeable corpus is considered efficacious in many natural language processing and text mining applications. However, distributional representation of a word is unable to capture distant relational knowledge, representing the relational semantics. In this paper, we propose a novel word representation approach using distributional and relational contexts, DRCoVe, which augments the distributional representation of a word using the relational semantics extracted as syntactic and semantic association among entities from the underlying corpus. Unlike existing approaches that use external knowledge bases representing the relational semantics for enhanced word representation, DRCoVe uses typed dependencies (aka syntactic dependencies) to extract relational knowledge from the underlying corpus. The proposed approach is applied over a biomedical text corpus to learn word representation and compared with GloVe, which is one of the most popular word embedding approaches. The evaluation results on various benchmark datasets for word similarity and word categorization tasks demonstrate the effectiveness of DRCoVe over the GloVe."",","{parwez-etal-2019-drcove,
    title = ""{DRC}o{V}e: An Augmented Word Representation Approach using Distributional and Relational Context"",
    author = ""Parwez, Md. Aslam  and
      Abulaish, Muhammad  and
      Fazil, Mohd"",
    editor = ""Sharma, Dipti Misra  and
      Bhattacharya, Pushpak"",
    booktitle = ""Proceedings of the 16th International Conference on Natural Language Processing"",
    month = dec,
    year = ""2019"",
    address = ""International Institute of Information Technology, Hyderabad, India"",
    publisher = ""NLP Association of India"",
    url = ""https://aclanthology.org/2019.icon-1.26"",
    pages = ""220--229"",
    abstract = ""Word representation using the distributional information of words from a sizeable corpus is considered efficacious in many natural language processing and text mining applications. However, distributional representation of a word is unable to capture distant relational knowledge, representing the relational semantics. In this paper, we propose a novel word representation approach using distributional and relational contexts, DRCoVe, which augments the distributional representation of a word using the relational semantics extracted as syntactic and semantic association among entities from the underlying corpus. Unlike existing approaches that use external knowledge bases representing the relational semantics for enhanced word representation, DRCoVe uses typed dependencies (aka syntactic dependencies) to extract relational knowledge from the underlying corpus. The proposed approach is applied over a biomedical text corpus to learn word representation and compared with GloVe, which is one of the most popular word embedding approaches. The evaluation results on various benchmark datasets for word similarity and word categorization tasks demonstrate the effectiveness of DRCoVe over the GloVe."",
}
@",medical text,natural language process,natural languag,language process,nlp,semant,benchmark,evalu
" ""Findings of the {WMT} 2018 Biomedical Translation Shared Task: Evaluation on {M}edline test sets"","," ""Machine translation enables the automatic translation of textual documents between languages and can facilitate access to information only available in a given language for non-speakers of this language, e.g. research results presented in scientific publications. In this paper, we provide an overview of the Biomedical Translation shared task in the Workshop on Machine Translation (WMT) 2018, which specifically examined the performance of machine translation systems for biomedical texts. This year, we provided test sets of scientific publications from two sources (EDP and Medline) and for six language pairs (English with each of Chinese, French, German, Portuguese, Romanian and Spanish). We describe the development of the various test sets, the submissions that we received and the evaluations that we carried out. We obtained a total of 39 runs from six teams and some of this year{'}s BLEU scores were somewhat higher that last year{'}s, especially for teams that made use of biomedical resources or state-of-the-art MT algorithms (e.g. Transformer). Finally, our manual evaluation scored automatic translations higher than the reference translations for German and Spanish."",","{neves-etal-2018-findings,
    title = ""Findings of the {WMT} 2018 Biomedical Translation Shared Task: Evaluation on {M}edline test sets"",
    author = ""Neves, Mariana  and
      Jimeno Yepes, Antonio  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Grozea, Cristian  and
      Siu, Amy  and
      Kittner, Madeleine  and
      Verspoor, Karin"",
    editor = ""Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin"",
    booktitle = ""Proceedings of the Third Conference on Machine Translation: Shared Task Papers"",
    month = oct,
    year = ""2018"",
    address = ""Belgium, Brussels"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-6403"",
    doi = ""10.18653/v1/W18-6403"",
    pages = ""324--339"",
    abstract = ""Machine translation enables the automatic translation of textual documents between languages and can facilitate access to information only available in a given language for non-speakers of this language, e.g. research results presented in scientific publications. In this paper, we provide an overview of the Biomedical Translation shared task in the Workshop on Machine Translation (WMT) 2018, which specifically examined the performance of machine translation systems for biomedical texts. This year, we provided test sets of scientific publications from two sources (EDP and Medline) and for six language pairs (English with each of Chinese, French, German, Portuguese, Romanian and Spanish). We describe the development of the various test sets, the submissions that we received and the evaluations that we carried out. We obtained a total of 39 runs from six teams and some of this year{'}s BLEU scores were somewhat higher that last year{'}s, especially for teams that made use of biomedical resources or state-of-the-art MT algorithms (e.g. Transformer). Finally, our manual evaluation scored automatic translations higher than the reference translations for German and Spanish."",
}
@",medical text,translat,shared task,evalu
" ""Translation of Biomedical Documents with Focus on {S}panish-{E}nglish"","," ""For the WMT 2018 shared task of translating documents pertaining to the Biomedical domain, we developed a scoring formula that uses an unsophisticated and effective method of weighting term frequencies and was integrated in a data selection pipeline. The method was applied on five language pairs and it performed best on Portuguese-English, where a BLEU score of 41.84 placed it third out of seven runs submitted by three institutions. In this paper, we describe our method and results with a special focus on Spanish-English where we compare it against a state-of-the-art method. Our contribution to the task lies in introducing a fast, unsupervised method for selecting domain-specific data for training models which obtain good results using only 10{\%} of the general domain data."",","{duma-menzel-2018-translation,
    title = ""Translation of Biomedical Documents with Focus on {S}panish-{E}nglish"",
    author = ""Duma, Mirela-Stefania  and
      Menzel, Wolfgang"",
    editor = ""Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin"",
    booktitle = ""Proceedings of the Third Conference on Machine Translation: Shared Task Papers"",
    month = oct,
    year = ""2018"",
    address = ""Belgium, Brussels"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-6444"",
    doi = ""10.18653/v1/W18-6444"",
    pages = ""637--643"",
    abstract = ""For the WMT 2018 shared task of translating documents pertaining to the Biomedical domain, we developed a scoring formula that uses an unsophisticated and effective method of weighting term frequencies and was integrated in a data selection pipeline. The method was applied on five language pairs and it performed best on Portuguese-English, where a BLEU score of 41.84 placed it third out of seven runs submitted by three institutions. In this paper, we describe our method and results with a special focus on Spanish-English where we compare it against a state-of-the-art method. Our contribution to the task lies in introducing a fast, unsupervised method for selecting domain-specific data for training models which obtain good results using only 10{\%} of the general domain data."",
}
@",medical domain,translat,shared task
" ""{LMU} {M}unich{'}s Neural Machine Translation Systems at {WMT} 2018"","," ""We present the LMU Munich machine translation systems for the English{--}German language pair. We have built neural machine translation systems for both translation directions (English→German and German→English) and for two different domains (the biomedical domain and the news domain). The systems were used for our participation in the WMT18 biomedical translation task and in the shared task on machine translation of news. The main focus of our recent system development efforts has been on achieving improvements in the biomedical domain over last year{'}s strong biomedical translation engine for English→German (Huck et al., 2017a). Considerable progress has been made in the latter task, which we report on in this paper."",","{huck-etal-2018-lmu,
    title = ""{LMU} {M}unich{'}s Neural Machine Translation Systems at {WMT} 2018"",
    author = ""Huck, Matthias  and
      Stojanovski, Dario  and
      Hangya, Viktor  and
      Fraser, Alexander"",
    editor = ""Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin"",
    booktitle = ""Proceedings of the Third Conference on Machine Translation: Shared Task Papers"",
    month = oct,
    year = ""2018"",
    address = ""Belgium, Brussels"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-6446"",
    doi = ""10.18653/v1/W18-6446"",
    pages = ""648--654"",
    abstract = ""We present the LMU Munich machine translation systems for the English{--}German language pair. We have built neural machine translation systems for both translation directions (English→German and German→English) and for two different domains (the biomedical domain and the news domain). The systems were used for our participation in the WMT18 biomedical translation task and in the shared task on machine translation of news. The main focus of our recent system development efforts has been on achieving improvements in the biomedical domain over last year{'}s strong biomedical translation engine for English→German (Huck et al., 2017a). Considerable progress has been made in the latter task, which we report on in this paper."",
}
@",medical domain,translat,shared task
" ""Investigating the Challenges of Temporal Relation Extraction from Clinical Text"","," ""Temporal reasoning remains as an unsolved task for Natural Language Processing (NLP), particularly demonstrated in the clinical domain. The complexity of temporal representation in language is evident as results of the 2016 Clinical TempEval challenge indicate: the current state-of-the-art systems perform well in solving mention-identification tasks of event and time expressions but poorly in temporal relation extraction, showing a gap of around 0.25 point below human performance. We explore to adapt the tree-based LSTM-RNN model proposed by Miwa and Bansal (2016) to temporal relation extraction from clinical text, obtaining a five point improvement over the best 2016 Clinical TempEval system and two points over the state-of-the-art. We deliver a deep analysis of the results and discuss the next step towards human-like temporal reasoning."",","{galvan-etal-2018-investigating,
    title = ""Investigating the Challenges of Temporal Relation Extraction from Clinical Text"",
    author = ""Galvan, Diana  and
      Okazaki, Naoaki  and
      Matsuda, Koji  and
      Inui, Kentaro"",
    editor = ""Lavelli, Alberto  and
      Minard, Anne-Lyse  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis"",
    month = oct,
    year = ""2018"",
    address = ""Brussels, Belgium"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-5607"",
    doi = ""10.18653/v1/W18-5607"",
    pages = ""55--64"",
    abstract = ""Temporal reasoning remains as an unsolved task for Natural Language Processing (NLP), particularly demonstrated in the clinical domain. The complexity of temporal representation in language is evident as results of the 2016 Clinical TempEval challenge indicate: the current state-of-the-art systems perform well in solving mention-identification tasks of event and time expressions but poorly in temporal relation extraction, showing a gap of around 0.25 point below human performance. We explore to adapt the tree-based LSTM-RNN model proposed by Miwa and Bansal (2016) to temporal relation extraction from clinical text, obtaining a five point improvement over the best 2016 Clinical TempEval system and two points over the state-of-the-art. We deliver a deep analysis of the results and discuss the next step towards human-like temporal reasoning."",
}
@",clinical domain,clinical text,natural language process,natural languag,language process,nlp,relation extract,challeng
" ""Syntax-based Transfer Learning for the Task of Biomedical Relation Extraction"","," ""Transfer learning (TL) proposes to enhance machine learning performance on a problem, by reusing labeled data originally designed for a related problem. In particular, domain adaptation consists, for a specific task, in reusing training data developed for the same task but a distinct domain. This is particularly relevant to the applications of deep learning in Natural Language Processing, because those usually require large annotated corpora that may not exist for the targeted domain, but exist for side domains. In this paper, we experiment with TL for the task of Relation Extraction (RE) from biomedical texts, using the TreeLSTM model. We empirically show the impact of TreeLSTM alone and with domain adaptation by obtaining better performances than the state of the art on two biomedical RE tasks and equal performances for two others, for which few annotated data are available. Furthermore, we propose an analysis of the role that syntactic features may play in TL for RE."",","{legrand-etal-2018-syntax,
    title = ""Syntax-based Transfer Learning for the Task of Biomedical Relation Extraction"",
    author = {Legrand, Jo{\""e}l  and
      Toussaint, Yannick  and
      Ra{\""\i}ssi, Chedy  and
      Coulet, Adrien},
    editor = ""Lavelli, Alberto  and
      Minard, Anne-Lyse  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis"",
    month = oct,
    year = ""2018"",
    address = ""Brussels, Belgium"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-5617"",
    doi = ""10.18653/v1/W18-5617"",
    pages = ""149--159"",
    abstract = ""Transfer learning (TL) proposes to enhance machine learning performance on a problem, by reusing labeled data originally designed for a related problem. In particular, domain adaptation consists, for a specific task, in reusing training data developed for the same task but a distinct domain. This is particularly relevant to the applications of deep learning in Natural Language Processing, because those usually require large annotated corpora that may not exist for the targeted domain, but exist for side domains. In this paper, we experiment with TL for the task of Relation Extraction (RE) from biomedical texts, using the TreeLSTM model. We empirically show the impact of TreeLSTM alone and with domain adaptation by obtaining better performances than the state of the art on two biomedical RE tasks and equal performances for two others, for which few annotated data are available. Furthermore, we propose an analysis of the role that syntactic features may play in TL for RE."",
}
@",medical text,natural language process,natural languag,language process,relation extract,annotat
" ""In-domain Context-aware Token Embeddings Improve Biomedical Named Entity Recognition"","," ""Rapidly expanding volume of publications in the biomedical domain makes it increasingly difficult for a timely evaluation of the latest literature. That, along with a push for automated evaluation of clinical reports, present opportunities for effective natural language processing methods. In this study we target the problem of named entity recognition, where texts are processed to annotate terms that are relevant for biomedical studies. Terms of interest in the domain include gene and protein names, and cell lines and types. Here we report on a pipeline built on Embeddings from Language Models (ELMo) and a deep learning package for natural language processing (AllenNLP). We trained context-aware token embeddings on a dataset of biomedical papers using ELMo, and incorporated these embeddings in the LSTM-CRF model used by AllenNLP for named entity recognition. We show these representations improve named entity recognition for different types of biomedical named entities. We also achieve a new state of the art in gene mention detection on the BioCreative II gene mention shared task."",","{sheikhshabbafghi-etal-2018-domain,
    title = ""In-domain Context-aware Token Embeddings Improve Biomedical Named Entity Recognition"",
    author = ""Sheikhshabbafghi, Golnar  and
      Birol, Inanc  and
      Sarkar, Anoop"",
    editor = ""Lavelli, Alberto  and
      Minard, Anne-Lyse  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis"",
    month = oct,
    year = ""2018"",
    address = ""Brussels, Belgium"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-5618"",
    doi = ""10.18653/v1/W18-5618"",
    pages = ""160--164"",
    abstract = ""Rapidly expanding volume of publications in the biomedical domain makes it increasingly difficult for a timely evaluation of the latest literature. That, along with a push for automated evaluation of clinical reports, present opportunities for effective natural language processing methods. In this study we target the problem of named entity recognition, where texts are processed to annotate terms that are relevant for biomedical studies. Terms of interest in the domain include gene and protein names, and cell lines and types. Here we report on a pipeline built on Embeddings from Language Models (ELMo) and a deep learning package for natural language processing (AllenNLP). We trained context-aware token embeddings on a dataset of biomedical papers using ELMo, and incorporated these embeddings in the LSTM-CRF model used by AllenNLP for named entity recognition. We show these representations improve named entity recognition for different types of biomedical named entities. We also achieve a new state of the art in gene mention detection on the BioCreative II gene mention shared task."",
}
@",medical domain,natural language process,natural languag,language process,nlp,entity recognit,shared task,annotat,evalu
" ""Time Expressions in Mental Health Records for Symptom Onset Extraction"","," ""For psychiatric disorders such as schizophrenia, longer durations of untreated psychosis are associated with worse intervention outcomes. Data included in electronic health records (EHRs) can be useful for retrospective clinical studies, but much of this is stored as unstructured text which cannot be directly used in computation. Natural Language Processing (NLP) methods can be used to extract this data, in order to identify symptoms and treatments from mental health records, and temporally anchor the first emergence of these. We are developing an EHR corpus annotated with time expressions, clinical entities and their relations, to be used for NLP development. In this study, we focus on the first step, identifying time expressions in EHRs for patients with schizophrenia. We developed a gold standard corpus, compared this corpus to other related corpora in terms of content and time expression prevalence, and adapted two NLP systems for extracting time expressions. To the best of our knowledge, this is the first resource annotated for temporal entities in the mental health domain."",","{viani-etal-2018-time,
    title = ""Time Expressions in Mental Health Records for Symptom Onset Extraction"",
    author = ""Viani, Natalia  and
      Yin, Lucia  and
      Kam, Joyce  and
      Alawi, Ayunni  and
      Bittar, Andr{\'e}  and
      Dutta, Rina  and
      Patel, Rashmi  and
      Stewart, Robert  and
      Velupillai, Sumithra"",
    editor = ""Lavelli, Alberto  and
      Minard, Anne-Lyse  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis"",
    month = oct,
    year = ""2018"",
    address = ""Brussels, Belgium"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-5621"",
    doi = ""10.18653/v1/W18-5621"",
    pages = ""183--192"",
    abstract = ""For psychiatric disorders such as schizophrenia, longer durations of untreated psychosis are associated with worse intervention outcomes. Data included in electronic health records (EHRs) can be useful for retrospective clinical studies, but much of this is stored as unstructured text which cannot be directly used in computation. Natural Language Processing (NLP) methods can be used to extract this data, in order to identify symptoms and treatments from mental health records, and temporally anchor the first emergence of these. We are developing an EHR corpus annotated with time expressions, clinical entities and their relations, to be used for NLP development. In this study, we focus on the first step, identifying time expressions in EHRs for patients with schizophrenia. We developed a gold standard corpus, compared this corpus to other related corpora in terms of content and time expression prevalence, and adapted two NLP systems for extracting time expressions. To the best of our knowledge, this is the first resource annotated for temporal entities in the mental health domain."",
}
@",electronic health record,health record,natural language process,natural languag,language process,nlp,annotat
" ""Evaluation of a Sequence Tagging Tool for Biomedical Texts"","," ""Many applications in biomedical natural language processing rely on sequence tagging as an initial step to perform more complex analysis. To support text analysis in the biomedical domain, we introduce Yet Another SEquence Tagger (YASET), an open-source multi purpose sequence tagger that implements state-of-the-art deep learning algorithms for sequence tagging. Herein, we evaluate YASET on part-of-speech tagging and named entity recognition in a variety of text genres including articles from the biomedical literature in English and clinical narratives in French. To further characterize performance, we report distributions over 30 runs and different sizes of training datasets. YASET provides state-of-the-art performance on the CoNLL 2003 NER dataset (F1","{tourille-etal-2018-evaluation,
    title = ""Evaluation of a Sequence Tagging Tool for Biomedical Texts"",
    author = ""Tourille, Julien  and
      Doutreligne, Matthieu  and
      Ferret, Olivier  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Paris, Nicolas  and
      Tannier, Xavier"",
    editor = ""Lavelli, Alberto  and
      Minard, Anne-Lyse  and
      Rinaldi, Fabio"",
    booktitle = ""Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis"",
    month = oct,
    year = ""2018"",
    address = ""Brussels, Belgium"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-5622"",
    doi = ""10.18653/v1/W18-5622"",
    pages = ""193--203"",
    abstract = ""Many applications in biomedical natural language processing rely on sequence tagging as an initial step to perform more complex analysis. To support text analysis in the biomedical domain, we introduce Yet Another SEquence Tagger (YASET), an open-source multi purpose sequence tagger that implements state-of-the-art deep learning algorithms for sequence tagging. Herein, we evaluate YASET on part-of-speech tagging and named entity recognition in a variety of text genres including articles from the biomedical literature in English and clinical narratives in French. To further characterize performance, we report distributions over 30 runs and different sizes of training datasets. YASET provides state-of-the-art performance on the CoNLL 2003 NER dataset (F1=0.87), MEDPOST corpus (F1=0.97), MERLoT corpus (F1=0.99) and NCBI disease corpus (F1=0.81). We believe that YASET is a versatile and efficient tool that can be used for sequence tagging in biomedical and clinical texts."",
}
@",clinical narr,medical domain,clinical text,medical text,natural language process,natural languag,language process,entity recognit,evalu
" ""Semantic role labeling tools for biomedical question answering: a study of selected tools on the {B}io{ASQ} datasets"","," ""Question answering (QA) systems usually rely on advanced natural language processing components to precisely understand the questions and extract the answers. Semantic role labeling (SRL) is known to boost performance for QA, but its use for biomedical texts has not yet been fully studied. We analyzed the performance of three SRL tools (BioKIT, BIOSMILE and PathLSTM) on 1776 questions from the BioASQ challenge. We compared the systems regarding the coverage of the questions and snippets, as well as based on pre-defined criteria, such as easiness of installation, supported formats and usability. Finally, we integrated two of the tools in a simple QA system to further evaluate their performance over the official BioASQ test sets."",","{eckert-neves-2018-semantic,
    title = ""Semantic role labeling tools for biomedical question answering: a study of selected tools on the {B}io{ASQ} datasets"",
    author = ""Eckert, Fabian  and
      Neves, Mariana"",
    editor = ""Kakadiaris, Ioannis A.  and
      Paliouras, George  and
      Krithara, Anastasia"",
    booktitle = ""Proceedings of the 6th {B}io{ASQ} Workshop A challenge on large-scale biomedical semantic indexing and question answering"",
    month = nov,
    year = ""2018"",
    address = ""Brussels, Belgium"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-5302"",
    doi = ""10.18653/v1/W18-5302"",
    pages = ""11--21"",
    abstract = ""Question answering (QA) systems usually rely on advanced natural language processing components to precisely understand the questions and extract the answers. Semantic role labeling (SRL) is known to boost performance for QA, but its use for biomedical texts has not yet been fully studied. We analyzed the performance of three SRL tools (BioKIT, BIOSMILE and PathLSTM) on 1776 questions from the BioASQ challenge. We compared the systems regarding the coverage of the questions and snippets, as well as based on pre-defined criteria, such as easiness of installation, supported formats and usability. Finally, we integrated two of the tools in a simple QA system to further evaluate their performance over the official BioASQ test sets."",
}
@",medical text,natural language process,natural languag,language process,semant,challeng,evalu
" ""{A}ttention{M}e{SH}: Simple, Effective and Interpretable Automatic {M}e{SH} Indexer"","," ""There are millions of articles in PubMed database. To facilitate information retrieval, curators in the National Library of Medicine (NLM) assign a set of Medical Subject Headings (MeSH) to each article. MeSH is a hierarchically-organized vocabulary, containing about 28K different concepts, covering the fields from clinical medicine to information sciences. Several automatic MeSH indexing models have been developed to improve the time-consuming and financially expensive manual annotation, including the NLM official tool {--} Medical Text Indexer, and the winner of BioASQ Task5a challenge {--} DeepMeSH. However, these models are complex and not interpretable. We propose a novel end-to-end model, AttentionMeSH, which utilizes deep learning and attention mechanism to index MeSH terms to biomedical text. The attention mechanism enables the model to associate textual evidence with annotations, thus providing interpretability at the word level. The model also uses a novel masking mechanism to enhance accuracy and speed. In the final week of BioASQ Chanllenge Task6a, we ranked 2nd by average MiF using an on-construction model. After the contest, we achieve close to state-of-the-art MiF performance of ∼ 0.684 using our final model. Human evaluations show AttentionMeSH also provides high level of interpretability, retrieving about 90{\%} of all expert-labeled relevant words given an MeSH-article pair at 20 output."",","{jin-etal-2018-attentionmesh,
    title = ""{A}ttention{M}e{SH}: Simple, Effective and Interpretable Automatic {M}e{SH} Indexer"",
    author = ""Jin, Qiao  and
      Dhingra, Bhuwan  and
      Cohen, William  and
      Lu, Xinghua"",
    editor = ""Kakadiaris, Ioannis A.  and
      Paliouras, George  and
      Krithara, Anastasia"",
    booktitle = ""Proceedings of the 6th {B}io{ASQ} Workshop A challenge on large-scale biomedical semantic indexing and question answering"",
    month = nov,
    year = ""2018"",
    address = ""Brussels, Belgium"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-5306"",
    doi = ""10.18653/v1/W18-5306"",
    pages = ""47--56"",
    abstract = ""There are millions of articles in PubMed database. To facilitate information retrieval, curators in the National Library of Medicine (NLM) assign a set of Medical Subject Headings (MeSH) to each article. MeSH is a hierarchically-organized vocabulary, containing about 28K different concepts, covering the fields from clinical medicine to information sciences. Several automatic MeSH indexing models have been developed to improve the time-consuming and financially expensive manual annotation, including the NLM official tool {--} Medical Text Indexer, and the winner of BioASQ Task5a challenge {--} DeepMeSH. However, these models are complex and not interpretable. We propose a novel end-to-end model, AttentionMeSH, which utilizes deep learning and attention mechanism to index MeSH terms to biomedical text. The attention mechanism enables the model to associate textual evidence with annotations, thus providing interpretability at the word level. The model also uses a novel masking mechanism to enhance accuracy and speed. In the final week of BioASQ Chanllenge Task6a, we ranked 2nd by average MiF using an on-construction model. After the contest, we achieve close to state-of-the-art MiF performance of ∼ 0.684 using our final model. Human evaluations show AttentionMeSH also provides high level of interpretability, retrieving about 90{\%} of all expert-labeled relevant words given an MeSH-article pair at 20 output."",
}
@",medical text,information retriev,semant,annotat,challeng,evalu
" ""The Interplay of Form and Meaning in Complex Medical Terms: Evidence from a Clinical Corpus"","," ""We conduct a corpus study to investigate the structure of multi-word expressions (MWEs) in the clinical domain. Based on an existing medical taxonomy, we develop an annotation scheme and label a sample of MWEs from a Dutch corpus with semantic and grammatical features. The analysis of the annotated data shows that the formal structure of clinical MWEs correlates with their conceptual properties. The insights gained from this study could inform the design of Natural Language Processing (NLP) systems for clinical writing, but also for other specialized genres."",","{gron-etal-2018-interplay,
    title = ""The Interplay of Form and Meaning in Complex Medical Terms: Evidence from a Clinical Corpus"",
    author = {Gr{\""o}n, Leonie  and
      Bertels, Ann  and
      Heylen, Kris},
    editor = ""Savary, Agata  and
      Ramisch, Carlos  and
      Hwang, Jena D.  and
      Schneider, Nathan  and
      Andresen, Melanie  and
      Pradhan, Sameer  and
      Petruck, Miriam R. L."",
    booktitle = ""Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)"",
    month = aug,
    year = ""2018"",
    address = ""Santa Fe, New Mexico, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-4905"",
    pages = ""18--29"",
    abstract = ""We conduct a corpus study to investigate the structure of multi-word expressions (MWEs) in the clinical domain. Based on an existing medical taxonomy, we develop an annotation scheme and label a sample of MWEs from a Dutch corpus with semantic and grammatical features. The analysis of the annotated data shows that the formal structure of clinical MWEs correlates with their conceptual properties. The insights gained from this study could inform the design of Natural Language Processing (NLP) systems for clinical writing, but also for other specialized genres."",
}
@",clinical domain,natural language process,natural languag,language process,nlp,semant,annotat
" ""Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility"","," ""Functioning is gaining recognition as an important indicator of global health, but remains under-studied in medical natural language processing research. We present the first analysis of automatically extracting descriptions of patient mobility, using a recently-developed dataset of free text electronic health records. We frame the task as a named entity recognition (NER) problem, and investigate the applicability of NER techniques to mobility extraction. As text corpora focused on patient functioning are scarce, we explore domain adaptation of word embeddings for use in a recurrent neural network NER system. We find that embeddings trained on a small in-domain corpus perform nearly as well as those learned from large out-of-domain corpora, and that domain adaptation techniques yield additional improvements in both precision and recall. Our analysis identifies several significant challenges in extracting descriptions of patient mobility, including the length and complexity of annotated entities and high linguistic variability in mobility descriptions."",","{newman-griffis-zirikly-2018-embedding,
    title = ""Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility"",
    author = ""Newman-Griffis, Denis  and
      Zirikly, Ayah"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the {B}io{NLP} 2018 workshop"",
    month = jul,
    year = ""2018"",
    address = ""Melbourne, Australia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-2301"",
    doi = ""10.18653/v1/W18-2301"",
    pages = ""1--11"",
    abstract = ""Functioning is gaining recognition as an important indicator of global health, but remains under-studied in medical natural language processing research. We present the first analysis of automatically extracting descriptions of patient mobility, using a recently-developed dataset of free text electronic health records. We frame the task as a named entity recognition (NER) problem, and investigate the applicability of NER techniques to mobility extraction. As text corpora focused on patient functioning are scarce, we explore domain adaptation of word embeddings for use in a recurrent neural network NER system. We find that embeddings trained on a small in-domain corpus perform nearly as well as those learned from large out-of-domain corpora, and that domain adaptation techniques yield additional improvements in both precision and recall. Our analysis identifies several significant challenges in extracting descriptions of patient mobility, including the length and complexity of annotated entities and high linguistic variability in mobility descriptions."",
}
@",electronic health record,health record,natural language process,natural languag,language process,nlp,entity recognit,annotat,challeng
" ""Identifying Risk Factors For Heart Disease in Electronic Medical Records: A Deep Learning Approach"","," ""Automatic identification of heart disease risk factors in clinical narratives can expedite disease progression modelling and support clinical decisions. Existing practical solutions for cardiovascular risk detection are mostly hybrid systems entailing the integration of knowledge-driven and data-driven methods, relying on dictionaries, rules and machine learning methods that require a substantial amount of human effort. This paper proposes a comparative analysis on the applicability of deep learning, a re-emerged data-driven technique, in the context of clinical text classification. Various deep learning architectures were devised and evaluated for extracting heart disease risk factors from clinical documents. The data provided for the 2014 i2b2/UTHealth shared task focusing on identifying risk factors for heart disease was used for system development and evaluation. Results have shown that a relatively simple deep learning model can achieve a high micro-averaged F-measure of 0.9081, which is comparable to the best systems from the shared task. This is highly encouraging given the simplicity of the deep learning approach compared to the heavily feature-engineered hybrid approaches that were required to achieve state-of-the-art performances."",","{chokwijitkul-etal-2018-identifying,
    title = ""Identifying Risk Factors For Heart Disease in Electronic Medical Records: A Deep Learning Approach"",
    author = ""Chokwijitkul, Thanat  and
      Nguyen, Anthony  and
      Hassanzadeh, Hamed  and
      Perez, Siegfried"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the {B}io{NLP} 2018 workshop"",
    month = jul,
    year = ""2018"",
    address = ""Melbourne, Australia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-2303"",
    doi = ""10.18653/v1/W18-2303"",
    pages = ""18--27"",
    abstract = ""Automatic identification of heart disease risk factors in clinical narratives can expedite disease progression modelling and support clinical decisions. Existing practical solutions for cardiovascular risk detection are mostly hybrid systems entailing the integration of knowledge-driven and data-driven methods, relying on dictionaries, rules and machine learning methods that require a substantial amount of human effort. This paper proposes a comparative analysis on the applicability of deep learning, a re-emerged data-driven technique, in the context of clinical text classification. Various deep learning architectures were devised and evaluated for extracting heart disease risk factors from clinical documents. The data provided for the 2014 i2b2/UTHealth shared task focusing on identifying risk factors for heart disease was used for system development and evaluation. Results have shown that a relatively simple deep learning model can achieve a high micro-averaged F-measure of 0.9081, which is comparable to the best systems from the shared task. This is highly encouraging given the simplicity of the deep learning approach compared to the heavily feature-engineered hybrid approaches that were required to achieve state-of-the-art performances."",
}
@",medical record,clinical narr,clinical text,nlp,shared task,evalu
" ""Ontology alignment in the biomedical domain using entity definitions and context"","," ""Ontology alignment is the task of identifying semantically equivalent entities from two given ontologies. Different ontologies have different representations of the same entity, resulting in a need to de-duplicate entities when merging ontologies. We propose a method for enriching entities in an ontology with external definition and context information, and use this additional information for ontology alignment. We develop a neural architecture capable of encoding the additional information when available, and show that the addition of external data results in an F1-score of 0.69 on the Ontology Alignment Evaluation Initiative (OAEI) largebio SNOMED-NCI subtask, comparable with the entity-level matchers in a SOTA system."",","{wang-etal-2018-ontology,
    title = ""Ontology alignment in the biomedical domain using entity definitions and context"",
    author = ""Wang, Lucy Lu  and
      Bhagavatula, Chandra  and
      Neumann, Mark  and
      Lo, Kyle  and
      Wilhelm, Chris  and
      Ammar, Waleed"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the {B}io{NLP} 2018 workshop"",
    month = jul,
    year = ""2018"",
    address = ""Melbourne, Australia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-2306"",
    doi = ""10.18653/v1/W18-2306"",
    pages = ""47--55"",
    abstract = ""Ontology alignment is the task of identifying semantically equivalent entities from two given ontologies. Different ontologies have different representations of the same entity, resulting in a need to de-duplicate entities when merging ontologies. We propose a method for enriching entities in an ontology with external definition and context information, and use this additional information for ontology alignment. We develop a neural architecture capable of encoding the additional information when available, and show that the addition of external data results in an F1-score of 0.69 on the Ontology Alignment Evaluation Initiative (OAEI) largebio SNOMED-NCI subtask, comparable with the entity-level matchers in a SOTA system."",
}
@",medical domain,nlp,semant,evalu
" ""Sub-word information in pre-trained biomedical word representations: evaluation and hyper-parameter optimization"","," ""Word2vec embeddings are limited to computing vectors for in-vocabulary terms and do not take into account sub-word information. Character-based representations, such as fastText, mitigate such limitations. We optimize and compare these representations for the biomedical domain. fastText was found to consistently outperform word2vec in named entity recognition tasks for entities such as chemicals and genes. This is likely due to gained information from computed out-of-vocabulary term vectors, as well as the word compositionality of such entities. Contrastingly, performance varied on intrinsic datasets. Optimal hyper-parameters were intrinsic dataset-dependent, likely due to differences in term types distributions. This indicates embeddings should be chosen based on the task at hand. We therefore provide a number of optimized hyper-parameter sets and pre-trained word2vec and fastText models, available on \url{https://github.com/dterg/bionlp-embed}."",","{galea-etal-2018-sub,
    title = ""Sub-word information in pre-trained biomedical word representations: evaluation and hyper-parameter optimization"",
    author = ""Galea, Dieter  and
      Laponogov, Ivan  and
      Veselkov, Kirill"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the {B}io{NLP} 2018 workshop"",
    month = jul,
    year = ""2018"",
    address = ""Melbourne, Australia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-2307"",
    doi = ""10.18653/v1/W18-2307"",
    pages = ""56--66"",
    abstract = ""Word2vec embeddings are limited to computing vectors for in-vocabulary terms and do not take into account sub-word information. Character-based representations, such as fastText, mitigate such limitations. We optimize and compare these representations for the biomedical domain. fastText was found to consistently outperform word2vec in named entity recognition tasks for entities such as chemicals and genes. This is likely due to gained information from computed out-of-vocabulary term vectors, as well as the word compositionality of such entities. Contrastingly, performance varied on intrinsic datasets. Optimal hyper-parameters were intrinsic dataset-dependent, likely due to differences in term types distributions. This indicates embeddings should be chosen based on the task at hand. We therefore provide a number of optimized hyper-parameter sets and pre-trained word2vec and fastText models, available on \url{https://github.com/dterg/bionlp-embed}."",
}
@",medical domain,nlp,entity recognit,evalu
" ""{PICO} Element Detection in Medical Text via Long Short-Term Memory Neural Networks"","," ""Successful evidence-based medicine (EBM) applications rely on answering clinical questions by analyzing large medical literature databases. In order to formulate a well-defined, focused clinical question, a framework called PICO is widely used, which identifies the sentences in a given medical text that belong to the four components: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome (O). In this work, we present a Long Short-Term Memory (LSTM) neural network based model to automatically detect PICO elements. By jointly classifying subsequent sentences in the given text, we achieve state-of-the-art results on PICO element classification compared to several strong baseline models. We also make our curated data public as a benchmarking dataset so that the community can benefit from it."",","{jin-szolovits-2018-pico,
    title = ""{PICO} Element Detection in Medical Text via Long Short-Term Memory Neural Networks"",
    author = ""Jin, Di  and
      Szolovits, Peter"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the {B}io{NLP} 2018 workshop"",
    month = jul,
    year = ""2018"",
    address = ""Melbourne, Australia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-2308"",
    doi = ""10.18653/v1/W18-2308"",
    pages = ""67--75"",
    abstract = ""Successful evidence-based medicine (EBM) applications rely on answering clinical questions by analyzing large medical literature databases. In order to formulate a well-defined, focused clinical question, a framework called PICO is widely used, which identifies the sentences in a given medical text that belong to the four components: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome (O). In this work, we present a Long Short-Term Memory (LSTM) neural network based model to automatically detect PICO elements. By jointly classifying subsequent sentences in the given text, we achieve state-of-the-art results on PICO element classification compared to several strong baseline models. We also make our curated data public as a benchmarking dataset so that the community can benefit from it."",
}
@",medical text,nlp,benchmark
" ""Biomedical Event Extraction Using Convolutional Neural Networks and Dependency Parsing"","," ""Event and relation extraction are central tasks in biomedical text mining. Where relation extraction concerns the detection of semantic connections between pairs of entities, event extraction expands this concept with the addition of trigger words, multiple arguments and nested events, in order to more accurately model the diversity of natural language. In this work we develop a convolutional neural network that can be used for both event and relation extraction. We use a linear representation of the input text, where information is encoded with various vector space embeddings. Most notably, we encode the parse graph into this linear space using dependency path embeddings. We integrate our neural network into the open source Turku Event Extraction System (TEES) framework. Using this system, our machine learning model can be easily applied to a large set of corpora from e.g. the BioNLP, DDI Extraction and BioCreative shared tasks. We evaluate our system on 12 different event, relation and NER corpora, showing good generalizability to many tasks and achieving improved performance on several corpora."",","{bjorne-salakoski-2018-biomedical,
    title = ""Biomedical Event Extraction Using Convolutional Neural Networks and Dependency Parsing"",
    author = {Bj{\""o}rne, Jari  and
      Salakoski, Tapio},
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the {B}io{NLP} 2018 workshop"",
    month = jul,
    year = ""2018"",
    address = ""Melbourne, Australia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-2311"",
    doi = ""10.18653/v1/W18-2311"",
    pages = ""98--108"",
    abstract = ""Event and relation extraction are central tasks in biomedical text mining. Where relation extraction concerns the detection of semantic connections between pairs of entities, event extraction expands this concept with the addition of trigger words, multiple arguments and nested events, in order to more accurately model the diversity of natural language. In this work we develop a convolutional neural network that can be used for both event and relation extraction. We use a linear representation of the input text, where information is encoded with various vector space embeddings. Most notably, we encode the parse graph into this linear space using dependency path embeddings. We integrate our neural network into the open source Turku Event Extraction System (TEES) framework. Using this system, our machine learning model can be easily applied to a large set of corpora from e.g. the BioNLP, DDI Extraction and BioCreative shared tasks. We evaluate our system on 12 different event, relation and NER corpora, showing good generalizability to many tasks and achieving improved performance on several corpora."",
}
@",medical text,natural languag,nlp,relation extract,semant,shared task,evalu
" ""Predicting Discharge Disposition Using Patient Complaint Notes in Electronic Medical Records"","," ""Overcrowding in emergency rooms is a major challenge faced by hospitals across the United States. Overcrowding can result in longer wait times, which, in turn, has been shown to adversely affect patient satisfaction, clinical outcomes, and procedure reimbursements. This paper presents research that aims to automatically predict discharge disposition of patients who received medical treatment in an emergency department. We make use of a corpus that consists of notes containing patient complaints, diagnosis information, and disposition, entered by health care providers. We use this corpus to develop a model that uses the complaint and diagnosis information to predict patient disposition. We show that the proposed model substantially outperforms the baseline of predicting the most common disposition type. The long-term goal of this research is to build a model that can be implemented as a real-time service in an application to predict disposition as patients arrive."",","{salimi-rozovskaya-2018-predicting,
    title = ""Predicting Discharge Disposition Using Patient Complaint Notes in Electronic Medical Records"",
    author = ""Salimi, Mohamad  and
      Rozovskaya, Alla"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the {B}io{NLP} 2018 workshop"",
    month = jul,
    year = ""2018"",
    address = ""Melbourne, Australia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-2316"",
    doi = ""10.18653/v1/W18-2316"",
    pages = ""142--146"",
    abstract = ""Overcrowding in emergency rooms is a major challenge faced by hospitals across the United States. Overcrowding can result in longer wait times, which, in turn, has been shown to adversely affect patient satisfaction, clinical outcomes, and procedure reimbursements. This paper presents research that aims to automatically predict discharge disposition of patients who received medical treatment in an emergency department. We make use of a corpus that consists of notes containing patient complaints, diagnosis information, and disposition, entered by health care providers. We use this corpus to develop a model that uses the complaint and diagnosis information to predict patient disposition. We show that the proposed model substantially outperforms the baseline of predicting the most common disposition type. The long-term goal of this research is to build a model that can be implemented as a real-time service in an application to predict disposition as patients arrive."",
}
@",medical record,nlp,challeng
" ""On Learning Better Embeddings from {C}hinese Clinical Records: Study on Combining In-Domain and Out-Domain Data"","," ""High quality word embeddings are of great significance to advance applications of biomedical natural language processing. In recent years, a surge of interest on how to learn good embeddings and evaluate embedding quality based on English medical text has become increasing evident, however a limited number of studies based on Chinese medical text, particularly Chinese clinical records, were performed. Herein, we proposed a novel approach of improving the quality of learned embeddings using out-domain data as a supplementary in the case of limited Chinese clinical records. Moreover, the embedding quality evaluation method was conducted based on Medical Conceptual Similarity Property. The experimental results revealed that selecting good training samples was necessary, and collecting right amount of out-domain data and trading off between the quality of embeddings and the training time consumption were essential factors for better embeddings."",","{wang-etal-2018-learning-better,
    title = ""On Learning Better Embeddings from {C}hinese Clinical Records: Study on Combining In-Domain and Out-Domain Data"",
    author = ""Wang, Yaqiang  and
      Chen, Yunhui  and
      Shu, Hongping  and
      Jiang, Yongguang"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the {B}io{NLP} 2018 workshop"",
    month = jul,
    year = ""2018"",
    address = ""Melbourne, Australia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-2323"",
    doi = ""10.18653/v1/W18-2323"",
    pages = ""177--182"",
    abstract = ""High quality word embeddings are of great significance to advance applications of biomedical natural language processing. In recent years, a surge of interest on how to learn good embeddings and evaluate embedding quality based on English medical text has become increasing evident, however a limited number of studies based on Chinese medical text, particularly Chinese clinical records, were performed. Herein, we proposed a novel approach of improving the quality of learned embeddings using out-domain data as a supplementary in the case of limited Chinese clinical records. Moreover, the embedding quality evaluation method was conducted based on Medical Conceptual Similarity Property. The experimental results revealed that selecting good training samples was necessary, and collecting right amount of out-domain data and trading off between the quality of embeddings and the training time consumption were essential factors for better embeddings."",
}
@",clinical record,medical text,natural language process,natural languag,language process,nlp,evalu
" ""Proceedings of the {AMTA} 2018 Workshop on Technologies for {MT} of Low Resource Languages ({L}o{R}es{MT} 2018)"","," ""We present a novel annotation task evaluating a patient{'}s engagement with their health care regimen. The concept of engagement supplements the traditional concept of adherence with a focus on the patient{'}s affect, lifestyle choices, and health goal status. We describe an engagement annotation task across two patient note domains: traditional clinical notes and a novel domain, care manager notes, where we find engagement to be more common. The annotation task resulted in a kappa of .53, suggesting strong annotator intuitions regarding engagement-bearing language. In addition, we report the results of a series of preliminary engagement classification experiments using domain adaptation."",","{rosenthal-faulkner-2018-toward,
    title = ""Toward Cross-Domain Engagement Analysis in Medical Notes"",
    author = ""Rosenthal, Sara  and
      Faulkner, Adam"",
    editor = ""Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""Proceedings of the {B}io{NLP} 2018 workshop"",
    month = jul,
    year = ""2018"",
    address = ""Melbourne, Australia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-2325"",
    doi = ""10.18653/v1/W18-2325"",
    pages = ""189--193"",
    abstract = ""We present a novel annotation task evaluating a patient{'}s engagement with their health care regimen. The concept of engagement supplements the traditional concept of adherence with a focus on the patient{'}s affect, lifestyle choices, and health goal status. We describe an engagement annotation task across two patient note domains: traditional clinical notes and a novel domain, care manager notes, where we find engagement to be more common. The annotation task resulted in a kappa of .53, suggesting strong annotator intuitions regarding engagement-bearing language. In addition, we report the results of a series of preliminary engagement classification experiments using domain adaptation."",
}
@proceedings{ws-2018-amta-2018-technologies,
    title = ""Proceedings of the {AMTA} 2018 Workshop on Technologies for {MT} of Low Resource Languages ({L}o{R}es{MT} 2018)"",
    editor = ""Liu, Chao-Hong"",
    month = mar,
    year = ""2018"",
    address = ""Boston, MA"",
    publisher = ""Association for Machine Translation in the Americas"",
    url = ""https://aclanthology.org/W18-2200"",
}
@",clinical not,translat,nlp,annotat,evalu
" ""Paths for uncertainty: Exploring the intricacies of uncertainty identification for news"","," ""Currently, news articles are produced, shared and consumed at an extremely rapid rate. Although their quantity is increasing, at the same time, their quality and trustworthiness is becoming fuzzier. Hence, it is important not only to automate information extraction but also to quantify the certainty of this information. Automated identification of certainty has been studied both in the scientific and newswire domains, but performance is considerably higher in tasks focusing on scientific text. We compare the differences in the definition and expression of uncertainty between a scientific domain, i.e., biomedicine, and newswire. We delve into the different aspects that affect the certainty of an extracted event in a news article and examine whether they can be easily identified by techniques already validated in the biomedical domain. Finally, we present a comparison of the syntactic and lexical differences between the the expression of certainty in the biomedical and newswire domains, using two annotated corpora."",","{zerva-ananiadou-2018-paths,
    title = ""Paths for uncertainty: Exploring the intricacies of uncertainty identification for news"",
    author = ""Zerva, Chrysoula  and
      Ananiadou, Sophia"",
    editor = ""Blanco, Eduardo  and
      Morante, Roser"",
    booktitle = ""Proceedings of the Workshop on Computational Semantics beyond Events and Roles"",
    month = jun,
    year = ""2018"",
    address = ""New Orleans, Louisiana"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W18-1302"",
    doi = ""10.18653/v1/W18-1302"",
    pages = ""6--20"",
    abstract = ""Currently, news articles are produced, shared and consumed at an extremely rapid rate. Although their quantity is increasing, at the same time, their quality and trustworthiness is becoming fuzzier. Hence, it is important not only to automate information extraction but also to quantify the certainty of this information. Automated identification of certainty has been studied both in the scientific and newswire domains, but performance is considerably higher in tasks focusing on scientific text. We compare the differences in the definition and expression of uncertainty between a scientific domain, i.e., biomedicine, and newswire. We delve into the different aspects that affect the certainty of an extracted event in a news article and examine whether they can be easily identified by techniques already validated in the biomedical domain. Finally, we present a comparison of the syntactic and lexical differences between the the expression of certainty in the biomedical and newswire domains, using two annotated corpora."",
}
@",medical domain,semant,annotat
" ""{ADAPT} at {S}em{E}val-2018 Task 9: Skip-Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora"","," ""This paper describes a simple but competitive unsupervised system for hypernym discovery. The system uses skip-gram word embeddings with negative sampling, trained on specialised corpora. Candidate hypernyms for an input word are predicted based based on cosine similarity scores. Two sets of word embedding models were trained separately on two specialised corpora: a medical corpus and a music industry corpus. Our system scored highest in the medical domain among the competing unsupervised systems but performed poorly on the music industry domain. Our system does not depend on any external data other than raw specialised corpora."",","{maldonado-klubicka-2018-adapt,
    title = ""{ADAPT} at {S}em{E}val-2018 Task 9: Skip-Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora"",
    author = ""Maldonado, Alfredo  and
      Klubi{\v{c}}ka, Filip"",
    editor = ""Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      May, Jonathan  and
      Shutova, Ekaterina  and
      Bethard, Steven  and
      Carpuat, Marine"",
    booktitle = ""Proceedings of the 12th International Workshop on Semantic Evaluation"",
    month = jun,
    year = ""2018"",
    address = ""New Orleans, Louisiana"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S18-1151"",
    doi = ""10.18653/v1/S18-1151"",
    pages = ""924--927"",
    abstract = ""This paper describes a simple but competitive unsupervised system for hypernym discovery. The system uses skip-gram word embeddings with negative sampling, trained on specialised corpora. Candidate hypernyms for an input word are predicted based based on cosine similarity scores. Two sets of word embedding models were trained separately on two specialised corpora: a medical corpus and a music industry corpus. Our system scored highest in the medical domain among the competing unsupervised systems but performed poorly on the music industry domain. Our system does not depend on any external data other than raw specialised corpora."",
}
@",medical domain,semant,evalu
" ""Pushing the Limits of Radiology with Joint Modeling of Visual and Textual Information"","," ""Recently, there has been increasing interest in the intersection of computer vision and natural language processing. Researchers have studied several interesting tasks, including generating text descriptions from images and videos and language embedding of images. More recent work has further extended the scope of this area to combine videos and language, learning to solve non-visual tasks using visual cues, visual question answering, and visual dialog. Despite a large body of research on the intersection of vision-language technology, its adaption to the medical domain is not fully explored. To address this research gap, we aim to develop machine learning models that can reason jointly on medical images and clinical text for advanced search, retrieval, annotation and description of medical images."",","{singh-2018-pushing,
    title = ""Pushing the Limits of Radiology with Joint Modeling of Visual and Textual Information"",
    author = ""Singh, Sonit"",
    editor = ""Shwartz, Vered  and
      Tabassum, Jeniya  and
      Voigt, Rob  and
      Che, Wanxiang  and
      de Marneffe, Marie-Catherine  and
      Nissim, Malvina"",
    booktitle = ""Proceedings of {ACL} 2018, Student Research Workshop"",
    month = jul,
    year = ""2018"",
    address = ""Melbourne, Australia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/P18-3005"",
    doi = ""10.18653/v1/P18-3005"",
    pages = ""28--36"",
    abstract = ""Recently, there has been increasing interest in the intersection of computer vision and natural language processing. Researchers have studied several interesting tasks, including generating text descriptions from images and videos and language embedding of images. More recent work has further extended the scope of this area to combine videos and language, learning to solve non-visual tasks using visual cues, visual question answering, and visual dialog. Despite a large body of research on the intersection of vision-language technology, its adaption to the medical domain is not fully explored. To address this research gap, we aim to develop machine learning models that can reason jointly on medical images and clinical text for advanced search, retrieval, annotation and description of medical images."",
}
@",medical domain,clinical text,natural language process,natural languag,language process,generat,annotat
" ""Generating Continuous Representations of Medical Texts"","," ""We present an architecture that generates medical texts while learning an informative, continuous representation with discriminative features. During training the input to the system is a dataset of captions for medical X-Rays. The acquired continuous representations are of particular interest for use in many machine learning techniques where the discrete and high-dimensional nature of textual input is an obstacle. We use an Adversarially Regularized Autoencoder to create realistic text in both an unconditional and conditional setting. We show that this technique is applicable to medical texts which often contain syntactic and domain-specific shorthands. A quantitative evaluation shows that we achieve a lower model perplexity than a traditional LSTM generator."",","{spinks-moens-2018-generating,
    title = ""Generating Continuous Representations of Medical Texts"",
    author = ""Spinks, Graham  and
      Moens, Marie-Francine"",
    editor = ""Liu, Yang  and
      Paek, Tim  and
      Patwardhan, Manasi"",
    booktitle = ""Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations"",
    month = jun,
    year = ""2018"",
    address = ""New Orleans, Louisiana"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/N18-5014"",
    doi = ""10.18653/v1/N18-5014"",
    pages = ""66--70"",
    abstract = ""We present an architecture that generates medical texts while learning an informative, continuous representation with discriminative features. During training the input to the system is a dataset of captions for medical X-Rays. The acquired continuous representations are of particular interest for use in many machine learning techniques where the discrete and high-dimensional nature of textual input is an obstacle. We use an Adversarially Regularized Autoencoder to create realistic text in both an unconditional and conditional setting. We show that this technique is applicable to medical texts which often contain syntactic and domain-specific shorthands. A quantitative evaluation shows that we achieve a lower model perplexity than a traditional LSTM generator."",
}
@",medical text,generat,evalu
" ""Label-Aware Double Transfer Learning for Cross-Specialty Medical Named Entity Recognition"","," ""We study the problem of named entity recognition (NER) from electronic medical records, which is one of the most fundamental and critical problems for medical text mining. Medical records which are written by clinicians from different specialties usually contain quite different terminologies and writing styles. The difference of specialties and the cost of human annotation makes it particularly difficult to train a universal medical NER system. In this paper, we propose a label-aware double transfer learning framework (La-DTL) for cross-specialty NER, so that a medical NER system designed for one specialty could be conveniently applied to another one with minimal annotation efforts. The transferability is guaranteed by two components: (i) we propose label-aware MMD for feature representation transfer, and (ii) we perform parameter transfer with a theoretical upper bound which is also label aware. We conduct extensive experiments on 12 cross-specialty NER tasks. The experimental results demonstrate that La-DTL provides consistent accuracy improvement over strong baselines. Besides, the promising experimental results on non-medical NER scenarios indicate that La-DTL is potential to be seamlessly adapted to a wide range of NER tasks."",","{wang-etal-2018-label-aware,
    title = ""Label-Aware Double Transfer Learning for Cross-Specialty Medical Named Entity Recognition"",
    author = ""Wang, Zhenghui  and
      Qu, Yanru  and
      Chen, Liheng  and
      Shen, Jian  and
      Zhang, Weinan  and
      Zhang, Shaodian  and
      Gao, Yimei  and
      Gu, Gen  and
      Chen, Ken  and
      Yu, Yong"",
    editor = ""Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda"",
    booktitle = ""Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"",
    month = jun,
    year = ""2018"",
    address = ""New Orleans, Louisiana"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/N18-1001"",
    doi = ""10.18653/v1/N18-1001"",
    pages = ""1--15"",
    abstract = ""We study the problem of named entity recognition (NER) from electronic medical records, which is one of the most fundamental and critical problems for medical text mining. Medical records which are written by clinicians from different specialties usually contain quite different terminologies and writing styles. The difference of specialties and the cost of human annotation makes it particularly difficult to train a universal medical NER system. In this paper, we propose a label-aware double transfer learning framework (La-DTL) for cross-specialty NER, so that a medical NER system designed for one specialty could be conveniently applied to another one with minimal annotation efforts. The transferability is guaranteed by two components: (i) we propose label-aware MMD for feature representation transfer, and (ii) we perform parameter transfer with a theoretical upper bound which is also label aware. We conduct extensive experiments on 12 cross-specialty NER tasks. The experimental results demonstrate that La-DTL provides consistent accuracy improvement over strong baselines. Besides, the promising experimental results on non-medical NER scenarios indicate that La-DTL is potential to be seamlessly adapted to a wide range of NER tasks."",
}
@",medical record,medical text,entity recognit,annotat
" ""{C}li{CR}: a Dataset of Clinical Case Reports for Machine Reading Comprehension"","," ""We present a new dataset for machine comprehension in the medical domain. Our dataset uses clinical case reports with around 100,000 gap-filling queries about these cases. We apply several baselines and state-of-the-art neural readers to the dataset, and observe a considerable gap in performance (20{\%} F1) between the best human and machine readers. We analyze the skills required for successful answering and show how reader performance varies depending on the applicable skills. We find that inferences using domain knowledge and object tracking are the most frequently required skills, and that recognizing omitted information and spatio-temporal reasoning are the most difficult for the machines."",","{suster-daelemans-2018-clicr,
    title = ""{C}li{CR}: a Dataset of Clinical Case Reports for Machine Reading Comprehension"",
    author = ""{\v{S}}uster, Simon  and
      Daelemans, Walter"",
    editor = ""Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda"",
    booktitle = ""Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"",
    month = jun,
    year = ""2018"",
    address = ""New Orleans, Louisiana"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/N18-1140"",
    doi = ""10.18653/v1/N18-1140"",
    pages = ""1551--1563"",
    abstract = ""We present a new dataset for machine comprehension in the medical domain. Our dataset uses clinical case reports with around 100,000 gap-filling queries about these cases. We apply several baselines and state-of-the-art neural readers to the dataset, and observe a considerable gap in performance (20{\%} F1) between the best human and machine readers. We analyze the skills required for successful answering and show how reader performance varies depending on the applicable skills. We find that inferences using domain knowledge and object tracking are the most frequently required skills, and that recognizing omitted information and spatio-temporal reasoning are the most difficult for the machines."",
}
@",medical domain,infer,track
" ""Lessons from Natural Language Inference in the Clinical Domain"","," ""State of the art models using deep neural networks have become very good in learning an accurate mapping from inputs to outputs. However, they still lack generalization capabilities in conditions that differ from the ones encountered during training. This is even more challenging in specialized, and knowledge intensive domains, where training data is limited. To address this gap, we introduce MedNLI - a dataset annotated by doctors, performing a natural language inference task (NLI), grounded in the medical history of patients. We present strategies to: 1) leverage transfer learning using datasets from the open domain, (e.g. SNLI) and 2) incorporate domain knowledge from external data and lexical sources (e.g. medical terminologies). Our results demonstrate performance gains using both strategies."",","{romanov-shivade-2018-lessons,
    title = ""Lessons from Natural Language Inference in the Clinical Domain"",
    author = ""Romanov, Alexey  and
      Shivade, Chaitanya"",
    editor = ""Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi"",
    booktitle = ""Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"",
    month = oct # ""-"" # nov,
    year = ""2018"",
    address = ""Brussels, Belgium"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D18-1187"",
    doi = ""10.18653/v1/D18-1187"",
    pages = ""1586--1596"",
    abstract = ""State of the art models using deep neural networks have become very good in learning an accurate mapping from inputs to outputs. However, they still lack generalization capabilities in conditions that differ from the ones encountered during training. This is even more challenging in specialized, and knowledge intensive domains, where training data is limited. To address this gap, we introduce MedNLI - a dataset annotated by doctors, performing a natural language inference task (NLI), grounded in the medical history of patients. We present strategies to: 1) leverage transfer learning using datasets from the open domain, (e.g. SNLI) and 2) incorporate domain knowledge from external data and lexical sources (e.g. medical terminologies). Our results demonstrate performance gains using both strategies."",
}
@",clinical domain,natural language process,natural languag,language process,infer,annotat,challeng
" ""Annotation of a Large Clinical Entity Corpus"","," ""Having an entity annotated corpus of the clinical domain is one of the basic requirements for detection of clinical entities using machine learning (ML) approaches. Past researches have shown the superiority of statistical/ML approaches over the rule based approaches. But in order to take full advantage of the ML approaches, an accurately annotated corpus becomes an essential requirement. Though there are a few annotated corpora available either on a small data set, or covering a narrower domain (like cancer patients records, lab reports), annotation of a large data set representing the entire clinical domain has not been created yet. In this paper, we have described in detail the annotation guidelines, annotation process and our approaches in creating a CER (clinical entity recognition) corpus of 5,160 clinical documents from forty different clinical specialities. The clinical entities range across various types such as diseases, procedures, medications, medical devices and so on. We have classified them into eleven categories for annotation. Our annotation also reflects the relations among the group of entities that constitute larger concepts altogether."",","{patel-etal-2018-annotation,
    title = ""Annotation of a Large Clinical Entity Corpus"",
    author = ""Patel, Pinal  and
      Davey, Disha  and
      Panchal, Vishal  and
      Pathak, Parth"",
    editor = ""Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi"",
    booktitle = ""Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"",
    month = oct # ""-"" # nov,
    year = ""2018"",
    address = ""Brussels, Belgium"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D18-1228"",
    doi = ""10.18653/v1/D18-1228"",
    pages = ""2033--2042"",
    abstract = ""Having an entity annotated corpus of the clinical domain is one of the basic requirements for detection of clinical entities using machine learning (ML) approaches. Past researches have shown the superiority of statistical/ML approaches over the rule based approaches. But in order to take full advantage of the ML approaches, an accurately annotated corpus becomes an essential requirement. Though there are a few annotated corpora available either on a small data set, or covering a narrower domain (like cancer patients records, lab reports), annotation of a large data set representing the entire clinical domain has not been created yet. In this paper, we have described in detail the annotation guidelines, annotation process and our approaches in creating a CER (clinical entity recognition) corpus of 5,160 clinical documents from forty different clinical specialities. The clinical entities range across various types such as diseases, procedures, medications, medical devices and so on. We have classified them into eleven categories for annotation. Our annotation also reflects the relations among the group of entities that constitute larger concepts altogether."",
}
@",clinical domain,natural language process,natural languag,language process,entity recognit,annotat
" ""emr{QA}: A Large Corpus for Question Answering on Electronic Medical Records"","," ""We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets. The resulting corpus (emrQA) has 1 million questions-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping."",","{pampari-etal-2018-emrqa,
    title = ""emr{QA}: A Large Corpus for Question Answering on Electronic Medical Records"",
    author = ""Pampari, Anusri  and
      Raghavan, Preethi  and
      Liang, Jennifer  and
      Peng, Jian"",
    editor = ""Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi"",
    booktitle = ""Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"",
    month = oct # ""-"" # nov,
    year = ""2018"",
    address = ""Brussels, Belgium"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D18-1258"",
    doi = ""10.18653/v1/D18-1258"",
    pages = ""2357--2368"",
    abstract = ""We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets. The resulting corpus (emrQA) has 1 million questions-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping."",
}
@",medical record,clinical not,natural language process,natural languag,language process,nlp,generat,annotat,question-answ
" ""Deep Exhaustive Model for Nested Named Entity Recognition"","," ""We propose a simple deep neural model for nested named entity recognition (NER). Most NER models focused on flat entities and ignored nested entities, which failed to fully capture underlying semantic information in texts. The key idea of our model is to enumerate all possible regions or spans as potential entity mentions and classify them with deep neural networks. To reduce the computational costs and capture the information of the contexts around the regions, the model represents the regions using the outputs of shared underlying bidirectional long short-term memory. We evaluate our exhaustive model on the GENIA and JNLPBA corpora in biomedical domain, and the results show that our model outperforms state-of-the-art models on nested and flat NER, achieving 77.1{\%} and 78.4{\%} respectively in terms of F-score, without any external knowledge resources."",","{sohrab-miwa-2018-deep,
    title = ""Deep Exhaustive Model for Nested Named Entity Recognition"",
    author = ""Sohrab, Mohammad Golam  and
      Miwa, Makoto"",
    editor = ""Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi"",
    booktitle = ""Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"",
    month = oct # ""-"" # nov,
    year = ""2018"",
    address = ""Brussels, Belgium"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D18-1309"",
    doi = ""10.18653/v1/D18-1309"",
    pages = ""2843--2849"",
    abstract = ""We propose a simple deep neural model for nested named entity recognition (NER). Most NER models focused on flat entities and ignored nested entities, which failed to fully capture underlying semantic information in texts. The key idea of our model is to enumerate all possible regions or spans as potential entity mentions and classify them with deep neural networks. To reduce the computational costs and capture the information of the contexts around the regions, the model represents the regions using the outputs of shared underlying bidirectional long short-term memory. We evaluate our exhaustive model on the GENIA and JNLPBA corpora in biomedical domain, and the results show that our model outperforms state-of-the-art models on nested and flat NER, achieving 77.1{\%} and 78.4{\%} respectively in terms of F-score, without any external knowledge resources."",
}
@",medical domain,natural language process,natural languag,language process,nlp,entity recognit,semant,evalu
" ""Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces"","," ""Large multi-label datasets contain labels that occur thousands of times (frequent group), those that occur only a few times (few-shot group), and labels that never appear in the training dataset (zero-shot group). Multi-label few- and zero-shot label prediction is mostly unexplored on datasets with large label spaces, especially for text classification. In this paper, we perform a fine-grained evaluation to understand how state-of-the-art methods perform on infrequent labels. Furthermore, we develop few- and zero-shot methods for multi-label text classification when there is a known structure over the label space, and evaluate them on two publicly available medical text datasets: MIMIC II and MIMIC III. For few-shot labels we achieve improvements of 6.2{\%} and 4.8{\%} in R@10 for MIMIC II and MIMIC III, respectively, over prior efforts; the corresponding R@10 improvements for zero-shot labels are 17.3{\%} and 19{\%}."",","{rios-kavuluru-2018-shot,
    title = ""Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces"",
    author = ""Rios, Anthony  and
      Kavuluru, Ramakanth"",
    editor = ""Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi"",
    booktitle = ""Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"",
    month = oct # ""-"" # nov,
    year = ""2018"",
    address = ""Brussels, Belgium"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D18-1352"",
    doi = ""10.18653/v1/D18-1352"",
    pages = ""3132--3142"",
    abstract = ""Large multi-label datasets contain labels that occur thousands of times (frequent group), those that occur only a few times (few-shot group), and labels that never appear in the training dataset (zero-shot group). Multi-label few- and zero-shot label prediction is mostly unexplored on datasets with large label spaces, especially for text classification. In this paper, we perform a fine-grained evaluation to understand how state-of-the-art methods perform on infrequent labels. Furthermore, we develop few- and zero-shot methods for multi-label text classification when there is a known structure over the label space, and evaluate them on two publicly available medical text datasets: MIMIC II and MIMIC III. For few-shot labels we achieve improvements of 6.2{\%} and 4.8{\%} in R@10 for MIMIC II and MIMIC III, respectively, over prior efforts; the corresponding R@10 improvements for zero-shot labels are 17.3{\%} and 19{\%}."",
}
@",medical text,natural language process,natural languag,language process,evalu
" ""Word-Level Loss Extensions for Neural Temporal Relation Classification"","," ""Unsupervised pre-trained word embeddings are used effectively for many tasks in natural language processing to leverage unlabeled textual data. Often these embeddings are either used as initializations or as fixed word representations for task-specific classification models. In this work, we extend our classification model{'}s task loss with an unsupervised auxiliary loss on the word-embedding level of the model. This is to ensure that the learned word representations contain both task-specific features, learned from the supervised loss component, and more general features learned from the unsupervised loss component. We evaluate our approach on the task of temporal relation extraction, in particular, narrative containment relation extraction from clinical records, and show that continued training of the embeddings on the unsupervised objective together with the task objective gives better task-specific embeddings, and results in an improvement over the state of the art on the THYME dataset, using only a general-domain part-of-speech tagger as linguistic resource."",","{leeuwenberg-moens-2018-word,
    title = ""Word-Level Loss Extensions for Neural Temporal Relation Classification"",
    author = ""Leeuwenberg, Artuur  and
      Moens, Marie-Francine"",
    editor = ""Bender, Emily M.  and
      Derczynski, Leon  and
      Isabelle, Pierre"",
    booktitle = ""Proceedings of the 27th International Conference on Computational Linguistics"",
    month = aug,
    year = ""2018"",
    address = ""Santa Fe, New Mexico, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/C18-1291"",
    pages = ""3436--3447"",
    abstract = ""Unsupervised pre-trained word embeddings are used effectively for many tasks in natural language processing to leverage unlabeled textual data. Often these embeddings are either used as initializations or as fixed word representations for task-specific classification models. In this work, we extend our classification model{'}s task loss with an unsupervised auxiliary loss on the word-embedding level of the model. This is to ensure that the learned word representations contain both task-specific features, learned from the supervised loss component, and more general features learned from the unsupervised loss component. We evaluate our approach on the task of temporal relation extraction, in particular, narrative containment relation extraction from clinical records, and show that continued training of the embeddings on the unsupervised objective together with the task objective gives better task-specific embeddings, and results in an improvement over the state of the art on the THYME dataset, using only a general-domain part-of-speech tagger as linguistic resource."",
}
@",clinical record,natural language process,natural languag,language process,relation extract,relation classif,evalu
" ""{WME} 3.0: An Enhanced and Validated Lexicon of Medical Concepts"","," ""Information extraction in the medical domain is laborious and time-consuming due to the insufficient number of domain-specific lexicons and lack of involvement of domain experts such as doctors and medical practitioners. Thus, in the present work, we are motivated to design a new lexicon, WME 3.0 (WordNet of Medical Events), which contains over 10,000 medical concepts along with their part of speech, gloss (descriptive explanations), polarity score, sentiment, similar sentiment words, category, affinity score and gravity score features. In addition, the manual annotators help to validate the overall as well as individual category level of medical concepts of WME 3.0 using Cohen{'}s Kappa agreement metric. The agreement score indicates almost correct identification of medical concepts and their assigned features in WME 3.0."",","{mondal-etal-2018-wme,
    title = ""{WME} 3.0: An Enhanced and Validated Lexicon of Medical Concepts"",
    author = ""Mondal, Anupam  and
      Das, Dipankar  and
      Cambria, Erik  and
      Bandyopadhyay, Sivaji"",
    editor = ""Bond, Francis  and
      Vossen, Piek  and
      Fellbaum, Christiane"",
    booktitle = ""Proceedings of the 9th Global Wordnet Conference"",
    month = jan,
    year = ""2018"",
    address = ""Nanyang Technological University (NTU), Singapore"",
    publisher = ""Global Wordnet Association"",
    url = ""https://aclanthology.org/2018.gwc-1.2"",
    pages = ""10--16"",
    abstract = ""Information extraction in the medical domain is laborious and time-consuming due to the insufficient number of domain-specific lexicons and lack of involvement of domain experts such as doctors and medical practitioners. Thus, in the present work, we are motivated to design a new lexicon, WME 3.0 (WordNet of Medical Events), which contains over 10,000 medical concepts along with their part of speech, gloss (descriptive explanations), polarity score, sentiment, similar sentiment words, category, affinity score and gravity score features. In addition, the manual annotators help to validate the overall as well as individual category level of medical concepts of WME 3.0 using Cohen{'}s Kappa agreement metric. The agreement score indicates almost correct identification of medical concepts and their assigned features in WME 3.0."",
}
@",medical domain,sentiment,annotat
" ""Entity-Centric Information Access with Human in the Loop for the Biomedical Domain"","," ""In this paper, we describe the concept of entity-centric information access for the biomedical domain. With entity recognition technologies approaching acceptable levels of accuracy, we put forward a paradigm of document browsing and searching where the entities of the domain and their relations are explicitly modeled to provide users the possibility of collecting exhaustive information on relations of interest. We describe three working prototypes along these lines: NEW/S/LEAK, which was developed for investigative journalists who need a quick overview of large leaked document collections; STORYFINDER, which is a personalized organizer for information found in web pages that allows adding entities as well as relations, and is capable of personalized information management; and adaptive annotation capabilities of WEBANNO, which is a general-purpose linguistic annotation tool. We will discuss future steps towards the adaptation of these tools to biomedical data, which is subject to a recently started project on biomedical knowledge acquisition. A key difference to other approaches is the centering around the user in a Human-in-the-Loop machine learning approach, where users define and extend categories and enable the system to improve via feedback and interaction."",","{yimam-etal-2017-entity,
    title = ""Entity-Centric Information Access with Human in the Loop for the Biomedical Domain"",
    author = ""Yimam, Seid Muhie  and
      Remus, Steffen  and
      Panchenko, Alexander  and
      Holzinger, Andreas  and
      Biemann, Chris"",
    editor = ""Boytcheva, Svetla  and
      Cohen, Kevin Bretonnel  and
      Savova, Guergana  and
      Angelova, Galia"",
    booktitle = ""Proceedings of the Biomedical {NLP} Workshop associated with {RANLP} 2017"",
    month = sep,
    year = ""2017"",
    address = ""Varna, Bulgaria"",
    publisher = ""INCOMA Ltd."",
    url = ""https://doi.org/10.26615/978-954-452-044-1_006"",
    doi = ""10.26615/978-954-452-044-1_006"",
    pages = ""42--48"",
    abstract = ""In this paper, we describe the concept of entity-centric information access for the biomedical domain. With entity recognition technologies approaching acceptable levels of accuracy, we put forward a paradigm of document browsing and searching where the entities of the domain and their relations are explicitly modeled to provide users the possibility of collecting exhaustive information on relations of interest. We describe three working prototypes along these lines: NEW/S/LEAK, which was developed for investigative journalists who need a quick overview of large leaked document collections; STORYFINDER, which is a personalized organizer for information found in web pages that allows adding entities as well as relations, and is capable of personalized information management; and adaptive annotation capabilities of WEBANNO, which is a general-purpose linguistic annotation tool. We will discuss future steps towards the adaptation of these tools to biomedical data, which is subject to a recently started project on biomedical knowledge acquisition. A key difference to other approaches is the centering around the user in a Human-in-the-Loop machine learning approach, where users define and extend categories and enable the system to improve via feedback and interaction."",
}
@",medical domain,nlp,entity recognit,annotat
" ""{POMELO}: {M}edline corpus with manually annotated food-drug interactions"","," ""When patients take more than one medication, they may be at risk of drug interactions, which means that a given drug can cause unexpected effects when taken in combination with other drugs. Similar effects may occur when drugs are taken together with some food or beverages. For instance, grapefruit has interactions with several drugs, because its active ingredients inhibit enzymes involved in the drugs metabolism and can then cause an excessive dosage of these drugs. Yet, information on food/drug interactions is poorly researched. The current research is mainly provided by the medical domain and a very tentative work is provided by computer sciences and NLP domains. One factor that motivates the research is related to the availability of the annotated corpora and the reference data. The purpose of our work is to describe the rationale and approach for creation and annotation of scientific corpus with information on food/drug interactions. This corpus contains 639 MEDLINE citations (titles and abstracts), corresponding to 5,752 sentences. It is manually annotated by two experts. The corpus is named POMELO. This annotated corpus will be made available for the research purposes."",","{hamon-etal-2017-pomelo,
    title = ""{POMELO}: {M}edline corpus with manually annotated food-drug interactions"",
    author = ""Hamon, Thierry  and
      Tabanou, Vincent  and
      Mougin, Fleur  and
      Grabar, Natalia  and
      Thiessard, Frantz"",
    editor = ""Boytcheva, Svetla  and
      Cohen, Kevin Bretonnel  and
      Savova, Guergana  and
      Angelova, Galia"",
    booktitle = ""Proceedings of the Biomedical {NLP} Workshop associated with {RANLP} 2017"",
    month = sep,
    year = ""2017"",
    address = ""Varna, Bulgaria"",
    publisher = ""INCOMA Ltd."",
    url = ""https://doi.org/10.26615/978-954-452-044-1_010"",
    doi = ""10.26615/978-954-452-044-1_010"",
    pages = ""73--80"",
    abstract = ""When patients take more than one medication, they may be at risk of drug interactions, which means that a given drug can cause unexpected effects when taken in combination with other drugs. Similar effects may occur when drugs are taken together with some food or beverages. For instance, grapefruit has interactions with several drugs, because its active ingredients inhibit enzymes involved in the drugs metabolism and can then cause an excessive dosage of these drugs. Yet, information on food/drug interactions is poorly researched. The current research is mainly provided by the medical domain and a very tentative work is provided by computer sciences and NLP domains. One factor that motivates the research is related to the availability of the annotated corpora and the reference data. The purpose of our work is to describe the rationale and approach for creation and annotation of scientific corpus with information on food/drug interactions. This corpus contains 639 MEDLINE citations (titles and abstracts), corresponding to 5,752 sentences. It is manually annotated by two experts. The corpus is named POMELO. This annotated corpus will be made available for the research purposes."",
}
@",medical domain,nlp,annotat
" ""Proceedings of the Workshop Human-Informed Translation and Interpreting Technology"","," ""In this paper we describe annotation process of clinical texts with morphosyntactic and semantic information. The corpus contains 1,300 discharge letters in Bulgarian language for patients with Endocrinology and Metabolic disorders. The annotated corpus will be used as a Gold standard for information extraction evaluation of test corpus of 6,200 discharge letters. The annotation is performed within Clark system {---} an XML Based System For Corpora Development. It provides mechanism for semi-automatic annotation first running a pipeline for Bulgarian morphosyntactic annotation and a cascaded regular grammar for semantic annotation is run, then rules for cleaning of frequent errors are applied. At the end the result is manually checked. At the end we hope also to be able to adapted the morphosyntactic tagger to the domain of clinical narratives as well."",","{radev-etal-2017-annotation,
    title = ""Annotation of Clinical Narratives in {B}ulgarian language"",
    author = ""Radev, Ivajlo  and
      Simov, Kiril  and
      Angelova, Galia  and
      Boytcheva, Svetla"",
    editor = ""Boytcheva, Svetla  and
      Cohen, Kevin Bretonnel  and
      Savova, Guergana  and
      Angelova, Galia"",
    booktitle = ""Proceedings of the Biomedical {NLP} Workshop associated with {RANLP} 2017"",
    month = sep,
    year = ""2017"",
    address = ""Varna, Bulgaria"",
    publisher = ""INCOMA Ltd."",
    url = ""https://doi.org/10.26615/978-954-452-044-1_011"",
    doi = ""10.26615/978-954-452-044-1_011"",
    pages = ""81--87"",
    abstract = ""In this paper we describe annotation process of clinical texts with morphosyntactic and semantic information. The corpus contains 1,300 discharge letters in Bulgarian language for patients with Endocrinology and Metabolic disorders. The annotated corpus will be used as a Gold standard for information extraction evaluation of test corpus of 6,200 discharge letters. The annotation is performed within Clark system {---} an XML Based System For Corpora Development. It provides mechanism for semi-automatic annotation first running a pipeline for Bulgarian morphosyntactic annotation and a cascaded regular grammar for semantic annotation is run, then rules for cleaning of frequent errors are applied. At the end the result is manually checked. At the end we hope also to be able to adapted the morphosyntactic tagger to the domain of clinical narratives as well."",
}
@proceedings{ws-2017-human,
    title = ""Proceedings of the Workshop Human-Informed Translation and Interpreting Technology"",
    editor = ""Temnikova, Irina  and
      Orasan, Constantin  and
      Pastor, Gloria Corpas  and
      Vogel, Stephan"",
    month = sep,
    year = ""2017"",
    address = ""Varna, Bulgaria"",
    publisher = ""Association for Computational Linguistics, Shoumen, Bulgaria"",
    url = ""https://aclanthology.org/W17-7900"",
    doi = ""10.26615/978-954-452-042-7_"",
}
@",clinical narr,clinical text,translat,nlp,semant,annotat,evalu
" ""A Method to Generate a Machine-Labeled Data for Biomedical Named Entity Recognition with Various Sub-Domains"","," ""Biomedical Named Entity (NE) recognition is a core technique for various works in the biomedical domain. In previous studies, using machine learning algorithm shows better performance than dictionary-based and rule-based approaches because there are too many terminological variations of biomedical NEs and new biomedical NEs are constantly generated. To achieve the high performance with a machine-learning algorithm, good-quality corpora are required. However, it is difficult to obtain the good-quality corpora because an-notating a biomedical corpus for ma-chine-learning is extremely time-consuming and costly. In addition, most previous corpora are insufficient for high-level tasks because they cannot cover various domains. Therefore, we propose a method for generating a large amount of machine-labeled data that covers various domains. To generate a large amount of machine-labeled data, firstly we generate an initial machine-labeled data by using a chunker and MetaMap. The chunker is developed to extract only biomedical NEs with manually annotated data. MetaMap is used to annotate the category of bio-medical NE. Then we apply the self-training approach to bootstrap the performance of initial machine-labeled data. In our experiments, the biomedical NE recognition system that is trained with our proposed machine-labeled data achieves much high performance. As a result, our system outperforms biomedical NE recognition system that using MetaMap only with 26.03{\%}p improvements on F1-score."",","{kim-etal-2017-method,
    title = ""A Method to Generate a Machine-Labeled Data for Biomedical Named Entity Recognition with Various Sub-Domains"",
    author = ""Kim, Juae  and
      Kwon, Sunjae  and
      Ko, Youngjoong  and
      Seo, Jungyun"",
    editor = ""Jonnagaddala, Jitendra  and
      Dai, Hong-Jie  and
      Chang, Yung-Chun"",
    booktitle = ""Proceedings of the International Workshop on Digital Disease Detection using Social Media 2017 ({DDDSM}-2017)"",
    month = nov,
    year = ""2017"",
    address = ""Taipei, Taiwan"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-5807"",
    pages = ""47--51"",
    abstract = ""Biomedical Named Entity (NE) recognition is a core technique for various works in the biomedical domain. In previous studies, using machine learning algorithm shows better performance than dictionary-based and rule-based approaches because there are too many terminological variations of biomedical NEs and new biomedical NEs are constantly generated. To achieve the high performance with a machine-learning algorithm, good-quality corpora are required. However, it is difficult to obtain the good-quality corpora because an-notating a biomedical corpus for ma-chine-learning is extremely time-consuming and costly. In addition, most previous corpora are insufficient for high-level tasks because they cannot cover various domains. Therefore, we propose a method for generating a large amount of machine-labeled data that covers various domains. To generate a large amount of machine-labeled data, firstly we generate an initial machine-labeled data by using a chunker and MetaMap. The chunker is developed to extract only biomedical NEs with manually annotated data. MetaMap is used to annotate the category of bio-medical NE. Then we apply the self-training approach to bootstrap the performance of initial machine-labeled data. In our experiments, the biomedical NE recognition system that is trained with our proposed machine-labeled data achieves much high performance. As a result, our system outperforms biomedical NE recognition system that using MetaMap only with 26.03{\%}p improvements on F1-score."",
}
@",medical domain,generat,entity recognit,annotat
" ""A parallel collection of clinical trials in {P}ortuguese and {E}nglish"","," ""Parallel collections of documents are crucial resources for training and evaluating machine translation (MT) systems. Even though large collections are available for certain domains and language pairs, these are still scarce in the biomedical domain. We developed a parallel corpus of clinical trials in Portuguese and English. The documents are derived from the Brazilian Clinical Trials Registry and the corpus currently contains a total of 1188 documents. In this paper, we describe the corpus construction and discuss the quality of the translation and the sentence alignment that we obtained."",","{neves-2017-parallel,
    title = ""A parallel collection of clinical trials in {P}ortuguese and {E}nglish"",
    author = ""Neves, Mariana"",
    editor = ""Sharoff, Serge  and
      Zweigenbaum, Pierre  and
      Rapp, Reinhard"",
    booktitle = ""Proceedings of the 10th Workshop on Building and Using Comparable Corpora"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-2507"",
    doi = ""10.18653/v1/W17-2507"",
    pages = ""36--40"",
    abstract = ""Parallel collections of documents are crucial resources for training and evaluating machine translation (MT) systems. Even though large collections are available for certain domains and language pairs, these are still scarce in the biomedical domain. We developed a parallel corpus of clinical trials in Portuguese and English. The documents are derived from the Brazilian Clinical Trials Registry and the corpus currently contains a total of 1188 documents. In this paper, we describe the corpus construction and discuss the quality of the translation and the sentence alignment that we obtained."",
}
@",medical domain,translat,evalu
" ""Insights into Analogy Completion from the Biomedical Domain"","," ""Analogy completion has been a popular task in recent years for evaluating the semantic properties of word embeddings, but the standard methodology makes a number of assumptions about analogies that do not always hold, either in recent benchmark datasets or when expanding into other domains. Through an analysis of analogies in the biomedical domain, we identify three assumptions: that of a Single Answer for any given analogy, that the pairs involved describe the Same Relationship, and that each pair is Informative with respect to the other. We propose modifying the standard methodology to relax these assumptions by allowing for multiple correct answers, reporting MAP and MRR in addition to accuracy, and using multiple example pairs. We further present BMASS, a novel dataset for evaluating linguistic regularities in biomedical embeddings, and demonstrate that the relationships described in the dataset pose significant semantic challenges to current word embedding methods."",","{newman-griffis-etal-2017-insights,
    title = ""Insights into Analogy Completion from the Biomedical Domain"",
    author = ""Newman-Griffis, Denis  and
      Lai, Albert  and
      Fosler-Lussier, Eric"",
    editor = ""Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""{B}io{NLP} 2017"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada,"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-2303"",
    doi = ""10.18653/v1/W17-2303"",
    pages = ""19--28"",
    abstract = ""Analogy completion has been a popular task in recent years for evaluating the semantic properties of word embeddings, but the standard methodology makes a number of assumptions about analogies that do not always hold, either in recent benchmark datasets or when expanding into other domains. Through an analysis of analogies in the biomedical domain, we identify three assumptions: that of a Single Answer for any given analogy, that the pairs involved describe the Same Relationship, and that each pair is Informative with respect to the other. We propose modifying the standard methodology to relax these assumptions by allowing for multiple correct answers, reporting MAP and MRR in addition to accuracy, and using multiple example pairs. We further present BMASS, a novel dataset for evaluating linguistic regularities in biomedical embeddings, and demonstrate that the relationships described in the dataset pose significant semantic challenges to current word embedding methods."",
}
@",medical domain,nlp,semant,challeng,benchmark,evalu
" ""Results of the fifth edition of the {B}io{ASQ} Challenge"","," ""The goal of the BioASQ challenge is to engage researchers into creating cuttingedge biomedical information systems. Specifically, it aims at the promotion of systems and methodologies that are able to deal with a plethora of different tasks in the biomedical domain. This is achieved through the organization of challenges. The fifth challenge consisted of three tasks: semantic indexing, question answering and a new task on information extraction. In total, 29 teams with more than 95 systems participated in the challenge. Overall, as in previous years, the best systems were able to outperform the strong baselines. This suggests that state-of-the art systems are continuously improving, pushing the frontier of research."",","{nentidis-etal-2017-results,
    title = ""Results of the fifth edition of the {B}io{ASQ} Challenge"",
    author = ""Nentidis, Anastasios  and
      Bougiatiotis, Konstantinos  and
      Krithara, Anastasia  and
      Paliouras, Georgios  and
      Kakadiaris, Ioannis"",
    editor = ""Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""{B}io{NLP} 2017"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada,"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-2306"",
    doi = ""10.18653/v1/W17-2306"",
    pages = ""48--57"",
    abstract = ""The goal of the BioASQ challenge is to engage researchers into creating cuttingedge biomedical information systems. Specifically, it aims at the promotion of systems and methodologies that are able to deal with a plethora of different tasks in the biomedical domain. This is achieved through the organization of challenges. The fifth challenge consisted of three tasks: semantic indexing, question answering and a new task on information extraction. In total, 29 teams with more than 95 systems participated in the challenge. Overall, as in previous years, the best systems were able to outperform the strong baselines. This suggests that state-of-the art systems are continuously improving, pushing the frontier of research."",
}
@",medical domain,nlp,semant,challeng
" ""Tackling Biomedical Text Summarization: {OAQA} at {B}io{ASQ} 5{B}"","," ""In this paper, we describe our participation in phase B of task 5b of the fifth edition of the annual BioASQ challenge, which includes answering factoid, list, yes-no and summary questions from biomedical data. We describe our techniques with an emphasis on ideal answer generation, where the goal is to produce a relevant, precise, non-redundant, query-oriented summary from multiple relevant documents. We make use of extractive summarization techniques to address this task and experiment with different biomedical ontologies and various algorithms including agglomerative clustering, Maximum Marginal Relevance (MMR) and sentence compression. We propose a novel word embedding based tf-idf similarity metric and a soft positional constraint which improve our system performance. We evaluate our techniques on test batch 4 from the fourth edition of the challenge. Our best system achieves a ROUGE-2 score of 0.6534 and ROUGE-SU4 score of 0.6536."",","{chandu-etal-2017-tackling,
    title = ""Tackling Biomedical Text Summarization: {OAQA} at {B}io{ASQ} 5{B}"",
    author = ""Chandu, Khyathi  and
      Naik, Aakanksha  and
      Chandrasekar, Aditya  and
      Yang, Zi  and
      Gupta, Niloy  and
      Nyberg, Eric"",
    editor = ""Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""{B}io{NLP} 2017"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada,"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-2307"",
    doi = ""10.18653/v1/W17-2307"",
    pages = ""58--66"",
    abstract = ""In this paper, we describe our participation in phase B of task 5b of the fifth edition of the annual BioASQ challenge, which includes answering factoid, list, yes-no and summary questions from biomedical data. We describe our techniques with an emphasis on ideal answer generation, where the goal is to produce a relevant, precise, non-redundant, query-oriented summary from multiple relevant documents. We make use of extractive summarization techniques to address this task and experiment with different biomedical ontologies and various algorithms including agglomerative clustering, Maximum Marginal Relevance (MMR) and sentence compression. We propose a novel word embedding based tf-idf similarity metric and a soft positional constraint which improve our system performance. We evaluate our techniques on test batch 4 from the fourth edition of the challenge. Our best system achieves a ROUGE-2 score of 0.6534 and ROUGE-SU4 score of 0.6536."",
}
@",medical text,nlp,generat,summar,challeng,evalu
" ""Biomedical Event Extraction using {A}bstract {M}eaning {R}epresentation"","," ""We propose a novel, Abstract Meaning Representation (AMR) based approach to identifying molecular events/interactions in biomedical text. Our key contributions are: (1) an empirical validation of our hypothesis that an event is a subgraph of the AMR graph, (2) a neural network-based model that identifies such an event subgraph given an AMR, and (3) a distant supervision based approach to gather additional training data. We evaluate our approach on the 2013 Genia Event Extraction dataset and show promising results."",","{rao-etal-2017-biomedical,
    title = ""Biomedical Event Extraction using {A}bstract {M}eaning {R}epresentation"",
    author = ""Rao, Sudha  and
      Marcu, Daniel  and
      Knight, Kevin  and
      Daum{\'e} III, Hal"",
    editor = ""Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""{B}io{NLP} 2017"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada,"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-2315"",
    doi = ""10.18653/v1/W17-2315"",
    pages = ""126--135"",
    abstract = ""We propose a novel, Abstract Meaning Representation (AMR) based approach to identifying molecular events/interactions in biomedical text. Our key contributions are: (1) an empirical validation of our hypothesis that an event is a subgraph of the AMR graph, (2) a neural network-based model that identifies such an event subgraph given an AMR, and (3) a distant supervision based approach to gather additional training data. We evaluate our approach on the 2013 Genia Event Extraction dataset and show promising results."",
}
@",medical text,nlp,evalu
" ""Role-Preserving Redaction of Medical Records to Enable Ontology-Driven Processing"","," ""Electronic medical records (EMR) have largely replaced hand-written patient files in healthcare. The growing pool of EMR data presents a significant resource in medical research, but the U.S. Health Insurance Portability and Accountability Act (HIPAA) mandates redacting medical records before performing any analysis on the same. This process complicates obtaining medical data and can remove much useful information from the record. As part of a larger project involving ontology-driven medical processing, we employ a method of recognizing protected health information (PHI) that maps to ontological terms. We then use the relationships defined in the ontology to redact medical texts so that roles and semantics of terms are retained without compromising anonymity. The method is evaluated by clinical experts on several hundred medical documents, achieving up to a 98.8{\%} f-score, and has already shown promise for retaining semantic information in later processing."",","{polsley-etal-2017-role,
    title = ""Role-Preserving Redaction of Medical Records to Enable Ontology-Driven Processing"",
    author = ""Polsley, Seth  and
      Tahir, Atif  and
      Raju, Muppala  and
      Akinleye, Akintayo  and
      Steward, Duane"",
    editor = ""Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""{B}io{NLP} 2017"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada,"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-2324"",
    doi = ""10.18653/v1/W17-2324"",
    pages = ""194--199"",
    abstract = ""Electronic medical records (EMR) have largely replaced hand-written patient files in healthcare. The growing pool of EMR data presents a significant resource in medical research, but the U.S. Health Insurance Portability and Accountability Act (HIPAA) mandates redacting medical records before performing any analysis on the same. This process complicates obtaining medical data and can remove much useful information from the record. As part of a larger project involving ontology-driven medical processing, we employ a method of recognizing protected health information (PHI) that maps to ontological terms. We then use the relationships defined in the ontology to redact medical texts so that roles and semantics of terms are retained without compromising anonymity. The method is evaluated by clinical experts on several hundred medical documents, achieving up to a 98.8{\%} f-score, and has already shown promise for retaining semantic information in later processing."",
}
@",medical record,medical text,nlp,semant,evalu
" ""Annotation of pain and anesthesia events for surgery-related processes and outcomes extraction"","," ""Pain and anesthesia information are crucial elements to identifying surgery-related processes and outcomes. However pain is not consistently recorded in the electronic medical record. Even when recorded, the rich complex granularity of the pain experience may be lost. Similarly, anesthesia information is recorded using local electronic collection systems; though the accuracy and completeness of the information is unknown. We propose an annotation schema to capture pain, pain management, and anesthesia event information."",","{yim-etal-2017-annotation,
    title = ""Annotation of pain and anesthesia events for surgery-related processes and outcomes extraction"",
    author = ""Yim, Wen-wai  and
      Tedesco, Dario  and
      Curtin, Catherine  and
      Hernandez-Boussard, Tina"",
    editor = ""Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""{B}io{NLP} 2017"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada,"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-2325"",
    doi = ""10.18653/v1/W17-2325"",
    pages = ""200--205"",
    abstract = ""Pain and anesthesia information are crucial elements to identifying surgery-related processes and outcomes. However pain is not consistently recorded in the electronic medical record. Even when recorded, the rich complex granularity of the pain experience may be lost. Similarly, anesthesia information is recorded using local electronic collection systems; though the accuracy and completeness of the information is unknown. We propose an annotation schema to capture pain, pain management, and anesthesia event information."",
}
@",medical record,nlp,annotat
" ""Identifying Comparative Structures in Biomedical Text"","," ""Comparison sentences are very commonly used by authors in biomedical literature to report results of experiments. In such comparisons, authors typically make observations under two different scenarios. In this paper, we present a system to automatically identify such comparative sentences and their components i.e. the compared entities, the scale of the comparison and the aspect on which the entities are being compared. Our methodology is based on dependencies obtained by applying a parser to extract a wide range of comparison structures. We evaluated our system for its effectiveness in identifying comparisons and their components. The system achieved a F-score of 0.87 for comparison sentence identification and 0.77-0.81 for identifying its components."",","{gupta-etal-2017-identifying,
    title = ""Identifying Comparative Structures in Biomedical Text"",
    author = ""Gupta, Samir  and
      Mahmood, A.S.M. Ashique  and
      Ross, Karen  and
      Wu, Cathy  and
      Vijay-Shanker, K."",
    editor = ""Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""{B}io{NLP} 2017"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada,"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-2326"",
    doi = ""10.18653/v1/W17-2326"",
    pages = ""206--215"",
    abstract = ""Comparison sentences are very commonly used by authors in biomedical literature to report results of experiments. In such comparisons, authors typically make observations under two different scenarios. In this paper, we present a system to automatically identify such comparative sentences and their components i.e. the compared entities, the scale of the comparison and the aspect on which the entities are being compared. Our methodology is based on dependencies obtained by applying a parser to extract a wide range of comparison structures. We evaluated our system for its effectiveness in identifying comparisons and their components. The system achieved a F-score of 0.87 for comparison sentence identification and 0.77-0.81 for identifying its components."",
}
@",medical text,nlp,evalu
" ""Toward Automated Early Sepsis Alerting: Identifying Infection Patients from Nursing Notes"","," ""Severe sepsis and septic shock are conditions that affect millions of patients and have close to 50{\%} mortality rate. Early identification of at-risk patients significantly improves outcomes. Electronic surveillance tools have been developed to monitor structured Electronic Medical Records and automatically recognize early signs of sepsis. However, many sepsis risk factors (e.g. symptoms and signs of infection) are often captured only in free text clinical notes. In this study, we developed a method for automatic monitoring of nursing notes for signs and symptoms of infection. We utilized a creative approach to automatically generate an annotated dataset. The dataset was used to create a Machine Learning model that achieved an F1-score ranging from 79 to 96{\%}."",","{apostolova-velez-2017-toward,
    title = ""Toward Automated Early Sepsis Alerting: Identifying Infection Patients from Nursing Notes"",
    author = ""Apostolova, Emilia  and
      Velez, Tom"",
    editor = ""Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""{B}io{NLP} 2017"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada,"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-2332"",
    doi = ""10.18653/v1/W17-2332"",
    pages = ""257--262"",
    abstract = ""Severe sepsis and septic shock are conditions that affect millions of patients and have close to 50{\%} mortality rate. Early identification of at-risk patients significantly improves outcomes. Electronic surveillance tools have been developed to monitor structured Electronic Medical Records and automatically recognize early signs of sepsis. However, many sepsis risk factors (e.g. symptoms and signs of infection) are often captured only in free text clinical notes. In this study, we developed a method for automatic monitoring of nursing notes for signs and symptoms of infection. We utilized a creative approach to automatically generate an annotated dataset. The dataset was used to create a Machine Learning model that achieved an F1-score ranging from 79 to 96{\%}."",
}
@",medical record,clinical not,nlp,generat,annotat
" ""Enhancing Automatic {ICD}-9-{CM} Code Assignment for Medical Texts with {P}ub{M}ed"","," ""Assigning a standard ICD-9-CM code to disease symptoms in medical texts is an important task in the medical domain. Automating this process could greatly reduce the costs. However, the effectiveness of an automatic ICD-9-CM code classifier faces a serious problem, which can be triggered by unbalanced training data. Frequent diseases often have more training data, which helps its classification to perform better than that of an infrequent disease. However, a disease{'}s frequency does not necessarily reflect its importance. To resolve this training data shortage problem, we propose to strategically draw data from PubMed to enrich the training data when there is such need. We validate our method on the CMC dataset, and the evaluation results indicate that our method can significantly improve the code assignment classifiers{'} performance at the macro-averaging level."",","{zhang-etal-2017-enhancing,
    title = ""Enhancing Automatic {ICD}-9-{CM} Code Assignment for Medical Texts with {P}ub{M}ed"",
    author = ""Zhang, Danchen  and
      He, Daqing  and
      Zhao, Sanqiang  and
      Li, Lei"",
    editor = ""Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""{B}io{NLP} 2017"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada,"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-2333"",
    doi = ""10.18653/v1/W17-2333"",
    pages = ""263--271"",
    abstract = ""Assigning a standard ICD-9-CM code to disease symptoms in medical texts is an important task in the medical domain. Automating this process could greatly reduce the costs. However, the effectiveness of an automatic ICD-9-CM code classifier faces a serious problem, which can be triggered by unbalanced training data. Frequent diseases often have more training data, which helps its classification to perform better than that of an infrequent disease. However, a disease{'}s frequency does not necessarily reflect its importance. To resolve this training data shortage problem, we propose to strategically draw data from PubMed to enrich the training data when there is such need. We validate our method on the CMC dataset, and the evaluation results indicate that our method can significantly improve the code assignment classifiers{'} performance at the macro-averaging level."",
}
@",medical domain,medical text,nlp,evalu
" ""A Biomedical Question Answering System in {B}io{ASQ} 2017"","," ""Question answering, the identification of short accurate answers to users questions, is a longstanding challenge widely studied over the last decades in the open domain. However, it still requires further efforts in the biomedical domain. In this paper, we describe our participation in phase B of task 5b in the 2017 BioASQ challenge using our biomedical question answering system. Our system, dealing with four types of questions (i.e., yes/no, factoid, list, and summary), is based on (1) a dictionary-based approach for generating the exact answers of yes/no questions, (2) UMLS metathesaurus and term frequency metric for extracting the exact answers of factoid and list questions, and (3) the BM25 model and UMLS concepts for retrieving the ideal answers (i.e., paragraph-sized summaries). Preliminary results show that our system achieves good and competitive results in both exact and ideal answers extraction tasks as compared with the participating systems."",","{sarrouti-ouatik-el-alaoui-2017-biomedical,
    title = ""A Biomedical Question Answering System in {B}io{ASQ} 2017"",
    author = ""Sarrouti, Mourad  and
      Ouatik El Alaoui, Said"",
    editor = ""Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""{B}io{NLP} 2017"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada,"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-2337"",
    doi = ""10.18653/v1/W17-2337"",
    pages = ""296--301"",
    abstract = ""Question answering, the identification of short accurate answers to users questions, is a longstanding challenge widely studied over the last decades in the open domain. However, it still requires further efforts in the biomedical domain. In this paper, we describe our participation in phase B of task 5b in the 2017 BioASQ challenge using our biomedical question answering system. Our system, dealing with four types of questions (i.e., yes/no, factoid, list, and summary), is based on (1) a dictionary-based approach for generating the exact answers of yes/no questions, (2) UMLS metathesaurus and term frequency metric for extracting the exact answers of factoid and list questions, and (3) the BM25 model and UMLS concepts for retrieving the ideal answers (i.e., paragraph-sized summaries). Preliminary results show that our system achieves good and competitive results in both exact and ideal answers extraction tasks as compared with the participating systems."",
}
@",medical domain,nlp,generat,summar,challeng
" ""Initializing neural networks for hierarchical multi-label text classification"","," ""Many tasks in the biomedical domain require the assignment of one or more predefined labels to input text, where the labels are a part of a hierarchical structure (such as a taxonomy). The conventional approach is to use a one-vs.-rest (OVR) classification setup, where a binary classifier is trained for each label in the taxonomy or ontology where all instances not belonging to the class are considered negative examples. The main drawbacks to this approach are that dependencies between classes are not leveraged in the training and classification process, and the additional computational cost of training parallel classifiers. In this paper, we apply a new method for hierarchical multi-label text classification that initializes a neural network model final hidden layer such that it leverages label co-occurrence relations such as hypernymy. This approach elegantly lends itself to hierarchical classification. We evaluated this approach using two hierarchical multi-label text classification tasks in the biomedical domain using both sentence- and document-level classification. Our evaluation shows promising results for this approach."",","{baker-korhonen-2017-initializing,
    title = ""Initializing neural networks for hierarchical multi-label text classification"",
    author = ""Baker, Simon  and
      Korhonen, Anna"",
    editor = ""Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""{B}io{NLP} 2017"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada,"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-2339"",
    doi = ""10.18653/v1/W17-2339"",
    pages = ""307--315"",
    abstract = ""Many tasks in the biomedical domain require the assignment of one or more predefined labels to input text, where the labels are a part of a hierarchical structure (such as a taxonomy). The conventional approach is to use a one-vs.-rest (OVR) classification setup, where a binary classifier is trained for each label in the taxonomy or ontology where all instances not belonging to the class are considered negative examples. The main drawbacks to this approach are that dependencies between classes are not leveraged in the training and classification process, and the additional computational cost of training parallel classifiers. In this paper, we apply a new method for hierarchical multi-label text classification that initializes a neural network model final hidden layer such that it leverages label co-occurrence relations such as hypernymy. This approach elegantly lends itself to hierarchical classification. We evaluated this approach using two hierarchical multi-label text classification tasks in the biomedical domain using both sentence- and document-level classification. Our evaluation shows promising results for this approach."",
}
@",medical domain,nlp,evalu
" ""Automatic classification of doctor-patient questions for a virtual patient record query task"","," ""We present the work-in-progress of automating the classification of doctor-patient questions in the context of a simulated consultation with a virtual patient. We classify questions according to the computational strategy (rule-based or other) needed for looking up data in the clinical record. We compare {`}traditional{'} machine learning methods (Gaussian and Multinomial Naive Bayes, and Support Vector Machines) and a neural network classifier (FastText). We obtained the best results with the SVM using semantic annotations, whereas the neural classifier achieved promising results without it."",","{campillos-llanos-etal-2017-automatic,
    title = ""Automatic classification of doctor-patient questions for a virtual patient record query task"",
    author = ""Campillos Llanos, Leonardo  and
      Rosset, Sophie  and
      Zweigenbaum, Pierre"",
    editor = ""Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""{B}io{NLP} 2017"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada,"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-2343"",
    doi = ""10.18653/v1/W17-2343"",
    pages = ""333--341"",
    abstract = ""We present the work-in-progress of automating the classification of doctor-patient questions in the context of a simulated consultation with a virtual patient. We classify questions according to the computational strategy (rule-based or other) needed for looking up data in the clinical record. We compare {`}traditional{'} machine learning methods (Gaussian and Multinomial Naive Bayes, and Support Vector Machines) and a neural network classifier (FastText). We obtained the best results with the SVM using semantic annotations, whereas the neural classifier achieved promising results without it."",
}
@",clinical record,nlp,semant,annotat
" ""Detecting mentions of pain and acute confusion in {F}innish clinical text"","," ""We study and compare two different approaches to the task of automatic assignment of predefined classes to clinical free-text narratives. In the first approach this is treated as a traditional mention-level named-entity recognition task, while the second approach treats it as a sentence-level multi-label classification task. Performance comparison across these two approaches is conducted in the form of sentence-level evaluation and state-of-the-art methods for both approaches are evaluated. The experiments are done on two data sets consisting of Finnish clinical text, manually annotated with respect to the topics pain and acute confusion. Our results suggest that the mention-level named-entity recognition approach outperforms sentence-level classification overall, but the latter approach still manages to achieve the best prediction scores on several annotation classes."",","{moen-etal-2017-detecting,
    title = ""Detecting mentions of pain and acute confusion in {F}innish clinical text"",
    author = {Moen, Hans  and
      Hakala, Kai  and
      Mehryary, Farrokh  and
      Peltonen, Laura-Maria  and
      Salakoski, Tapio  and
      Ginter, Filip  and
      Salanter{\""a}, Sanna},
    editor = ""Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Tsujii, Junichi"",
    booktitle = ""{B}io{NLP} 2017"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada,"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-2347"",
    doi = ""10.18653/v1/W17-2347"",
    pages = ""365--372"",
    abstract = ""We study and compare two different approaches to the task of automatic assignment of predefined classes to clinical free-text narratives. In the first approach this is treated as a traditional mention-level named-entity recognition task, while the second approach treats it as a sentence-level multi-label classification task. Performance comparison across these two approaches is conducted in the form of sentence-level evaluation and state-of-the-art methods for both approaches are evaluated. The experiments are done on two data sets consisting of Finnish clinical text, manually annotated with respect to the topics pain and acute confusion. Our results suggest that the mention-level named-entity recognition approach outperforms sentence-level classification overall, but the latter approach still manages to achieve the best prediction scores on several annotation classes."",
}
@",clinical text,nlp,entity recognit,annotat,evalu
" ""Annotation of negation in the {IULA} {S}panish Clinical Record Corpus"","," ""This paper presents the IULA Spanish Clinical Record Corpus, a corpus of 3,194 sentences extracted from anonymized clinical records and manually annotated with negation markers and their scope. The corpus was conceived as a resource to support clinical text-mining systems, but it is also a useful resource for other Natural Language Processing systems handling clinical texts: automatic encoding of clinical records, diagnosis support, term extraction, among others, as well as for the study of clinical texts. The corpus is publicly available with a CC-BY-SA 3.0 license."",","{marimon-etal-2017-annotation,
    title = ""Annotation of negation in the {IULA} {S}panish Clinical Record Corpus"",
    author = ""Marimon, Montserrat  and
      Vivaldi, Jorge  and
      Bel, N{\'u}ria"",
    editor = ""Blanco, Eduardo  and
      Morante, Roser  and
      Saur{\'\i}, Roser"",
    booktitle = ""Proceedings of the Workshop Computational Semantics Beyond Events and Roles"",
    month = apr,
    year = ""2017"",
    address = ""Valencia, Spain"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-1807"",
    doi = ""10.18653/v1/W17-1807"",
    pages = ""43--52"",
    abstract = ""This paper presents the IULA Spanish Clinical Record Corpus, a corpus of 3,194 sentences extracted from anonymized clinical records and manually annotated with negation markers and their scope. The corpus was conceived as a resource to support clinical text-mining systems, but it is also a useful resource for other Natural Language Processing systems handling clinical texts: automatic encoding of clinical records, diagnosis support, term extraction, among others, as well as for the study of clinical texts. The corpus is publicly available with a CC-BY-SA 3.0 license."",
}
@",clinical record,clinical text,natural language process,natural languag,language process,semant,annotat
" ""Annotating Negation in {S}panish Clinical Texts"","," ""In this paper we present on-going work on annotating negation in Spanish clinical documents. A corpus of anamnesis and radiology reports has been annotated by two domain expert annotators with negation markers and negated events. The Dice coefficient for inter-annotator agreement is higher than 0.94 for negation markers and higher than 0.72 for negated events. The corpus will be publicly released when the annotation process is finished, constituting the first corpus annotated with negation for Spanish clinical reports available for the NLP community."",","{cruz-etal-2017-annotating,
    title = ""Annotating Negation in {S}panish Clinical Texts"",
    author = ""Cruz, Noa  and
      Morante, Roser  and
      Ma{\~n}a L{\'o}pez, Manuel J.  and
      Mata V{\'a}zquez, Jacinto  and
      Parra Calder{\'o}n, Carlos L."",
    editor = ""Blanco, Eduardo  and
      Morante, Roser  and
      Saur{\'\i}, Roser"",
    booktitle = ""Proceedings of the Workshop Computational Semantics Beyond Events and Roles"",
    month = apr,
    year = ""2017"",
    address = ""Valencia, Spain"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-1808"",
    doi = ""10.18653/v1/W17-1808"",
    pages = ""53--58"",
    abstract = ""In this paper we present on-going work on annotating negation in Spanish clinical documents. A corpus of anamnesis and radiology reports has been annotated by two domain expert annotators with negation markers and negated events. The Dice coefficient for inter-annotator agreement is higher than 0.94 for negation markers and higher than 0.72 for negated events. The corpus will be publicly released when the annotation process is finished, constituting the first corpus annotated with negation for Spanish clinical reports available for the NLP community."",
}
@",clinical text,nlp,semant,annotat
" ""A Short Review of Ethical Challenges in Clinical Natural Language Processing"","," ""Clinical NLP has an immense potential in contributing to how clinical practice will be revolutionized by the advent of large scale processing of clinical records. However, this potential has remained largely untapped due to slow progress primarily caused by strict data access policies for researchers. In this paper, we discuss the concern for privacy and the measures it entails. We also suggest sources of less sensitive data. Finally, we draw attention to biases that can compromise the validity of empirical research and lead to socially harmful applications."",","{suster-etal-2017-short,
    title = ""A Short Review of Ethical Challenges in Clinical Natural Language Processing"",
    author = ""{\v{S}}uster, Simon  and
      Tulkens, St{\'e}phan  and
      Daelemans, Walter"",
    editor = ""Hovy, Dirk  and
      Spruit, Shannon  and
      Mitchell, Margaret  and
      Bender, Emily M.  and
      Strube, Michael  and
      Wallach, Hanna"",
    booktitle = ""Proceedings of the First {ACL} Workshop on Ethics in Natural Language Processing"",
    month = apr,
    year = ""2017"",
    address = ""Valencia, Spain"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W17-1610"",
    doi = ""10.18653/v1/W17-1610"",
    pages = ""80--87"",
    abstract = ""Clinical NLP has an immense potential in contributing to how clinical practice will be revolutionized by the advent of large scale processing of clinical records. However, this potential has remained largely untapped due to slow progress primarily caused by strict data access policies for researchers. In this paper, we discuss the concern for privacy and the measures it entails. We also suggest sources of less sensitive data. Finally, we draw attention to biases that can compromise the validity of empirical research and lead to socially harmful applications."",
}
@",clinical record,natural language process,natural languag,language process,nlp,challeng
" ""{S}em{E}val-2017 Task 9: {A}bstract {M}eaning {R}epresentation Parsing and Generation"","," ""In this report we summarize the results of the 2017 AMR SemEval shared task. The task consisted of two separate yet related subtasks. In the parsing subtask, participants were asked to produce Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs for a set of English sentences in the biomedical domain. In the generation subtask, participants were asked to generate English sentences given AMR graphs in the news/forum domain. A total of five sites participated in the parsing subtask, and four participated in the generation subtask. Along with a description of the task and the participants{'} systems, we show various score ablations and some sample outputs."",","{may-priyadarshi-2017-semeval,
    title = ""{S}em{E}val-2017 Task 9: {A}bstract {M}eaning {R}epresentation Parsing and Generation"",
    author = ""May, Jonathan  and
      Priyadarshi, Jay"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S17-2090"",
    doi = ""10.18653/v1/S17-2090"",
    pages = ""536--545"",
    abstract = ""In this report we summarize the results of the 2017 AMR SemEval shared task. The task consisted of two separate yet related subtasks. In the parsing subtask, participants were asked to produce Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs for a set of English sentences in the biomedical domain. In the generation subtask, participants were asked to generate English sentences given AMR graphs in the news/forum domain. A total of five sites participated in the parsing subtask, and four participated in the generation subtask. Along with a description of the task and the participants{'} systems, we show various score ablations and some sample outputs."",
}
@",medical domain,generat,summar,semant,shared task,evalu
" ""{S}em{E}val-2017 Task 12: Clinical {T}emp{E}val"","," ""Clinical TempEval 2017 aimed to answer the question: how well do systems trained on annotated timelines for one medical condition (colon cancer) perform in predicting timelines on another medical condition (brain cancer)? Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification. Participant systems were evaluated on clinical and pathology notes from Mayo Clinic cancer patients, annotated with an extension of TimeML for the clinical domain. 11 teams participated in the tasks, with the best systems achieving F1 scores above 0.55 for time expressions, above 0.70 for event expressions, and above 0.40 for temporal relations. Most tasks observed about a 20 point drop over Clinical TempEval 2016, where systems were trained and evaluated on the same domain (colon cancer)."",","{bethard-etal-2017-semeval,
    title = ""{S}em{E}val-2017 Task 12: Clinical {T}emp{E}val"",
    author = ""Bethard, Steven  and
      Savova, Guergana  and
      Palmer, Martha  and
      Pustejovsky, James"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S17-2093"",
    doi = ""10.18653/v1/S17-2093"",
    pages = ""565--572"",
    abstract = ""Clinical TempEval 2017 aimed to answer the question: how well do systems trained on annotated timelines for one medical condition (colon cancer) perform in predicting timelines on another medical condition (brain cancer)? Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification. Participant systems were evaluated on clinical and pathology notes from Mayo Clinic cancer patients, annotated with an extension of TimeML for the clinical domain. 11 teams participated in the tasks, with the best systems achieving F1 scores above 0.55 for time expressions, above 0.70 for event expressions, and above 0.40 for temporal relations. Most tasks observed about a 20 point drop over Clinical TempEval 2016, where systems were trained and evaluated on the same domain (colon cancer)."",
}
@",clinical domain,semant,annotat,evalu
" ""{LIMSI}-{COT} at {S}em{E}val-2017 Task 12: Neural Architecture for Temporal Information Extraction from Clinical Narratives"","," ""In this paper we present our participation to SemEval 2017 Task 12. We used a neural network based approach for entity and temporal relation extraction, and experimented with two domain adaptation strategies. We achieved competitive performance for both tasks."",","{tourille-etal-2017-limsi,
    title = ""{LIMSI}-{COT} at {S}em{E}val-2017 Task 12: Neural Architecture for Temporal Information Extraction from Clinical Narratives"",
    author = ""Tourille, Julien  and
      Ferret, Olivier  and
      Tannier, Xavier  and
      N{\'e}v{\'e}ol, Aur{\'e}lie"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S17-2098"",
    doi = ""10.18653/v1/S17-2098"",
    pages = ""597--602"",
    abstract = ""In this paper we present our participation to SemEval 2017 Task 12. We used a neural network based approach for entity and temporal relation extraction, and experimented with two domain adaptation strategies. We achieved competitive performance for both tasks."",
}
@",clinical narr,relation extract,semant,evalu
" ""{LABDA} at {S}em{E}val-2017 Task 10: Extracting Keyphrases from Scientific Publications by combining the {BANNER} tool and the {UMLS} Semantic Network"","," ""This paper describes the system presented by the LABDA group at SemEval 2017 Task 10 ScienceIE, specifically for the subtasks of identification and classification of keyphrases from scientific articles. For the task of identification, we use the BANNER tool, a named entity recognition system, which is based on conditional random fields (CRF) and has obtained successful results in the biomedical domain. To classify keyphrases, we study the UMLS semantic network and propose a possible linking between the keyphrase types and the UMLS semantic groups. Based on this semantic linking, we create a dictionary for each keyphrase type. Then, a feature indicating if a token is found in one of these dictionaries is incorporated to feature set used by the BANNER tool. The final results on the test dataset show that our system still needs to be improved, but the conditional random fields and, consequently, the BANNER system can be used as a first approximation to identify and classify keyphrases."",","{segura-bedmar-etal-2017-labda,
    title = ""{LABDA} at {S}em{E}val-2017 Task 10: Extracting Keyphrases from Scientific Publications by combining the {BANNER} tool and the {UMLS} Semantic Network"",
    author = ""Segura-Bedmar, Isabel  and
      Col{\'o}n-Ruiz, Crist{\'o}bal  and
      Mart{\'\i}nez, Paloma"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S17-2164"",
    doi = ""10.18653/v1/S17-2164"",
    pages = ""947--950"",
    abstract = ""This paper describes the system presented by the LABDA group at SemEval 2017 Task 10 ScienceIE, specifically for the subtasks of identification and classification of keyphrases from scientific articles. For the task of identification, we use the BANNER tool, a named entity recognition system, which is based on conditional random fields (CRF) and has obtained successful results in the biomedical domain. To classify keyphrases, we study the UMLS semantic network and propose a possible linking between the keyphrase types and the UMLS semantic groups. Based on this semantic linking, we create a dictionary for each keyphrase type. Then, a feature indicating if a token is found in one of these dictionaries is incorporated to feature set used by the BANNER tool. The final results on the test dataset show that our system still needs to be improved, but the conditional random fields and, consequently, the BANNER system can be used as a first approximation to identify and classify keyphrases."",
}
@",medical domain,entity recognit,semant,evalu
" ""Hitachi at {S}em{E}val-2017 Task 12: System for temporal information extraction from clinical notes"","," ""This paper describes the system developed for the task of temporal information extraction from clinical narratives in the context of the 2017 Clinical TempEval challenge. Clinical TempEval 2017 addressed the problem of temporal reasoning in the clinical domain by providing annotated clinical notes, pathology and radiology reports in line with Clinical TempEval challenges 2015/16, across two different evaluation phases focusing on cross domain adaptation. Our team focused on subtasks involving extractions of temporal spans and relations for which the developed systems showed average F-score of 0.45 and 0.47 across the two phases of evaluations."",","{p-r-etal-2017-hitachi,
    title = ""Hitachi at {S}em{E}val-2017 Task 12: System for temporal information extraction from clinical notes"",
    author = ""P R, Sarath  and
      R, Manikandan  and
      Niwa, Yoshiki"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S17-2176"",
    doi = ""10.18653/v1/S17-2176"",
    pages = ""1005--1009"",
    abstract = ""This paper describes the system developed for the task of temporal information extraction from clinical narratives in the context of the 2017 Clinical TempEval challenge. Clinical TempEval 2017 addressed the problem of temporal reasoning in the clinical domain by providing annotated clinical notes, pathology and radiology reports in line with Clinical TempEval challenges 2015/16, across two different evaluation phases focusing on cross domain adaptation. Our team focused on subtasks involving extractions of temporal spans and relations for which the developed systems showed average F-score of 0.45 and 0.47 across the two phases of evaluations."",
}
@",clinical narr,clinical domain,clinical not,semant,annotat,challeng,evalu
" ""{XJNLP} at {S}em{E}val-2017 Task 12: Clinical temporal information ex-traction with a Hybrid Model"","," ""Temporality is crucial in understanding the course of clinical events from a patient{'}s electronic health recordsand temporal processing is becoming more and more important for improving access to content. SemEval 2017 Task 12 (Clinical TempEval) addressed this challenge using the THYME corpus, a corpus of clinical narratives annotated with a schema based on TimeML2 guidelines. We developed and evaluated approaches for: extraction of temporal expressions (TIMEX3) and EVENTs; EVENT attributes; document-time relations. Our approach is a hybrid model which is based on rule based methods, semi-supervised learning, and semantic features with addition of manually crafted rules."",","{long-etal-2017-xjnlp,
    title = ""{XJNLP} at {S}em{E}val-2017 Task 12: Clinical temporal information ex-traction with a Hybrid Model"",
    author = ""Long, Yu  and
      Li, Zhijing  and
      Wang, Xuan  and
      Li, Chen"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S17-2178"",
    doi = ""10.18653/v1/S17-2178"",
    pages = ""1014--1018"",
    abstract = ""Temporality is crucial in understanding the course of clinical events from a patient{'}s electronic health recordsand temporal processing is becoming more and more important for improving access to content. SemEval 2017 Task 12 (Clinical TempEval) addressed this challenge using the THYME corpus, a corpus of clinical narratives annotated with a schema based on TimeML2 guidelines. We developed and evaluated approaches for: extraction of temporal expressions (TIMEX3) and EVENTs; EVENT attributes; document-time relations. Our approach is a hybrid model which is based on rule based methods, semi-supervised learning, and semantic features with addition of manually crafted rules."",
}
@",electronic health record,health record,clinical narr,nlp,semant,annotat,challeng,evalu
" ""{GUIR} at {S}em{E}val-2017 Task 12: A Framework for Cross-Domain Clinical Temporal Information Extraction"","," ""Clinical TempEval 2017 (SemEval 2017 Task 12) addresses the task of cross-domain temporal extraction from clinical text. We present a system for this task that uses supervised learning for the extraction of temporal expression and event spans with corresponding attributes and narrative container relations. Approaches include conditional random fields and decision tree ensembles, using lexical, syntactic, semantic, distributional, and rule-based features. Our system received best or second best scores in TIMEX3 span, EVENT span, and CONTAINS relation extraction."",","{macavaney-etal-2017-guir,
    title = ""{GUIR} at {S}em{E}val-2017 Task 12: A Framework for Cross-Domain Clinical Temporal Information Extraction"",
    author = ""MacAvaney, Sean  and
      Cohan, Arman  and
      Goharian, Nazli"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S17-2180"",
    doi = ""10.18653/v1/S17-2180"",
    pages = ""1024--1029"",
    abstract = ""Clinical TempEval 2017 (SemEval 2017 Task 12) addresses the task of cross-domain temporal extraction from clinical text. We present a system for this task that uses supervised learning for the extraction of temporal expression and event spans with corresponding attributes and narrative container relations. Approaches include conditional random fields and decision tree ensembles, using lexical, syntactic, semantic, distributional, and rule-based features. Our system received best or second best scores in TIMEX3 span, EVENT span, and CONTAINS relation extraction."",
}
@",clinical text,relation extract,semant,evalu
" ""Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017)"","," ""In this paper, we describe the system of the KULeuven-LIIR submission for Clinical TempEval 2017. We participated in all six subtasks, using a combination of Support Vector Machines (SVM) for event and temporal expression detection, and a structured perceptron for extracting temporal relations. Moreover, we present and analyze the results from our submissions, and verify the effectiveness of several system components. Our system performed above average for all subtasks in both phases."",","{leeuwenberg-moens-2017-kuleuven,
    title = ""{KUL}euven-{LIIR} at {S}em{E}val-2017 Task 12: Cross-Domain Temporal Information Extraction from Clinical Records"",
    author = ""Leeuwenberg, Artuur  and
      Moens, Marie-Francine"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S17-2181"",
    doi = ""10.18653/v1/S17-2181"",
    pages = ""1030--1034"",
    abstract = ""In this paper, we describe the system of the KULeuven-LIIR submission for Clinical TempEval 2017. We participated in all six subtasks, using a combination of Support Vector Machines (SVM) for event and temporal expression detection, and a structured perceptron for extracting temporal relations. Moreover, we present and analyze the results from our submissions, and verify the effectiveness of several system components. Our system performed above average for all subtasks in both phases."",
}
@proceedings{semeval-2017-joint,
    title = ""Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017)"",
    editor = ""Ide, Nancy  and
      Herbelot, Aur{\'e}lie  and
      M{\`a}rquez, Llu{\'\i}s"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S17-1000"",
    doi = ""10.18653/v1/S17-1"",
}
@",clinical record,semant,evalu
" ""Mining Association Rules from Clinical Narratives"","," ""Shallow text analysis (Text Mining) uses mainly Information Extraction techniques. The low resource languages do not allow application of such traditional techniques with sufficient accuracy and recall on big data. In contrast, Data Mining approaches provide an opportunity to make deep analysis and to discover new knowledge. Frequent pattern mining approaches are used mainly for structured information in databases and are a quite challenging task in text mining. Unfortunately, most frequent pattern mining approaches do not use contextual information for extracted patterns: general patterns are extracted regardless of the context. We propose a method that processes raw informal texts (from health discussion forums) and formal texts (outpatient records) in Bulgarian language. In addition we use some context information and small terminological lexicons to generalize extracted frequent patterns. This allows to map informal expression of medical terminology to the formal one and to generate automatically resources."",","{boytcheva-etal-2017-mining,
    title = ""Mining Association Rules from Clinical Narratives"",
    author = ""Boytcheva, Svetla  and
      Nikolova, Ivelina  and
      Angelova, Galia"",
    editor = ""Mitkov, Ruslan  and
      Angelova, Galia"",
    booktitle = ""Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017"",
    month = sep,
    year = ""2017"",
    address = ""Varna, Bulgaria"",
    publisher = ""INCOMA Ltd."",
    url = ""https://doi.org/10.26615/978-954-452-049-6_019"",
    doi = ""10.26615/978-954-452-049-6_019"",
    pages = ""130--138"",
    abstract = ""Shallow text analysis (Text Mining) uses mainly Information Extraction techniques. The low resource languages do not allow application of such traditional techniques with sufficient accuracy and recall on big data. In contrast, Data Mining approaches provide an opportunity to make deep analysis and to discover new knowledge. Frequent pattern mining approaches are used mainly for structured information in databases and are a quite challenging task in text mining. Unfortunately, most frequent pattern mining approaches do not use contextual information for extracted patterns: general patterns are extracted regardless of the context. We propose a method that processes raw informal texts (from health discussion forums) and formal texts (outpatient records) in Bulgarian language. In addition we use some context information and small terminological lexicons to generalize extracted frequent patterns. This allows to map informal expression of medical terminology to the formal one and to generate automatically resources."",
}
@",clinical narr,natural language process,natural languag,language process,nlp,generat,challeng
" ""Bootstrapping a {R}omanian Corpus for Medical Named Entity Recognition"","," ""Named Entity Recognition (NER) is an important component of natural language processing (NLP), with applicability in biomedical domain, enabling knowledge-discovery from medical texts. Due to the fact that for the Romanian language there are only a few linguistic resources specific to the biomedical domain, it was created a sub-corpus specific to this domain. In this paper we present a newly developed Romanian sub-corpus for medical-domain NER, which is a valuable asset for the field of biomedical text processing. We provide a description of the sub-corpus, informative statistics about data-composition and we evaluate an automatic NER tool on the newly created resource."",","{mitrofan-2017-bootstrapping,
    title = ""Bootstrapping a {R}omanian Corpus for Medical Named Entity Recognition"",
    author = ""Mitrofan, Maria"",
    editor = ""Mitkov, Ruslan  and
      Angelova, Galia"",
    booktitle = ""Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017"",
    month = sep,
    year = ""2017"",
    address = ""Varna, Bulgaria"",
    publisher = ""INCOMA Ltd."",
    url = ""https://doi.org/10.26615/978-954-452-049-6_066"",
    doi = ""10.26615/978-954-452-049-6_066"",
    pages = ""501--509"",
    abstract = ""Named Entity Recognition (NER) is an important component of natural language processing (NLP), with applicability in biomedical domain, enabling knowledge-discovery from medical texts. Due to the fact that for the Romanian language there are only a few linguistic resources specific to the biomedical domain, it was created a sub-corpus specific to this domain. In this paper we present a newly developed Romanian sub-corpus for medical-domain NER, which is a valuable asset for the field of biomedical text processing. We provide a description of the sub-corpus, informative statistics about data-composition and we evaluate an automatic NER tool on the newly created resource."",
}
@",medical domain,medical text,natural language process,natural languag,language process,nlp,entity recognit,evalu
" ""Learning local and global contexts using a convolutional recurrent network model for relation classification in biomedical text"","," ""The task of relation classification in the biomedical domain is complex due to the presence of samples obtained from heterogeneous sources such as research articles, discharge summaries, or electronic health records. It is also a constraint for classifiers which employ manual feature engineering. In this paper, we propose a convolutional recurrent neural network (CRNN) architecture that combines RNNs and CNNs in sequence to solve this problem. The rationale behind our approach is that CNNs can effectively identify coarse-grained local features in a sentence, while RNNs are more suited for long-term dependencies. We compare our CRNN model with several baselines on two biomedical datasets, namely the i2b2-2010 clinical relation extraction challenge dataset, and the SemEval-2013 DDI extraction dataset. We also evaluate an attentive pooling technique and report its performance in comparison with the conventional max pooling method. Our results indicate that the proposed model achieves state-of-the-art performance on both datasets."",","{raj-etal-2017-learning,
    title = ""Learning local and global contexts using a convolutional recurrent network model for relation classification in biomedical text"",
    author = ""Raj, Desh  and
      Sahu, Sunil  and
      Anand, Ashish"",
    editor = ""Levy, Roger  and
      Specia, Lucia"",
    booktitle = ""Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)"",
    month = aug,
    year = ""2017"",
    address = ""Vancouver, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/K17-1032"",
    doi = ""10.18653/v1/K17-1032"",
    pages = ""311--321"",
    abstract = ""The task of relation classification in the biomedical domain is complex due to the presence of samples obtained from heterogeneous sources such as research articles, discharge summaries, or electronic health records. It is also a constraint for classifiers which employ manual feature engineering. In this paper, we propose a convolutional recurrent neural network (CRNN) architecture that combines RNNs and CNNs in sequence to solve this problem. The rationale behind our approach is that CNNs can effectively identify coarse-grained local features in a sentence, while RNNs are more suited for long-term dependencies. We compare our CRNN model with several baselines on two biomedical datasets, namely the i2b2-2010 clinical relation extraction challenge dataset, and the SemEval-2013 DDI extraction dataset. We also evaluate an attentive pooling technique and report its performance in comparison with the conventional max pooling method. Our results indicate that the proposed model achieves state-of-the-art performance on both datasets."",
}
@",electronic health record,health record,discharge summar,medical domain,medical text,natural languag,relation extract,relation classif,summar,challeng,evalu
" ""Predicting Counselor Behaviors in Motivational Interviewing Encounters"","," ""As the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors{'} language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. We introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90{\%}, which represent a significant improvement with respect to features used in the past."",","{perez-rosas-etal-2017-predicting,
    title = ""Predicting Counselor Behaviors in Motivational Interviewing Encounters"",
    author = ""P{\'e}rez-Rosas, Ver{\'o}nica  and
      Mihalcea, Rada  and
      Resnicow, Kenneth  and
      Singh, Satinder  and
      An, Lawrence  and
      Goggin, Kathy J.  and
      Catley, Delwyn"",
    editor = ""Lapata, Mirella  and
      Blunsom, Phil  and
      Koller, Alexander"",
    booktitle = ""Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers"",
    month = apr,
    year = ""2017"",
    address = ""Valencia, Spain"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/E17-1106"",
    pages = ""1128--1137"",
    abstract = ""As the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors{'} language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. We introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90{\%}, which represent a significant improvement with respect to features used in the past."",
}
@",clinical domain,semant,challeng,evalu
" ""An Insight Extraction System on {B}io{M}edical Literature with Deep Neural Networks"","," ""Mining biomedical text offers an opportunity to automatically discover important facts and infer associations among them. As new scientific findings appear across a large collection of biomedical publications, our aim is to tap into this literature to automate biomedical knowledge extraction and identify important insights from them. Towards that goal, we develop a system with novel deep neural networks to extract insights on biomedical literature. Evaluation shows our system is able to provide insights with competitive accuracy of human acceptance and its relation extraction component outperforms previous work."",","{he-etal-2017-insight,
    title = ""An Insight Extraction System on {B}io{M}edical Literature with Deep Neural Networks"",
    author = ""He, Hua  and
      Ganjam, Kris  and
      Jain, Navendu  and
      Lundin, Jessica  and
      White, Ryen  and
      Lin, Jimmy"",
    editor = ""Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian"",
    booktitle = ""Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"",
    month = sep,
    year = ""2017"",
    address = ""Copenhagen, Denmark"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/D17-1285"",
    doi = ""10.18653/v1/D17-1285"",
    pages = ""2691--2701"",
    abstract = ""Mining biomedical text offers an opportunity to automatically discover important facts and infer associations among them. As new scientific findings appear across a large collection of biomedical publications, our aim is to tap into this literature to automate biomedical knowledge extraction and identify important insights from them. Towards that goal, we develop a system with novel deep neural networks to extract insights on biomedical literature. Evaluation shows our system is able to provide insights with competitive accuracy of human acceptance and its relation extraction component outperforms previous work."",
}
@",medical text,natural language process,natural languag,language process,infer,relation extract,evalu
" ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"","," ""Different types of users require different functions in NLP software. It is difficult for a single platform to cover all types of users. When a framework aims to provide more interoperability, users are required to learn more concepts; users{'} application designs are restricted to be compliant with the framework. While an interoperability framework is useful in certain cases, some types of users will not select the framework due to the learning cost and design restrictions. We suggest a rather simple framework for the interoperability aiming at developers. Reusing an existing NLP platform Kachako, we created an API oriented NLP system. This system loosely couples rich high-end functions, including annotation visualizations, statistical evaluations, an-notation searching, etc. This API do not require users much learning cost, providing customization ability for power users while also allowing easy users to employ many GUI functions."",","{kano-2016-platform,
    title = ""Between Platform and {API}s: {K}achako {API} for Developers"",
    author = ""Kano, Yoshinobu"",
    editor = ""Murakami, Yohei  and
      Lin, Donghui  and
      Ide, Nancy  and
      Pustejovsky, James"",
    booktitle = ""Proceedings of the Third International Workshop on Worldwide Language Service Infrastructure and Second Workshop on Open Infrastructures and Analysis Frameworks for Human Language Technologies ({WLSI}/{OIAF}4{HLT}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5210"",
    pages = ""70--75"",
    abstract = ""Different types of users require different functions in NLP software. It is difficult for a single platform to cover all types of users. When a framework aims to provide more interoperability, users are required to learn more concepts; users{'} application designs are restricted to be compliant with the framework. While an interoperability framework is useful in certain cases, some types of users will not select the framework due to the learning cost and design restrictions. We suggest a rather simple framework for the interoperability aiming at developers. Reusing an existing NLP platform Kachako, we created an API oriented NLP system. This system loosely couples rich high-end functions, including annotation visualizations, statistical evaluations, an-notation searching, etc. This API do not require users much learning cost, providing customization ability for power users while also allowing easy users to employ many GUI functions."",
}
@proceedings{ws-2016-building,
    title = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5100"",
}
@",medical text,nlp,annotat,evalu
" ""Cancer Hallmark Text Classification Using Convolutional Neural Networks"","," ""Methods based on deep learning approaches have recently achieved state-of-the-art performance in a range of machine learning tasks and are increasingly applied to natural language processing (NLP). Despite strong results in various established NLP tasks involving general domain texts, there is only limited work applying these models to biomedical NLP. In this paper, we consider a Convolutional Neural Network (CNN) approach to biomedical text classification. Evaluation using a recently introduced cancer domain dataset involving the categorization of documents according to the well-established hallmarks of cancer shows that a basic CNN model can achieve a level of performance competitive with a Support Vector Machine (SVM) trained using complex manually engineered features optimized to the task. We further show that simple modifications to the CNN hyperparameters, initialization, and training process allow the model to notably outperform the SVM, establishing a new state of the art result at this task. We make all of the resources and tools introduced in this study available under open licenses from \url{https://cambridgeltl.github.io/cancer-hallmark-cnn/}."",","{baker-etal-2016-cancer,
    title = ""Cancer Hallmark Text Classification Using Convolutional Neural Networks"",
    author = ""Baker, Simon  and
      Korhonen, Anna  and
      Pyysalo, Sampo"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    booktitle = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5101"",
    pages = ""1--9"",
    abstract = ""Methods based on deep learning approaches have recently achieved state-of-the-art performance in a range of machine learning tasks and are increasingly applied to natural language processing (NLP). Despite strong results in various established NLP tasks involving general domain texts, there is only limited work applying these models to biomedical NLP. In this paper, we consider a Convolutional Neural Network (CNN) approach to biomedical text classification. Evaluation using a recently introduced cancer domain dataset involving the categorization of documents according to the well-established hallmarks of cancer shows that a basic CNN model can achieve a level of performance competitive with a Support Vector Machine (SVM) trained using complex manually engineered features optimized to the task. We further show that simple modifications to the CNN hyperparameters, initialization, and training process allow the model to notably outperform the SVM, establishing a new state of the art result at this task. We make all of the resources and tools introduced in this study available under open licenses from \url{https://cambridgeltl.github.io/cancer-hallmark-cnn/}."",
}
@",medical text,natural language process,natural languag,language process,nlp,evalu
" ""Learning Orthographic Features in Bi-directional {LSTM} for Biomedical Named Entity Recognition"","," ""End-to-end neural network models for named entity recognition (NER) have shown to achieve effective performances on general domain datasets (e.g. newswire), without requiring additional hand-crafted features. However, in biomedical domain, recent studies have shown that hand-engineered features (e.g. orthographic features) should be used to attain effective performance, due to the complexity of biomedical terminology (e.g. the use of acronyms and complex gene names). In this work, we propose a novel approach that allows a neural network model based on a long short-term memory (LSTM) to automatically learn orthographic features and incorporate them into a model for biomedical NER. Importantly, our bi-directional LSTM model learns and leverages orthographic features on an end-to-end basis. We evaluate our approach by comparing against existing neural network models for NER using three well-established biomedical datasets. Our experimental results show that the proposed approach consistently outperforms these strong baselines across all of the three datasets."",","{limsopatham-collier-2016-learning,
    title = ""Learning Orthographic Features in Bi-directional {LSTM} for Biomedical Named Entity Recognition"",
    author = ""Limsopatham, Nut  and
      Collier, Nigel"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    booktitle = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5102"",
    pages = ""10--19"",
    abstract = ""End-to-end neural network models for named entity recognition (NER) have shown to achieve effective performances on general domain datasets (e.g. newswire), without requiring additional hand-crafted features. However, in biomedical domain, recent studies have shown that hand-engineered features (e.g. orthographic features) should be used to attain effective performance, due to the complexity of biomedical terminology (e.g. the use of acronyms and complex gene names). In this work, we propose a novel approach that allows a neural network model based on a long short-term memory (LSTM) to automatically learn orthographic features and incorporate them into a model for biomedical NER. Importantly, our bi-directional LSTM model learns and leverages orthographic features on an end-to-end basis. We evaluate our approach by comparing against existing neural network models for NER using three well-established biomedical datasets. Our experimental results show that the proposed approach consistently outperforms these strong baselines across all of the three datasets."",
}
@",medical domain,medical text,entity recognit,evalu
" ""Building Content-driven Entity Networks for Scarce Scientific Literature using Content Information"","," ""This paper proposes several network construction methods for collections of scarce scientific literature data. We define scarcity as lacking in value and in volume. Instead of using the paper{'}s metadata to construct several kinds of scientific networks, we use the full texts of the articles and automatically extract the entities needed to construct the networks. Specifically, we present seven kinds of networks using the proposed construction methods: co-occurrence networks for author, keyword, and biological entities, and citation networks for author, keyword, biological, and topic entities. We show two case studies that applies our proposed methods: CADASIL, a rare yet the most common form of hereditary stroke disorder, and Metformin, the first-line medication to the type 2 diabetes treatment. We apply our proposed method to four different applications for evaluation: finding prolific authors, finding important bio-entities, finding meaningful keywords, and discovering influential topics. The results show that the co-occurrence and citation networks constructed using the proposed method outperforms the traditional-based networks. We also compare our proposed networks to traditional citation networks constructed using enough data and infer that even with the same amount of enough data, our methods perform comparably or better than the traditional methods."",","{amplayo-song-2016-building,
    title = ""Building Content-driven Entity Networks for Scarce Scientific Literature using Content Information"",
    author = ""Amplayo, Reinald Kim  and
      Song, Min"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    booktitle = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5103"",
    pages = ""20--29"",
    abstract = ""This paper proposes several network construction methods for collections of scarce scientific literature data. We define scarcity as lacking in value and in volume. Instead of using the paper{'}s metadata to construct several kinds of scientific networks, we use the full texts of the articles and automatically extract the entities needed to construct the networks. Specifically, we present seven kinds of networks using the proposed construction methods: co-occurrence networks for author, keyword, and biological entities, and citation networks for author, keyword, biological, and topic entities. We show two case studies that applies our proposed methods: CADASIL, a rare yet the most common form of hereditary stroke disorder, and Metformin, the first-line medication to the type 2 diabetes treatment. We apply our proposed method to four different applications for evaluation: finding prolific authors, finding important bio-entities, finding meaningful keywords, and discovering influential topics. The results show that the co-occurrence and citation networks constructed using the proposed method outperforms the traditional-based networks. We also compare our proposed networks to traditional citation networks constructed using enough data and infer that even with the same amount of enough data, our methods perform comparably or better than the traditional methods."",
}
@",medical text,infer,evalu
" ""Named Entity Recognition in {S}wedish Health Records with Character-Based Deep Bidirectional {LSTM}s"","," ""We propose an approach for named entity recognition in medical data, using a character-based deep bidirectional recurrent neural network. Such models can learn features and patterns based on the character sequence, and are not limited to a fixed vocabulary. This makes them very well suited for the NER task in the medical domain. Our experimental evaluation shows promising results, with a 60{\%} improvement in F 1 score over the baseline, and our system generalizes well between different datasets."",","{almgren-etal-2016-named,
    title = ""Named Entity Recognition in {S}wedish Health Records with Character-Based Deep Bidirectional {LSTM}s"",
    author = ""Almgren, Simon  and
      Pavlov, Sean  and
      Mogren, Olof"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    booktitle = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5104"",
    pages = ""30--39"",
    abstract = ""We propose an approach for named entity recognition in medical data, using a character-based deep bidirectional recurrent neural network. Such models can learn features and patterns based on the character sequence, and are not limited to a fixed vocabulary. This makes them very well suited for the NER task in the medical domain. Our experimental evaluation shows promising results, with a 60{\%} improvement in F 1 score over the baseline, and our system generalizes well between different datasets."",
}
@",health record,medical domain,medical text,entity recognit,evalu
" ""Entity-Supported Summarization of Biomedical Abstracts"","," ""The increasing amount of biomedical information that is available for researchers and clinicians makes it harder to quickly find the right information. Automatic summarization of multiple texts can provide summaries specific to the user{'}s information needs. In this paper we look into the use named-entity recognition for graph-based summarization. We extend the LexRank algorithm with information about named entities and present EntityRank, a multi-document graph-based summarization algorithm that is solely based on named entities. We evaluate our system on a datasets of 1009 human written summaries provided by BioASQ and on 1974 gene summaries, fetched from the Entrez Gene database. The results show that the addition of named-entity information increases the performance of graph-based summarizers and that the EntityRank significantly outperforms the other methods with regard to the ROUGE measures."",","{schulze-neves-2016-entity,
    title = ""Entity-Supported Summarization of Biomedical Abstracts"",
    author = ""Schulze, Frederik  and
      Neves, Mariana"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    booktitle = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5105"",
    pages = ""40--49"",
    abstract = ""The increasing amount of biomedical information that is available for researchers and clinicians makes it harder to quickly find the right information. Automatic summarization of multiple texts can provide summaries specific to the user{'}s information needs. In this paper we look into the use named-entity recognition for graph-based summarization. We extend the LexRank algorithm with information about named entities and present EntityRank, a multi-document graph-based summarization algorithm that is solely based on named entities. We evaluate our system on a datasets of 1009 human written summaries provided by BioASQ and on 1974 gene summaries, fetched from the Entrez Gene database. The results show that the addition of named-entity information increases the performance of graph-based summarizers and that the EntityRank significantly outperforms the other methods with regard to the ROUGE measures."",
}
@",medical text,entity recognit,summar,evalu
" ""Fully unsupervised low-dimensional representation of adverse drug reaction events through distributional semantics"","," ""Electronic health records show great variability since the same concept is often expressed with different terms, either scientific latin forms, common or lay variants and even vernacular naming. Deep learning enables distributional representation of terms in a vector-space, and therefore, related terms tend to be close in the vector space. Accordingly, embedding words through these vectors opens the way towards accounting for semantic relatedness through classical algebraic operations. In this work we propose a simple though efficient unsupervised characterization of Adverse Drug Reactions (ADRs). This approach exploits the embedding representation of the terms involved in candidate ADR events, that is, drug-disease entity pairs. In brief, the ADRs are represented as vectors that link the drug with the disease in their context through a recursive additive model. We discovered that a low-dimensional representation that makes use of the modulus and argument of the embedded representation of the ADR event shows correlation with the manually annotated class. Thus, it can be derived that this characterization results in to be beneficial for further classification tasks as predictive features."",","{perez-etal-2016-fully,
    title = ""Fully unsupervised low-dimensional representation of adverse drug reaction events through distributional semantics"",
    author = ""P{\'e}rez, Alicia  and
      Casillas, Arantza  and
      Gojenola, Koldo"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    booktitle = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5106"",
    pages = ""50--59"",
    abstract = ""Electronic health records show great variability since the same concept is often expressed with different terms, either scientific latin forms, common or lay variants and even vernacular naming. Deep learning enables distributional representation of terms in a vector-space, and therefore, related terms tend to be close in the vector space. Accordingly, embedding words through these vectors opens the way towards accounting for semantic relatedness through classical algebraic operations. In this work we propose a simple though efficient unsupervised characterization of Adverse Drug Reactions (ADRs). This approach exploits the embedding representation of the terms involved in candidate ADR events, that is, drug-disease entity pairs. In brief, the ADRs are represented as vectors that link the drug with the disease in their context through a recursive additive model. We discovered that a low-dimensional representation that makes use of the modulus and argument of the embedded representation of the ADR event shows correlation with the manually annotated class. Thus, it can be derived that this characterization results in to be beneficial for further classification tasks as predictive features."",
}
@",electronic health record,health record,medical text,semant,annotat,evalu
" ""A Dataset for {ICD}-10 Coding of Death Certificates: Creation and Usage"","," ""Very few datasets have been released for the evaluation of diagnosis coding with the International Classification of Diseases, and only one so far in a language other than English. This paper describes a large-scale dataset prepared from French death certificates, and the problems which needed to be solved to turn it into a dataset suitable for the application of machine learning and natural language processing methods of ICD-10 coding. The dataset includes the free-text statements written by medical doctors, the associated meta-data, the human coder-assigned codes for each statement, as well as the statement segments which supported the coder{'}s decision for each code. The dataset comprises 93,694 death certificates totalling 276,103 statements and 377,677 ICD-10 code assignments (3,457 unique codes). It was made available for an international automated coding shared task, which attracted five participating teams. An extended version of the dataset will be used in a new edition of the shared task."",","{lavergne-etal-2016-dataset,
    title = ""A Dataset for {ICD}-10 Coding of Death Certificates: Creation and Usage"",
    author = ""Lavergne, Thomas  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Robert, Aude  and
      Grouin, Cyril  and
      Rey, Gr{\'e}goire  and
      Zweigenbaum, Pierre"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    booktitle = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5107"",
    pages = ""60--69"",
    abstract = ""Very few datasets have been released for the evaluation of diagnosis coding with the International Classification of Diseases, and only one so far in a language other than English. This paper describes a large-scale dataset prepared from French death certificates, and the problems which needed to be solved to turn it into a dataset suitable for the application of machine learning and natural language processing methods of ICD-10 coding. The dataset includes the free-text statements written by medical doctors, the associated meta-data, the human coder-assigned codes for each statement, as well as the statement segments which supported the coder{'}s decision for each code. The dataset comprises 93,694 death certificates totalling 276,103 statements and 377,677 ICD-10 code assignments (3,457 unique codes). It was made available for an international automated coding shared task, which attracted five participating teams. An extended version of the dataset will be used in a new edition of the shared task."",
}
@",medical text,natural language process,natural languag,language process,shared task,evalu
" ""Supervised classification of end-of-lines in clinical text with no manual annotation"","," ""In some plain text documents, end-of-line marks may or may not mark the boundary of a text unit (e.g., of a paragraph). This vexing problem is likely to impact subsequent natural language processing components, but is seldom addressed in the literature. We propose a method which uses no manual annotation to classify whether end-of-lines must actually be seen as simple spaces (soft line breaks) or as true text unit boundaries. This method, which includes self-training and co-training steps based on token and line length features, achieves 0.943 F-measure on a corpus of short e-books with controlled format, F","{zweigenbaum-etal-2016-supervised,
    title = ""Supervised classification of end-of-lines in clinical text with no manual annotation"",
    author = ""Zweigenbaum, Pierre  and
      Grouin, Cyril  and
      Lavergne, Thomas"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    booktitle = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5109"",
    pages = ""80--88"",
    abstract = ""In some plain text documents, end-of-line marks may or may not mark the boundary of a text unit (e.g., of a paragraph). This vexing problem is likely to impact subsequent natural language processing components, but is seldom addressed in the literature. We propose a method which uses no manual annotation to classify whether end-of-lines must actually be seen as simple spaces (soft line breaks) or as true text unit boundaries. This method, which includes self-training and co-training steps based on token and line length features, achieves 0.943 F-measure on a corpus of short e-books with controlled format, F=0.904 on a random sample of 24 clinical texts with soft line breaks, and F=0.898 on a larger set of mixed clinical texts which may or may not contain soft line breaks, a fairly high value for a method with no manual annotation."",
}
@",clinical text,medical text,natural language process,natural languag,language process,annotat,evalu
" ""{B}io{DCA} Identifier: A System for Automatic Identification of Discourse Connective and Arguments from Biomedical Text"","," ""This paper describes a Natural language processing system developed for automatic identification of explicit connectives, its sense and arguments. Prior work has shown that the difference in usage of connectives across corpora affects the cross domain connective identification task negatively. Hence the development of domain specific discourse parser has become indispensable. Here, we present a corpus annotated with discourse relations on Medline abstracts. Kappa score is calculated to check the annotation quality of our corpus. The previous works on discourse analysis in bio-medical data have concentrated only on the identification of connectives and hence we have developed an end-end parser for connective and argument identification using Conditional Random Fields algorithm. The type and sub-type of the connective sense is also identified. The results obtained are encouraging."",","{gopalan-lalitha-devi-2016-biodca,
    title = ""{B}io{DCA} Identifier: A System for Automatic Identification of Discourse Connective and Arguments from Biomedical Text"",
    author = ""Gopalan, Sindhuja  and
      Lalitha Devi, Sobha"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    booktitle = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5110"",
    pages = ""89--98"",
    abstract = ""This paper describes a Natural language processing system developed for automatic identification of explicit connectives, its sense and arguments. Prior work has shown that the difference in usage of connectives across corpora affects the cross domain connective identification task negatively. Hence the development of domain specific discourse parser has become indispensable. Here, we present a corpus annotated with discourse relations on Medline abstracts. Kappa score is calculated to check the annotation quality of our corpus. The previous works on discourse analysis in bio-medical data have concentrated only on the identification of connectives and hence we have developed an end-end parser for connective and argument identification using Conditional Random Fields algorithm. The type and sub-type of the connective sense is also identified. The results obtained are encouraging."",
}
@",medical text,natural language process,natural languag,language process,annotat,evalu
" ""Data, tools and resources for mining social media drug chatter"","," ""Social media has emerged into a crucial resource for obtaining population-based signals for various public health monitoring and surveillance tasks, such as pharmacovigilance. There is an abundance of knowledge hidden within social media data, and the volume is growing. Drug-related chatter on social media can include user-generated information that can provide insights into public health problems such as abuse, adverse reactions, long-term effects, and multi-drug interactions. Our objective in this paper is to present to the biomedical natural language processing, data science, and public health communities data sets (annotated and unannotated), tools and resources that we have collected and created from social media. The data we present was collected from Twitter using the generic and brand names of drugs as keywords, along with their common misspellings. Following the collection of the data, annotation guidelines were created over several iterations, which detail important aspects of social media data annotation and can be used by future researchers for developing similar data sets. The annotation guidelines were followed to prepare data sets for text classification, information extraction and normalization. In this paper, we discuss the preparation of these guidelines, outline the data sets prepared, and present an overview of our state-of-the-art systems for data collection, supervised classification, and information extraction. In addition to the development of supervised systems for classification and extraction, we developed and released unlabeled data and language models. We discuss the potential uses of these language models in data mining and the large volumes of unlabeled data from which they were generated. We believe that the summaries and repositories we present here of our data, annotation guidelines, models, and tools will be beneficial to the research community as a single-point entry for all these resources, and will promote further research in this area."",","{sarker-gonzalez-2016-data,
    title = ""Data, tools and resources for mining social media drug chatter"",
    author = ""Sarker, Abeed  and
      Gonzalez, Graciela"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    booktitle = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5111"",
    pages = ""99--107"",
    abstract = ""Social media has emerged into a crucial resource for obtaining population-based signals for various public health monitoring and surveillance tasks, such as pharmacovigilance. There is an abundance of knowledge hidden within social media data, and the volume is growing. Drug-related chatter on social media can include user-generated information that can provide insights into public health problems such as abuse, adverse reactions, long-term effects, and multi-drug interactions. Our objective in this paper is to present to the biomedical natural language processing, data science, and public health communities data sets (annotated and unannotated), tools and resources that we have collected and created from social media. The data we present was collected from Twitter using the generic and brand names of drugs as keywords, along with their common misspellings. Following the collection of the data, annotation guidelines were created over several iterations, which detail important aspects of social media data annotation and can be used by future researchers for developing similar data sets. The annotation guidelines were followed to prepare data sets for text classification, information extraction and normalization. In this paper, we discuss the preparation of these guidelines, outline the data sets prepared, and present an overview of our state-of-the-art systems for data collection, supervised classification, and information extraction. In addition to the development of supervised systems for classification and extraction, we developed and released unlabeled data and language models. We discuss the potential uses of these language models in data mining and the large volumes of unlabeled data from which they were generated. We believe that the summaries and repositories we present here of our data, annotation guidelines, models, and tools will be beneficial to the research community as a single-point entry for all these resources, and will promote further research in this area."",
}
@",medical text,natural language process,natural languag,language process,generat,summar,annotat,communiti,evalu
" ""Detection of Text Reuse in {F}rench Medical Corpora"","," ""Electronic Health Records (EHRs) are increasingly available in modern health care institutions either through the direct creation of electronic documents in hospitals{'} health information systems, or through the digitization of historical paper records. Each EHR creation method yields the need for sophisticated text reuse detection tools in order to prepare the EHR collections for efficient secondary use relying on Natural Language Processing methods. Herein, we address the detection of two types of text reuse in French EHRs: 1) the detection of updated versions of the same document and 2) the detection of document duplicates that still bear surface differences due to OCR or de-identification processing. We present a robust text reuse detection method to automatically identify redundant document pairs in two French EHR corpora that achieves an overall macro F-measure of 0.68 and 0.60, respectively and correctly identifies all redundant document pairs of interest."",","{dhondt-etal-2016-detection,
    title = ""Detection of Text Reuse in {F}rench Medical Corpora"",
    author = ""D{'}hondt, Eva  and
      Grouin, Cyril  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Stamatatos, Efstathios  and
      Zweigenbaum, Pierre"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    booktitle = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5112"",
    pages = ""108--114"",
    abstract = ""Electronic Health Records (EHRs) are increasingly available in modern health care institutions either through the direct creation of electronic documents in hospitals{'} health information systems, or through the digitization of historical paper records. Each EHR creation method yields the need for sophisticated text reuse detection tools in order to prepare the EHR collections for efficient secondary use relying on Natural Language Processing methods. Herein, we address the detection of two types of text reuse in French EHRs: 1) the detection of updated versions of the same document and 2) the detection of document duplicates that still bear surface differences due to OCR or de-identification processing. We present a robust text reuse detection method to automatically identify redundant document pairs in two French EHR corpora that achieves an overall macro F-measure of 0.68 and 0.60, respectively and correctly identifies all redundant document pairs of interest."",
}
@",electronic health record,health record,medical text,natural language process,natural languag,language process,evalu
" ""Negation Detection in Clinical Reports Written in {G}erman"","," ""An important subtask in clinical text mining tries to identify whether a clinical finding is expressed as present, absent or unsure in a text. This work presents a system for detecting mentions of clinical findings that are negated or just speculated. The system has been applied to two different types of German clinical texts: clinical notes and discharge summaries. Our approach is built on top of NegEx, a well known algorithm for identifying non-factive mentions of medical findings. In this work, we adjust a previous adaptation of NegEx to German and evaluate the system on our data to detect negation and speculation. The results are compared to a baseline algorithm and are analyzed for both types of clinical documents. Our system achieves an F1-Score above 0.9 on both types of reports."",","{cotik-etal-2016-negation,
    title = ""Negation Detection in Clinical Reports Written in {G}erman"",
    author = ""Cotik, Viviana  and
      Roller, Roland  and
      Xu, Feiyu  and
      Uszkoreit, Hans  and
      Budde, Klemens  and
      Schmidt, Danilo"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    booktitle = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5113"",
    pages = ""115--124"",
    abstract = ""An important subtask in clinical text mining tries to identify whether a clinical finding is expressed as present, absent or unsure in a text. This work presents a system for detecting mentions of clinical findings that are negated or just speculated. The system has been applied to two different types of German clinical texts: clinical notes and discharge summaries. Our approach is built on top of NegEx, a well known algorithm for identifying non-factive mentions of medical findings. In this work, we adjust a previous adaptation of NegEx to German and evaluate the system on our data to detect negation and speculation. The results are compared to a baseline algorithm and are analyzed for both types of clinical documents. Our system achieves an F1-Score above 0.9 on both types of reports."",
}
@",discharge summar,clinical not,clinical text,medical text,summar,evalu
" ""Scoring Disease-Medication Associations using Advanced {NLP}, Machine Learning, and Multiple Content Sources"","," ""Effective knowledge resources are critical for developing successful clinical decision support systems that alleviate the cognitive load on physicians in patient care. In this paper, we describe two new methods for building a knowledge resource of disease to medication associations. These methods use fundamentally different content and are based on advanced natural language processing and machine learning techniques. One method uses distributional semantics on large medical text, and the other uses data mining on a large number of patient records. The methods are evaluated using 25,379 unique disease-medication pairs extracted from 100 de-identified longitudinal patient records of a large multi-provider hospital system. We measured recall (R), precision (P), and F scores for positive and negative association prediction, along with coverage and accuracy. While individual methods performed well, a combined stacked classifier achieved the best performance, indicating the limitations and unique value of each resource and method. In predicting positive associations, the stacked combination significantly outperformed the baseline (a distant semi-supervised method on large medical text), achieving F scores of 0.75 versus 0.55 on the pairs seen in the patient records, and F scores of 0.69 and 0.35 on unique pairs."",","{dandala-etal-2016-scoring,
    title = ""Scoring Disease-Medication Associations using Advanced {NLP}, Machine Learning, and Multiple Content Sources"",
    author = ""Dandala, Bharath  and
      Devarakonda, Murthy  and
      Bornea, Mihaela  and
      Nielson, Christopher"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    booktitle = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5114"",
    pages = ""125--133"",
    abstract = ""Effective knowledge resources are critical for developing successful clinical decision support systems that alleviate the cognitive load on physicians in patient care. In this paper, we describe two new methods for building a knowledge resource of disease to medication associations. These methods use fundamentally different content and are based on advanced natural language processing and machine learning techniques. One method uses distributional semantics on large medical text, and the other uses data mining on a large number of patient records. The methods are evaluated using 25,379 unique disease-medication pairs extracted from 100 de-identified longitudinal patient records of a large multi-provider hospital system. We measured recall (R), precision (P), and F scores for positive and negative association prediction, along with coverage and accuracy. While individual methods performed well, a combined stacked classifier achieved the best performance, indicating the limitations and unique value of each resource and method. In predicting positive associations, the stacked combination significantly outperformed the baseline (a distant semi-supervised method on large medical text), achieving F scores of 0.75 versus 0.55 on the pairs seen in the patient records, and F scores of 0.69 and 0.35 on unique pairs."",
}
@",medical text,natural language process,natural languag,language process,nlp,semant,evalu
" ""Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics ({E}x{P}ro{M})"","," ""Author name disambiguation (AND) in publication and citation resources is a well-known problem. Often, information about email address and other details in the affiliation is missing. In cases where such information is not available, identifying the authorship of publications becomes very challenging. Consequently, there have been attempts to resolve such cases by utilizing external resources as references. However, such external resources are heterogeneous and are not always reliable regarding the correctness of information. To solve the AND task, especially when information about an author is not complete we suggest the use of new features such as journal descriptors (JD) and semantic types (ST). The evaluation of different feature models shows that their inclusion has an impact equivalent to that of other important features such as email address. Using such features we show that our system outperforms the state of the art."",","{vishnyakova-etal-2016-author,
    title = ""Author Name Disambiguation in {MEDLINE} Based on Journal Descriptors and Semantic Types"",
    author = ""Vishnyakova, Dina  and
      Rodriguez-Esteban, Raul  and
      Ozol, Khan  and
      Rinaldi, Fabio"",
    editor = ""Ananiadou, Sophia  and
      Batista-Navarro, Riza  and
      Cohen, Kevin Bretonnel  and
      Demner-Fushman, Dina  and
      Thompson, Paul"",
    booktitle = ""Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016)"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5115"",
    pages = ""134--142"",
    abstract = ""Author name disambiguation (AND) in publication and citation resources is a well-known problem. Often, information about email address and other details in the affiliation is missing. In cases where such information is not available, identifying the authorship of publications becomes very challenging. Consequently, there have been attempts to resolve such cases by utilizing external resources as references. However, such external resources are heterogeneous and are not always reliable regarding the correctness of information. To solve the AND task, especially when information about an author is not complete we suggest the use of new features such as journal descriptors (JD) and semantic types (ST). The evaluation of different feature models shows that their inclusion has an impact equivalent to that of other important features such as email address. Using such features we show that our system outperforms the state of the art."",
}
@proceedings{ws-2016-extra,
    title = ""Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics ({E}x{P}ro{M})"",
    editor = ""Blanco, Eduardo  and
      Morante, Roser  and
      Saur{\'\i}, Roser"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-5000"",
}
@",medical text,semant,challeng,evalu
" ""Bidirectional {LSTM}-{CRF} for Clinical Concept Extraction"","," ""Automated extraction of concepts from patient clinical records is an essential facilitator of clinical research. For this reason, the 2010 i2b2/VA Natural Language Processing Challenges for Clinical Records introduced a concept extraction task aimed at identifying and classifying concepts into predefined categories (i.e., treatments, tests and problems). State-of-the-art concept extraction approaches heavily rely on handcrafted features and domain-specific resources which are hard to collect and define. For this reason, this paper proposes an alternative, streamlined approach: a recurrent neural network (the bidirectional LSTM with CRF decoding) initialized with general-purpose, off-the-shelf word embeddings. The experimental results achieved on the 2010 i2b2/VA reference corpora using the proposed framework outperform all recent methods and ranks closely to the best submission from the original 2010 i2b2/VA challenge."",","{chalapathy-etal-2016-bidirectional,
    title = ""Bidirectional {LSTM}-{CRF} for Clinical Concept Extraction"",
    author = ""Chalapathy, Raghavendra  and
      Zare Borzeshi, Ehsan  and
      Piccardi, Massimo"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the Clinical Natural Language Processing Workshop ({C}linical{NLP})"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-4202"",
    pages = ""7--12"",
    abstract = ""Automated extraction of concepts from patient clinical records is an essential facilitator of clinical research. For this reason, the 2010 i2b2/VA Natural Language Processing Challenges for Clinical Records introduced a concept extraction task aimed at identifying and classifying concepts into predefined categories (i.e., treatments, tests and problems). State-of-the-art concept extraction approaches heavily rely on handcrafted features and domain-specific resources which are hard to collect and define. For this reason, this paper proposes an alternative, streamlined approach: a recurrent neural network (the bidirectional LSTM with CRF decoding) initialized with general-purpose, off-the-shelf word embeddings. The experimental results achieved on the 2010 i2b2/VA reference corpora using the proposed framework outperform all recent methods and ranks closely to the best submission from the original 2010 i2b2/VA challenge."",
}
@",clinical record,natural language process,natural languag,language process,nlp,challeng
" ""{M}ed{NLPD}oc: {J}apanese Shared Task for Clinical {NLP}"","," ""Due to the recent replacements of physical documents with electronic medical records (EMR), the importance of information processing in medical fields has been increased. We have been organizing the MedNLP task series in NTCIR-10 and 11. These workshops were the first shared tasks which attempt to evaluate technologies that retrieve important information from medical reports written in Japanese. In this report, we describe the NTCIR-12 MedNLPDoc task which is designed for more advanced and practical use for the medical fields. This task is considered as a multi-labeling task to a patient record. This report presents results of the shared task, discusses and illustrates remained issues in the medical natural language processing field."",","{aramaki-etal-2016-mednlpdoc,
    title = ""{M}ed{NLPD}oc: {J}apanese Shared Task for Clinical {NLP}"",
    author = ""Aramaki, Eiji  and
      Kano, Yoshinobu  and
      Ohkuma, Tomoko  and
      Morita, Mizuki"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the Clinical Natural Language Processing Workshop ({C}linical{NLP})"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-4203"",
    pages = ""13--16"",
    abstract = ""Due to the recent replacements of physical documents with electronic medical records (EMR), the importance of information processing in medical fields has been increased. We have been organizing the MedNLP task series in NTCIR-10 and 11. These workshops were the first shared tasks which attempt to evaluate technologies that retrieve important information from medical reports written in Japanese. In this report, we describe the NTCIR-12 MedNLPDoc task which is designed for more advanced and practical use for the medical fields. This task is considered as a multi-labeling task to a patient record. This report presents results of the shared task, discusses and illustrates remained issues in the medical natural language processing field."",
}
@",medical record,natural language process,natural languag,language process,nlp,shared task,evalu
" ""Semi-supervised Clustering of Medical Text"","," ""Semi-supervised clustering is an attractive alternative for traditional (unsupervised) clustering in targeted applications. By using the information of a small annotated dataset, semi-supervised clustering can produce clusters that are customized to the application domain. In this paper, we present a semi-supervised clustering technique based on a multi-objective evolutionary algorithm (NSGA-II-clus). We apply this technique to the task of clustering medical publications for Evidence Based Medicine (EBM) and observe an improvement of the results against unsupervised and other semi-supervised clustering techniques."",","{sahoo-etal-2016-semi,
    title = ""Semi-supervised Clustering of Medical Text"",
    author = ""Sahoo, Pracheta  and
      Ekbal, Asif  and
      Saha, Sriparna  and
      Moll{\'a}, Diego  and
      Nandan, Kaushik"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the Clinical Natural Language Processing Workshop ({C}linical{NLP})"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-4205"",
    pages = ""23--31"",
    abstract = ""Semi-supervised clustering is an attractive alternative for traditional (unsupervised) clustering in targeted applications. By using the information of a small annotated dataset, semi-supervised clustering can produce clusters that are customized to the application domain. In this paper, we present a semi-supervised clustering technique based on a multi-objective evolutionary algorithm (NSGA-II-clus). We apply this technique to the task of clustering medical publications for Evidence Based Medicine (EBM) and observe an improvement of the results against unsupervised and other semi-supervised clustering techniques."",
}
@",medical text,natural language process,natural languag,language process,nlp,annotat
" ""Deep Learning Architecture for Patient Data De-identification in Clinical Records"","," ""Rapid growth in Electronic Medical Records (EMR) has emerged to an expansion of data in the clinical domain. The majority of the available health care information is sealed in the form of narrative documents which form the rich source of clinical information. Text mining of such clinical records has gained huge attention in various medical applications like treatment and decision making. However, medical records enclose patient Private Health Information (PHI) which can reveal the identities of the patients. In order to retain the privacy of patients, it is mandatory to remove all the PHI information prior to making it publicly available. The aim is to de-identify or encrypt the PHI from the patient medical records. In this paper, we propose an algorithm based on deep learning architecture to solve this problem. We perform de-identification of seven PHI terms from the clinical records. Experiments on benchmark datasets show that our proposed approach achieves encouraging performance, which is better than the baseline model developed with Conditional Random Field."",","{yadav-etal-2016-deep,
    title = ""Deep Learning Architecture for Patient Data De-identification in Clinical Records"",
    author = ""Yadav, Shweta  and
      Ekbal, Asif  and
      Saha, Sriparna  and
      Bhattacharyya, Pushpak"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the Clinical Natural Language Processing Workshop ({C}linical{NLP})"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-4206"",
    pages = ""32--41"",
    abstract = ""Rapid growth in Electronic Medical Records (EMR) has emerged to an expansion of data in the clinical domain. The majority of the available health care information is sealed in the form of narrative documents which form the rich source of clinical information. Text mining of such clinical records has gained huge attention in various medical applications like treatment and decision making. However, medical records enclose patient Private Health Information (PHI) which can reveal the identities of the patients. In order to retain the privacy of patients, it is mandatory to remove all the PHI information prior to making it publicly available. The aim is to de-identify or encrypt the PHI from the patient medical records. In this paper, we propose an algorithm based on deep learning architecture to solve this problem. We perform de-identification of seven PHI terms from the clinical records. Experiments on benchmark datasets show that our proposed approach achieves encouraging performance, which is better than the baseline model developed with Conditional Random Field."",
}
@",medical record,clinical domain,clinical record,natural language process,natural languag,language process,nlp,benchmark
" ""Inference of {ICD} Codes from {J}apanese Medical Records by Searching Disease Names"","," ""Importance of utilizing medical information is getting increased as electronic health records (EHRs) are widely used nowadays. We aim to assign international standardized disease codes, ICD-10, to Japanese textual information in EHRs for users to reuse the information accurately. In this paper, we propose methods to automatically extract diagnosis and to assign ICD codes to Japanese medical records. Due to the lack of available training data, we dare employed rule-based methods rather than machine learning. We observed characteristics of medical records carefully, writing rules to make effective methods by hand. We applied our system to the NTCIR-12 MedNLPDoc shared task data where participants are required to assign ICD-10 codes of possible diagnosis in given EHRs. In this shared task, our system achieved the highest F-measure score among all participants in the most severe evaluation criteria. Through comparison with other approaches, we show that our approach could be a useful milestone for the future development of Japanese medical record processing."",","{sakishita-kano-2016-inference,
    title = ""Inference of {ICD} Codes from {J}apanese Medical Records by Searching Disease Names"",
    author = ""Sakishita, Masahito  and
      Kano, Yoshinobu"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the Clinical Natural Language Processing Workshop ({C}linical{NLP})"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-4209"",
    pages = ""64--68"",
    abstract = ""Importance of utilizing medical information is getting increased as electronic health records (EHRs) are widely used nowadays. We aim to assign international standardized disease codes, ICD-10, to Japanese textual information in EHRs for users to reuse the information accurately. In this paper, we propose methods to automatically extract diagnosis and to assign ICD codes to Japanese medical records. Due to the lack of available training data, we dare employed rule-based methods rather than machine learning. We observed characteristics of medical records carefully, writing rules to make effective methods by hand. We applied our system to the NTCIR-12 MedNLPDoc shared task data where participants are required to assign ICD-10 codes of possible diagnosis in given EHRs. In this shared task, our system achieved the highest F-measure score among all participants in the most severe evaluation criteria. Through comparison with other approaches, we show that our approach could be a useful milestone for the future development of Japanese medical record processing."",
}
@",electronic health record,health record,medical record,natural language process,natural languag,language process,nlp,infer,shared task,evalu
" ""Unsupervised Abbreviation Detection in Clinical Narratives"","," ""Clinical narratives in electronic health record systems are a rich resource of patient-based information. They constitute an ongoing challenge for natural language processing, due to their high compactness and abundance of short forms. German medical texts exhibit numerous ad-hoc abbreviations that terminate with a period character. The disambiguation of period characters is therefore an important task for sentence and abbreviation detection. This task is addressed by a combination of co-occurrence information of word types with trailing period characters, a large domain dictionary, and a simple rule engine, thus merging statistical and dictionary-based disambiguation strategies. An F-measure of 0.95 could be reached by using the unsupervised approach presented in this paper. The results are promising for a domain-independent abbreviation detection strategy, because our approach avoids retraining of models or use case specific feature engineering efforts required for supervised machine learning approaches."",","{kreuzthaler-etal-2016-unsupervised,
    title = ""Unsupervised Abbreviation Detection in Clinical Narratives"",
    author = ""Kreuzthaler, Markus  and
      Oleynik, Michel  and
      Avian, Alexander  and
      Schulz, Stefan"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the Clinical Natural Language Processing Workshop ({C}linical{NLP})"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-4213"",
    pages = ""91--98"",
    abstract = ""Clinical narratives in electronic health record systems are a rich resource of patient-based information. They constitute an ongoing challenge for natural language processing, due to their high compactness and abundance of short forms. German medical texts exhibit numerous ad-hoc abbreviations that terminate with a period character. The disambiguation of period characters is therefore an important task for sentence and abbreviation detection. This task is addressed by a combination of co-occurrence information of word types with trailing period characters, a large domain dictionary, and a simple rule engine, thus merging statistical and dictionary-based disambiguation strategies. An F-measure of 0.95 could be reached by using the unsupervised approach presented in this paper. The results are promising for a domain-independent abbreviation detection strategy, because our approach avoids retraining of models or use case specific feature engineering efforts required for supervised machine learning approaches."",
}
@",electronic health record,health record,clinical narr,medical text,natural language process,natural languag,language process,nlp,challeng
" ""Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity ({CL}4{LC})"","," ""The issue of privacy has always been a concern when clinical texts are used for research purposes. Personal health information (PHI) (such as name and identification number) needs to be removed so that patients cannot be identified. Manual anonymization is not feasible due to the large number of clinical texts to be anonymized. In this paper, we tackle the task of anonymizing clinical texts written in sentence fragments and which frequently contain symbols, abbreviations, and misspelled words. Our clinical texts therefore differ from those in the i2b2 shared tasks which are in prose form with complete sentences. Our clinical texts are also part of a structured database which contains patient name and identification number in structured fields. As such, we formulate our anonymization task as spelling variant detection, exploiting patients{'} personal information in the structured fields to detect their spelling variants in clinical texts. We successfully anonymized clinical texts consisting of more than 200 million words, using minimum edit distance and regular expression patterns."",","{yuwono-etal-2016-automated,
    title = ""Automated Anonymization as Spelling Variant Detection"",
    author = ""Yuwono, Steven Kester  and
      Ng, Hwee Tou  and
      Ngiam, Kee Yuan"",
    editor = ""Rumshisky, Anna  and
      Roberts, Kirk  and
      Bethard, Steven  and
      Naumann, Tristan"",
    booktitle = ""Proceedings of the Clinical Natural Language Processing Workshop ({C}linical{NLP})"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-4214"",
    pages = ""99--103"",
    abstract = ""The issue of privacy has always been a concern when clinical texts are used for research purposes. Personal health information (PHI) (such as name and identification number) needs to be removed so that patients cannot be identified. Manual anonymization is not feasible due to the large number of clinical texts to be anonymized. In this paper, we tackle the task of anonymizing clinical texts written in sentence fragments and which frequently contain symbols, abbreviations, and misspelled words. Our clinical texts therefore differ from those in the i2b2 shared tasks which are in prose form with complete sentences. Our clinical texts are also part of a structured database which contains patient name and identification number in structured fields. As such, we formulate our anonymization task as spelling variant detection, exploiting patients{'} personal information in the structured fields to detect their spelling variants in clinical texts. We successfully anonymized clinical texts consisting of more than 200 million words, using minimum edit distance and regular expression patterns."",
}
@proceedings{ws-2016-linguistics-linguistic,
    title = ""Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity ({CL}4{LC})"",
    editor = ""Brunato, Dominique  and
      Dell{'}Orletta, Felice  and
      Venturi, Giulia  and
      Fran{\c{c}}ois, Thomas  and
      Blache, Philippe"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/W16-4100"",
}
@",clinical text,natural language process,natural languag,language process,nlp,shared task
" ""Hitachi at {S}em{E}val-2016 Task 12: A Hybrid Approach for Temporal Information Extraction from Clinical Notes"",",,"{p-r-etal-2016-hitachi,
    title = ""Hitachi at {S}em{E}val-2016 Task 12: A Hybrid Approach for Temporal Information Extraction from Clinical Notes"",
    author = ""P R, Sarath  and
      R, Manikandan  and
      Niwa, Yoshiki"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Cer, Daniel  and
      Jurgens, David  and
      Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)"",
    month = jun,
    year = ""2016"",
    address = ""San Diego, California"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S16-1191"",
    doi = ""10.18653/v1/S16-1191"",
    pages = ""1231--1236"",
}
@",clinical not,semant,evalu
" ""{GUIR} at {S}em{E}val-2016 task 12: Temporal Information Processing for Clinical Narratives"",",,"{cohan-etal-2016-guir,
    title = ""{GUIR} at {S}em{E}val-2016 task 12: Temporal Information Processing for Clinical Narratives"",
    author = ""Cohan, Arman  and
      Meurer, Kevin  and
      Goharian, Nazli"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Cer, Daniel  and
      Jurgens, David  and
      Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)"",
    month = jun,
    year = ""2016"",
    address = ""San Diego, California"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S16-1194"",
    doi = ""10.18653/v1/S16-1194"",
    pages = ""1248--1255"",
}
@",clinical narr,semant,evalu
" ""{U}tah{BMI} at {S}em{E}val-2016 Task 12: Extracting Temporal Information from Clinical Text"",",,"{khalifa-etal-2016-utahbmi,
    title = ""{U}tah{BMI} at {S}em{E}val-2016 Task 12: Extracting Temporal Information from Clinical Text"",
    author = ""Khalifa, Abdulrahman  and
      Velupillai, Sumithra  and
      Meystre, Stephane"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Cer, Daniel  and
      Jurgens, David  and
      Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)"",
    month = jun,
    year = ""2016"",
    address = ""San Diego, California"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S16-1195"",
    doi = ""10.18653/v1/S16-1195"",
    pages = ""1256--1262"",
}
@",clinical text,semant,evalu
" ""{UTA} {DLNLP} at {S}em{E}val-2016 Task 12: Deep Learning Based Natural Language Processing System for Clinical Information Identification from Clinical Notes and Pathology Reports"",",,"{li-huang-2016-uta-dlnlp,
    title = ""{UTA} {DLNLP} at {S}em{E}val-2016 Task 12: Deep Learning Based Natural Language Processing System for Clinical Information Identification from Clinical Notes and Pathology Reports"",
    author = ""Li, Peng  and
      Huang, Heng"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Cer, Daniel  and
      Jurgens, David  and
      Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)"",
    month = jun,
    year = ""2016"",
    address = ""San Diego, California"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S16-1197"",
    doi = ""10.18653/v1/S16-1197"",
    pages = ""1268--1273"",
}
@",clinical not,natural language process,natural languag,language process,nlp,semant,evalu
" ""{KUL}euven-{LIIR} at {S}em{E}val 2016 Task 12: Detecting Narrative Containment in Clinical Records"",",,"{leeuwenberg-moens-2016-kuleuven,
    title = ""{KUL}euven-{LIIR} at {S}em{E}val 2016 Task 12: Detecting Narrative Containment in Clinical Records"",
    author = ""Leeuwenberg, Artuur  and
      Moens, Marie-Francine"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Cer, Daniel  and
      Jurgens, David  and
      Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)"",
    month = jun,
    year = ""2016"",
    address = ""San Diego, California"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S16-1199"",
    doi = ""10.18653/v1/S16-1199"",
    pages = ""1280--1285"",
}
@",clinical record,semant,evalu
" ""{UTH}ealth at {S}em{E}val-2016 Task 12: an End-to-End System for Temporal Information Extraction from Clinical Notes"",",,"{lee-etal-2016-uthealth,
    title = ""{UTH}ealth at {S}em{E}val-2016 Task 12: an End-to-End System for Temporal Information Extraction from Clinical Notes"",
    author = ""Lee, Hee-Jin  and
      Xu, Hua  and
      Wang, Jingqi  and
      Zhang, Yaoyun  and
      Moon, Sungrim  and
      Xu, Jun  and
      Wu, Yonghui"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Cer, Daniel  and
      Jurgens, David  and
      Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)"",
    month = jun,
    year = ""2016"",
    address = ""San Diego, California"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S16-1201"",
    doi = ""10.18653/v1/S16-1201"",
    pages = ""1292--1297"",
}
@",clinical not,semant,evalu
" ""Utilizing Temporal Information for Taxonomy Construction"","," ""Taxonomies play an important role in many applications by organizing domain knowledge into a hierarchy of {`}is-a{'} relations between terms. Previous work on automatic construction of taxonomies from text documents either ignored temporal information or used fixed time periods to discretize the time series of documents. In this paper, we propose a time-aware method to automatically construct and effectively maintain a taxonomy from a given series of documents preclustered for a domain of interest. The method extracts temporal information from the documents and uses a timestamp contribution function to score the temporal relevance of the evidence from source texts when identifying the taxonomic relations for constructing the taxonomy. Experimental results show that our proposed method outperforms the state-of-the-art methods by increasing F-measure up to 7{\%}{--}20{\%}. Furthermore, the proposed method can incrementally update the taxonomy by adding fresh relations from new data and removing outdated relations using an information decay function. It thus avoids rebuilding the whole taxonomy from scratch for every update and keeps the taxonomy effectively up-to-date in order to track the latest information trends in the rapidly evolving domain."",","{mcinnes-2016-vcu,
    title = ""{VCU} at {S}emeval-2016 Task 14: Evaluating definitional-based similarity measure for semantic taxonomy enrichment"",
    author = ""McInnes, Bridget"",
    editor = ""Bethard, Steven  and
      Carpuat, Marine  and
      Cer, Daniel  and
      Jurgens, David  and
      Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)"",
    month = jun,
    year = ""2016"",
    address = ""San Diego, California"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S16-1212"",
    doi = ""10.18653/v1/S16-1212"",
    pages = ""1351--1355"",
}
@proceedings{tacl-2016-transactions,
    title = ""Transactions of the Association for Computational Linguistics, Volume 4"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1000"",
}
@article{faruqui-etal-2016-morpho,
    title = ""Morpho-syntactic Lexicon Generation Using Graph-based Semi-supervised Learning"",
    author = ""Faruqui, Manaal  and
      McDonald, Ryan  and
      Soricut, Radu"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1001"",
    doi = ""10.1162/tacl_a_00079"",
    pages = ""1--16"",
    abstract = ""Morpho-syntactic lexicons provide information about the morphological and syntactic roles of words in a language. Such lexicons are not available for all languages and even when available, their coverage can be limited. We present a graph-based semi-supervised learning method that uses the morphological, syntactic and semantic relations between words to automatically construct wide coverage lexicons from small seed sets. Our method is language-independent, and we show that we can expand a 1000 word seed lexicon to more than 100 times its size with high quality for 11 languages. In addition, the automatically created lexicons provide features that improve performance in two downstream tasks: morphological tagging and dependency parsing."",
}
@article{hill-etal-2016-learning-understand,
    title = ""Learning to Understand Phrases by Embedding the Dictionary"",
    author = ""Hill, Felix  and
      Cho, Kyunghyun  and
      Korhonen, Anna  and
      Bengio, Yoshua"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1002"",
    doi = ""10.1162/tacl_a_00080"",
    pages = ""17--30"",
    abstract = ""Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal semantics. Neural language embedding models can be effectively trained to map dictionary definitions (phrases) to (lexical) representations of the words defined by those definitions. We present two applications of these architectures: reverse dictionaries that return the name of a concept given a definition or description and general-knowledge crossword question answerers. On both tasks, neural language embedding models trained on definitions from a handful of freely-available lexical resources perform as well or better than existing commercial systems that rely on significant task-specific engineering. The results highlight the effectiveness of both neural embedding architectures and definition-based training for developing models that understand phrases and sentences."",
}
@article{frermann-lapata-2016-bayesian,
    title = ""A {B}ayesian Model of Diachronic Meaning Change"",
    author = ""Frermann, Lea  and
      Lapata, Mirella"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1003"",
    doi = ""10.1162/tacl_a_00081"",
    pages = ""31--45"",
    abstract = ""Word meanings change over time and an automated procedure for extracting this information from text would be useful for historical exploratory studies, information retrieval or question answering. We present a dynamic Bayesian model of diachronic meaning change, which infers temporal word representations as a set of senses and their prevalence. Unlike previous work, we explicitly model language change as a smooth, gradual process. We experimentally show that this modeling decision is beneficial: our model performs competitively on meaning change detection tasks whilst inducing discernible word senses and their development over time. Application of our model to the SemEval-2015 temporal classification benchmark datasets further reveals that it performs on par with highly optimized task-specific systems."",
}
@article{gutierrez-etal-2016-detecting,
    title = ""Detecting Cross-Cultural Differences Using a Multilingual Topic Model"",
    author = ""Guti{\'e}rrez, E.D.  and
      Shutova, Ekaterina  and
      Lichtenstein, Patricia  and
      de Melo, Gerard  and
      Gilardi, Luca"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1004"",
    doi = ""10.1162/tacl_a_00082"",
    pages = ""47--60"",
    abstract = ""Understanding cross-cultural differences has important implications for world affairs and many aspects of the life of society. Yet, the majority of text-mining methods to date focus on the analysis of monolingual texts. In contrast, we present a statistical model that simultaneously learns a set of common topics from multilingual, non-parallel data and automatically discovers the differences in perspectives on these topics across linguistic communities. We perform a behavioural evaluation of a subset of the differences identified by our model in English and Spanish to investigate their psychological validity."",
}
@article{pavlick-tetreault-2016-empirical,
    title = ""An Empirical Analysis of Formality in Online Communication"",
    author = ""Pavlick, Ellie  and
      Tetreault, Joel"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1005"",
    doi = ""10.1162/tacl_a_00083"",
    pages = ""61--74"",
    abstract = ""This paper presents an empirical study of linguistic formality. We perform an analysis of humans{'} perceptions of formality in four different genres. These findings are used to develop a statistical model for predicting formality, which is evaluated under different feature settings and genres. We apply our model to an investigation of formality in online discussion forums, and present findings consistent with theories of formality and linguistic coordination."",
}
@article{hauer-kondrak-2016-decoding,
    title = ""Decoding Anagrammed Texts Written in an Unknown Language and Script"",
    author = ""Hauer, Bradley  and
      Kondrak, Grzegorz"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1006"",
    doi = ""10.1162/tacl_a_00084"",
    pages = ""75--86"",
    abstract = ""Algorithmic decipherment is a prime example of a truly unsupervised problem. The first step in the decipherment process is the identification of the encrypted language. We propose three methods for determining the source language of a document enciphered with a monoalphabetic substitution cipher. The best method achieves 97{\%} accuracy on 380 languages. We then present an approach to decoding anagrammed substitution ciphers, in which the letters within words have been arbitrarily transposed. It obtains the average decryption word accuracy of 93{\%} on a set of 50 ciphertexts in 5 languages. Finally, we report the results on the Voynich manuscript, an unsolved fifteenth century cipher, which suggest Hebrew as the language of the document."",
}
@article{jardine-heinz-2016-learning,
    title = ""Learning Tier-based Strictly 2-Local Languages"",
    author = ""Jardine, Adam  and
      Heinz, Jeffrey"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1007"",
    doi = ""10.1162/tacl_a_00085"",
    pages = ""87--98"",
    abstract = ""The Tier-based Strictly 2-Local (TSL2) languages are a class of formal languages which have been shown to model long-distance phonotactic generalizations in natural language (Heinz et al., 2011). This paper introduces the Tier-based Strictly 2-Local Inference Algorithm (2TSLIA), the first nonenumerative learner for the TSL2 languages. We prove the 2TSLIA is guaranteed to converge in polynomial time on a data sample whose size is bounded by a constant."",
}
@article{cuong-etal-2016-adapting,
    title = ""Adapting to All Domains at Once: Rewarding Domain Invariance in {SMT}"",
    author = ""Cuong, Hoang  and
      Sima{'}an, Khalil  and
      Titov, Ivan"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1008"",
    doi = ""10.1162/tacl_a_00086"",
    pages = ""99--112"",
    abstract = ""Existing work on domain adaptation for statistical machine translation has consistently assumed access to a small sample from the test distribution (target domain) at training time. In practice, however, the target domain may not be known at training time or it may change to match user needs. In such situations, it is natural to push the system to make safer choices, giving higher preference to domain-invariant translations, which work well across domains, over risky domain-specific alternatives. We encode this intuition by (1) inducing latent subdomains from the training data only; (2) introducing features which measure how specialized phrases are to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance."",
}
@article{sultan-etal-2016-joint,
    title = ""A Joint Model for Answer Sentence Ranking and Answer Extraction"",
    author = ""Sultan, Md Arafat  and
      Castelli, Vittorio  and
      Florian, Radu"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1009"",
    doi = ""10.1162/tacl_a_00087"",
    pages = ""113--125"",
    abstract = ""Answer sentence ranking and answer extraction are two key challenges in question answering that have traditionally been treated in isolation, i.e., as independent tasks. In this article, we (1) explain how both tasks are related at their core by a common quantity, and (2) propose a simple and intuitive joint probabilistic model that addresses both via joint computation but task-specific application of that quantity. In our experiments with two TREC datasets, our joint model substantially outperforms state-of-the-art systems in both tasks."",
}
@article{reddy-etal-2016-transforming,
    title = ""Transforming Dependency Structures to Logical Forms for Semantic Parsing"",
    author = {Reddy, Siva  and
      T{\""a}ckstr{\""o}m, Oscar  and
      Collins, Michael  and
      Kwiatkowski, Tom  and
      Das, Dipanjan  and
      Steedman, Mark  and
      Lapata, Mirella},
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1010"",
    doi = ""10.1162/tacl_a_00088"",
    pages = ""127--140"",
    abstract = ""The strongly typed syntax of grammar formalisms such as CCG, TAG, LFG and HPSG offers a synchronous framework for deriving syntactic structures and semantic logical forms. In contrast{---}partly due to the lack of a strong type system{---}dependency structures are easy to annotate and have become a widely used form of syntactic analysis for many languages. However, the lack of a type system makes a formal mechanism for deriving logical forms from dependency structures challenging. We address this by introducing a robust system based on the lambda calculus for deriving neo-Davidsonian logical forms from dependency trees. These logical forms are then used for semantic parsing of natural language to Freebase. Experiments on the Free917 and Web-Questions datasets show that our representation is superior to the original dependency trees and that it outperforms a CCG-based representation on this task. Compared to prior work, we obtain the strongest result to date on Free917 and competitive results on WebQuestions."",
}
@article{tsai-roth-2016-concept,
    title = ""Concept Grounding to Multiple Knowledge Bases via Indirect Supervision"",
    author = ""Tsai, Chen-Tse  and
      Roth, Dan"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1011"",
    doi = ""10.1162/tacl_a_00089"",
    pages = ""141--154"",
    abstract = ""We consider the problem of disambiguating concept mentions appearing in documents and grounding them in multiple knowledge bases, where each knowledge base addresses some aspects of the domain. This problem poses a few additional challenges beyond those addressed in the popular Wikification problem. Key among them is that most knowledge bases do not contain the rich textual and structural information Wikipedia does; consequently, the main supervision signal used to train Wikification rankers does not exist anymore. In this work we develop an algorithmic approach that, by carefully examining the relations between various related knowledge bases, generates an indirect supervision signal it uses to train a ranking model that accurately chooses knowledge base entries for a given mention; moreover, it also induces prior knowledge that can be used to support a global coherent mapping of all the concepts in a given document to the knowledge bases. Using the biomedical domain as our application, we show that our indirectly supervised ranking model outperforms other unsupervised baselines and that the quality of this indirect supervision scheme is very close to a supervised model. We also show that considering multiple knowledge bases together has an advantage over grounding concepts to each knowledge base individually."",
}
@article{richardson-kuhn-2016-learning,
    title = ""Learning to Make Inferences in a Semantic Parsing Task"",
    author = ""Richardson, Kyle  and
      Kuhn, Jonas"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1012"",
    doi = ""10.1162/tacl_a_00090"",
    pages = ""155--168"",
    abstract = ""We introduce a new approach to training a semantic parser that uses textual entailment judgements as supervision. These judgements are based on high-level inferences about whether the meaning of one sentence follows from another. When applied to an existing semantic parsing task, they prove to be a useful tool for revealing semantic distinctions and background knowledge not captured in the target representations. This information is used to improve the quality of the semantic representations being learned and to acquire generic knowledge for reasoning. Experiments are done on the benchmark Sportscaster corpus (Chen and Mooney, 2008), and a novel RTE-inspired inference dataset is introduced. On this new dataset our method strongly outperforms several strong baselines. Separately, we obtain state-of-the-art results on the original Sportscaster semantic parsing task."",
}
@article{sakaguchi-etal-2016-reassessing,
    title = ""Reassessing the Goals of Grammatical Error Correction: Fluency Instead of Grammaticality"",
    author = ""Sakaguchi, Keisuke  and
      Napoles, Courtney  and
      Post, Matt  and
      Tetreault, Joel"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1013"",
    doi = ""10.1162/tacl_a_00091"",
    pages = ""169--182"",
    abstract = ""The field of grammatical error correction (GEC) has grown substantially in recent years, with research directed at both evaluation metrics and improved system performance against those metrics. One unvisited assumption, however, is the reliance of GEC evaluation on error-coded corpora, which contain specific labeled corrections. We examine current practices and show that GEC{'}s reliance on such corpora unnaturally constrains annotation and automatic evaluation, resulting in (a) sentences that do not sound acceptable to native speakers and (b) system rankings that do not correlate with human judgments. In light of this, we propose an alternate approach that jettisons costly error coding in favor of unannotated, whole-sentence rewrites. We compare the performance of existing metrics over different gold-standard annotations, and show that automatic evaluation with our new annotation scheme has very strong correlation with expert rankings (ρ = 0.82). As a result, we advocate for a fundamental and necessary shift in the goal of GEC, from correcting small, labeled error types, to producing text that has native fluency."",
}
@article{vaswani-sagae-2016-efficient,
    title = ""Efficient Structured Inference for Transition-Based Parsing with Neural Networks and Error States"",
    author = ""Vaswani, Ashish  and
      Sagae, Kenji"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1014"",
    doi = ""10.1162/tacl_a_00092"",
    pages = ""183--196"",
    abstract = ""Transition-based approaches based on local classification are attractive for dependency parsing due to their simplicity and speed, despite producing results slightly below the state-of-the-art. In this paper, we propose a new approach for approximate structured inference for transition-based parsing that produces scores suitable for global scoring using local models. This is accomplished with the introduction of error states in local training, which add information about incorrect derivation paths typically left out completely in locally-trained models. Using neural networks for our local classifiers, our approach achieves 93.61{\%} accuracy for transition-based dependency parsing in English."",
}
@article{hartmann-etal-2016-generating,
    title = ""Generating Training Data for Semantic Role Labeling based on Label Transfer from Linked Lexical Resources"",
    author = ""Hartmann, Silvana  and
      Eckle-Kohler, Judith  and
      Gurevych, Iryna"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1015"",
    doi = ""10.1162/tacl_a_00093"",
    pages = ""197--213"",
    abstract = ""We present a new approach for generating role-labeled training data using Linked Lexical Resources, i.e., integrated lexical resources that combine several resources (e.g., Word-Net, FrameNet, Wiktionary) by linking them on the sense or on the role level. Unlike resource-based supervision in relation extraction, we focus on complex linguistic annotations, more specifically FrameNet senses and roles. The automatically labeled training data (www.ukp.tu-darmstadt.de/knowledge-based-srl/) are evaluated on four corpora from different domains for the tasks of word sense disambiguation and semantic role classification. Results show that classifiers trained on our generated data equal those resulting from a standard supervised setting."",
}
@article{nguyen-etal-2016-j,
    title = ""{J}-{NERD}: Joint Named Entity Recognition and Disambiguation with Rich Linguistic Features"",
    author = ""Nguyen, Dat Ba  and
      Theobald, Martin  and
      Weikum, Gerhard"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1016"",
    doi = ""10.1162/tacl_a_00094"",
    pages = ""215--229"",
    abstract = ""Methods for Named Entity Recognition and Disambiguation (NERD) perform NER and NED in two separate stages. Therefore, NED may be penalized with respect to precision by NER false positives, and suffers in recall from NER false negatives. Conversely, NED does not fully exploit information computed by NER such as types of mentions. This paper presents J-NERD, a new approach to perform NER and NED jointly, by means of a probabilistic graphical model that captures mention spans, mention types, and the mapping of mentions to entities in a knowledge base. We present experiments with different kinds of texts from the CoNLL{'}03, ACE{'}05, and ClueWeb{'}09-FACC1 corpora. J-NERD consistently outperforms state-of-the-art competitors in end-to-end NERD precision, recall, and F1."",
}
@article{marcheggiani-titov-2016-discrete,
    title = ""Discrete-State Variational Autoencoders for Joint Discovery and Factorization of Relations"",
    author = ""Marcheggiani, Diego  and
      Titov, Ivan"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1017"",
    doi = ""10.1162/tacl_a_00095"",
    pages = ""231--244"",
    abstract = ""We present a method for unsupervised open-domain relation discovery. In contrast to previous (mostly generative and agglomerative clustering) approaches, our model relies on rich contextual features and makes minimal independence assumptions. The model is composed of two parts: a feature-rich relation extractor, which predicts a semantic relation between two entities, and a factorization model, which reconstructs arguments (i.e., the entities) relying on the predicted relation. The two components are estimated jointly so as to minimize errors in recovering arguments. We study factorization models inspired by previous work in relation factorization and selectional preference modeling. Our models substantially outperform the generative and agglomerative-clustering counterparts and achieve state-of-the-art performance."",
}
@article{stratos-etal-2016-unsupervised,
    title = ""Unsupervised Part-Of-Speech Tagging with Anchor Hidden {M}arkov Models"",
    author = ""Stratos, Karl  and
      Collins, Michael  and
      Hsu, Daniel"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1018"",
    doi = ""10.1162/tacl_a_00096"",
    pages = ""245--257"",
    abstract = ""We tackle unsupervised part-of-speech (POS) tagging by learning hidden Markov models (HMMs) that are particularly well-suited for the problem. These HMMs, which we call anchor HMMs, assume that each tag is associated with at least one word that can have no other tag, which is a relatively benign condition for POS tagging (e.g., {``}the{''} is a word that appears only under the determiner tag). We exploit this assumption and extend the non-negative matrix factorization framework of Arora et al. (2013) to design a consistent estimator for anchor HMMs. In experiments, our algorithm is competitive with strong baselines such as the clustering method of Brown et al. (1992) and the log-linear model of Berg-Kirkpatrick et al. (2010). Furthermore, it produces an interpretable model in which hidden states are automatically lexicalized by words."",
}
@article{yin-etal-2016-abcnn,
    title = ""{ABCNN}: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs"",
    author = {Yin, Wenpeng  and
      Sch{\""u}tze, Hinrich  and
      Xiang, Bing  and
      Zhou, Bowen},
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1019"",
    doi = ""10.1162/tacl_a_00097"",
    pages = ""259--272"",
    abstract = ""How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE). Most prior work (i) deals with one individual task by fine-tuning a specific system; (ii) models each sentence{'}s representation separately, rarely considering the impact of the other sentence; or (iii) relies fully on manually designed, task-specific linguistic features. This work presents a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences. We make three contributions. (i) The ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs. (ii) We propose three attention schemes that integrate mutual influence between sentences into CNNs; thus, the representation of each sentence takes into consideration its counterpart. These interdependent sentence pair representations are more powerful than isolated sentence representations. (iii) ABCNNs achieve state-of-the-art performance on AS, PI and TE tasks. We release code at: \url{https://github.com/yinwenpeng/Answer_Selection}."",
}
@article{hashimoto-etal-2016-word,
    title = ""Word Embeddings as Metric Recovery in Semantic Spaces"",
    author = ""Hashimoto, Tatsunori B.  and
      Alvarez-Melis, David  and
      Jaakkola, Tommi S."",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1020"",
    doi = ""10.1162/tacl_a_00098"",
    pages = ""273--286"",
    abstract = ""Continuous word representations have been remarkably useful across NLP tasks but remain poorly understood. We ground word embeddings in semantic spaces studied in the cognitive-psychometric literature, taking these spaces as the primary objects to recover. To this end, we relate log co-occurrences of words in large corpora to semantic similarity assessments and show that co-occurrences are indeed consistent with an Euclidean semantic space hypothesis. Framing word embedding as metric recovery of a semantic space unifies existing word embedding algorithms, ties them to manifold learning, and demonstrates that existing algorithms are consistent metric recovery methods given co-occurrence counts from random walks. Furthermore, we propose a simple, principled, direct metric recovery algorithm that performs on par with the state-of-the-art word embedding and manifold learning methods. Finally, we complement recent focus on analogies by constructing two new inductive reasoning datasets{---}series completion and classification{---}and demonstrate that word embeddings can be used to solve them as well."",
}
@article{schofield-mimno-2016-comparing,
    title = ""Comparing Apples to Apple: The Effects of Stemmers on Topic Models"",
    author = ""Schofield, Alexandra  and
      Mimno, David"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1021"",
    doi = ""10.1162/tacl_a_00099"",
    pages = ""287--300"",
    abstract = ""Rule-based stemmers such as the Porter stemmer are frequently used to preprocess English corpora for topic modeling. In this work, we train and evaluate topic models on a variety of corpora using several different stemming algorithms. We examine several different quantitative measures of the resulting models, including likelihood, coherence, model stability, and entropy. Despite their frequent use in topic modeling, we find that stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability."",
}
@article{agic-etal-2016-multilingual,
    title = ""Multilingual Projection for Parsing Truly Low-Resource Languages"",
    author = ""Agi{\'c}, {\v{Z}}eljko  and
      Johannsen, Anders  and
      Plank, Barbara  and
      Mart{\'\i}nez Alonso, H{\'e}ctor  and
      Schluter, Natalie  and
      S{\o}gaard, Anders"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1022"",
    doi = ""10.1162/tacl_a_00100"",
    pages = ""301--312"",
    abstract = ""We propose a novel approach to cross-lingual part-of-speech tagging and dependency parsing for truly low-resource languages. Our annotation projection-based approach yields tagging and parsing models for over 100 languages. All that is needed are freely available parallel texts, and taggers and parsers for resource-rich languages. The empirical evaluation across 30 test languages shows that our method consistently provides top-level accuracies, close to established upper bounds, and outperforms several competitive baselines."",
}
@article{kiperwasser-goldberg-2016-simple,
    title = ""Simple and Accurate Dependency Parsing Using Bidirectional {LSTM} Feature Representations"",
    author = ""Kiperwasser, Eliyahu  and
      Goldberg, Yoav"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1023"",
    doi = ""10.1162/tacl_a_00101"",
    pages = ""313--327"",
    abstract = ""We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition-based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese."",
}
@article{pelemans-etal-2016-sparse,
    title = ""Sparse Non-negative Matrix Language Modeling"",
    author = ""Pelemans, Joris  and
      Shazeer, Noam  and
      Chelba, Ciprian"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1024"",
    doi = ""10.1162/tacl_a_00102"",
    pages = ""329--342"",
    abstract = ""We present Sparse Non-negative Matrix (SNM) estimation, a novel probability estimation technique for language modeling that can efficiently incorporate arbitrary features. We evaluate SNM language models on two corpora: the One Billion Word Benchmark and a subset of the LDC English Gigaword corpus. Results show that SNM language models trained with n-gram features are a close match for the well-established Kneser-Ney models. The addition of skip-gram features yields a model that is in the same league as the state-of-the-art recurrent neural network language models, as well as complementary: combining the two modeling techniques yields the best known result on the One Billion Word Benchmark. On the Gigaword corpus further improvements are observed using features that cross sentence boundaries. The computational advantages of SNM estimation over both maximum entropy and neural network estimation are probably its main strength, promising an approach that has large flexibility in combining arbitrary features and yet scales gracefully to large amounts of data."",
}
@article{gulordava-merlo-2016-multi,
    title = ""Multi-lingual Dependency Parsing Evaluation: a Large-scale Analysis of Word Order Properties using Artificial Data"",
    author = ""Gulordava, Kristina  and
      Merlo, Paola"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1025"",
    doi = ""10.1162/tacl_a_00103"",
    pages = ""343--356"",
    abstract = ""The growing work in multi-lingual parsing faces the challenge of fair comparative evaluation and performance analysis across languages and their treebanks. The difficulty lies in teasing apart the properties of treebanks, such as their size or average sentence length, from those of the annotation scheme, and from the linguistic properties of languages. We propose a method to evaluate the effects of word order of a language on dependency parsing performance, while controlling for confounding treebank properties. The method uses artificially-generated treebanks that are minimal permutations of actual treebanks with respect to two word order properties: word order variation and dependency lengths. Based on these artificial data on twelve languages, we show that longer dependencies and higher word order variability degrade parsing performance. Our method also extends to minimal pairs of individual sentences, leading to a finer-grained understanding of parsing errors."",
}
@article{chiu-nichols-2016-named,
    title = ""Named Entity Recognition with Bidirectional {LSTM}-{CNN}s"",
    author = ""Chiu, Jason P.C.  and
      Nichols, Eric"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1026"",
    doi = ""10.1162/tacl_a_00104"",
    pages = ""357--370"",
    abstract = ""Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."",
}
@article{zhou-etal-2016-deep,
    title = ""Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation"",
    author = ""Zhou, Jie  and
      Cao, Ying  and
      Wang, Xuguang  and
      Li, Peng  and
      Xu, Wei"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1027"",
    doi = ""10.1162/tacl_a_00105"",
    pages = ""371--383"",
    abstract = ""Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT{'}14 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT{'}14 English-to-German task."",
}
@article{arora-etal-2016-latent,
    title = ""A Latent Variable Model Approach to {PMI}-based Word Embeddings"",
    author = ""Arora, Sanjeev  and
      Li, Yuanzhi  and
      Liang, Yingyu  and
      Ma, Tengyu  and
      Risteski, Andrej"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1028"",
    doi = ""10.1162/tacl_a_00106"",
    pages = ""385--399"",
    abstract = ""Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods. This paper proposes a new generative model, a dynamic version of the log-linear topic model of Mnih and Hinton (2007). The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by Mikolov et al. (2013a) and many subsequent papers. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space."",
}
@article{xu-etal-2016-optimizing,
    title = ""Optimizing Statistical Machine Translation for Text Simplification"",
    author = ""Xu, Wei  and
      Napoles, Courtney  and
      Pavlick, Ellie  and
      Chen, Quanze  and
      Callison-Burch, Chris"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1029"",
    doi = ""10.1162/tacl_a_00107"",
    pages = ""401--415"",
    abstract = ""Most recent sentence simplification systems use basic machine translation models to learn lexical and syntactic paraphrases from a manually simplified parallel corpus. These methods are limited by the quality and quantity of manually simplified corpora, which are expensive to build. In this paper, we conduct an in-depth adaptation of statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task."",
}
@article{osborne-etal-2016-encoding,
    title = ""Encoding Prior Knowledge with Eigenword Embeddings"",
    author = ""Osborne, Dominique  and
      Narayan, Shashi  and
      Cohen, Shay B."",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1030"",
    doi = ""10.1162/tacl_a_00108"",
    pages = ""417--430"",
    abstract = ""Canonical correlation analysis (CCA) is a method for reducing the dimension of data represented using two views. It has been previously used to derive word embeddings, where one view indicates a word, and the other view indicates its context. We describe a way to incorporate prior knowledge into CCA, give a theoretical justification for it, and test it by deriving word embeddings and evaluating them on a myriad of datasets."",
}
@article{ammar-etal-2016-many,
    title = ""Many Languages, One Parser"",
    author = ""Ammar, Waleed  and
      Mulcaire, George  and
      Ballesteros, Miguel  and
      Dyer, Chris  and
      Smith, Noah A."",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1031"",
    doi = ""10.1162/tacl_a_00109"",
    pages = ""431--444"",
    abstract = ""We train one multilingual model for dependency parsing and use it to parse sentences in several languages. The parsing model uses (i) multilingual word clusters and embeddings; (ii) token-level language information; and (iii) language-specific features (fine-grained POS tags). This input representation enables the parser not only to parse effectively in multiple languages, but also to generalize across languages based on linguistic universals and typological similarities, making it more effective to learn from limited annotations. Our parser{'}s performance compares favorably to strong baselines in a range of data scenarios, including when the target language has a large treebank, a small treebank, or no treebank for training."",
}
@article{kiperwasser-goldberg-2016-easy,
    title = ""Easy-First Dependency Parsing with Hierarchical Tree {LSTM}s"",
    author = ""Kiperwasser, Eliyahu  and
      Goldberg, Yoav"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1032"",
    doi = ""10.1162/tacl_a_00110"",
    pages = ""445--461"",
    abstract = ""We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders. To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving very strong accuracies for English and Chinese, without relying on external word embeddings. The parser{'}s implementation is available for download at the first author{'}s webpage."",
}
@article{althoff-etal-2016-large,
    title = ""Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health"",
    author = ""Althoff, Tim  and
      Clark, Kevin  and
      Leskovec, Jure"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1033"",
    doi = ""10.1162/tacl_a_00111"",
    pages = ""463--476"",
    abstract = ""Mental illness is one of the most pressing public health issues of our time. While counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. In this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. Applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes."",
}
@article{shareghi-etal-2016-fast,
    title = ""Fast, Small and Exact: Infinite-order Language Modelling with Compressed Suffix Trees"",
    author = ""Shareghi, Ehsan  and
      Petri, Matthias  and
      Haffari, Gholamreza  and
      Cohn, Trevor"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1034"",
    doi = ""10.1162/tacl_a_00112"",
    pages = ""477--490"",
    abstract = ""Efficient methods for storing and querying are critical for scaling high-order m-gram language models to large corpora. We propose a language model based on compressed suffix trees, a representation that is highly compact and can be easily held in memory, while supporting queries needed in computing language model probabilities on-the-fly. We present several optimisations which improve query runtimes up to 2500{\mbox{$\times$}}, despite only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying)."",
}
@article{wang-eisner-2016-galactic,
    title = ""The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages"",
    author = ""Wang, Dingquan  and
      Eisner, Jason"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1035"",
    doi = ""10.1162/tacl_a_00113"",
    pages = ""491--505"",
    abstract = ""We release Galactic Dependencies 1.0{---}a large set of synthetic languages not found on Earth, but annotated in Universal Dependencies format. This new resource aims to provide training and development data for NLP methods that aim to adapt to unfamiliar languages. Each synthetic treebank is produced from a real treebank by stochastically permuting the dependents of nouns and/or verbs to match the word order of other real languages. We discuss the usefulness, realism, parsability, perplexity, and diversity of the synthetic languages. As a simple demonstration of the use of Galactic Dependencies, we consider single-source transfer, which attempts to parse a real target language using a parser trained on a {``}nearby{''} source language. We find that including synthetic source languages somewhat increases the diversity of the source pool, which significantly improves results for most target languages."",
}
@article{gorman-sproat-2016-minimally,
    title = ""Minimally Supervised Number Normalization"",
    author = ""Gorman, Kyle  and
      Sproat, Richard"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1036"",
    doi = ""10.1162/tacl_a_00114"",
    pages = ""507--519"",
    abstract = ""We propose two models for verbalizing numbers, a key component in speech recognition and synthesis systems. The first model uses an end-to-end recurrent neural network. The second model, drawing inspiration from the linguistics literature, uses finite-state transducers constructed with a minimal amount of training data. While both models achieve near-perfect performance, the latter model can be trained using several orders of magnitude less data than the former, making it particularly useful for low-resource languages."",
}
@article{linzen-etal-2016-assessing,
    title = ""Assessing the Ability of {LSTM}s to Learn Syntax-Sensitive Dependencies"",
    author = ""Linzen, Tal  and
      Dupoux, Emmanuel  and
      Goldberg, Yoav"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1037"",
    doi = ""10.1162/tacl_a_00115"",
    pages = ""521--535"",
    abstract = ""The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture{'}s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1{\%} errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured."",
}
@article{goldwasser-zhang-2016-understanding,
    title = ""Understanding Satirical Articles Using Common-Sense"",
    author = ""Goldwasser, Dan  and
      Zhang, Xiao"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1038"",
    doi = ""10.1162/tacl_a_00116"",
    pages = ""537--549"",
    abstract = ""Automatic satire detection is a subtle text classification task, for machines and at times, even for humans. In this paper we argue that satire detection should be approached using common-sense inferences, rather than traditional text classification methods. We present a highly structured latent variable model capturing the required inferences. The model abstracts over the specific entities appearing in the articles, grouping them into generalized categories, thus allowing the model to adapt to previously unseen situations."",
}
@article{tuan-etal-2016-utilizing,
    title = ""Utilizing Temporal Information for Taxonomy Construction"",
    author = ""Tuan, Luu Anh  and
      Hui, Siu Cheung  and
      Ng, See Kiong"",
    editor = ""Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q16-1039"",
    doi = ""10.1162/tacl_a_00117"",
    pages = ""551--564"",
    abstract = ""Taxonomies play an important role in many applications by organizing domain knowledge into a hierarchy of {`}is-a{'} relations between terms. Previous work on automatic construction of taxonomies from text documents either ignored temporal information or used fixed time periods to discretize the time series of documents. In this paper, we propose a time-aware method to automatically construct and effectively maintain a taxonomy from a given series of documents preclustered for a domain of interest. The method extracts temporal information from the documents and uses a timestamp contribution function to score the temporal relevance of the evidence from source texts when identifying the taxonomic relations for constructing the taxonomy. Experimental results show that our proposed method outperforms the state-of-the-art methods by increasing F-measure up to 7{\%}{--}20{\%}. Furthermore, the proposed method can incrementally update the taxonomy by adding fresh relations from new data and removing outdated relations using an information decay function. It thus avoids rebuilding the whole taxonomy from scratch for every update and keeps the taxonomy effectively up-to-date in order to track the latest information trends in the rapidly evolving domain."",
}
@",medical domain,natural language process,natural languag,language process,translat,nlp,infer,generat,relation extract,entity recognit,information retriev,semant,annotat,challeng,communiti,benchmark,evalu,assess,track
" ""Staggered {NLP}-assisted refinement for Clinical Annotations of Chronic Disease Events"","," ""Domain-specific annotations for NLP are often centered on real-world applications of text, and incorrect annotations may be particularly unacceptable. In medical text, the process of manual chart review (of a patient{'}s medical record) is error-prone due to its complexity. We propose a staggered NLP-assisted approach to the refinement of clinical annotations, an interactive process that allows initial human judgments to be verified or falsified by means of comparison with an improving NLP system. We show on our internal Asthma Timelines dataset that this approach improves the quality of the human-produced clinical annotations."",","{wu-etal-2016-staggered,
    title = ""Staggered {NLP}-assisted refinement for Clinical Annotations of Chronic Disease Events"",
    author = ""Wu, Stephen  and
      Wi, Chung-Il  and
      Sohn, Sunghwan  and
      Liu, Hongfang  and
      Juhn, Young"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)"",
    month = may,
    year = ""2016"",
    address = ""Portoro{\v{z}}, Slovenia"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""https://aclanthology.org/L16-1068"",
    pages = ""426--429"",
    abstract = ""Domain-specific annotations for NLP are often centered on real-world applications of text, and incorrect annotations may be particularly unacceptable. In medical text, the process of manual chart review (of a patient{'}s medical record) is error-prone due to its complexity. We propose a staggered NLP-assisted approach to the refinement of clinical annotations, an interactive process that allows initial human judgments to be verified or falsified by means of comparison with an improving NLP system. We show on our internal Asthma Timelines dataset that this approach improves the quality of the human-produced clinical annotations."",
}
@",medical record,medical text,nlp,annotat,evalu
" ""The Scielo Corpus: a Parallel Corpus of Scientific Publications for Biomedicine"","," ""The biomedical scientific literature is a rich source of information not only in the English language, for which it is more abundant, but also in other languages, such as Portuguese, Spanish and French. We present the first freely available parallel corpus of scientific publications for the biomedical domain. Documents from the {''}Biological Sciences{''} and {''}Health Sciences{''} categories were retrieved from the Scielo database and parallel titles and abstracts are available for the following language pairs: Portuguese/English (about 86,000 documents in total), Spanish/English (about 95,000 documents) and French/English (about 2,000 documents). Additionally, monolingual data was also collected for all four languages. Sentences in the parallel corpus were automatically aligned and a manual analysis of 200 documents by native experts found that a minimum of 79{\%} of sentences were correctly aligned in all language pairs. We demonstrate the utility of the corpus by running baseline machine translation experiments. We show that for all language pairs, a statistical machine translation system trained on the parallel corpora achieves performance that rivals or exceeds the state of the art in the biomedical domain. Furthermore, the corpora are currently being used in the biomedical task in the First Conference on Machine Translation (WMT{'}16)."",","{neves-etal-2016-scielo,
    title = ""The Scielo Corpus: a Parallel Corpus of Scientific Publications for Biomedicine"",
    author = ""Neves, Mariana  and
      Yepes, Antonio Jimeno  and
      N{\'e}v{\'e}ol, Aur{\'e}lie"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)"",
    month = may,
    year = ""2016"",
    address = ""Portoro{\v{z}}, Slovenia"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""https://aclanthology.org/L16-1470"",
    pages = ""2942--2948"",
    abstract = ""The biomedical scientific literature is a rich source of information not only in the English language, for which it is more abundant, but also in other languages, such as Portuguese, Spanish and French. We present the first freely available parallel corpus of scientific publications for the biomedical domain. Documents from the {''}Biological Sciences{''} and {''}Health Sciences{''} categories were retrieved from the Scielo database and parallel titles and abstracts are available for the following language pairs: Portuguese/English (about 86,000 documents in total), Spanish/English (about 95,000 documents) and French/English (about 2,000 documents). Additionally, monolingual data was also collected for all four languages. Sentences in the parallel corpus were automatically aligned and a manual analysis of 200 documents by native experts found that a minimum of 79{\%} of sentences were correctly aligned in all language pairs. We demonstrate the utility of the corpus by running baseline machine translation experiments. We show that for all language pairs, a statistical machine translation system trained on the parallel corpora achieves performance that rivals or exceeds the state of the art in the biomedical domain. Furthermore, the corpora are currently being used in the biomedical task in the First Conference on Machine Translation (WMT{'}16)."",
}
@",medical domain,translat,evalu
" ""Annotating and Detecting Medical Events in Clinical Notes"","," ""Early detection and treatment of diseases that onset after a patient is admitted to a hospital, such as pneumonia, is critical to improving and reducing costs in healthcare. Previous studies (Tepper et al., 2013) showed that change-of-state events in clinical notes could be important cues for phenotype detection. In this paper, we extend the annotation schema proposed in (Klassen et al., 2014) to mark change-of-state events, diagnosis events, coordination, and negation. After we have completed the annotation, we build NLP systems to automatically identify named entities and medical events, which yield an f-score of 94.7{\%} and 91.8{\%}, respectively."",","{klassen-etal-2016-annotating,
    title = ""Annotating and Detecting Medical Events in Clinical Notes"",
    author = ""Klassen, Prescott  and
      Xia, Fei  and
      Yetisgen, Meliha"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)"",
    month = may,
    year = ""2016"",
    address = ""Portoro{\v{z}}, Slovenia"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""https://aclanthology.org/L16-1545"",
    pages = ""3417--3421"",
    abstract = ""Early detection and treatment of diseases that onset after a patient is admitted to a hospital, such as pneumonia, is critical to improving and reducing costs in healthcare. Previous studies (Tepper et al., 2013) showed that change-of-state events in clinical notes could be important cues for phenotype detection. In this paper, we extend the annotation schema proposed in (Klassen et al., 2014) to mark change-of-state events, diagnosis events, coordination, and negation. After we have completed the annotation, we build NLP systems to automatically identify named entities and medical events, which yield an f-score of 94.7{\%} and 91.8{\%}, respectively."",
}
@",clinical not,nlp,annotat,evalu
" ""Annotating Logical Forms for {EHR} Questions"","," ""This paper discusses the creation of a semantically annotated corpus of questions about patient data in electronic health records (EHRs). The goal is provide the training data necessary for semantic parsers to automatically convert EHR questions into a structured query. A layered annotation strategy is used which mirrors a typical natural language processing (NLP) pipeline. First, questions are syntactically analyzed to identify multi-part questions. Second, medical concepts are recognized and normalized to a clinical ontology. Finally, logical forms are created using a lambda calculus representation. We use a corpus of 446 questions asking for patient-specific information. From these, 468 specific questions are found containing 259 unique medical concepts and requiring 53 unique predicates to represent the logical forms. We further present detailed characteristics of the corpus, including inter-annotator agreement results, and describe the challenges automatic NLP systems will face on this task."",","{roberts-demner-fushman-2016-annotating,
    title = ""Annotating Logical Forms for {EHR} Questions"",
    author = ""Roberts, Kirk  and
      Demner-Fushman, Dina"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)"",
    month = may,
    year = ""2016"",
    address = ""Portoro{\v{z}}, Slovenia"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""https://aclanthology.org/L16-1598"",
    pages = ""3772--3778"",
    abstract = ""This paper discusses the creation of a semantically annotated corpus of questions about patient data in electronic health records (EHRs). The goal is provide the training data necessary for semantic parsers to automatically convert EHR questions into a structured query. A layered annotation strategy is used which mirrors a typical natural language processing (NLP) pipeline. First, questions are syntactically analyzed to identify multi-part questions. Second, medical concepts are recognized and normalized to a clinical ontology. Finally, logical forms are created using a lambda calculus representation. We use a corpus of 446 questions asking for patient-specific information. From these, 468 specific questions are found containing 259 unique medical concepts and requiring 53 unique predicates to represent the logical forms. We further present detailed characteristics of the corpus, including inter-annotator agreement results, and describe the challenges automatic NLP systems will face on this task."",
}
@",electronic health record,health record,natural language process,natural languag,language process,nlp,semant,annotat,challeng,evalu
" ""Embedding Open-domain Common-sense Knowledge from Text"","," ""Our ability to understand language often relies on common-sense knowledge ― background information the speaker can assume is known by the reader. Similarly, our comprehension of the language used in complex domains relies on access to domain-specific knowledge. Capturing common-sense and domain-specific knowledge can be achieved by taking advantage of recent advances in open information extraction (IE) techniques and, more importantly, of knowledge embeddings, which are multi-dimensional representations of concepts and relations. Building a knowledge graph for representing common-sense knowledge in which concepts discerned from noun phrases are cast as vertices and lexicalized relations are cast as edges leads to learning the embeddings of common-sense knowledge accounting for semantic compositionality as well as implied knowledge. Common-sense knowledge is acquired from a vast collection of blogs and books as well as from WordNet. Similarly, medical knowledge is learned from two large sets of electronic health records. The evaluation results of these two forms of knowledge are promising: the same knowledge acquisition methodology based on learning knowledge embeddings works well both for common-sense knowledge and for medical knowledge Interestingly, the common-sense knowledge that we have acquired was evaluated as being less neutral than than the medical knowledge, as it often reflected the opinion of the knowledge utterer. In addition, the acquired medical knowledge was evaluated as more plausible than the common-sense knowledge, reflecting the complexity of acquiring common-sense knowledge due to the pragmatics and economicity of language."",","{goodwin-harabagiu-2016-embedding,
    title = ""Embedding Open-domain Common-sense Knowledge from Text"",
    author = ""Goodwin, Travis  and
      Harabagiu, Sanda"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)"",
    month = may,
    year = ""2016"",
    address = ""Portoro{\v{z}}, Slovenia"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""https://aclanthology.org/L16-1732"",
    pages = ""4621--4628"",
    abstract = ""Our ability to understand language often relies on common-sense knowledge ― background information the speaker can assume is known by the reader. Similarly, our comprehension of the language used in complex domains relies on access to domain-specific knowledge. Capturing common-sense and domain-specific knowledge can be achieved by taking advantage of recent advances in open information extraction (IE) techniques and, more importantly, of knowledge embeddings, which are multi-dimensional representations of concepts and relations. Building a knowledge graph for representing common-sense knowledge in which concepts discerned from noun phrases are cast as vertices and lexicalized relations are cast as edges leads to learning the embeddings of common-sense knowledge accounting for semantic compositionality as well as implied knowledge. Common-sense knowledge is acquired from a vast collection of blogs and books as well as from WordNet. Similarly, medical knowledge is learned from two large sets of electronic health records. The evaluation results of these two forms of knowledge are promising: the same knowledge acquisition methodology based on learning knowledge embeddings works well both for common-sense knowledge and for medical knowledge Interestingly, the common-sense knowledge that we have acquired was evaluated as being less neutral than than the medical knowledge, as it often reflected the opinion of the knowledge utterer. In addition, the acquired medical knowledge was evaluated as more plausible than the common-sense knowledge, reflecting the complexity of acquiring common-sense knowledge due to the pragmatics and economicity of language."",
}
@",electronic health record,health record,semant,evalu
" ""Appraising {UMLS} Coverage for Summarizing Medical Evidence"","," ""When making clinical decisions, practitioners need to rely on the most relevant evidence available. However, accessing a vast body of medical evidence and confronting with the issue of information overload can be challenging and time consuming. This paper proposes an effective summarizer for medical evidence by utilizing both UMLS and WordNet. Given a clinical query and a set of relevant abstracts, our aim is to generate a fluent, well-organized, and compact summary that answers the query. Analysis via ROUGE metrics shows that using WordNet as a general-purpose lexicon helps to capture the concepts not covered by the UMLS Metathesaurus, and hence significantly increases the performance. The effectiveness of our proposed approach is demonstrated by conducting a set of experiments over a specialized evidence-based medicine (EBM) corpus - which has been gathered and annotated for the purpose of biomedical text summarization."",","{shafieibavani-etal-2016-appraising,
    title = ""Appraising {UMLS} Coverage for Summarizing Medical Evidence"",
    author = ""ShafieiBavani, Elaheh  and
      Ebrahimi, Mohammad  and
      Wong, Raymond  and
      Chen, Fang"",
    editor = ""Matsumoto, Yuji  and
      Prasad, Rashmi"",
    booktitle = ""Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers"",
    month = dec,
    year = ""2016"",
    address = ""Osaka, Japan"",
    publisher = ""The COLING 2016 Organizing Committee"",
    url = ""https://aclanthology.org/C16-1050"",
    pages = ""513--524"",
    abstract = ""When making clinical decisions, practitioners need to rely on the most relevant evidence available. However, accessing a vast body of medical evidence and confronting with the issue of information overload can be challenging and time consuming. This paper proposes an effective summarizer for medical evidence by utilizing both UMLS and WordNet. Given a clinical query and a set of relevant abstracts, our aim is to generate a fluent, well-organized, and compact summary that answers the query. Analysis via ROUGE metrics shows that using WordNet as a general-purpose lexicon helps to capture the concepts not covered by the UMLS Metathesaurus, and hence significantly increases the performance. The effectiveness of our proposed approach is demonstrated by conducting a set of experiments over a specialized evidence-based medicine (EBM) corpus - which has been gathered and annotated for the purpose of biomedical text summarization."",
}
@",medical text,generat,summar,annotat,challeng
" ""Analysis of Temporal Expressions Annotated in Clinical Notes"",",,"{tissot-etal-2015-analysis,
    title = ""Analysis of Temporal Expressions Annotated in Clinical Notes"",
    author = ""Tissot, Hegler  and
      Roberts, Angus  and
      Derczynski, Leon  and
      Gorrell, Genevieve  and
      Del Fabro, Marcus Didonet"",
    booktitle = ""Proceedings of the 11th Joint {ACL}-{ISO} Workshop on Interoperable Semantic Annotation ({ISA}-11)"",
    month = apr,
    year = ""2015"",
    address = ""London, UK"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W15-0211"",
}
@",clinical not,semant,annotat
" ""{S}em{E}val-2015 Task 14: Analysis of Clinical Text"",",,"{elhadad-etal-2015-semeval,
    title = ""{S}em{E}val-2015 Task 14: Analysis of Clinical Text"",
    author = ""Elhadad, No{\'e}mie  and
      Pradhan, Sameer  and
      Gorman, Sharon  and
      Manandhar, Suresh  and
      Chapman, Wendy  and
      Savova, Guergana"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015)"",
    month = jun,
    year = ""2015"",
    address = ""Denver, Colorado"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S15-2051"",
    doi = ""10.18653/v1/S15-2051"",
    pages = ""303--310"",
}
@",clinical text,semant,evalu
" ""{T}eam{HCMUS}: Analysis of Clinical Text"",",,"{huynh-ho-2015-teamhcmus,
    title = ""{T}eam{HCMUS}: Analysis of Clinical Text"",
    author = ""Huynh, Nghia  and
      Ho, Quoc"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015)"",
    month = jun,
    year = ""2015"",
    address = ""Denver, Colorado"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S15-2063"",
    doi = ""10.18653/v1/S15-2063"",
    pages = ""370--374"",
}
@",clinical text,semant,evalu
" ""{IHS}-{RD}-{B}elarus: Identification and Normalization of Disorder Concepts in Clinical Notes"",",,"{chernyshevich-stankevitch-2015-ihs,
    title = ""{IHS}-{RD}-{B}elarus: Identification and Normalization of Disorder Concepts in Clinical Notes"",
    author = ""Chernyshevich, Maryna  and
      Stankevitch, Vadim"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015)"",
    month = jun,
    year = ""2015"",
    address = ""Denver, Colorado"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S15-2065"",
    doi = ""10.18653/v1/S15-2065"",
    pages = ""380--384"",
}
@",clinical not,semant,evalu
" ""{UWM}: A Simple Baseline Method for Identifying Attributes of Disease and Disorder Mentions in Clinical Text"",",,"{ghiasvand-kate-2015-uwm,
    title = ""{UWM}: A Simple Baseline Method for Identifying Attributes of Disease and Disorder Mentions in Clinical Text"",
    author = ""Ghiasvand, Omid  and
      Kate, Rohit"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015)"",
    month = jun,
    year = ""2015"",
    address = ""Denver, Colorado"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S15-2066"",
    doi = ""10.18653/v1/S15-2066"",
    pages = ""385--388"",
}
@",clinical text,semant,evalu
" ""{TMUNSW}: Identification of Disorders and Normalization to {SNOMED}-{CT} Terminology in Unstructured Clinical Notes"",",,"{jonnagaddala-etal-2015-tmunsw,
    title = ""{TMUNSW}: Identification of Disorders and Normalization to {SNOMED}-{CT} Terminology in Unstructured Clinical Notes"",
    author = ""Jonnagaddala, Jitendra  and
      Liaw, Siaw-Teng  and
      Ray, Pradeep  and
      Kumar, Manish  and
      Dai, Hong-Jie"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015)"",
    month = jun,
    year = ""2015"",
    address = ""Denver, Colorado"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S15-2068"",
    doi = ""10.18653/v1/S15-2068"",
    pages = ""394--398"",
}
@",clinical not,semant,evalu
" ""ez{DI}: A Supervised {NLP} System for Clinical Narrative Analysis"",",,"{pathak-etal-2015-ezdi,
    title = ""ez{DI}: A Supervised {NLP} System for Clinical Narrative Analysis"",
    author = ""Pathak, Parth  and
      Patel, Pinal  and
      Panchal, Vishal  and
      Soni, Sagar  and
      Dani, Kinjal  and
      Patel, Amrish  and
      Choudhary, Narayan"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015)"",
    month = jun,
    year = ""2015"",
    address = ""Denver, Colorado"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S15-2071"",
    doi = ""10.18653/v1/S15-2071"",
    pages = ""412--416"",
}
@",clinical narr,nlp,semant,evalu
" ""{LIST}-{LUX}: Disorder Identification from Clinical Texts"",",,"{ben-abacha-etal-2015-list,
    title = ""{LIST}-{LUX}: Disorder Identification from Clinical Texts"",
    author = ""Ben Abacha, Asma  and
      Karanasiou, Aikaterini  and
      Mrabet, Yassine  and
      Dos Reis, Julio Cesar"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten  and
      Cer, Daniel  and
      Jurgens, David"",
    booktitle = ""Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015)"",
    month = jun,
    year = ""2015"",
    address = ""Denver, Colorado"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S15-2074"",
    doi = ""10.18653/v1/S15-2074"",
    pages = ""427--432"",
}
@",clinical text,semant,evalu
" ""Building a semantically annotated corpus for congestive heart and renal failure from clinical records and the literature"",",,"{alnazzawi-etal-2014-building,
    title = ""Building a semantically annotated corpus for congestive heart and renal failure from clinical records and the literature"",
    author = ""Alnazzawi, Noha  and
      Thompson, Paul  and
      Ananiadou, Sophia"",
    editor = ""Velupillai, Sumithra  and
      Duneld, Martin  and
      Kvist, Maria  and
      Dalianis, Hercules  and
      Skeppstedt, Maria  and
      Henriksson, Aron"",
    booktitle = ""Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi)"",
    month = apr,
    year = ""2014"",
    address = ""Gothenburg, Sweden"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W14-1110"",
    doi = ""10.3115/v1/W14-1110"",
    pages = ""69--74"",
}
@",clinical record,semant,annotat
" ""{S}em{E}val-2014 Task 7: Analysis of Clinical Text"",",,"{pradhan-etal-2014-semeval,
    title = ""{S}em{E}val-2014 Task 7: Analysis of Clinical Text"",
    author = ""Pradhan, Sameer  and
      Elhadad, No{\'e}mie  and
      Chapman, Wendy  and
      Manandhar, Suresh  and
      Savova, Guergana"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)"",
    month = aug,
    year = ""2014"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S14-2007"",
    doi = ""10.3115/v1/S14-2007"",
    pages = ""54--62"",
}
@",clinical text,semant,evalu
" ""{B}ioinformatics{UA}: Concept Recognition in Clinical Narratives Using a Modular and Highly Efficient Text Processing Framework"",",,"{matos-etal-2014-bioinformaticsua,
    title = ""{B}ioinformatics{UA}: Concept Recognition in Clinical Narratives Using a Modular and Highly Efficient Text Processing Framework"",
    author = ""Matos, S{\'e}rgio  and
      Nunes, Tiago  and
      Oliveira, Jos{\'e} Lu{\'\i}s"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)"",
    month = aug,
    year = ""2014"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S14-2019"",
    doi = ""10.3115/v1/S14-2019"",
    pages = ""135--139"",
}
@",clinical narr,semant,evalu
" ""ez{DI}: A Hybrid {CRF} and {SVM} based Model for Detecting and Encoding Disorder Mentions in Clinical Notes"",",,"{pathak-etal-2014-ezdi,
    title = ""ez{DI}: A Hybrid {CRF} and {SVM} based Model for Detecting and Encoding Disorder Mentions in Clinical Notes"",
    author = ""Pathak, Parth  and
      Patel, Pinal  and
      Panchal, Vishal  and
      Choudhary, Narayan  and
      Patel, Amrish  and
      Joshi, Gautam"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)"",
    month = aug,
    year = ""2014"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S14-2045"",
    doi = ""10.3115/v1/S14-2045"",
    pages = ""278--283"",
}
@",clinical not,semant,evalu
" ""{I}xa{M}ed: Applying Freeling and a Perceptron Sequential Tagger at the Shared Task on Analyzing Clinical Texts"",",,"{gojenola-etal-2014-ixamed,
    title = ""{I}xa{M}ed: Applying Freeling and a Perceptron Sequential Tagger at the Shared Task on Analyzing Clinical Texts"",
    author = ""Gojenola, Koldo  and
      Oronoz, Maite  and
      P{\'e}rez, Alicia  and
      Casillas, Arantza"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)"",
    month = aug,
    year = ""2014"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S14-2061"",
    doi = ""10.3115/v1/S14-2061"",
    pages = ""361--365"",
}
@",clinical text,semant,shared task,evalu
" ""{R}el{A}gent: Entity Detection and Normalization for Diseases in Clinical Records: a Linguistically Driven Approach"",",,"{ramanan-nathan-2014-relagent,
    title = ""{R}el{A}gent: Entity Detection and Normalization for Diseases in Clinical Records: a Linguistically Driven Approach"",
    author = ""Ramanan, Sv  and
      Nathan, Senthil"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)"",
    month = aug,
    year = ""2014"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S14-2083"",
    doi = ""10.3115/v1/S14-2083"",
    pages = ""477--481"",
}
@",clinical record,semant,evalu
" ""{SZTE}-{NLP}: Clinical Text Analysis with Named Entity Recognition"",",,"{katona-farkas-2014-szte,
    title = ""{SZTE}-{NLP}: Clinical Text Analysis with Named Entity Recognition"",
    author = ""Katona, Melinda  and
      Farkas, Rich{\'a}rd"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)"",
    month = aug,
    year = ""2014"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S14-2108"",
    doi = ""10.3115/v1/S14-2108"",
    pages = ""615--618"",
}
@",clinical text,nlp,entity recognit,semant,evalu
" ""{TMUNSW}: Disorder Concept Recognition and Normalization in Clinical Notes for {S}em{E}val-2014 Task 7"",",,"{jonnagaddala-etal-2014-tmunsw,
    title = ""{TMUNSW}: Disorder Concept Recognition and Normalization in Clinical Notes for {S}em{E}val-2014 Task 7"",
    author = ""Jonnagaddala, Jitendra  and
      Kumar, Manish  and
      Dai, Hong-Jie  and
      Rachmani, Enny  and
      Hsu, Chien-Yeh"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)"",
    month = aug,
    year = ""2014"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S14-2118"",
    doi = ""10.3115/v1/S14-2118"",
    pages = ""663--667"",
}
@",clinical not,semant,evalu
" ""{U}ni{P}i: Recognition of Mentions of Disorders in Clinical Text"",",,"{attardi-etal-2014-unipi,
    title = ""{U}ni{P}i: Recognition of Mentions of Disorders in Clinical Text"",
    author = ""Attardi, Giuseppe  and
      Cozza, Vittoria  and
      Sartiano, Daniele"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)"",
    month = aug,
    year = ""2014"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S14-2134"",
    doi = ""10.3115/v1/S14-2134"",
    pages = ""754--760"",
}
@",clinical text,semant,evalu
" ""{UTH}{\_}{CCB}: A report for {S}em{E}val 2014 {--} Task 7 Analysis of Clinical Text"",",,"{zhang-etal-2014-uth,
    title = ""{UTH}{\_}{CCB}: A report for {S}em{E}val 2014 {--} Task 7 Analysis of Clinical Text"",
    author = ""Zhang, Yaoyun  and
      Wang, Jingqi  and
      Tang, Buzhou  and
      Wu, Yonghui  and
      Jiang, Min  and
      Chen, Yukun  and
      Xu, Hua"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)"",
    month = aug,
    year = ""2014"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S14-2142"",
    doi = ""10.3115/v1/S14-2142"",
    pages = ""802--806"",
}
@",clinical text,semant,evalu
" ""{UWM}: Disorder Mention Extraction from Clinical Text Using {CRF}s and Normalization Using Learned Edit Distance Patterns"",",,"{ghiasvand-kate-2014-uwm,
    title = ""{UWM}: Disorder Mention Extraction from Clinical Text Using {CRF}s and Normalization Using Learned Edit Distance Patterns"",
    author = ""Ghiasvand, Omid  and
      Kate, Rohit"",
    editor = ""Nakov, Preslav  and
      Zesch, Torsten"",
    booktitle = ""Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)"",
    month = aug,
    year = ""2014"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S14-2147"",
    doi = ""10.3115/v1/S14-2147"",
    pages = ""828--832"",
}
@",clinical text,semant,evalu
" ""Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials"","," ""Prepositional phrase (PP) attachment disambiguation is a known challenge in syntactic parsing. The lexical sparsity associated with PP attachments motivates research in word representations that can capture pertinent syntactic and semantic features of the word. One promising solution is to use word vectors induced from large amounts of raw text. However, state-of-the-art systems that employ such representations yield modest gains in PP attachment accuracy. In this paper, we show that word vector representations can yield significant PP attachment performance gains. This is achieved via a non-linear architecture that is discriminatively trained to maximize PP attachment accuracy. The architecture is initialized with word vectors trained from unlabeled data, and relearns those to maximize attachment accuracy. We obtain additional performance gains with alternative representations such as dependency-based word vectors. When tested on both English and Arabic datasets, our method outperforms both a strong SVM classifier and state-of-the-art parsers. For instance, we achieve 82.6{\%} PP attachment accuracy on Arabic, while the Turbo and Charniak self-trained parsers obtain 76.7{\%} and 80.8{\%} respectively."",","{bott-schulte-im-walde-2014-syntactic,
    title = ""Syntactic Transfer Patterns of {G}erman Particle Verbs and their Impact on Lexical Semantics"",
    author = ""Bott, Stefan  and
      Schulte im Walde, Sabine"",
    editor = ""Bos, Johan  and
      Frank, Anette  and
      Navigli, Roberto"",
    booktitle = ""Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*{SEM} 2014)"",
    month = aug,
    year = ""2014"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics and Dublin City University"",
    url = ""https://aclanthology.org/S14-1022"",
    doi = ""10.3115/v1/S14-1022"",
    pages = ""182--192"",
}
@proceedings{tacl-2014-transactions,
    title = ""Transactions of the Association for Computational Linguistics, Volume 2"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1000"",
}
@article{king-etal-2014-heterogeneous,
    title = ""Heterogeneous Networks and Their Applications: Scientometrics, Name Disambiguation, and Topic Modeling"",
    author = ""King, Ben  and
      Jha, Rahul  and
      Radev, Dragomir R."",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1001"",
    doi = ""10.1162/tacl_a_00161"",
    pages = ""1--14"",
    abstract = ""We present heterogeneous networks as a way to unify lexical networks with relational data. We build a unified ACL Anthology network, tying together the citation, author collaboration, and term-cooccurence networks with affiliation and venue relations. This representation proves to be convenient and allows problems such as name disambiguation, topic modeling, and the measurement of scientific impact to be easily solved using only this network and off-the-shelf graph algorithms."",
}
@article{schnabel-schutze-2014-flors,
    title = ""{FLORS}: Fast and Simple Domain Adaptation for Part-of-Speech Tagging"",
    author = {Schnabel, Tobias  and
      Sch{\""u}tze, Hinrich},
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1002"",
    doi = ""10.1162/tacl_a_00162"",
    pages = ""15--26"",
    abstract = ""We present FLORS, a new part-of-speech tagger for domain adaptation. FLORS uses robust representations that work especially well for unknown words and for known words with unseen tags. FLORS is simpler and faster than previous domain adaptation methods, yet it has significantly better accuracy than several baselines."",
}
@article{lui-etal-2014-automatic,
    title = ""Automatic Detection and Language Identification of Multilingual Documents"",
    author = ""Lui, Marco  and
      Lau, Jey Han  and
      Baldwin, Timothy"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1003"",
    doi = ""10.1162/tacl_a_00163"",
    pages = ""27--40"",
    abstract = ""Language identification is the task of automatically detecting the language(s) present in a document based on the content of the document. In this work, we address the problem of detecting documents that contain text from more than one language (multilingual documents). We introduce a method that is able to detect that a document is multilingual, identify the languages present, and estimate their relative proportions. We demonstrate the effectiveness of our method over synthetic data, as well as real-world multilingual documents collected from the web."",
}
@article{pitler-2014-crossing,
    title = ""A Crossing-Sensitive Third-Order Factorization for Dependency Parsing"",
    author = ""Pitler, Emily"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1004"",
    doi = ""10.1162/tacl_a_00164"",
    pages = ""41--54"",
    abstract = ""Parsers that parametrize over wider scopes are generally more accurate than edge-factored models. For graph-based non-projective parsers, wider factorizations have so far implied large increases in the computational complexity of the parsing problem. This paper introduces a {``}crossing-sensitive{''} generalization of a third-order factorization that trades off complexity in the model structure (i.e., scoring with features over multiple edges) with complexity in the output structure (i.e., producing crossing edges). Under this model, the optimal 1-Endpoint-Crossing tree can be found in O(n4) time, matching the asymptotic run-time of both the third-order projective parser and the edge-factored 1-Endpoint-Crossing parser. The crossing-sensitive third-order parser is significantly more accurate than the third-order projective parser under many experimental settings and significantly less accurate on none."",
}
@article{wang-manning-2014-cross,
    title = ""Cross-lingual Projected Expectation Regularization for Weakly Supervised Learning"",
    author = ""Wang, Mengqiu  and
      Manning, Christopher D."",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1005"",
    doi = ""10.1162/tacl_a_00165"",
    pages = ""55--66"",
    abstract = ""We consider a multilingual weakly supervised learning scenario where knowledge from annotated corpora in a resource-rich language is transferred via bitext to guide the learning in other languages. Past approaches project labels across bitext and use them as features or gold labels for training. We propose a new method that projects model expectations rather than labels, which facilities transfer of model uncertainty across language boundaries. We encode expectations as constraints and train a discriminative CRF model using Generalized Expectation Criteria (Mann and McCallum, 2010). Evaluated on standard Chinese-English and German-English NER datasets, our method demonstrates F1 scores of 64{\%} and 60{\%} when no labeled data is used. Attaining the same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences. Furthermore, when combined with labeled examples, our method yields significant improvements over state-of-the-art supervised methods, achieving best reported numbers to date on Chinese OntoNotes and German CoNLL-03 datasets."",
}
@article{young-etal-2014-image,
    title = ""From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"",
    author = ""Young, Peter  and
      Lai, Alice  and
      Hodosh, Micah  and
      Hockenmaier, Julia"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1006"",
    doi = ""10.1162/tacl_a_00166"",
    pages = ""67--78"",
    abstract = ""We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions."",
}
@article{pavlick-etal-2014-language,
    title = ""The Language Demographics of {A}mazon {M}echanical {T}urk"",
    author = ""Pavlick, Ellie  and
      Post, Matt  and
      Irvine, Ann  and
      Kachaev, Dmitry  and
      Callison-Burch, Chris"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1007"",
    doi = ""10.1162/tacl_a_00167"",
    pages = ""79--92"",
    abstract = ""We present a large scale study of the languages spoken by bilingual workers on Mechanical Turk (MTurk). We establish a methodology for determining the language skills of anonymous crowd workers that is more robust than simple surveying. We validate workers{'} self-reported language skill claims by measuring their ability to correctly translate words, and by geolocating workers to see if they reside in countries where the languages are likely to be spoken. Rather than posting a one-off survey, we posted paid tasks consisting of 1,000 assignments to translate a total of 10,000 words in each of 100 languages. Our study ran for several months, and was highly visible on the MTurk crowdsourcing platform, increasing the chances that bilingual workers would complete it. Our study was useful both to create bilingual dictionaries and to act as census of the bilingual speakers on MTurk. We use this data to recommend languages with the largest speaker populations as good candidates for other researchers who want to develop crowdsourced, multilingual technologies. To further demonstrate the value of creating data via crowdsourcing, we hire workers to create bilingual parallel corpora in six Indian languages, and use them to train statistical machine translation systems."",
}
@article{borschinger-johnson-2014-exploring,
    title = ""Exploring the Role of Stress in {B}ayesian Word Segmentation using {A}daptor {G}rammars"",
    author = {B{\""o}rschinger, Benjamin  and
      Johnson, Mark},
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1008"",
    doi = ""10.1162/tacl_a_00168"",
    pages = ""93--104"",
    abstract = ""Stress has long been established as a major cue in word segmentation for English infants. We show that enabling a current state-of-the-art Bayesian word segmentation model to take advantage of stress cues noticeably improves its performance. We find that the improvements range from 10 to 4{\%}, depending on both the use of phonotactic cues and, to a lesser extent, the amount of evidence available to the learner. We also find that in particular early on, stress cues are much more useful for our model than phonotactic cues by themselves, consistent with the finding that children do seem to use stress cues before they use phonotactic cues. Finally, we study how the model{'}s knowledge about stress patterns evolves over time. We not only find that our model correctly acquires the most frequent patterns relatively quickly but also that the Unique Stress Constraint that is at the heart of a previously proposed model does not need to be built in but can be acquired jointly with word segmentation."",
}
@article{ravi-etal-2014-parallel,
    title = ""Parallel Algorithms for Unsupervised Tagging"",
    author = ""Ravi, Sujith  and
      Vassilivitskii, Sergei  and
      Rastogi, Vibhor"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1009"",
    doi = ""10.1162/tacl_a_00169"",
    pages = ""105--118"",
    abstract = ""We propose a new method for unsupervised tagging that finds minimal models which are then further improved by Expectation Maximization training. In contrast to previous approaches that rely on manually specified and multi-step heuristics for model minimization, our approach is a simple greedy approximation algorithm DMLC (Distributed-Minimum-Label-Cover) that solves this objective in a single step. We extend the method and show how to efficiently parallelize the algorithm on modern parallel computing platforms while preserving approximation guarantees. The new method easily scales to large data and grammar sizes, overcoming the memory bottleneck in previous approaches. We demonstrate the power of the new algorithm by evaluating on various sequence labeling tasks: Part-of-Speech tagging for multiple languages (including low-resource languages), with complete and incomplete dictionaries, and supertagging, a complex sequence labeling task, where the grammar size alone can grow to millions of entries. Our results show that for all of these settings, our method achieves state-of-the-art scalable performance that yields high quality tagging outputs."",
}
@article{goldberg-etal-2014-tabular,
    title = ""A Tabular Method for Dynamic Oracles in Transition-Based Parsing"",
    author = ""Goldberg, Yoav  and
      Sartorio, Francesco  and
      Satta, Giorgio"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1010"",
    doi = ""10.1162/tacl_a_00170"",
    pages = ""119--130"",
    abstract = ""We develop parsing oracles for two transition-based dependency parsers, including the arc-standard parser, solving a problem that was left open in (Goldberg and Nivre, 2013). We experimentally show that using these oracles during training yields superior parsing accuracies on many languages."",
}
@article{honnibal-johnson-2014-joint,
    title = ""Joint Incremental Disfluency Detection and Dependency Parsing"",
    author = ""Honnibal, Matthew  and
      Johnson, Mark"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1011"",
    doi = ""10.1162/tacl_a_00171"",
    pages = ""131--142"",
    abstract = ""We present an incremental dependency parsing model that jointly performs disfluency detection. The model handles speech repairs using a novel non-monotonic transition system, and includes several novel classes of features. For comparison, we evaluated two pipeline systems, using state-of-the-art disfluency detectors. The joint model performed better on both tasks, with a parse accuracy of 90.5{\%} and 84.0{\%} accuracy at disfluency detection. The model runs in expected linear time, and processes over 550 tokens a second."",
}
@article{styler-iv-etal-2014-temporal,
    title = ""Temporal Annotation in the Clinical Domain"",
    author = ""Styler IV, William F.  and
      Bethard, Steven  and
      Finan, Sean  and
      Palmer, Martha  and
      Pradhan, Sameer  and
      de Groen, Piet C  and
      Erickson, Brad  and
      Miller, Timothy  and
      Lin, Chen  and
      Savova, Guergana  and
      Pustejovsky, James"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1012"",
    doi = ""10.1162/tacl_a_00172"",
    pages = ""143--154"",
    abstract = ""This article discusses the requirements of a formal specification for the annotation of temporal information in clinical narratives. We discuss the implementation and extension of ISO-TimeML for annotating a corpus of clinical notes, known as the THYME corpus. To reflect the information task and the heavily inference-based reasoning demands in the domain, a new annotation guideline has been developed, {``}the THYME Guidelines to ISO-TimeML (THYME-TimeML){''}. To clarify what relations merit annotation, we distinguish between linguistically-derived and inferentially-derived temporal orderings in the text. We also apply a top performing TempEval 2013 system against this new resource to measure the difficulty of adapting systems to the clinical domain. The corpus is available to the community and has been proposed for use in a SemEval 2015 task."",
}
@article{qu-etal-2014-senti,
    title = ""Senti-{LSSVM}: Sentiment-Oriented Multi-Relation Extraction with Latent Structural {SVM}"",
    author = ""Qu, Lizhen  and
      Zhang, Yi  and
      Wang, Rui  and
      Jiang, Lili  and
      Gemulla, Rainer  and
      Weikum, Gerhard"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1013"",
    doi = ""10.1162/tacl_a_00173"",
    pages = ""155--168"",
    abstract = ""Extracting instances of sentiment-oriented relations from user-generated web documents is important for online marketing analysis. Unlike previous work, we formulate this extraction task as a structured prediction problem and design the corresponding inference as an integer linear program. Our latent structural SVM based model can learn from training corpora that do not contain explicit annotations of sentiment-bearing expressions, and it can simultaneously recognize instances of both binary (polarity) and ternary (comparative) relations with regard to entity mentions of interest. The empirical evaluation shows that our approach significantly outperforms state-of-the-art systems across domains (cameras and movies) and across genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community."",
}
@article{sperber-etal-2014-segmentation,
    title = ""Segmentation for Efficient Supervised Language Annotation with an Explicit Cost-Utility Tradeoff"",
    author = ""Sperber, Matthias  and
      Simantzik, Mirjam  and
      Neubig, Graham  and
      Nakamura, Satoshi  and
      Waibel, Alex"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1014"",
    doi = ""10.1162/tacl_a_00174"",
    pages = ""169--180"",
    abstract = ""In this paper, we study the problem of manually correcting automatic annotations of natural language in as efficient a manner as possible. We introduce a method for automatically segmenting a corpus into chunks such that many uncertain labels are grouped into the same chunk, while human supervision can be omitted altogether for other segments. A tradeoff must be found for segment sizes. Choosing short segments allows us to reduce the number of highly confident labels that are supervised by the annotator, which is useful because these labels are often already correct and supervising correct labels is a waste of effort. In contrast, long segments reduce the cognitive effort due to context switches. Our method helps find the segmentation that optimizes supervision efficiency by defining user models to predict the cost and utility of supervising each segment and solving a constrained optimization problem balancing these contradictory objectives. A user study demonstrates noticeable gains over pre-segmented, confidence-ordered baselines on two natural language processing tasks: speech transcription and word segmentation."",
}
@article{yogatama-etal-2014-dynamic,
    title = ""Dynamic Language Models for Streaming Text"",
    author = ""Yogatama, Dani  and
      Wang, Chong  and
      Routledge, Bryan R.  and
      Smith, Noah A.  and
      Xing, Eric P."",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1015"",
    doi = ""10.1162/tacl_a_00175"",
    pages = ""181--192"",
    abstract = ""We present a probabilistic language model that captures temporal dynamics and conditions on arbitrary non-linguistic context features. These context features serve as important indicators of language changes that are otherwise difficult to capture using text data by itself. We learn our model in an efficient online fashion that is scalable for large, streaming data. With five streaming datasets from two different genres{---}economics news articles and social media{---}we evaluate our model on the task of sequential language modeling. Our model consistently outperforms competing models."",
}
@article{schneider-etal-2014-discriminative,
    title = ""Discriminative Lexical Semantic Segmentation with Gaps: Running the {MWE} Gamut"",
    author = ""Schneider, Nathan  and
      Danchik, Emily  and
      Dyer, Chris  and
      Smith, Noah A."",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1016"",
    doi = ""10.1162/tacl_a_00176"",
    pages = ""193--206"",
    abstract = ""We present a novel representation, evaluation measure, and supervised models for the task of identifying the multiword expressions (MWEs) in a sentence, resulting in a lexical semantic segmentation. Our approach generalizes a standard chunking representation to encode MWEs containing gaps, thereby enabling efficient sequence tagging algorithms for feature-rich discriminative models. Experiments on a new dataset of English web text offer the first linguistically-driven evaluation of MWE identification with truly heterogeneous expression types. Our statistical sequence model greatly outperforms a lookup-based segmentation procedure, achieving nearly 60{\%} F1 for MWE identification."",
}
@article{socher-etal-2014-grounded,
    title = ""Grounded Compositional Semantics for Finding and Describing Images with Sentences"",
    author = ""Socher, Richard  and
      Karpathy, Andrej  and
      Le, Quoc V.  and
      Manning, Christopher D.  and
      Ng, Andrew Y."",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1017"",
    doi = ""10.1162/tacl_a_00177"",
    pages = ""207--218"",
    abstract = ""Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image."",
}
@article{sultan-etal-2014-back,
    title = ""Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence"",
    author = ""Sultan, Md Arafat  and
      Bethard, Steven  and
      Sumner, Tamara"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1018"",
    doi = ""10.1162/tacl_a_00178"",
    pages = ""219--230"",
    abstract = ""We present a simple, easy-to-replicate monolingual aligner that demonstrates state-of-the-art performance while relying on almost no supervision and a very small number of external resources. Based on the hypothesis that words with similar meanings represent potential pairs for alignment if located in similar contexts, we propose a system that operates by finding such pairs. In two intrinsic evaluations on alignment test data, our system achieves F1 scores of 88{--}92{\%}, demonstrating 1{--}3{\%} absolute improvement over the previous best system. Moreover, in two extrinsic evaluations our aligner outperforms existing aligners, and even a naive application of the aligner approaches state-of-the-art performance in each extrinsic task."",
}
@article{moro-etal-2014-entity,
    title = ""Entity Linking meets Word Sense Disambiguation: a Unified Approach"",
    author = ""Moro, Andrea  and
      Raganato, Alessandro  and
      Navigli, Roberto"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1019"",
    doi = ""10.1162/tacl_a_00179"",
    pages = ""231--244"",
    abstract = ""Entity Linking (EL) and Word Sense Disambiguation (WSD) both address the lexical ambiguity of language. But while the two tasks are pretty similar, they differ in a fundamental respect: in EL the textual mention can be linked to a named entity which may or may not contain the exact mention, while in WSD there is a perfect match between the word form (better, its lemma) and a suitable word sense. In this paper we present Babelfy, a unified graph-based approach to EL and WSD based on a loose identification of candidate meanings coupled with a densest subgraph heuristic which selects high-coherence semantic interpretations. Our experiments show state-of-the-art performances on both tasks on 6 different datasets, including a multilingual setting. Babelfy is online at \url{http://babelfy.org}"",
}
@article{utt-pado-2014-crosslingual,
    title = ""Crosslingual and Multilingual Construction of Syntax-Based Vector Space Models"",
    author = ""Utt, Jason  and
      Pad{\'o}, Sebastian"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1020"",
    doi = ""10.1162/tacl_a_00180"",
    pages = ""245--258"",
    abstract = ""Syntax-based distributional models of lexical semantics provide a flexible and linguistically adequate representation of co-occurrence information. However, their construction requires large, accurately parsed corpora, which are unavailable for most languages. In this paper, we develop a number of methods to overcome this obstacle. We describe (a) a crosslingual approach that constructs a syntax-based model for a new language requiring only an English resource and a translation lexicon; and (b) multilingual approaches that combine crosslingual with monolingual information, subject to availability. We evaluate on two lexical semantic benchmarks in German and Croatian. We find that the models exhibit complementary profiles: crosslingual models yield higher accuracies while monolingual models provide better coverage. In addition, we show that simple multilingual models can successfully combine their strengths."",
}
@article{fang-chang-2014-entity,
    title = ""Entity Linking on Microblogs with Spatial and Temporal Signals"",
    author = ""Fang, Yuan  and
      Chang, Ming-Wei"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1021"",
    doi = ""10.1162/tacl_a_00181"",
    pages = ""259--272"",
    abstract = ""Microblogs present an excellent opportunity for monitoring and analyzing world happenings. Given that words are often ambiguous, entity linking becomes a crucial step towards understanding microblogs. In this paper, we re-examine the problem of entity linking on microblogs. We first observe that spatiotemporal (i.e., spatial and temporal) signals play a key role, but they are not utilized in existing approaches. Thus, we propose a novel entity linking framework that incorporates spatiotemporal signals through a weakly supervised process. Using entity annotations on real-world data, our experiments show that the spatiotemporal model improves F1 by more than 10 points over existing systems. Finally, we present a qualitative study to visualize the effectiveness of our approach."",
}
@article{chambers-etal-2014-dense,
    title = ""Dense Event Ordering with a Multi-Pass Architecture"",
    author = ""Chambers, Nathanael  and
      Cassidy, Taylor  and
      McDowell, Bill  and
      Bethard, Steven"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1022"",
    doi = ""10.1162/tacl_a_00182"",
    pages = ""273--284"",
    abstract = ""The past 10 years of event ordering research has focused on learning partial orderings over document events and time expressions. The most popular corpus, the TimeBank, contains a small subset of the possible ordering graph. Many evaluations follow suit by only testing certain pairs of events (e.g., only main verbs of neighboring sentences). This has led most research to focus on specific learners for partial labelings. This paper attempts to nudge the discussion from identifying some relations to all relations. We present new experiments on strongly connected event graphs that contain ∼10 times more relations per document than the TimeBank. We also describe a shift away from the single learner to a sieve-based architecture that naturally blends multiple learners into a precision-ranked cascade of sieves. Each sieve adds labels to the event graph one at a time, and earlier sieves inform later ones through transitive closure. This paper thus describes innovations in both approach and task. We experiment on the densest event graphs to date and show a 14{\%} gain over state-of-the-art."",
}
@article{hill-etal-2014-multi,
    title = ""Multi-Modal Models for Concrete and Abstract Concept Meaning"",
    author = ""Hill, Felix  and
      Reichart, Roi  and
      Korhonen, Anna"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1023"",
    doi = ""10.1162/tacl_a_00183"",
    pages = ""285--296"",
    abstract = ""Multi-modal models that learn semantic representations from both linguistic and perceptual input outperform language-only models on a range of evaluations, and better reflect human concept acquisition. Most perceptual input to such models corresponds to concrete noun concepts and the superiority of the multi-modal approach has only been established when evaluating on such concepts. We therefore investigate which concepts can be effectively learned by multi-modal models. We show that concreteness determines both which linguistic features are most informative and the impact of perceptual input in such models. We then introduce ridge regression as a means of propagating perceptual information from concrete nouns to more abstract concepts that is more robust than previous approaches. Finally, we present weighted gram matrix combination, a means of combining representations from distinct modalities that outperforms alternatives when both modalities are sufficiently rich."",
}
@article{west-etal-2014-exploiting,
    title = ""Exploiting Social Network Structure for Person-to-Person Sentiment Analysis"",
    author = ""West, Robert  and
      Paskov, Hristo S.  and
      Leskovec, Jure  and
      Potts, Christopher"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1024"",
    doi = ""10.1162/tacl_a_00184"",
    pages = ""297--310"",
    abstract = ""Person-to-person evaluations are prevalent in all kinds of discourse and important for establishing reputations, building social bonds, and shaping public opinion. Such evaluations can be analyzed separately using signed social networks and textual sentiment analysis, but this misses the rich interactions between language and social context. To capture such interactions, we develop a model that predicts individual A{'}s opinion of individual B by synthesizing information from the signed social network in which A and B are embedded with sentiment analysis of the evaluative texts relating A to B. We prove that this problem is NP-hard but can be relaxed to an efficiently solvable hinge-loss Markov random field, and we show that this implementation outperforms text-only and network-only versions in two very different datasets involving community-level decision-making: the Wikipedia Requests for Adminship corpus and the Convote U.S. Congressional speech corpus."",
}
@article{passonneau-carpenter-2014-benefits,
    title = ""The Benefits of a Model of Annotation"",
    author = ""Passonneau, Rebecca J.  and
      Carpenter, Bob"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1025"",
    doi = ""10.1162/tacl_a_00185"",
    pages = ""311--326"",
    abstract = ""Standard agreement measures for interannotator reliability are neither necessary nor sufficient to ensure a high quality corpus. In a case study of word sense annotation, conventional methods for evaluating labels from trained annotators are contrasted with a probabilistic annotation model applied to crowdsourced data. The annotation model provides far more information, including a certainty measure for each gold standard label; the crowdsourced data was collected at less than half the cost of the conventional approach."",
}
@article{lewis-steedman-2014-improved,
    title = ""Improved {CCG} Parsing with Semi-supervised Supertagging"",
    author = ""Lewis, Mike  and
      Steedman, Mark"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1026"",
    doi = ""10.1162/tacl_a_00186"",
    pages = ""327--338"",
    abstract = ""Current supervised parsers are limited by the size of their labelled training data, making improving them with unlabelled data an important goal. We show how a state-of-the-art CCG parser can be enhanced, by predicting lexical categories using unsupervised vector-space embeddings of words. The use of word embeddings enables our model to better generalize from the labelled data, and allows us to accurately assign lexical categories without depending on a POS-tagger. Our approach leads to substantial improvements in dependency parsing results over the standard supervised CCG parser when evaluated on Wall Street Journal (0.8{\%}), Wikipedia (1.8{\%}) and biomedical (3.4{\%}) text. We compare the performance of two recently proposed approaches for classification using a wide variety of word embeddings. We also give a detailed error analysis demonstrating where using embeddings outperforms traditional feature sets, and showing how including POS features can decrease accuracy."",
}
@article{qian-liu-2014-2,
    title = ""2-Slave Dual Decomposition for Generalized Higher Order {CRF}s"",
    author = ""Qian, Xian  and
      Liu, Yang"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1027"",
    doi = ""10.1162/tacl_a_00187"",
    pages = ""339--350"",
    abstract = ""We show that the decoding problem in generalized Higher Order Conditional Random Fields (CRFs) can be decomposed into two parts: one is a tree labeling problem that can be solved in linear time using dynamic programming; the other is a supermodular quadratic pseudo-Boolean maximization problem, which can be solved in cubic time using a minimum cut algorithm. We use dual decomposition to force their agreement. Experimental results on Twitter named entity recognition and sentence dependency tagging tasks show that our method outperforms spanning tree based dual decomposition."",
}
@article{kuznetsova-etal-2014-treetalk,
    title = ""{T}ree{T}alk: Composition and Compression of Trees for Image Descriptions"",
    author = ""Kuznetsova, Polina  and
      Ordonez, Vicente  and
      Berg, Tamara L.  and
      Choi, Yejin"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1028"",
    doi = ""10.1162/tacl_a_00188"",
    pages = ""351--362"",
    abstract = ""We present a new tree based approach to composing expressive image descriptions that makes use of naturally occuring web images with captions. We investigate two related tasks: image caption generalization and generation, where the former is an optional subtask of the latter. The high-level idea of our approach is to harvest expressive phrases (as tree fragments) from existing image descriptions, then to compose a new description by selectively combining the extracted (and optionally pruned) tree fragments. Key algorithmic components are tree composition and compression, both integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation. In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions."",
}
@article{bamman-smith-2014-unsupervised,
    title = ""Unsupervised Discovery of Biographical Structure from Text"",
    author = ""Bamman, David  and
      Smith, Noah A."",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1029"",
    doi = ""10.1162/tacl_a_00189"",
    pages = ""363--376"",
    abstract = ""We present a method for discovering abstract event classes in biographies, based on a probabilistic latent-variable model. Taking as input timestamped text, we exploit latent correlations among events to learn a set of event classes (such as Born, Graduates High School, and Becomes Citizen), along with the typical times in a person{'}s life when those events occur. In a quantitative evaluation at the task of predicting a person{'}s age for a given event, we find that our generative model outperforms a strong linear regression baseline, along with simpler variants of the model that ablate some features. The abstract event classes that we learn allow us to perform a large-scale analysis of 242,970 Wikipedia biographies. Though it is known that women are greatly underrepresented on Wikipedia{---}not only as editors (Wikipedia, 2011) but also as subjects of articles (Reagle and Rhue, 2011){---}we find that there is a bias in their characterization as well, with biographies of women containing significantly more emphasis on events of marriage and divorce than biographies of men."",
}
@article{reddy-etal-2014-large,
    title = ""Large-scale Semantic Parsing without Question-Answer Pairs"",
    author = ""Reddy, Siva  and
      Lapata, Mirella  and
      Steedman, Mark"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1030"",
    doi = ""10.1162/tacl_a_00190"",
    pages = ""377--392"",
    abstract = ""In this paper we introduce a novel semantic parsing approach to query Freebase in natural language without requiring manual annotations or question-answer pairs. Our key insight is to represent natural language via semantic graphs whose topology shares many commonalities with Freebase. Given this representation, we conceptualize semantic parsing as a graph matching problem. Our model converts sentences to semantic graphs using CCG and subsequently grounds them to Freebase guided by denotations as a form of weak supervision. Evaluation experiments on a subset of the Free917 and WebQuestions benchmark datasets show our semantic parser improves over the state of the art."",
}
@article{clark-etal-2014-locally,
    title = ""Locally Non-Linear Learning for Statistical Machine Translation via Discretization and Structured Regularization"",
    author = ""Clark, Jonathan H.  and
      Dyer, Chris  and
      Lavie, Alon"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1031"",
    doi = ""10.1162/tacl_a_00191"",
    pages = ""393--404"",
    abstract = ""Linear models, which support efficient learning and inference, are the workhorses of statistical machine translation; however, linear decision rules are less attractive from a modeling perspective. In this work, we introduce a technique for learning arbitrary, rule-local, non-linear feature transforms that improve model expressivity, but do not sacrifice the efficient inference and learning associated with linear models. To demonstrate the value of our technique, we discard the customary log transform of lexical probabilities and drop the phrasal translation probability in favor of raw counts. We observe that our algorithm learns a variation of a log transform that leads to better translation quality compared to the explicit log transform. We conclude that non-linear responses play an important role in SMT, an observation that we hope will inform the efforts of feature engineers."",
}
@article{kuhlmann-satta-2014-new,
    title = ""A New Parsing Algorithm for {C}ombinatory {C}ategorial {G}rammar"",
    author = ""Kuhlmann, Marco  and
      Satta, Giorgio"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1032"",
    doi = ""10.1162/tacl_a_00192"",
    pages = ""405--418"",
    abstract = ""We present a polynomial-time parsing algorithm for CCG, based on a new decomposition of derivations into small, shareable parts. Our algorithm has the same asymptotic complexity, O(n6), as a previous algorithm by Vijay-Shanker and Weir (1993), but is easier to understand, implement, and prove correct."",
}
@article{rozovskaya-roth-2014-building,
    title = ""Building a State-of-the-Art Grammatical Error Correction System"",
    author = ""Rozovskaya, Alla  and
      Roth, Dan"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1033"",
    doi = ""10.1162/tacl_a_00193"",
    pages = ""419--434"",
    abstract = ""This paper identifies and examines the key principles underlying building a state-of-the-art grammatical error correction system. We do this by analyzing the Illinois system that placed first among seventeen teams in the recent CoNLL-2013 shared task on grammatical error correction. The system focuses on five different types of errors common among non-native English writers. We describe four design principles that are relevant for correcting all of these errors, analyze the system along these dimensions, and show how each of these dimensions contributes to the performance."",
}
@article{xu-etal-2014-extracting,
    title = ""Extracting Lexically Divergent Paraphrases from {T}witter"",
    author = ""Xu, Wei  and
      Ritter, Alan  and
      Callison-Burch, Chris  and
      Dolan, William B.  and
      Ji, Yangfeng"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1034"",
    doi = ""10.1162/tacl_a_00194"",
    pages = ""435--448"",
    abstract = ""We present MultiP (Multi-instance Learning Paraphrase Model), a new model suited to identify paraphrases within the short messages on Twitter. We jointly model paraphrase relations between word and sentence pairs and assume only sentence-level annotations during learning. Using this principled latent variable model alone, we achieve the performance competitive with a state-of-the-art method which combines a latent space model with a feature-based supervised classifier. Our model also captures lexically divergent paraphrases that differ from yet complement previous methods; combining our model with previous work significantly outperforms the state-of-the-art. In addition, we present a novel annotation methodology that has allowed us to crowdsource a paraphrase corpus from Twitter. We make this new dataset available to the research community."",
}
@article{jurgens-navigli-2014-fun,
    title = ""It{'}s All Fun and Games until Someone Annotates: Video Games with a Purpose for Linguistic Annotation"",
    author = ""Jurgens, David  and
      Navigli, Roberto"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1035"",
    doi = ""10.1162/tacl_a_00195"",
    pages = ""449--464"",
    abstract = ""Annotated data is prerequisite for many NLP applications. Acquiring large-scale annotated corpora is a major bottleneck, requiring significant time and resources. Recent work has proposed turning annotation into a game to increase its appeal and lower its cost; however, current games are largely text-based and closely resemble traditional annotation tasks. We propose a new linguistic annotation paradigm that produces annotations from playing graphical video games. The effectiveness of this design is demonstrated using two video games: one to create a mapping from WordNet senses to images, and a second game that performs Word Sense Disambiguation. Both games produce accurate results. The first game yields annotation quality equal to that of experts and a cost reduction of 73{\%} over equivalent crowdsourcing; the second game provides a 16.3{\%} improvement in accuracy over current state-of-the-art sense disambiguation games with WordNet."",
}
@article{zhai-etal-2014-online,
    title = ""Online {A}daptor {G}rammars with Hybrid Inference"",
    author = ""Zhai, Ke  and
      Boyd-Graber, Jordan  and
      Cohen, Shay B."",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1036"",
    doi = ""10.1162/tacl_a_00196"",
    pages = ""465--476"",
    abstract = ""Adaptor grammars are a flexible, powerful formalism for defining nonparametric, unsupervised models of grammar productions. This flexibility comes at the cost of expensive inference. We address the difficulty of inference through an online algorithm which uses a hybrid of Markov chain Monte Carlo and variational inference. We show that this inference strategy improves scalability without sacrificing performance on unsupervised word segmentation and topic modeling tasks."",
}
@article{durrett-klein-2014-joint,
    title = ""A Joint Model for Entity Analysis: Coreference, Typing, and Linking"",
    author = ""Durrett, Greg  and
      Klein, Dan"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1037"",
    doi = ""10.1162/tacl_a_00197"",
    pages = ""477--490"",
    abstract = ""We present a joint model of three core tasks in the entity analysis stack: coreference resolution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia entities). Our model is formally a structured conditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines."",
}
@article{chandlee-etal-2014-learning,
    title = ""Learning Strictly Local Subsequential Functions"",
    author = ""Chandlee, Jane  and
      Eyraud, R{\'e}mi  and
      Heinz, Jeffrey"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1038"",
    doi = ""10.1162/tacl_a_00198"",
    pages = ""491--504"",
    abstract = ""We define two proper subclasses of subsequential functions based on the concept of Strict Locality (McNaughton and Papert, 1971; Rogers and Pullum, 2011; Rogers et al., 2013) for formal languages. They are called Input and Output Strictly Local (ISL and OSL). We provide an automata-theoretic characterization of the ISL class and theorems establishing how the classes are related to each other and to Strictly Local languages. We give evidence that local phonological and morphological processes belong to these classes. Finally we provide a learning algorithm which provably identifies the class of ISL functions in the limit from positive data in polynomial time and data. We demonstrate this learning result on appropriately synthesized artificial corpora. We leave a similar learning result for OSL functions for future work and suggest future directions for addressing non-local phonological processes."",
}
@article{yang-cardie-2014-joint,
    title = ""Joint Modeling of Opinion Expression Extraction and Attribute Classification"",
    author = ""Yang, Bishan  and
      Cardie, Claire"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1039"",
    doi = ""10.1162/tacl_a_00199"",
    pages = ""505--516"",
    abstract = ""In this paper, we study the problems of opinion expression extraction and expression-level polarity and intensity classification. Traditional fine-grained opinion analysis systems address these problems in isolation and thus cannot capture interactions among the textual spans of opinion expressions and their opinion-related properties. We present two types of joint approaches that can account for such interactions during 1) both learning and inference or 2) only during inference. Extensive experiments on a standard dataset demonstrate that our approaches provide substantial improvements over previously published results. By analyzing the results, we gain some insight into the advantages of different joint models."",
}
@article{beinborn-etal-2014-predicting,
    title = ""Predicting the Difficulty of Language Proficiency Tests"",
    author = ""Beinborn, Lisa  and
      Zesch, Torsten  and
      Gurevych, Iryna"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1040"",
    doi = ""10.1162/tacl_a_00200"",
    pages = ""517--530"",
    abstract = ""Language proficiency tests are used to evaluate and compare the progress of language learners. We present an approach for automatic difficulty prediction of C-tests that performs on par with human experts. On the basis of detailed analysis of newly collected data, we develop a model for C-test difficulty introducing four dimensions: solution difficulty, candidate ambiguity, inter-gap dependency, and paragraph difficulty. We show that cues from all four dimensions contribute to C-test difficulty."",
}
@article{lapesa-evert-2014-large,
    title = ""A Large Scale Evaluation of Distributional Semantic Models: Parameters, Interactions and Model Selection"",
    author = ""Lapesa, Gabriella  and
      Evert, Stefan"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1041"",
    doi = ""10.1162/tacl_a_00201"",
    pages = ""531--546"",
    abstract = ""This paper presents the results of a large-scale evaluation study of window-based Distributional Semantic Models on a wide variety of tasks. Our study combines a broad coverage of model parameters with a model selection methodology that is robust to overfitting and able to capture parameter interactions. We show that our strategy allows us to identify parameter configurations that achieve good performance across different datasets and tasks."",
}
@article{vlachos-clark-2014-new,
    title = ""A New Corpus and Imitation Learning Framework for Context-Dependent Semantic Parsing"",
    author = ""Vlachos, Andreas  and
      Clark, Stephen"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1042"",
    doi = ""10.1162/tacl_a_00202"",
    pages = ""547--560"",
    abstract = ""Semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation. Most approaches to this task have been evaluated on a small number of existing corpora which assume that all utterances must be interpreted according to a database and typically ignore context. In this paper we present a new, publicly available corpus for context-dependent semantic parsing. The MRL used for the annotation was designed to support a portable, interactive tourist information system. We develop a semantic parser for this corpus by adapting the imitation learning algorithm DAgger without requiring alignment information during training. DAgger improves upon independently trained classifiers by 9.0 and 4.8 points in F-score on the development and test sets respectively."",
}
@article{belinkov-etal-2014-exploring,
    title = ""Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment"",
    author = ""Belinkov, Yonatan  and
      Lei, Tao  and
      Barzilay, Regina  and
      Globerson, Amir"",
    editor = ""Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""2"",
    year = ""2014"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q14-1043"",
    doi = ""10.1162/tacl_a_00203"",
    pages = ""561--572"",
    abstract = ""Prepositional phrase (PP) attachment disambiguation is a known challenge in syntactic parsing. The lexical sparsity associated with PP attachments motivates research in word representations that can capture pertinent syntactic and semantic features of the word. One promising solution is to use word vectors induced from large amounts of raw text. However, state-of-the-art systems that employ such representations yield modest gains in PP attachment accuracy. In this paper, we show that word vector representations can yield significant PP attachment performance gains. This is achieved via a non-linear architecture that is discriminatively trained to maximize PP attachment accuracy. The architecture is initialized with word vectors trained from unlabeled data, and relearns those to maximize attachment accuracy. We obtain additional performance gains with alternative representations such as dependency-based word vectors. When tested on both English and Arabic datasets, our method outperforms both a strong SVM classifier and state-of-the-art parsers. For instance, we achieve 82.6{\%} PP attachment accuracy on Arabic, while the Turbo and Charniak self-trained parsers obtain 76.7{\%} and 80.8{\%} respectively."",
}
@proceedings{acl-2014-association-linguistics-tutorials,
    title = ""Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials"",
    editor = ""Fraser, Alex  and
      Liu, Yang"",
    month = jun,
    year = ""2014"",
    address = ""Baltimore, Maryland, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/P14-6000"",
    doi = ""10.3115/v1/P14-6"",
}
@",clinical narr,clinical domain,clinical not,natural language process,natural languag,language process,translat,nlp,infer,generat,relation extract,entity recognit,sequence label,sentiment,semant,shared task,annotat,question-answ,challeng,benchmark,evalu
" ""Generating and using probabilistic morphological resources for the biomedical domain"","," ""In most Indo-European languages, many biomedical terms are rich morphological structures composed of several constituents mainly originating from Greek or Latin. The interpretation of these compounds are keystones to access information. In this paper, we present morphological resources aiming at coping with these biomedical morphological compounds. Following previous work (Claveau et al. 2011,Claveau et al. 12), these resources are automatically built using Japanese terms in Kanjis as a pivot language and alignment techniques. We show how these alignment information can be used for segmenting compounds, attaching semantic interpretation to each part, proposing definitions (gloses) of the compounds... When possible, these tasks are compared with state-of-the-art tools, and the results show the interest of our automatically built probabilistic resources."",","{claveau-kijak-2014-generating,
    title = ""Generating and using probabilistic morphological resources for the biomedical domain"",
    author = ""Claveau, Vincent  and
      Kijak, Ewa"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)"",
    month = may,
    year = ""2014"",
    address = ""Reykjavik, Iceland"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2014/pdf/1003_Paper.pdf"",
    pages = ""3348--3354"",
    abstract = ""In most Indo-European languages, many biomedical terms are rich morphological structures composed of several constituents mainly originating from Greek or Latin. The interpretation of these compounds are keystones to access information. In this paper, we present morphological resources aiming at coping with these biomedical morphological compounds. Following previous work (Claveau et al. 2011,Claveau et al. 12), these resources are automatically built using Japanese terms in Kanjis as a pivot language and alignment techniques. We show how these alignment information can be used for segmenting compounds, attaching semantic interpretation to each part, proposing definitions (gloses) of the compounds... When possible, these tasks are compared with state-of-the-art tools, and the results show the interest of our automatically built probabilistic resources."",
}
@",medical domain,generat,semant,evalu
" ""Collaboratively Annotating Multilingual Parallel Corpora in the Biomedical Domain{---}some {MANTRA}s"","," ""The coverage of multilingual biomedical resources is high for the English language, yet sparse for non-English languages―an observation which holds for seemingly well-resourced, yet still dramatically low-resourced ones such as Spanish, French or German but even more so for really under-resourced ones such as Dutch. We here present experimental results for automatically annotating parallel corpora and simultaneously acquiring new biomedical terminology for these under-resourced non-English languages on the basis of two types of language resources, namely parallel corpora (i.e. full translation equivalents at the document unit level) and (admittedly deficient) multilingual biomedical terminologies, with English as their anchor language. We automatically annotate these parallel corpora with biomedical named entities by an ensemble of named entity taggers and harmonize non-identical annotations the outcome of which is a so-called silver standard corpus. We conclude with an empirical assessment of this approach to automatically identify both known and new terms in multilingual corpora."",","{hellrich-etal-2014-collaboratively,
    title = ""Collaboratively Annotating Multilingual Parallel Corpora in the Biomedical Domain{---}some {MANTRA}s"",
    author = ""Hellrich, Johannes  and
      Clematide, Simon  and
      Hahn, Udo  and
      Rebholz-Schuhmann, Dietrich"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)"",
    month = may,
    year = ""2014"",
    address = ""Reykjavik, Iceland"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2014/pdf/1064_Paper.pdf"",
    pages = ""4033--4040"",
    abstract = ""The coverage of multilingual biomedical resources is high for the English language, yet sparse for non-English languages―an observation which holds for seemingly well-resourced, yet still dramatically low-resourced ones such as Spanish, French or German but even more so for really under-resourced ones such as Dutch. We here present experimental results for automatically annotating parallel corpora and simultaneously acquiring new biomedical terminology for these under-resourced non-English languages on the basis of two types of language resources, namely parallel corpora (i.e. full translation equivalents at the document unit level) and (admittedly deficient) multilingual biomedical terminologies, with English as their anchor language. We automatically annotate these parallel corpora with biomedical named entities by an ensemble of named entity taggers and harmonize non-identical annotations the outcome of which is a so-called silver standard corpus. We conclude with an empirical assessment of this approach to automatically identify both known and new terms in multilingual corpora."",
}
@",medical domain,translat,annotat,evalu,assess
" ""Using Large Biomedical Databases as Gold Annotations for Automatic Relation Extraction"","," ""We show how to use large biomedical databases in order to obtain a gold standard for training a machine learning system over a corpus of biomedical text. As an example we use the Comparative Toxicogenomics Database (CTD) and describe by means of a short case study how the obtained data can be applied. We explain how we exploit the structure of the database for compiling training material and a testset. Using a Naive Bayes document classification approach based on words, stem bigrams and MeSH descriptors we achieve a macro-average F-score of 61{\%} on a subset of 8 action terms. This outperforms a baseline system based on a lookup of stemmed keywords by more than 20{\%}. Furthermore, we present directions of future work, taking the described system as a vantage point. Future work will be aiming towards a weakly supervised system capable of discovering complete biomedical interactions and events."",","{ellendorff-etal-2014-using,
    title = ""Using Large Biomedical Databases as Gold Annotations for Automatic Relation Extraction"",
    author = ""Ellendorff, Tilia  and
      Rinaldi, Fabio  and
      Clematide, Simon"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)"",
    month = may,
    year = ""2014"",
    address = ""Reykjavik, Iceland"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2014/pdf/1156_Paper.pdf"",
    pages = ""3736--3741"",
    abstract = ""We show how to use large biomedical databases in order to obtain a gold standard for training a machine learning system over a corpus of biomedical text. As an example we use the Comparative Toxicogenomics Database (CTD) and describe by means of a short case study how the obtained data can be applied. We explain how we exploit the structure of the database for compiling training material and a testset. Using a Naive Bayes document classification approach based on words, stem bigrams and MeSH descriptors we achieve a macro-average F-score of 61{\%} on a subset of 8 action terms. This outperforms a baseline system based on a lookup of stemmed keywords by more than 20{\%}. Furthermore, we present directions of future work, taking the described system as a vantage point. Future work will be aiming towards a weakly supervised system capable of discovering complete biomedical interactions and events."",
}
@",medical text,relation extract,annotat,evalu
" ""Annotating Inter-Sentence Temporal Relations in Clinical Notes"","," ""Owing in part to the surge of interest in temporal relation extraction, a number of datasets manually annotated with temporal relations between event-event pairs and event-time pairs have been produced recently. However, it is not uncommon to find missing annotations in these manually annotated datasets. Many researchers attributed this problem to {``}annotator fatigue{''}. While some of these missing relations can be recovered automatically, many of them cannot. Our goals in this paper are to (1) manually annotate certain types of missing links that cannot be automatically recovered in the i2b2 Clinical Temporal Relations Challenge Corpus, one of the recently released evaluation corpora for temporal relation extraction; and (2) empirically determine the usefulness of these additional annotations. We will make our annotations publicly available, in hopes of enabling a more accurate evaluation of temporal relation extraction systems."",","{dsouza-ng-2014-annotating,
    title = ""Annotating Inter-Sentence Temporal Relations in Clinical Notes"",
    author = ""D{'}Souza, Jennifer  and
      Ng, Vincent"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)"",
    month = may,
    year = ""2014"",
    address = ""Reykjavik, Iceland"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2014/pdf/1185_Paper.pdf"",
    pages = ""2758--2765"",
    abstract = ""Owing in part to the surge of interest in temporal relation extraction, a number of datasets manually annotated with temporal relations between event-event pairs and event-time pairs have been produced recently. However, it is not uncommon to find missing annotations in these manually annotated datasets. Many researchers attributed this problem to {``}annotator fatigue{''}. While some of these missing relations can be recovered automatically, many of them cannot. Our goals in this paper are to (1) manually annotate certain types of missing links that cannot be automatically recovered in the i2b2 Clinical Temporal Relations Challenge Corpus, one of the recently released evaluation corpora for temporal relation extraction; and (2) empirically determine the usefulness of these additional annotations. We will make our annotations publicly available, in hopes of enabling a more accurate evaluation of temporal relation extraction systems."",
}
@",clinical not,relation extract,annotat,challeng,evalu
" ""Annotation of specialized corpora using a comprehensive entity and relation scheme"","," ""Annotated corpora are essential resources for many applications in Natural Language Processing. They provide insight on the linguistic and semantic characteristics of the genre and domain covered, and can be used for the training and evaluation of automatic tools. In the biomedical domain, annotated corpora of English texts have become available for several genres and subfields. However, very few similar resources are available for languages other than English. In this paper we present an effort to produce a high-quality corpus of clinical documents in French, annotated with a comprehensive scheme of entities and relations. We present the annotation scheme as well as the results of a pilot annotation study covering 35 clinical documents in a variety of subfields and genres. We show that high inter-annotator agreement can be achieved using a complex annotation scheme."",","{deleger-etal-2014-annotation,
    title = ""Annotation of specialized corpora using a comprehensive entity and relation scheme"",
    author = ""Del{\'e}ger, Louise  and
      Ligozat, Anne-Laure  and
      Grouin, Cyril  and
      Zweigenbaum, Pierre  and
      N{\'e}v{\'e}ol, Aur{\'e}lie"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)"",
    month = may,
    year = ""2014"",
    address = ""Reykjavik, Iceland"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2014/pdf/552_Paper.pdf"",
    pages = ""1267--1274"",
    abstract = ""Annotated corpora are essential resources for many applications in Natural Language Processing. They provide insight on the linguistic and semantic characteristics of the genre and domain covered, and can be used for the training and evaluation of automatic tools. In the biomedical domain, annotated corpora of English texts have become available for several genres and subfields. However, very few similar resources are available for languages other than English. In this paper we present an effort to produce a high-quality corpus of clinical documents in French, annotated with a comprehensive scheme of entities and relations. We present the annotation scheme as well as the results of a pilot annotation study covering 35 clinical documents in a variety of subfields and genres. We show that high inter-annotator agreement can be achieved using a complex annotation scheme."",
}
@",medical domain,natural language process,natural languag,language process,semant,annotat,evalu
" ""Language Resources for {F}rench in the Biomedical Domain"","," ""The biomedical domain offers a wealth of linguistic resources for Natural Language Processing, including terminologies and corpora. While many of these resources are prominently available for English, other languages including French benefit from substantial coverage thanks to the contribution of an active community over the past decades. However, access to terminological resources in languages other than English may not be as straight-forward as access to their English counterparts. Herein, we review the extent of resource coverage for French and give pointers to access French-language resources. We also discuss the sources and methods for making additional material available for French."",","{neveol-etal-2014-language,
    title = ""Language Resources for {F}rench in the Biomedical Domain"",
    author = ""N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Grosjean, Julien  and
      Darmoni, St{\'e}fan  and
      Zweigenbaum, Pierre"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)"",
    month = may,
    year = ""2014"",
    address = ""Reykjavik, Iceland"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2014/pdf/604_Paper.pdf"",
    pages = ""2146--2151"",
    abstract = ""The biomedical domain offers a wealth of linguistic resources for Natural Language Processing, including terminologies and corpora. While many of these resources are prominently available for English, other languages including French benefit from substantial coverage thanks to the contribution of an active community over the past decades. However, access to terminological resources in languages other than English may not be as straight-forward as access to their English counterparts. Herein, we review the extent of resource coverage for French and give pointers to access French-language resources. We also discuss the sources and methods for making additional material available for French."",
}
@",medical domain,natural language process,natural languag,language process,evalu
" ""Clinical Data-Driven Probabilistic Graph Processing"","," ""Electronic Medical Records (EMRs) encode an extraordinary amount of medical knowledge. Collecting and interpreting this knowledge, however, belies a significant level of clinical understanding. Automatically capturing the clinical information is crucial for performing comparative effectiveness research. In this paper, we present a data-driven approach to model semantic dependencies between medical concepts, qualified by the beliefs of physicians. The dependencies, captured in a patient cohort graph of clinical pictures and therapies is further refined into a probabilistic graphical model which enables efficient inference of patient-centered treatment or test recommendations (based on probabilities). To perform inference on the graphical model, we describe a technique of smoothing the conditional likelihood of medical concepts by their semantically-similar belief values. The experimental results, as compared against clinical guidelines are very promising."",","{goodwin-harabagiu-2014-clinical,
    title = ""Clinical Data-Driven Probabilistic Graph Processing"",
    author = ""Goodwin, Travis  and
      Harabagiu, Sanda"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)"",
    month = may,
    year = ""2014"",
    address = ""Reykjavik, Iceland"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2014/pdf/618_Paper.pdf"",
    pages = ""101--108"",
    abstract = ""Electronic Medical Records (EMRs) encode an extraordinary amount of medical knowledge. Collecting and interpreting this knowledge, however, belies a significant level of clinical understanding. Automatically capturing the clinical information is crucial for performing comparative effectiveness research. In this paper, we present a data-driven approach to model semantic dependencies between medical concepts, qualified by the beliefs of physicians. The dependencies, captured in a patient cohort graph of clinical pictures and therapies is further refined into a probabilistic graphical model which enables efficient inference of patient-centered treatment or test recommendations (based on probabilities). To perform inference on the graphical model, we describe a technique of smoothing the conditional likelihood of medical concepts by their semantically-similar belief values. The experimental results, as compared against clinical guidelines are very promising."",
}
@",medical record,infer,semant,evalu
" ""Sublanguage Corpus Analysis Toolkit: A tool for assessing the representativeness and sublanguage characteristics of corpora"","," ""Sublanguages are varieties of language that form subsets of the general language, typically exhibiting particular types of lexical, semantic, and other restrictions and deviance. SubCAT, the Sublanguage Corpus Analysis Toolkit, assesses the representativeness and closure properties of corpora to analyze the extent to which they are either sublanguages, or representative samples of the general language. The current version of SubCAT contains scripts and applications for assessing lexical closure, morphological closure, sentence type closure, over-represented words, and syntactic deviance. Its operation is illustrated with three case studies concerning scientific journal articles, patents, and clinical records. Materials from two language families are analyzed―English (Germanic), and Bulgarian (Slavic). The software is available at sublanguage.sourceforge.net under a liberal Open Source license."",","{temnikova-etal-2014-sublanguage,
    title = ""Sublanguage Corpus Analysis Toolkit: A tool for assessing the representativeness and sublanguage characteristics of corpora"",
    author = ""Temnikova, Irina  and
      Baumgartner Jr., William A.  and
      Hailu, Negacy D.  and
      Nikolova, Ivelina  and
      McEnery, Tony  and
      Kilgarriff, Adam  and
      Angelova, Galia  and
      Cohen, K. Bretonnel"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)"",
    month = may,
    year = ""2014"",
    address = ""Reykjavik, Iceland"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2014/pdf/675_Paper.pdf"",
    pages = ""1714--1718"",
    abstract = ""Sublanguages are varieties of language that form subsets of the general language, typically exhibiting particular types of lexical, semantic, and other restrictions and deviance. SubCAT, the Sublanguage Corpus Analysis Toolkit, assesses the representativeness and closure properties of corpora to analyze the extent to which they are either sublanguages, or representative samples of the general language. The current version of SubCAT contains scripts and applications for assessing lexical closure, morphological closure, sentence type closure, over-represented words, and syntactic deviance. Its operation is illustrated with three case studies concerning scientific journal articles, patents, and clinical records. Materials from two language families are analyzed―English (Germanic), and Bulgarian (Slavic). The software is available at sublanguage.sourceforge.net under a liberal Open Source license."",
}
@",clinical record,semant,evalu,assess
" ""Towards Multilingual Conversations in the Medical Domain: Development of Multilingual Medical Data and A Network-based {ASR} System"","," ""This paper outlines the recent development on multilingual medical data and multilingual speech recognition system for network-based speech-to-speech translation in the medical domain. The overall speech-to-speech translation (S2ST) system was designed to translate spoken utterances from a given source language into a target language in order to facilitate multilingual conversations and reduce the problems caused by language barriers in medical situations. Our final system utilizes a weighted finite-state transducers with n-gram language models. Currently, the system successfully covers three languages: Japanese, English, and Chinese. The difficulties involved in connecting Japanese, English and Chinese speech recognition systems through Web servers will be discussed, and the experimental results in simulated medical conversation will also be presented."",","{sakti-etal-2014-towards,
    title = ""Towards Multilingual Conversations in the Medical Domain: Development of Multilingual Medical Data and A Network-based {ASR} System"",
    author = ""Sakti, Sakriani  and
      Kubo, Keigo  and
      Matsumiya, Sho  and
      Neubig, Graham  and
      Toda, Tomoki  and
      Nakamura, Satoshi  and
      Adachi, Fumihiro  and
      Isotani, Ryosuke"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)"",
    month = may,
    year = ""2014"",
    address = ""Reykjavik, Iceland"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2014/pdf/709_Paper.pdf"",
    pages = ""2639--2643"",
    abstract = ""This paper outlines the recent development on multilingual medical data and multilingual speech recognition system for network-based speech-to-speech translation in the medical domain. The overall speech-to-speech translation (S2ST) system was designed to translate spoken utterances from a given source language into a target language in order to facilitate multilingual conversations and reduce the problems caused by language barriers in medical situations. Our final system utilizes a weighted finite-state transducers with n-gram language models. Currently, the system successfully covers three languages: Japanese, English, and Chinese. The difficulties involved in connecting Japanese, English and Chinese speech recognition systems through Web servers will be discussed, and the experimental results in simulated medical conversation will also be presented."",
}
@",medical domain,translat,evalu
" ""Multilingual Test Sets for Machine Translation of Search Queries for Cross-Lingual Information Retrieval in the Medical Domain"","," ""This paper presents development and test sets for machine translation of search queries in cross-lingual information retrieval in the medical domain. The data consists of the total of 1,508 real user queries in English translated to Czech, German, and French. We describe the translation and review process involving medical professionals and present a baseline experiment where our data sets are used for tuning and evaluation of a machine translation system."",","{uresova-etal-2014-multilingual,
    title = ""Multilingual Test Sets for Machine Translation of Search Queries for Cross-Lingual Information Retrieval in the Medical Domain"",
    author = ""Ure{\v{s}}ov{\'a}, Zde{\v{n}}ka  and
      Haji{\v{c}}, Jan  and
      Pecina, Pavel  and
      Du{\v{s}}ek, Ond{\v{r}}ej"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)"",
    month = may,
    year = ""2014"",
    address = ""Reykjavik, Iceland"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2014/pdf/99_Paper.pdf"",
    abstract = ""This paper presents development and test sets for machine translation of search queries in cross-lingual information retrieval in the medical domain. The data consists of the total of 1,508 real user queries in English translated to Czech, German, and French. We describe the translation and review process involving medical professionals and present a baseline experiment where our data sets are used for tuning and evaluation of a machine translation system."",
}
@",medical domain,translat,information retriev,evalu
" ""Exploring a Probabilistic {E}arley Parser for Event Composition in Biomedical Texts"",",,"{tran-etal-2013-exploring,
    title = ""Exploring a Probabilistic {E}arley Parser for Event Composition in Biomedical Texts"",
    author = ""Tran, Mai-Vu  and
      Collier, Nigel  and
      Le, Hoang-Quynh  and
      Phi, Van-Thuy  and
      Pham, Thanh-Binh"",
    editor = ""N{\'e}dellec, Claire  and
      Bossy, Robert  and
      Kim, Jin-Dong  and
      Kim, Jung-jae  and
      Ohta, Tomoko  and
      Pyysalo, Sampo  and
      Zweigenbaum, Pierre"",
    booktitle = ""Proceedings of the {B}io{NLP} Shared Task 2013 Workshop"",
    month = aug,
    year = ""2013"",
    address = ""Sofia, Bulgaria"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W13-2019"",
    pages = ""130--134"",
}
@",medical text,nlp,shared task
" ""Proceedings of the {IWCS} 2013 Workshop on Computational Semantics in Clinical Text ({CSCT} 2013)"",",,"{pustejovsky-2013-inference,
    title = ""Inference Patterns with Intensional Adjectives"",
    author = ""Pustejovsky, James"",
    editor = ""Bunt, Harry"",
    booktitle = ""Proceedings of the 9th Joint {ISO} - {ACL} {SIGSEM} Workshop on Interoperable Semantic Annotation"",
    month = mar,
    year = ""2013"",
    address = ""Potsdam, Germany"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W13-0509"",
    pages = ""85--89"",
}
@proceedings{ws-2013-iwcs-2013,
    title = ""Proceedings of the {IWCS} 2013 Workshop on Computational Semantics in Clinical Text ({CSCT} 2013)"",
    editor = ""Wu, Stephen  and
      Shah, Nigam  and
      Cohen, Kevin Bretonnel"",
    month = mar,
    year = ""2013"",
    address = ""Potsdam, Germany"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W13-0400"",
}
@",clinical text,infer,semant,annotat
" ""Evaluating the Use of Empirically Constructed Lexical Resources for Named Entity Recognition"",",,"{jonnalagadda-etal-2013-evaluating,
    title = ""Evaluating the Use of Empirically Constructed Lexical Resources for Named Entity Recognition"",
    author = ""Jonnalagadda, Siddhartha  and
      Cohen, Trevor  and
      Wu, Stephen  and
      Liu, Hongfang  and
      Gonzalez, Graciela"",
    editor = ""Wu, Stephen  and
      Shah, Nigam  and
      Cohen, Kevin Bretonnel"",
    booktitle = ""Proceedings of the {IWCS} 2013 Workshop on Computational Semantics in Clinical Text ({CSCT} 2013)"",
    month = mar,
    year = ""2013"",
    address = ""Potsdam, Germany"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W13-0404"",
    pages = ""23--33"",
}
@",clinical text,entity recognit,semant,evalu
" ""Analysis of Cross-Institutional Medication Information Annotations in Clinical Notes"",",,"{sohn-etal-2013-analysis,
    title = ""Analysis of Cross-Institutional Medication Information Annotations in Clinical Notes"",
    author = ""Sohn, Sunghwan  and
      Clark, Cheryl  and
      Halgrim, Scott  and
      Murphy, Sean  and
      Jonnalagadda, Siddhartha  and
      Wagholikar, Kavishwar  and
      Wu, Stephen  and
      Chute, Christopher  and
      Liu, Hongfang"",
    editor = ""Wu, Stephen  and
      Shah, Nigam  and
      Cohen, Kevin Bretonnel"",
    booktitle = ""Proceedings of the {IWCS} 2013 Workshop on Computational Semantics in Clinical Text ({CSCT} 2013)"",
    month = mar,
    year = ""2013"",
    address = ""Potsdam, Germany"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W13-0406"",
    pages = ""44--53"",
}
@",clinical not,clinical text,semant,annotat
" ""Proceedings of the {IWCS} 2013 Workshop on Annotation of Modal Meanings in Natural Language ({WAMM})"",",,"{thorne-etal-2013-vericlig,
    title = ""The {VERICLIG} Project: Extraction of Computer Interpretable Guidelines via Syntactic and Semantic Annotation"",
    author = ""Thorne, Camilo  and
      Montali, Marco  and
      Calvanese, Diego  and
      Cardillo, Elena  and
      Eccher, Claudio"",
    editor = ""Wu, Stephen  and
      Shah, Nigam  and
      Cohen, Kevin Bretonnel"",
    booktitle = ""Proceedings of the {IWCS} 2013 Workshop on Computational Semantics in Clinical Text ({CSCT} 2013)"",
    month = mar,
    year = ""2013"",
    address = ""Potsdam, Germany"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W13-0407"",
    pages = ""54--58"",
}
@proceedings{ws-2013-iwcs,
    title = ""Proceedings of the {IWCS} 2013 Workshop on Annotation of Modal Meanings in Natural Language ({WAMM})"",
    editor = ""Portner, Paul  and
      Rubinstein, Aynat  and
      Katz, Graham"",
    month = mar,
    year = ""2013"",
    address = ""Potsdam, Germany"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W13-0300"",
}
@",clinical text,natural languag,semant,annotat
" ""{S}em{E}val-2013 Task 9 : Extraction of Drug-Drug Interactions from Biomedical Texts ({DDIE}xtraction 2013)"",",,"{segura-bedmar-etal-2013-semeval,
    title = ""{S}em{E}val-2013 Task 9 : Extraction of Drug-Drug Interactions from Biomedical Texts ({DDIE}xtraction 2013)"",
    author = ""Segura-Bedmar, Isabel  and
      Mart{\'\i}nez, Paloma  and
      Herrero-Zazo, Mar{\'\i}a"",
    editor = ""Manandhar, Suresh  and
      Yuret, Deniz"",
    booktitle = ""Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)"",
    month = jun,
    year = ""2013"",
    address = ""Atlanta, Georgia, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S13-2056"",
    pages = ""341--350"",
}
@",medical text,semant,evalu
" ""{UC}3{M}: A kernel-based approach to identify and classify {DDI}s in bio-medical texts."",",,"{sanchez-cisneros-2013-uc3m,
    title = ""{UC}3{M}: A kernel-based approach to identify and classify {DDI}s in bio-medical texts."",
    author = ""Sanchez-Cisneros, Daniel"",
    editor = ""Manandhar, Suresh  and
      Yuret, Deniz"",
    booktitle = ""Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)"",
    month = jun,
    year = ""2013"",
    address = ""Atlanta, Georgia, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S13-2103"",
    pages = ""617--621"",
}
@",medical text,semant,evalu
" ""{UEM}-{UC}3{M}: An Ontology-based named entity recognition system for biomedical texts."",",,"{sanchez-cisneros-aparicio-gali-2013-uem,
    title = ""{UEM}-{UC}3{M}: An Ontology-based named entity recognition system for biomedical texts."",
    author = ""Sanchez-Cisneros, Daniel  and
      Aparicio Gali, Fernando"",
    editor = ""Manandhar, Suresh  and
      Yuret, Deniz"",
    booktitle = ""Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)"",
    month = jun,
    year = ""2013"",
    address = ""Atlanta, Georgia, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S13-2104"",
    pages = ""622--627"",
}
@",medical text,entity recognit,semant,evalu
" ""{UMCC}{\_}{DLSI}: Semantic and Lexical features for detection and classification Drugs in biomedical texts"",",,"{collazo-etal-2013-umcc,
    title = ""{UMCC}{\_}{DLSI}: Semantic and Lexical features for detection and classification Drugs in biomedical texts"",
    author = ""Collazo, Armando  and
      Ceballo, Alberto  and
      Puig, Dennys D.  and
      Guti{\'e}rrez, Yoan  and
      Abreu, Jos{\'e} I.  and
      P{\'e}rez, Roger  and
      Fern{\'a}ndez Orqu{\'\i}n, Antonio  and
      Montoyo, Andr{\'e}s  and
      Mu{\~n}oz, Rafael  and
      Camara, Franc"",
    editor = ""Manandhar, Suresh  and
      Yuret, Deniz"",
    booktitle = ""Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)"",
    month = jun,
    year = ""2013"",
    address = ""Atlanta, Georgia, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S13-2106"",
    pages = ""636--643"",
}
@",medical text,semant,evalu
" ""{UC}olorado{\_}{SOM}: Extraction of Drug-Drug Interactions from Biomedical Text using Knowledge-rich and Knowledge-poor Features"",",,"{hailu-etal-2013-ucolorado,
    title = ""{UC}olorado{\_}{SOM}: Extraction of Drug-Drug Interactions from Biomedical Text using Knowledge-rich and Knowledge-poor Features"",
    author = ""Hailu, Negacy  and
      Hunter, Lawrence E.  and
      Cohen, K. Bretonnel"",
    editor = ""Manandhar, Suresh  and
      Yuret, Deniz"",
    booktitle = ""Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)"",
    month = jun,
    year = ""2013"",
    address = ""Atlanta, Georgia, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/S13-2112"",
    pages = ""684--688"",
}
@",medical text,semant,evalu
" ""Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Tutorials)"","," ""We develop two techniques for analyzing the effect of porting a machine translation system to a new domain. One is a macro-level analysis that measures how domain shift affects corpus-level evaluation; the second is a micro-level analysis for word-level errors. We apply these methods to understand what happens when a Parliament-trained phrase-based machine translation system is applied in four very different domains: news, medical texts, scientific articles and movie subtitles. We present quantitative and qualitative experiments that highlight opportunities for future research in domain adaptation for machine translation."",","{zsibrita-etal-2013-magyarlanc,
    title = ""magyarlanc: A Tool for Morphological and Dependency Parsing of {H}ungarian"",
    author = ""Zsibrita, J{\'a}nos  and
      Vincze, Veronika  and
      Farkas, Rich{\'a}rd"",
    editor = ""Mitkov, Ruslan  and
      Angelova, Galia  and
      Bontcheva, Kalina"",
    booktitle = ""Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013"",
    month = sep,
    year = ""2013"",
    address = ""Hissar, Bulgaria"",
    publisher = ""INCOMA Ltd. Shoumen, BULGARIA"",
    url = ""https://aclanthology.org/R13-1099"",
    pages = ""763--771"",
}
@proceedings{tacl-2013-transactions,
    title = ""Transactions of the Association for Computational Linguistics, Volume 1"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1000"",
}
@article{tackstrom-etal-2013-token,
    title = ""Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging"",
    author = {T{\""a}ckstr{\""o}m, Oscar  and
      Das, Dipanjan  and
      Petrov, Slav  and
      McDonald, Ryan  and
      Nivre, Joakim},
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1001"",
    doi = ""10.1162/tacl_a_00205"",
    pages = ""1--12"",
    abstract = ""We consider the construction of part-of-speech taggers for resource-poor languages. Recently, manually constructed tag dictionaries from Wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting. In this paper, we show that additional token constraints can be projected from a resource-rich source language to a resource-poor target language via word-aligned bitext. We present several models to this end; in particular a partially observed conditional random field model, where coupled token and type constraints provide a partial signal for training. Averaged across eight previously studied Indo-European languages, our model achieves a 25{\%} relative error reduction over the prior state of the art. We further present successful results on seven additional languages from different families, empirically demonstrating the applicability of coupled token and type constraints across a diverse set of languages."",
}
@article{pitler-etal-2013-finding,
    title = ""Finding Optimal 1-Endpoint-Crossing Trees"",
    author = ""Pitler, Emily  and
      Kannan, Sampath  and
      Marcus, Mitchell"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1002"",
    doi = ""10.1162/tacl_a_00206"",
    pages = ""13--24"",
    abstract = ""Dependency parsing algorithms capable of producing the types of crossing dependencies seen in natural language sentences have traditionally been orders of magnitude slower than algorithms for projective trees. For 95.8{--}99.8{\%} of dependency parses in various natural language treebanks, whenever an edge is crossed, the edges that cross it all have a common vertex. The optimal dependency tree that satisfies this 1-Endpoint-Crossing property can be found with an O(n4) parsing algorithm that recursively combines forests over intervals with one exterior point. 1-Endpoint-Crossing trees also have natural connections to linguistics and another class of graphs that has been studied in NLP."",
}
@article{regneri-etal-2013-grounding,
    title = ""Grounding Action Descriptions in Videos"",
    author = ""Regneri, Michaela  and
      Rohrbach, Marcus  and
      Wetzel, Dominikus  and
      Thater, Stefan  and
      Schiele, Bernt  and
      Pinkal, Manfred"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1003"",
    doi = ""10.1162/tacl_a_00207"",
    pages = ""25--36"",
    abstract = ""Recent work has shown that the integration of visual information into text-based models can substantially improve model predictions, but so far only visual information extracted from static images has been used. In this paper, we consider the problem of grounding sentences describing actions in visual information extracted from videos. We present a general purpose corpus that aligns high quality videos with multiple natural language descriptions of the actions portrayed in the videos, together with an annotation of how similar the action descriptions are to each other. Experimental results demonstrate that a text-based model of similarity between actions improves substantially when combined with visual information from videos depicting the described actions."",
}
@article{qian-liu-2013-branch,
    title = ""Branch and Bound Algorithm for Dependency Parsing with Non-local Features"",
    author = ""Qian, Xian  and
      Liu, Yang"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1004"",
    doi = ""10.1162/tacl_a_00208"",
    pages = ""37--48"",
    abstract = ""Graph based dependency parsing is inefficient when handling non-local features due to high computational complexity of inference. In this paper, we proposed an exact and efficient decoding algorithm based on the Branch and Bound (B{\&}B) framework where non-local features are bounded by a linear combination of local features. Dynamic programming is used to search the upper bound. Experiments are conducted on English PTB and Chinese CTB datasets. We achieved competitive Unlabeled Attachment Score (UAS) when no additional resources are available: 93.17{\%} for English and 87.25{\%} for Chinese. Parsing speed is 177 words per second for English and 97 words per second for Chinese. Our algorithm is general and can be adapted to non-projective dependency parsing or other graphical models."",
}
@article{artzi-zettlemoyer-2013-weakly,
    title = ""Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions"",
    author = ""Artzi, Yoav  and
      Zettlemoyer, Luke"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1005"",
    doi = ""10.1162/tacl_a_00209"",
    pages = ""49--62"",
    abstract = ""The context in which language is used provides a strong signal for learning to recover its meaning. In this paper, we show it can be used within a grounded CCG semantic parsing approach that learns a joint model of meaning and context for interpreting and executing natural language instructions, using various types of weak supervision. The joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to directly influence learning. It also enables algorithms that learn while executing instructions, for example by trying to replicate human actions. Experiments on a benchmark navigational dataset demonstrate strong performance under differing forms of supervision, including correctly executing 60{\%} more instruction sets relative to the previous state of the art."",
}
@article{pate-goldwater-2013-unsupervised,
    title = ""Unsupervised Dependency Parsing with Acoustic Cues"",
    author = ""Pate, John K  and
      Goldwater, Sharon"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1006"",
    doi = ""10.1162/tacl_a_00210"",
    pages = ""63--74"",
    abstract = ""Unsupervised parsing is a difficult task that infants readily perform. Progress has been made on this task using text-based models, but few computational approaches have considered how infants might benefit from acoustic cues. This paper explores the hypothesis that word duration can help with learning syntax. We describe how duration information can be incorporated into an unsupervised Bayesian dependency parser whose only other source of information is the words themselves (without punctuation or parts of speech). Our results, evaluated on both adult-directed and child-directed utterances, show that using word duration can improve parse quality relative to words-only baselines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech."",
}
@article{bisk-hockenmaier-2013-hdp,
    title = ""An {HDP} Model for Inducing {C}ombinatory {C}ategorial {G}rammars"",
    author = ""Bisk, Yonatan  and
      Hockenmaier, Julia"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1007"",
    doi = ""10.1162/tacl_a_00211"",
    pages = ""75--88"",
    abstract = ""We introduce a novel nonparametric Bayesian model for the induction of Combinatory Categorial Grammars from POS-tagged text. It achieves state of the art performance on a number of languages, and induces linguistically plausible lexicons."",
}
@article{li-li-2013-novel,
    title = ""A Novel Feature-based {B}ayesian Model for Query Focused Multi-document Summarization"",
    author = ""Li, Jiwei  and
      Li, Sujian"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1008"",
    doi = ""10.1162/tacl_a_00212"",
    pages = ""89--98"",
    abstract = ""Supervised learning methods and LDA based topic model have been successfully applied in the field of multi-document summarization. In this paper, we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. Experimental results on DUC2007, TAC2008 and TAC2009 demonstrate the effectiveness of our approach."",
}
@article{beigman-klebanov-etal-2013-using,
    title = ""Using Pivot-Based Paraphrasing and Sentiment Profiles to Improve a Subjectivity Lexicon for Essay Data"",
    author = ""Beigman Klebanov, Beata  and
      Madnani, Nitin  and
      Burstein, Jill"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1009"",
    doi = ""10.1162/tacl_a_00213"",
    pages = ""99--110"",
    abstract = ""We demonstrate a method of improving a seed sentiment lexicon developed on essay data by using a pivot-based paraphrasing system for lexical expansion coupled with sentiment profile enrichment using crowdsourcing. Profile enrichment alone yields up to 15{\%} improvement in the accuracy of the seed lexicon on 3-way sentence-level sentiment polarity classification of essay data. Using lexical expansion in addition to sentiment profiles provides a further 7{\%} improvement in performance. Additional experiments show that the proposed method is also effective with other subjectivity lexicons and in a different domain of application (product reviews)."",
}
@article{sangati-keller-2013-incremental,
    title = ""Incremental Tree Substitution Grammar for Parsing and Sentence Prediction"",
    author = ""Sangati, Federico  and
      Keller, Frank"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1010"",
    doi = ""10.1162/tacl_a_00214"",
    pages = ""111--124"",
    abstract = ""In this paper, we present the first incremental parser for Tree Substitution Grammar (TSG). A TSG allows arbitrarily large syntactic fragments to be combined into complete trees; we show how constraints (including lexicalization) can be imposed on the shape of the TSG fragments to enable incremental processing. We propose an efficient Earley-based algorithm for incremental TSG parsing and report an F-score competitive with other incremental parsers. In addition to whole-sentence F-score, we also evaluate the partial trees that the parser constructs for sentence prefixes; partial trees play an important role in incremental interpretation, language modeling, and psycholinguistics. Unlike existing parsers, our incremental TSG parser can generate partial trees that include predictions about the upcoming words in a sentence. We show that it outperforms an n-gram model in predicting more than one upcoming word."",
}
@article{sahakian-snyder-2013-modeling,
    title = ""Modeling Child Divergences from Adult Grammar"",
    author = ""Sahakian, Sam  and
      Snyder, Benjamin"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1011"",
    doi = ""10.1162/tacl_a_00215"",
    pages = ""125--138"",
    abstract = ""During the course of first language acquisition, children produce linguistic forms that do not conform to adult grammar. In this paper, we introduce a data set and approach for systematically modeling this child-adult grammar divergence. Our corpus consists of child sentences with corrected adult forms. We bridge the gap between these forms with a discriminatively reranked noisy channel model that translates child sentences into equivalent adult utterances. Our method outperforms MT and ESL baselines, reducing child error by 20{\%}. Our model allows us to chart specific aspects of grammar development in longitudinal studies of children, and investigate the hypothesis that children share a common developmental path in language acquisition."",
}
@article{hayashi-etal-2013-efficient,
    title = ""Efficient Stacked Dependency Parsing by Forest Reranking"",
    author = ""Hayashi, Katsuhiko  and
      Kondo, Shuhei  and
      Matsumoto, Yuji"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1012"",
    doi = ""10.1162/tacl_a_00216"",
    pages = ""139--150"",
    abstract = ""This paper proposes a discriminative forest reranking algorithm for dependency parsing that can be seen as a form of efficient stacked parsing. A dynamic programming shift-reduce parser produces a packed derivation forest which is then scored by a discriminative reranker, using the 1-best tree output by the shift-reduce parser as guide features in addition to third-order graph-based features. To improve efficiency and accuracy, this paper also proposes a novel shift-reduce parser that eliminates the spurious ambiguity of arc-standard transition systems. Testing on the English Penn Treebank data, forest reranking gave a state-of-the-art unlabeled dependency accuracy of 93.12."",
}
@article{matuschek-gurevych-2013-dijkstra,
    title = ""Dijkstra-{WSA}: A Graph-Based Approach to Word Sense Alignment"",
    author = ""Matuschek, Michael  and
      Gurevych, Iryna"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1013"",
    doi = ""10.1162/tacl_a_00217"",
    pages = ""151--164"",
    abstract = ""In this paper, we present Dijkstra-WSA, a novel graph-based algorithm for word sense alignment. We evaluate it on four different pairs of lexical-semantic resources with different characteristics (WordNet-OmegaWiki, WordNet-Wiktionary, GermaNet-Wiktionary and WordNet-Wikipedia) and show that it achieves competitive performance on 3 out of 4 datasets. Dijkstra-WSA outperforms the state of the art on every dataset if it is combined with a back-off based on gloss similarity. We also demonstrate that Dijkstra-WSA is not only flexibly applicable to different resources but also highly parameterizable to optimize for precision or recall."",
}
@article{lopez-etal-2013-learning,
    title = ""Learning to translate with products of novices: a suite of open-ended challenge problems for teaching {MT}"",
    author = ""Lopez, Adam  and
      Post, Matt  and
      Callison-Burch, Chris  and
      Weese, Jonathan  and
      Ganitkevitch, Juri  and
      Ahmidi, Narges  and
      Buzek, Olivia  and
      Hanson, Leah  and
      Jamil, Beenish  and
      Lee, Matthias  and
      Lin, Ya-Ting  and
      Pao, Henry  and
      Rivera, Fatima  and
      Shahriyari, Leili  and
      Sinha, Debu  and
      Teichert, Adam  and
      Wampler, Stephen  and
      Weinberger, Michael  and
      Xu, Daguang  and
      Yang, Lin  and
      Zhao, Shang"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1014"",
    doi = ""10.1162/tacl_a_00218"",
    pages = ""165--178"",
    abstract = ""Machine translation (MT) draws from several different disciplines, making it a complex subject to teach. There are excellent pedagogical texts, but problems in MT and current algorithms for solving them are best learned by doing. As a centerpiece of our MT course, we devised a series of open-ended challenges for students in which the goal was to improve performance on carefully constrained instances of four key MT tasks: alignment, decoding, evaluation, and reranking. Students brought a diverse set of techniques to the problems, including some novel solutions which performed remarkably well. A surprising and exciting outcome was that student solutions or their combinations fared competitively on some tasks, demonstrating that even newcomers to the field can help improve the state-of-the-art on hard NLP problems while simultaneously learning a great deal. The problems, baseline code, and results are freely available."",
}
@article{lewis-steedman-2013-combined,
    title = ""Combined Distributional and Logical Semantics"",
    author = ""Lewis, Mike  and
      Steedman, Mark"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1015"",
    doi = ""10.1162/tacl_a_00219"",
    pages = ""179--192"",
    abstract = ""We introduce a new approach to semantics which combines the benefits of distributional and formal logical semantics. Distributional models have been successful in modelling the meanings of content words, but logical semantics is necessary to adequately represent many function words. We follow formal semantics in mapping language to logical representations, but differ in that the relational constants used are induced by offline distributional clustering at the level of predicate-argument structure. Our clustering algorithm is highly scalable, allowing us to run on corpora the size of Gigaword. Different senses of a word are disambiguated based on their induced types. We outperform a variety of existing approaches on a wide-coverage question answering task, and demonstrate the ability to make complex multi-sentence inferences involving quantifiers on the FraCaS suite."",
}
@article{krishnamurthy-kollar-2013-jointly,
    title = ""Jointly Learning to Parse and Perceive: Connecting Natural Language to the Physical World"",
    author = ""Krishnamurthy, Jayant  and
      Kollar, Thomas"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1016"",
    doi = ""10.1162/tacl_a_00220"",
    pages = ""193--206"",
    abstract = ""This paper introduces Logical Semantics with Perception (LSP), a model for grounded language acquisition that learns to map natural language statements to their referents in a physical environment. For example, given an image, LSP can map the statement {``}blue mug on the table{''} to the set of image segments showing blue mugs on tables. LSP learns physical representations for both categorical ({``}blue,{''} {``}mug{''}) and relational ({``}on{''}) language, and also learns to compose these representations to produce the referents of entire statements. We further introduce a weakly supervised training procedure that estimates LSP{'}s parameters using annotated referents for entire statements, without annotated referents for individual words or the parse structure of the statement. We perform experiments on two applications: scene understanding and geographical question answering. We find that LSP outperforms existing, less expressive models that cannot represent relational language. We further find that weakly supervised training is competitive with fully supervised training while requiring significantly less annotation effort."",
}
@article{chang-yih-2013-dual,
    title = ""Dual Coordinate Descent Algorithms for Efficient Large Margin Structured Prediction"",
    author = ""Chang, Ming-Wei  and
      Yih, Wen-tau"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1017"",
    doi = ""10.1162/tacl_a_00221"",
    pages = ""207--218"",
    abstract = ""Due to the nature of complex NLP problems, structured prediction algorithms have been important modeling tools for a wide range of tasks. While there exists evidence showing that linear Structural Support Vector Machine (SSVM) algorithm performs better than structured Perceptron, the SSVM algorithm is still less frequently chosen in the NLP community because of its relatively slow training speed. In this paper, we propose a fast and easy-to-implement dual coordinate descent algorithm for SSVMs. Unlike algorithms such as Perceptron and stochastic gradient descent, our method keeps track of dual variables and updates the weight vector more aggressively. As a result, this training process is as efficient as existing online learning methods, and yet derives consistently better models, as evaluated on four benchmark NLP datasets for part-of-speech tagging, named-entity recognition and dependency parsing."",
}
@article{lluis-etal-2013-joint,
    title = ""Joint Arc-factored Parsing of Syntactic and Semantic Dependencies"",
    author = ""Llu{\'\i}s, Xavier  and
      Carreras, Xavier  and
      M{\`a}rquez, Llu{\'\i}s"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1018"",
    doi = ""10.1162/tacl_a_00222"",
    pages = ""219--230"",
    abstract = ""In this paper we introduce a joint arc-factored model for syntactic and semantic dependency parsing. The semantic role labeler predicts the full syntactic paths that connect predicates with their arguments. This process is framed as a linear assignment task, which allows to control some well-formedness constraints. For the syntactic part, we define a standard arc-factored dependency model that predicts the full syntactic tree. Finally, we employ dual decomposition techniques to produce consistent syntactic and predicate-argument structures while searching over a large space of syntactic configurations. In experiments on the CoNLL-2009 English benchmark we observe very competitive results."",
}
@article{srikumar-roth-2013-modeling,
    title = ""Modeling Semantic Relations Expressed by Prepositions"",
    author = ""Srikumar, Vivek  and
      Roth, Dan"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1019"",
    doi = ""10.1162/tacl_a_00223"",
    pages = ""231--242"",
    abstract = ""This paper introduces the problem of predicting semantic relations expressed by prepositions and develops statistical learning models for predicting the relations, their arguments and the semantic types of the arguments. We define an inventory of 32 relations, building on the word sense disambiguation task for prepositions and collapsing related senses across prepositions. Given a preposition in a sentence, our computational task to jointly model the preposition relation and its arguments along with their semantic types, as a way to support the relation prediction. The annotated data, however, only provides labels for the relation label, and not the arguments and types. We address this by presenting two models for preposition relation labeling. Our generalization of latent structure SVM gives close to 90{\%} accuracy on relation labeling. Further, by jointly predicting the relation, arguments, and their types along with preposition sense, we show that we can not only improve the relation accuracy, but also significantly improve sense prediction accuracy."",
}
@article{zhai-etal-2013-unsupervised,
    title = ""Unsupervised Tree Induction for Tree-based Translation"",
    author = ""Zhai, Feifei  and
      Zhang, Jiajun  and
      Zhou, Yu  and
      Zong, Chengqing"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1020"",
    doi = ""10.1162/tacl_a_00224"",
    pages = ""243--254"",
    abstract = ""In current research, most tree-based translation models are built directly from parse trees. In this study, we go in another direction and build a translation model with an unsupervised tree structure derived from a novel non-parametric Bayesian model. In the model, we utilize synchronous tree substitution grammars (STSG) to capture the bilingual mapping between language pairs. To train the model efficiently, we develop a Gibbs sampler with three novel Gibbs operators. The sampler is capable of exploring the infinite space of tree structures by performing local changes on the tree nodes. Experimental results show that the string-to-tree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees."",
}
@article{sirts-goldwater-2013-minimally,
    title = ""Minimally-Supervised Morphological Segmentation using {A}daptor {G}rammars"",
    author = ""Sirts, Kairit  and
      Goldwater, Sharon"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1021"",
    doi = ""10.1162/tacl_a_00225"",
    pages = ""255--266"",
    abstract = ""This paper explores the use of Adaptor Grammars, a nonparametric Bayesian modelling framework, for minimally supervised morphological segmentation. We compare three training methods: unsupervised training, semi-supervised training, and a novel model selection method. In the model selection method, we train unsupervised Adaptor Grammars using an over-articulated metagrammar, then use a small labelled data set to select which potential morph boundaries identified by the metagrammar should be returned in the final output. We evaluate on five languages and show that semi-supervised training provides a boost over unsupervised training, while the model selection method yields the best average results over all languages and is competitive with state-of-the-art semi-supervised systems. Moreover, this method provides the potential to tune performance according to different evaluation metrics or downstream tasks."",
}
@article{satta-kuhlmann-2013-efficient,
    title = ""Efficient Parsing for Head-Split Dependency Trees"",
    author = ""Satta, Giorgio  and
      Kuhlmann, Marco"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1022"",
    doi = ""10.1162/tacl_a_00226"",
    pages = ""267--278"",
    abstract = ""Head splitting techniques have been successfully exploited to improve the asymptotic runtime of parsing algorithms for projective dependency trees, under the arc-factored model. In this article we extend these techniques to a class of non-projective dependency trees, called well-nested dependency trees with block-degree at most 2, which has been previously investigated in the literature. We define a structural property that allows head splitting for these trees, and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage."",
}
@article{de-melo-bansal-2013-good,
    title = ""Good, Great, Excellent: Global Inference of Semantic Intensities"",
    author = ""de Melo, Gerard  and
      Bansal, Mohit"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1023"",
    doi = ""10.1162/tacl_a_00227"",
    pages = ""279--290"",
    abstract = ""Adjectives like good, great, and excellent are similar in meaning, but differ in intensity. Intensity order information is very useful for language learners as well as in several NLP tasks, but is missing in most lexical resources (dictionaries, WordNet, and thesauri). In this paper, we present a primarily unsupervised approach that uses semantics from Web-scale data (e.g., phrases like good but not excellent) to rank words by assigning them positions on a continuous scale. We rely on Mixed Integer Linear Programming to jointly determine the ranks, such that individual decisions benefit from global information. When ranking English adjectives, our global algorithm achieves substantial improvements over previous work on both pairwise and rank correlation metrics (specifically, 70{\%} pairwise accuracy as compared to only 56{\%} by previous work). Moreover, our approach can incorporate external synonymy information (increasing its pairwise accuracy to 78{\%}) and extends easily to new languages. We also make our code and data freely available."",
}
@article{wang-zong-2013-large,
    title = ""Large-scale Word Alignment Using Soft Dependency Cohesion Constraints"",
    author = ""Wang, Zhiguo  and
      Zong, Chengqing"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1024"",
    doi = ""10.1162/tacl_a_00228"",
    pages = ""291--300"",
    abstract = ""Dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language. It has been verified to be a useful constraint for word alignment. However, previous work either treats this as a hard constraint or uses it as a feature in discriminative models, which is ineffective for large-scale tasks. In this paper, we take dependency cohesion as a soft constraint, and integrate it into a generative model for large-scale word alignment experiments. We also propose an approximate EM algorithm and a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality."",
}
@article{sun-wan-2013-data,
    title = ""Data-driven, {PCFG}-based and Pseudo-{PCFG}-based Models for {C}hinese Dependency Parsing"",
    author = ""Sun, Weiwei  and
      Wan, Xiaojun"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1025"",
    doi = ""10.1162/tacl_a_00229"",
    pages = ""301--314"",
    abstract = ""We present a comparative study of transition-, graph- and PCFG-based models aimed at illuminating more precisely the likely contribution of CFGs in improving Chinese dependency parsing accuracy, especially by combining heterogeneous models. Inspired by the impact of a constituency grammar on dependency parsing, we propose several strategies to acquire pseudo CFGs only from dependency annotations. Compared to linguistic grammars learned from rich phrase-structure treebanks, well designed pseudo grammars achieve similar parsing accuracy and have equivalent contributions to parser ensemble. Moreover, pseudo grammars increase the diversity of base models; therefore, together with all other models, further improve system combination. Based on automatic POS tagging, our final model achieves a UAS of 87.23{\%}, resulting in a significant improvement of the state of the art."",
}
@article{luong-etal-2013-parsing,
    title = ""Parsing entire discourses as very long strings: Capturing topic continuity in grounded language learning"",
    author = ""Luong, Minh-Thang  and
      Frank, Michael C.  and
      Johnson, Mark"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1026"",
    doi = ""10.1162/tacl_a_00230"",
    pages = ""315--326"",
    abstract = ""Grounded language learning, the task of mapping from natural language to a representation of meaning, has attracted more and more interest in recent years. In most work on this topic, however, utterances in a conversation are treated independently and discourse structure information is largely ignored. In the context of language acquisition, this independence assumption discards cues that are important to the learner, e.g., the fact that consecutive utterances are likely to share the same referent (Frank et al., 2013). The current paper describes an approach to the problem of simultaneously modeling grounded language at the sentence and discourse levels. We combine ideas from parsing and grammar induction to produce a parser that can handle long input strings with thousands of tokens, creating parse trees that represent full discourses. By casting grounded language learning as a grammatical inference task, we use our parser to extend the work of Johnson et al. (2012), investigating the importance of discourse continuity in children{'}s language acquisition and its interaction with social cues. Our model boosts performance in a language acquisition task and yields good discourse segmentations compared with human annotators."",
}
@article{bisazza-federico-2013-dynamically,
    title = ""Dynamically Shaping the Reordering Search Space of Phrase-Based Statistical Machine Translation"",
    author = ""Bisazza, Arianna  and
      Federico, Marcello"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1027"",
    doi = ""10.1162/tacl_a_00231"",
    pages = ""327--340"",
    abstract = ""Defining the reordering search space is a crucial issue in phrase-based SMT between distant languages. In fact, the optimal trade-off between accuracy and complexity of decoding is nowadays reached by harshly limiting the input permutation space. We propose a method to dynamically shape such space and, thus, capture long-range word movements without hurting translation quality nor decoding time. The space defined by loose reordering constraints is dynamically pruned through a binary classifier that predicts whether a given input word should be translated right after another. The integration of this model into a phrase-based decoder improves a strong Arabic-English baseline already including state-of-the-art early distortion cost (Moore and Quirk, 2007) and hierarchical phrase orientation models (Galley and Manning, 2008). Significant improvements in the reordering of verbs are achieved by a system that is notably faster than the baseline, while bleu and meteor remain stable, or even increase, at a very high distortion limit."",
}
@article{louis-nenkova-2013-makes,
    title = ""What Makes Writing Great? First Experiments on Article Quality Prediction in the Science Journalism Domain"",
    author = ""Louis, Annie  and
      Nenkova, Ani"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1028"",
    doi = ""10.1162/tacl_a_00232"",
    pages = ""341--352"",
    abstract = ""Great writing is rare and highly admired. Readers seek out articles that are beautifully written, informative and entertaining. Yet information-access technologies lack capabilities for predicting article quality at this level. In this paper we present first experiments on article quality prediction in the science journalism domain. We introduce a corpus of great pieces of science journalism, along with typical articles from the genre. We implement features to capture aspects of great writing, including surprising, visual and emotional content, as well as general features related to discourse organization and sentence structure. We show that the distinction between great and typical articles can be detected fairly accurately, and that the entire spectrum of our features contribute to the distinction."",
}
@article{turney-2013-distributional,
    title = ""Distributional Semantics Beyond Words: Supervised Learning of Analogy and Paraphrase"",
    author = ""Turney, Peter D."",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1029"",
    doi = ""10.1162/tacl_a_00233"",
    pages = ""353--366"",
    abstract = ""There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval 2012 Task 2) and measuring compositional similarity between noun-modifier phrases and unigrams (multiple-choice paraphrase questions)."",
}
@article{ritter-etal-2013-modeling,
    title = ""Modeling Missing Data in Distant Supervision for Information Extraction"",
    author = ""Ritter, Alan  and
      Zettlemoyer, Luke  and
      {Mausam}  and
      Etzioni, Oren"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1030"",
    doi = ""10.1162/tacl_a_00234"",
    pages = ""367--378"",
    abstract = ""Distant supervision algorithms learn information extraction models given only large readily available databases and text collections. Most previous work has used heuristics for generating labeled data, for example assuming that facts not contained in the database are not mentioned in the text, and facts in the database must be mentioned at least once. In this paper, we propose a new latent-variable approach that models missing data. This provides a natural way to incorporate side information, for instance modeling the intuition that text will often mention rare entities which are likely to be missing in the database. Despite the added complexity introduced by reasoning about missing data, we demonstrate that a carefully designed local search approach to inference is very accurate and scales to large datasets. Experiments demonstrate improved performance for binary and unary relation extraction when compared to learning with heuristic labels, including on average a 27{\%} increase in area under the precision recall curve in the binary case."",
}
@article{li-etal-2013-data,
    title = ""Data-Driven Metaphor Recognition and Explanation"",
    author = ""Li, Hongsong  and
      Zhu, Kenny Q.  and
      Wang, Haixun"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1031"",
    doi = ""10.1162/tacl_a_00235"",
    pages = ""379--390"",
    abstract = ""Recognizing metaphors and identifying the source-target mappings is an important task as metaphorical text poses a big challenge for machine reading. To address this problem, we automatically acquire a metaphor knowledge base and an isA knowledge base from billions of web pages. Using the knowledge bases, we develop an inference mechanism to recognize and explain the metaphors in the text. To our knowledge, this is the first purely data-driven approach of probabilistic metaphor acquisition, recognition, and explanation. Our results shows that it significantly outperforms other state-of-the-art methods in recognizing and explaining metaphors."",
}
@article{basu-etal-2013-powergrading,
    title = ""{P}owergrading: a Clustering Approach to Amplify Human Effort for Short Answer Grading"",
    author = ""Basu, Sumit  and
      Jacobs, Chuck  and
      Vanderwende, Lucy"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1032"",
    doi = ""10.1162/tacl_a_00236"",
    pages = ""391--402"",
    abstract = ""We introduce a new approach to the machine-assisted grading of short answer questions. We follow past work in automated grading by first training a similarity metric between student responses, but then go on to use this metric to group responses into clusters and subclusters. The resulting groupings allow teachers to grade multiple responses with a single action, provide rich feedback to groups of similar answers, and discover modalities of misunderstanding among students; we refer to this amplification of grader effort as {``}powergrading.{''} We develop the means to further reduce teacher effort by automatically performing actions when an answer key is available. We show results in terms of grading progress with a small {``}budget{''} of human actions, both from our method and an LDA-based approach, on a test corpus of 10 questions answered by 698 respondents."",
}
@article{goldberg-nivre-2013-training,
    title = ""Training Deterministic Parsers with Non-Deterministic Oracles"",
    author = ""Goldberg, Yoav  and
      Nivre, Joakim"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1033"",
    doi = ""10.1162/tacl_a_00237"",
    pages = ""403--414"",
    abstract = ""Greedy transition-based parsers are very fast but tend to suffer from error propagation. This problem is aggravated by the fact that they are normally trained using oracles that are deterministic and incomplete in the sense that they assume a unique canonical path through the transition system and are only valid as long as the parser does not stray from this path. In this paper, we give a general characterization of oracles that are nondeterministic and complete, present a method for deriving such oracles for transition systems that satisfy a property we call arc decomposition, and instantiate this method for three well-known transition systems from the literature. We say that these oracles are dynamic, because they allow us to dynamically explore alternative and nonoptimal paths during training {---} in contrast to oracles that statically assume a unique optimal path. Experimental evaluation on a wide range of data sets clearly shows that using dynamic oracles to train greedy parsers gives substantial improvements in accuracy. Moreover, this improvement comes at no cost in terms of efficiency, unlike other techniques like beam search."",
}
@article{bohnet-etal-2013-joint,
    title = ""Joint Morphological and Syntactic Analysis for Richly Inflected Languages"",
    author = ""Bohnet, Bernd  and
      Nivre, Joakim  and
      Boguslavsky, Igor  and
      Farkas, Rich{\'a}rd  and
      Ginter, Filip  and
      Haji{\v{c}}, Jan"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1034"",
    doi = ""10.1162/tacl_a_00238"",
    pages = ""415--428"",
    abstract = ""Joint morphological and syntactic analysis has been proposed as a way of improving parsing accuracy for richly inflected languages. Starting from a transition-based model for joint part-of-speech tagging and dependency parsing, we explore different ways of integrating morphological features into the model. We also investigate the use of rule-based morphological analyzers to provide hard or soft lexical constraints and the use of word clusters to tackle the sparsity of lexical features. Evaluation on five morphologically rich languages (Czech, Finnish, German, Hungarian, and Russian) shows consistent improvements in both morphological and syntactic accuracy for joint prediction over a pipeline model, with further improvements thanks to lexical constraints and word clusters. The final results improve the state of the art in dependency parsing for all languages."",
}
@article{irvine-etal-2013-measuring,
    title = ""Measuring Machine Translation Errors in New Domains"",
    author = ""Irvine, Ann  and
      Morgan, John  and
      Carpuat, Marine  and
      Daum{\'e} III, Hal  and
      Munteanu, Dragos"",
    editor = ""Lin, Dekang  and
      Collins, Michael"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""1"",
    year = ""2013"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/Q13-1035"",
    doi = ""10.1162/tacl_a_00239"",
    pages = ""429--440"",
    abstract = ""We develop two techniques for analyzing the effect of porting a machine translation system to a new domain. One is a macro-level analysis that measures how domain shift affects corpus-level evaluation; the second is a micro-level analysis for word-level errors. We apply these methods to understand what happens when a Parliament-trained phrase-based machine translation system is applied in four very different domains: news, medical texts, scientific articles and movie subtitles. We present quantitative and qualitative experiments that highlight opportunities for future research in domain adaptation for machine translation."",
}
@proceedings{acl-2013-association-linguistics-tutorials,
    title = ""Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Tutorials)"",
    editor = ""Bos, Johan  and
      Hall, Keith"",
    month = aug,
    year = ""2013"",
    address = ""Sofia, Bulgaria"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/P13-5000"",
}
@",medical text,natural language process,natural languag,language process,translat,nlp,infer,generat,relation extract,entity recognit,summar,sentiment,semant,annotat,challeng,benchmark,evalu,track
" ""Learning Domain-Specific, {L}1-Specific Measures of Word Readability"",",,"{estreen-2013-overview,
    title = ""Overview of the {XLIFF} 2.0 specification"",
    author = ""Estreen, Frederik"",
    booktitle = ""Proceedings of Translating and the Computer 35"",
    month = nov # "" 28-29"",
    year = ""2013"",
    address = ""London, UK"",
    publisher = ""Aslib"",
    url = ""https://aclanthology.org/2013.tc-1.17"",
}
@article{farzindar-roche-2013-natural,
    title = ""Les d{\'e}fis de l{'}analyse des r{\'e}seaux sociaux pour le traitement automatique des langues [Natural language processing challenges for analysing social networks]"",
    author = ""Farzindar, Atefeh  and
      Roche, Mathieu"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Lepage, Yves  and
      Minel, Jean-Luc  and
      S{\'e}billot, Pascale"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""3"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-3.1"",
    pages = ""7--16"",
    language = ""French"",
}
@article{dridi-lapalme-2013-detection,
    title = ""D{\'e}tection d{'}{\'e}v{\`e}nements {\`a} partir de {T}witter [Event Detection in Tweets]"",
    author = ""Dridi, Houssem Eddine  and
      Lapalme, Guy"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Lepage, Yves  and
      Minel, Jean-Luc  and
      S{\'e}billot, Pascale"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""3"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-3.2"",
    pages = ""17--39"",
    language = ""French"",
}
@article{das-gamback-2013-code,
    title = ""Code-Mixing in Social Media Text"",
    author = {Das, Amitava  and
      Gamb{\""a}ck, Bj{\""o}rn},
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Lepage, Yves  and
      Minel, Jean-Luc  and
      S{\'e}billot, Pascale"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""3"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-3.3"",
    pages = ""41--64"",
}
@article{anamiadou-etal-2013-preface,
    title = ""Pr{\'e}face [Foreword]"",
    author = ""Anamiadou, Sophia  and
      Friburger, Nathalie  and
      Rosset, Sophie"",
    editor = ""Anamiadou, Sophia  and
      Friburger, Nathalie  and
      Rosset, Sophie"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""2"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-2.1"",
    pages = ""7--11"",
    language = ""French"",
}
@article{nouvel-etal-2013-fouille,
    title = ""Fouille de r{\`e}gles d{'}annotation pour la reconnaissance d{'}entit{\'e}s nomm{\'e}es [Annotation rule mining for named entity recognition]"",
    author = ""Nouvel, Damien  and
      Antoine, Jean-Yves  and
      Friburger, Nathalie  and
      Soulet, Arnaud"",
    editor = ""Anamiadou, Sophia  and
      Friburger, Nathalie  and
      Rosset, Sophie"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""2"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-2.2"",
    pages = ""13--41"",
    language = ""French"",
}
@article{hatmi-etal-2013-integration,
    title = ""Int{\'e}gration de la reconnaissance des entit{\'e}s nomm{\'e}es au processus de reconnaissance de la parole [Integration of named entity recognition to automatic speech recognition]"",
    author = ""Hatmi, Mahamed  and
      Jacquin, Christine  and
      Meignier, Sylvain  and
      Morin, Emmanuel  and
      Quiniou, Solen"",
    editor = ""Anamiadou, Sophia  and
      Friburger, Nathalie  and
      Rosset, Sophie"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""2"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-2.3"",
    pages = ""43--68"",
    language = ""French"",
}
@article{wang-etal-2013-extraction,
    title = ""Extraction et regroupement de relations entre entit{\'e}s pour l{'}extraction d{'}information non supervis{\'e}e [Extraction and clustering of entity relations for unsupervised information extraction]"",
    author = ""Wang, Wei  and
      Besan{\c{c}}on, Romaric  and
      Ferret, Olivier  and
      Grau, Brigitte"",
    editor = ""Anamiadou, Sophia  and
      Friburger, Nathalie  and
      Rosset, Sophie"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""2"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-2.4"",
    pages = ""69--100"",
    language = ""French"",
}
@article{gahbiche-braham-etal-2013-traitement,
    title = ""Traitement automatique des entit{\'e}s nomm{\'e}es en arabe: d{\'e}tection et traduction [Automatic processing of {A}rabic named entities: detection and translation]"",
    author = ""Gahbiche-Braham, Souhir  and
      Bonneau-Maynard, H{\'e}l{\`e}ne  and
      Yvon, Fran{\c{c}}ois"",
    editor = ""Anamiadou, Sophia  and
      Friburger, Nathalie  and
      Rosset, Sophie"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""2"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-2.5"",
    pages = ""101--132"",
    language = ""French"",
}
@article{claveau-kijak-2013-analyse,
    title = ""Analyse morphologique non supervis{\'e}e en domaine biom{\'e}dical [Unsupervised morphological analysis in the biomedical domain]"",
    author = ""Claveau, Vincent  and
      Kijak, Ewa"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Minel, Jean-Luc"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""1"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-1.1"",
    pages = ""13--45"",
    language = ""French"",
}
@article{constant-etal-2013-strategies,
    title = ""Strat{\'e}gies discriminantes pour int{\'e}grer la reconnaissance des mots compos{\'e}s dans un analyseur syntaxique en constituants [Discriminative strategies for integrating multiword expression recognition in a constituent parser]"",
    author = ""Constant, Matthieu  and
      Sigogne, Anthony  and
      Watrin, Patrick"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Minel, Jean-Luc"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""1"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-1.2"",
    pages = ""47--70"",
    language = ""French"",
}
@article{adam-etal-2013-evaluer,
    title = ""Evaluer et am{\'e}liorer une ressource distributionnelle: protocole d{'}annotation de liens s{\'e}mantiques en contexte [Evaluating and improving a distributional resource: protocol for in-context annotation of semantic links]"",
    author = ""Adam, Cl{\'e}mentine  and
      Fabre, C{\'e}cile  and
      Muller, Philippe"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Minel, Jean-Luc"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""1"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-1.3"",
    pages = ""71--97"",
    language = ""French"",
}
@article{schwab-etal-2013-desambiguisation,
    title = {D{\'e}sambigu{\""\i}sation lexicale de textes : efficacit{\'e} qualitative et temporelle d{'}un algorithme {\`a} colonies de fourmis [Lexical disambiguation of texts: qualitative and temporal efficiency of an ant colony algorithm]},
    author = ""Schwab, Didier  and
      Goulian, J{\'e}r{\^o}me  and
      Tchechmedjiev, Andon"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Minel, Jean-Luc"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""1"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-1.4"",
    pages = ""99--138"",
    language = ""French"",
}
@article{jean-louis-etal-2013-une,
    title = ""Une m{\'e}thode d{'}extraction d{'}information fond{\'e}e sur les graphes pour le remplissage de formulaires [A graph-based information extraction method for filling forms]"",
    author = ""Jean-Louis, Ludovic  and
      Besan{\c{c}}on, Romaric  and
      Ferret, Olivier"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Minel, Jean-Luc"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""1"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-1.5"",
    pages = ""139--170"",
    language = ""French"",
}
@article{francois-fairon-2013-les,
    title = ""Les apports du {TAL} {\`a} la lisibilit{\'e} du fran{\c{c}}ais langue {\'e}trang{\`e}re [Contributions of {NLP} to the readability of {F}rench as a foreign language]"",
    author = ""Fran{\c{c}}ois, Thomas  and
      Fairon, C{\'e}drick"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Minel, Jean-Luc"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""1"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-1.6"",
    pages = ""171--202"",
    language = ""French"",
}
@article{bergsma-yarowsky-2013-learning,
    title = ""Learning Domain-Specific, {L}1-Specific Measures of Word Readability"",
    author = ""Bergsma, Shane  and
      Yarowsky, David"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Minel, Jean-Luc"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""54"",
    number = ""1"",
    year = ""2013"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2013.tal-1.7"",
    pages = ""203--226"",
}
@",medical domain,natural language process,natural languag,language process,translat,nlp,entity recognit,semant,annotat,challeng,evalu
" ""Automatic Approaches for Gene-Drug Interaction Extraction from Biomedical Text: Corpus and Comparative Evaluation"",",,"{sutton-etal-2012-automatic,
    title = ""Automatic Approaches for Gene-Drug Interaction Extraction from Biomedical Text: Corpus and Comparative Evaluation"",
    author = ""Sutton, Nate  and
      Wojtulewicz, Laura  and
      Mehta, Neel  and
      Gonzalez, Graciela"",
    editor = ""Cohen, Kevin B.  and
      Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Webber, Bonnie  and
      Tsujii, Jun{'}ichi  and
      Pestian, John"",
    booktitle = ""{B}io{NLP}: Proceedings of the 2012 Workshop on Biomedical Natural Language Processing"",
    month = jun,
    year = ""2012"",
    address = ""Montr{\'e}al, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W12-2427"",
    pages = ""214--222"",
}
@",medical text,natural language process,natural languag,language process,nlp,evalu
" ""{S}em{S}cribe: Natural Language Generation for Medical Reports"","," ""Natural language generation in the medical domain is heavily influenced by domain knowledge and genre-specific text characteristics. We present SemScribe, an implemented natural language generation system that produces doctor's letters, in particular descriptions of cardiological findings. Texts in this domain are characterized by a high density of information and a relatively telegraphic style. Domain knowledge is encoded in a medical ontology of about 80,000 concepts. The ontology is used in particular for concept generalizations during referring expression generation. Architecturally, the system is a generation pipeline that uses a corpus-informed syntactic frame approach for realizing sentences appropriate to the domain. The system reads XML documents conforming to the HL7 Clinical Document Architecture (CDA) Standard and enhances them with generated text and references to the used data elements. We conducted a first clinical trial evaluation with medical staff and report on the findings."",","{varges-etal-2012-semscribe,
    title = ""{S}em{S}cribe: Natural Language Generation for Medical Reports"",
    author = ""Varges, Sebastian  and
      Bieler, Heike  and
      Stede, Manfred  and
      Faulstich, Lukas C.  and
      Irsig, Kristin  and
      Atalla, Malik"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)"",
    month = may,
    year = ""2012"",
    address = ""Istanbul, Turkey"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2012/pdf/165_Paper.pdf"",
    pages = ""2674--2681"",
    abstract = ""Natural language generation in the medical domain is heavily influenced by domain knowledge and genre-specific text characteristics. We present SemScribe, an implemented natural language generation system that produces doctor's letters, in particular descriptions of cardiological findings. Texts in this domain are characterized by a high density of information and a relatively telegraphic style. Domain knowledge is encoded in a medical ontology of about 80,000 concepts. The ontology is used in particular for concept generalizations during referring expression generation. Architecturally, the system is a generation pipeline that uses a corpus-informed syntactic frame approach for realizing sentences appropriate to the domain. The system reads XML documents conforming to the HL7 Clinical Document Architecture (CDA) Standard and enhances them with generated text and references to the used data elements. We conducted a first clinical trial evaluation with medical staff and report on the findings."",
}
@",medical domain,natural languag,generat,evalu
" ""Biomedical {C}hinese-{E}nglish {CLIR} Using an Extended {CM}e{SH} Resource to Expand Queries"","," ""Cross-lingual information retrieval (CLIR) involving the Chinese language has been thoroughly studied in the general language domain, but rarely in the biomedical domain, due to the lack of suitable linguistic resources and parsing tools. In this paper, we describe a Chinese-English CLIR system for biomedical literature, which exploits a bilingual ontology, the ``eCMeSH Tree''''''''. This is an extension of the Chinese Medical Subject Headings (CMeSH) Tree, based on Medical Subject Headings (MeSH). Using the 2006 and 2007 TREC Genomics track data, we have evaluated the performance of the eCMeSH Tree in expanding queries. We have compared our results to those obtained using two other approaches, i.e. pseudo-relevance feedback (PRF) and document translation (DT). Subsequently, we evaluate the performance of different combinations of these three retrieval methods. Our results show that our method of expanding queries using the eCMeSH Tree can outperform the PRF method. Furthermore, combining this method with PRF and DT helps to smooth the differences in query expansion, and consequently results in the best performance amongst all experiments reported. All experiments compare the use of two different retrieval models, i.e. Okapi BM25 and a query likelihood language model. In general, the former performs slightly better."",","{wang-etal-2012-biomedical,
    title = ""Biomedical {C}hinese-{E}nglish {CLIR} Using an Extended {CM}e{SH} Resource to Expand Queries"",
    author = ""Wang, Xinkai  and
      Thompson, Paul  and
      Tsujii, Jun{'}ichi  and
      Ananiadou, Sophia"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)"",
    month = may,
    year = ""2012"",
    address = ""Istanbul, Turkey"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2012/pdf/316_Paper.pdf"",
    pages = ""1148--1155"",
    abstract = ""Cross-lingual information retrieval (CLIR) involving the Chinese language has been thoroughly studied in the general language domain, but rarely in the biomedical domain, due to the lack of suitable linguistic resources and parsing tools. In this paper, we describe a Chinese-English CLIR system for biomedical literature, which exploits a bilingual ontology, the ``eCMeSH Tree''''''''. This is an extension of the Chinese Medical Subject Headings (CMeSH) Tree, based on Medical Subject Headings (MeSH). Using the 2006 and 2007 TREC Genomics track data, we have evaluated the performance of the eCMeSH Tree in expanding queries. We have compared our results to those obtained using two other approaches, i.e. pseudo-relevance feedback (PRF) and document translation (DT). Subsequently, we evaluate the performance of different combinations of these three retrieval methods. Our results show that our method of expanding queries using the eCMeSH Tree can outperform the PRF method. Furthermore, combining this method with PRF and DT helps to smooth the differences in query expansion, and consequently results in the best performance amongst all experiments reported. All experiments compare the use of two different retrieval models, i.e. Okapi BM25 and a query likelihood language model. In general, the former performs slightly better."",
}
@",medical domain,translat,information retriev,evalu,track
" ""Creation and use of Language Resources in a Question-Answering e{H}ealth System"","," ""ESICT (Experience-oriented Sharing of health knowledge via Information and Communication Technology) is an ongoing research project funded by the Danish Council for Strategic Research. It aims at developing a health/disease related information system based on information technology, language technology, and formalized medical knowledge. The formalized medical knowledge consists partly of the terminology database SNOMED CT and partly of authorized medical texts on the domain. The system will allow users to ask questions in Danish and will provide natural language answers. Currently, the project is pursuing three basically different methods for question answering, and they are all described to some extent in this paper. A system prototype will handle questions related to diabetes and heart diseases. This paper concentrates on the methods employed for question answering and the language resources that are utilized. Some resources were existing, such as SNOMED CT, others, such as a corpus of sample questions, have had to be created or constructed."",","{andersen-etal-2012-creation,
    title = ""Creation and use of Language Resources in a Question-Answering e{H}ealth System"",
    author = {Andersen, Ulrich  and
      Braasch, Anna  and
      Henriksen, Lina  and
      Huszka, Csaba  and
      Johannsen, Anders  and
      Kayser, Lars  and
      Maegaard, Bente  and
      Norgaard, Ole  and
      Schulz, Stefan  and
      Wedekind, J{\""u}rgen},
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)"",
    month = may,
    year = ""2012"",
    address = ""Istanbul, Turkey"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2012/pdf/504_Paper.pdf"",
    pages = ""2536--2542"",
    abstract = ""ESICT (Experience-oriented Sharing of health knowledge via Information and Communication Technology) is an ongoing research project funded by the Danish Council for Strategic Research. It aims at developing a health/disease related information system based on information technology, language technology, and formalized medical knowledge. The formalized medical knowledge consists partly of the terminology database SNOMED CT and partly of authorized medical texts on the domain. The system will allow users to ask questions in Danish and will provide natural language answers. Currently, the project is pursuing three basically different methods for question answering, and they are all described to some extent in this paper. A system prototype will handle questions related to diabetes and heart diseases. This paper concentrates on the methods employed for question answering and the language resources that are utilized. Some resources were existing, such as SNOMED CT, others, such as a corpus of sample questions, have had to be created or constructed."",
}
@",medical text,natural languag,question-answ,evalu
" ""Rule-based Entity Recognition and Coverage of {SNOMED} {CT} in {S}wedish Clinical Text"","," ""Named entity recognition of the clinical entities disorders, findings and body structures is needed for information extraction from unstructured text in health records. Clinical notes from a Swedish emergency unit were annotated and used for evaluating a rule- and terminology-based entity recognition system. This system used different preprocessing techniques for matching terms to SNOMED CT, and, one by one, four other terminologies were added. For the class body structure, the results improved with preprocessing, whereas only small improvements were shown for the classes disorder and finding. The best average results were achieved when all terminologies were used together. The entity body structure was recognised with a precision of 0.74 and a recall of 0.80, whereas lower results were achieved for disorder (precision: 0.75, recall: 0.55) and for finding (precision: 0.57, recall: 0.30). The proportion of entities containing abbreviations were higher for false negatives than for correctly recognised entities, and no entities containing more than two tokens were recognised by the system. Low recall for disorders and findings shows both that additional methods are needed for entity recognition and that there are many expressions in clinical text that are not included in SNOMED CT."",","{skeppstedt-etal-2012-rule,
    title = ""Rule-based Entity Recognition and Coverage of {SNOMED} {CT} in {S}wedish Clinical Text"",
    author = ""Skeppstedt, Maria  and
      Kvist, Maria  and
      Dalianis, Hercules"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)"",
    month = may,
    year = ""2012"",
    address = ""Istanbul, Turkey"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2012/pdf/521_Paper.pdf"",
    pages = ""1250--1257"",
    abstract = ""Named entity recognition of the clinical entities disorders, findings and body structures is needed for information extraction from unstructured text in health records. Clinical notes from a Swedish emergency unit were annotated and used for evaluating a rule- and terminology-based entity recognition system. This system used different preprocessing techniques for matching terms to SNOMED CT, and, one by one, four other terminologies were added. For the class body structure, the results improved with preprocessing, whereas only small improvements were shown for the classes disorder and finding. The best average results were achieved when all terminologies were used together. The entity body structure was recognised with a precision of 0.74 and a recall of 0.80, whereas lower results were achieved for disorder (precision: 0.75, recall: 0.55) and for finding (precision: 0.57, recall: 0.30). The proportion of entities containing abbreviations were higher for false negatives than for correctly recognised entities, and no entities containing more than two tokens were recognised by the system. Low recall for disorders and findings shows both that additional methods are needed for entity recognition and that there are many expressions in clinical text that are not included in SNOMED CT."",
}
@",health record,clinical not,clinical text,entity recognit,annotat,evalu
" ""The {TARSQI} Toolkit"","," ""We present and demonstrate the updated version of the TARSQI Toolkit, a suite of temporal processing modules that extract temporal information from natural language texts. It parses the document and identifies temporal expressions, recognizes events, anchor events to temporal expressions and orders events relative to each other. The toolkit was previously demonstrated at COLING 2008, but has since seen substantial changes including: (1) incorporation of a new time expression tagger, (2){\textasciitilde}embracement of stand-off annotation, (3) application to the medical domain and (4) introduction of narrative containers."",","{verhagen-pustejovsky-2012-tarsqi,
    title = ""The {TARSQI} Toolkit"",
    author = ""Verhagen, Marc  and
      Pustejovsky, James"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)"",
    month = may,
    year = ""2012"",
    address = ""Istanbul, Turkey"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2012/pdf/656_Paper.pdf"",
    pages = ""2043--2048"",
    abstract = ""We present and demonstrate the updated version of the TARSQI Toolkit, a suite of temporal processing modules that extract temporal information from natural language texts. It parses the document and identifies temporal expressions, recognizes events, anchor events to temporal expressions and orders events relative to each other. The toolkit was previously demonstrated at COLING 2008, but has since seen substantial changes including: (1) incorporation of a new time expression tagger, (2){\textasciitilde}embracement of stand-off annotation, (3) application to the medical domain and (4) introduction of narrative containers."",
}
@",medical domain,natural languag,annotat,evalu
" ""Iterative Refinement and Quality Checking of Annotation Guidelines {---} How to Deal Effectively with Semantically Sloppy Named Entity Types, such as Pathological Phenomena"","," ""We here discuss a methodology for dealing with the annotation of semantically hard to delineate, i.e., sloppy, named entity types. To illustrate sloppiness of entities, we treat an example from the medical domain, namely pathological phenomena. Based on our experience with iterative guideline refinement we propose to carefully characterize the thematic scope of the annotation by positive and negative coding lists and allow for alternative, short vs. long mention span annotations. Short spans account for canonical entity mentions (e.g., standardized disease names), while long spans cover descriptive text snippets which contain entity-specific elaborations (e.g., anatomical locations, observational details, etc.). Using this stratified approach, evidence for increasing annotation performance is provided by kappa-based inter-annotator agreement measurements over several, iterative annotation rounds using continuously refined guidelines. The latter reflects the increasing understanding of the sloppy entity class both from the perspective of guideline writers and users (annotators). Given our data, we have gathered evidence that we can deal with sloppiness in a controlled manner and expect inter-annotator agreement values around 80{\%} for PathoJen, the pathological phenomena corpus currently under development in our lab."",","{hahn-etal-2012-iterative,
    title = ""Iterative Refinement and Quality Checking of Annotation Guidelines {---} How to Deal Effectively with Semantically Sloppy Named Entity Types, such as Pathological Phenomena"",
    author = {Hahn, Udo  and
      Beisswanger, Elena  and
      Buyko, Ekaterina  and
      Faessler, Erik  and
      Traum{\""u}ller, Jenny  and
      Schr{\""o}der, Susann  and
      Hornbostel, Kerstin},
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)"",
    month = may,
    year = ""2012"",
    address = ""Istanbul, Turkey"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2012/pdf/755_Paper.pdf"",
    pages = ""3881--3885"",
    abstract = ""We here discuss a methodology for dealing with the annotation of semantically hard to delineate, i.e., sloppy, named entity types. To illustrate sloppiness of entities, we treat an example from the medical domain, namely pathological phenomena. Based on our experience with iterative guideline refinement we propose to carefully characterize the thematic scope of the annotation by positive and negative coding lists and allow for alternative, short vs. long mention span annotations. Short spans account for canonical entity mentions (e.g., standardized disease names), while long spans cover descriptive text snippets which contain entity-specific elaborations (e.g., anatomical locations, observational details, etc.). Using this stratified approach, evidence for increasing annotation performance is provided by kappa-based inter-annotator agreement measurements over several, iterative annotation rounds using continuously refined guidelines. The latter reflects the increasing understanding of the sloppy entity class both from the perspective of guideline writers and users (annotators). Given our data, we have gathered evidence that we can deal with sloppiness in a controlled manner and expect inter-annotator agreement values around 80{\%} for PathoJen, the pathological phenomena corpus currently under development in our lab."",
}
@",medical domain,semant,annotat,evalu
" ""A Corpus of Scientific Biomedical Texts Spanning over 168 Years Annotated for Uncertainty"","," ""Uncertainty language permeates biomedical research and is fundamental for the computer interpretation of unstructured text. And yet, a coherent, cognitive-based theory to interpret Uncertainty language and guide Natural Language Processing is, to our knowledge, non-existing. The aim of our project was therefore to detect and annotate Uncertainty markers ― which play a significant role in building knowledge or beliefs in readers' minds ― in a biomedical research corpus. Our corpus includes 80 manually annotated articles from the British Medical Journal randomly sampled from a 168-year period. Uncertainty markers have been classified according to a theoretical framework based on a combined linguistic and cognitive theory. The corpus was manually annotated according to such principles. We performed preliminary experiments to assess the manually annotated corpus and establish a baseline for the automatic detection of Uncertainty markers. The results of the experiments show that most of the Uncertainty markers can be recognized with good accuracy."",","{bongelli-etal-2012-corpus,
    title = ""A Corpus of Scientific Biomedical Texts Spanning over 168 Years Annotated for Uncertainty"",
    author = ""Bongelli, Ramona  and
      Canestrari, Carla  and
      Riccioni, Ilaria  and
      Zuczkowski, Andrzej  and
      Buldorini, Cinzia  and
      Pietrobon, Ricardo  and
      Lavelli, Alberto  and
      Magnini, Bernardo"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)"",
    month = may,
    year = ""2012"",
    address = ""Istanbul, Turkey"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2012/pdf/823_Paper.pdf"",
    pages = ""2009--2014"",
    abstract = ""Uncertainty language permeates biomedical research and is fundamental for the computer interpretation of unstructured text. And yet, a coherent, cognitive-based theory to interpret Uncertainty language and guide Natural Language Processing is, to our knowledge, non-existing. The aim of our project was therefore to detect and annotate Uncertainty markers ― which play a significant role in building knowledge or beliefs in readers' minds ― in a biomedical research corpus. Our corpus includes 80 manually annotated articles from the British Medical Journal randomly sampled from a 168-year period. Uncertainty markers have been classified according to a theoretical framework based on a combined linguistic and cognitive theory. The corpus was manually annotated according to such principles. We performed preliminary experiments to assess the manually annotated corpus and establish a baseline for the automatic detection of Uncertainty markers. The results of the experiments show that most of the Uncertainty markers can be recognized with good accuracy."",
}
@",medical text,natural language process,natural languag,language process,annotat,evalu,assess
" ""{CALBC}: Releasing the Final Corpora"","," ""A number of gold standard corpora for named entity recognition are available to the public. However, the existing gold standard corpora are limited in size and semantic entity types. These usually lead to implementation of trained solutions (1) for a limited number of semantic entity types and (2) lacking in generalization capability. In order to overcome these problems, the CALBC project has aimed to automatically generate large scale corpora annotated with multiple semantic entity types in a community-wide manner based on the consensus of different named entity solutions. The generated corpus is called the silver standard corpus since the corpus generation process does not involve any manual curation. In this publication, we announce the release of the final CALBC corpora which include the silver standard corpus in different versions and several gold standard corpora for the further usage of the biomedical text mining community. The gold standard corpora are utilised to benchmark the methods used in the silver standard corpora generation process and released in a shared format. All the corpora are released in a shared format and accessible at www.calbc.eu."",","{kafkas-etal-2012-calbc,
    title = ""{CALBC}: Releasing the Final Corpora"",
    author = ""Kafkas, {\c{S}}enay  and
      Lewin, Ian  and
      Milward, David  and
      van Mulligen, Erik  and
      Kors, Jan  and
      Hahn, Udo  and
      Rebholz-Schuhmann, Dietrich"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)"",
    month = may,
    year = ""2012"",
    address = ""Istanbul, Turkey"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2012/pdf/827_Paper.pdf"",
    pages = ""2923--2926"",
    abstract = ""A number of gold standard corpora for named entity recognition are available to the public. However, the existing gold standard corpora are limited in size and semantic entity types. These usually lead to implementation of trained solutions (1) for a limited number of semantic entity types and (2) lacking in generalization capability. In order to overcome these problems, the CALBC project has aimed to automatically generate large scale corpora annotated with multiple semantic entity types in a community-wide manner based on the consensus of different named entity solutions. The generated corpus is called the silver standard corpus since the corpus generation process does not involve any manual curation. In this publication, we announce the release of the final CALBC corpora which include the silver standard corpus in different versions and several gold standard corpora for the further usage of the biomedical text mining community. The gold standard corpora are utilised to benchmark the methods used in the silver standard corpora generation process and released in a shared format. All the corpora are released in a shared format and accessible at www.calbc.eu."",
}
@",medical text,generat,entity recognit,semant,annotat,benchmark,evalu
" ""Statistical Section Segmentation in Free-Text Clinical Records"","," ""Automatically segmenting and classifying clinical free text into sections is an important first step to automatic information retrieval, information extraction and data mining tasks, as it helps to ground the significance of the text within. In this work we describe our approach to automatic section segmentation of clinical records such as hospital discharge summaries and radiology reports, along with section classification into pre-defined section categories. We apply machine learning to the problems of section segmentation and section classification, comparing a joint (one-step) and a pipeline (two-step) approach. We demonstrate that our systems perform well when tested on three data sets, two for hospital discharge summaries and one for radiology reports. We then show the usefulness of section information by incorporating it in the task of extracting comorbidities from discharge summaries."",","{tepper-etal-2012-statistical,
    title = ""Statistical Section Segmentation in Free-Text Clinical Records"",
    author = ""Tepper, Michael  and
      Capurro, Daniel  and
      Xia, Fei  and
      Vanderwende, Lucy  and
      Yetisgen-Yildiz, Meliha"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)"",
    month = may,
    year = ""2012"",
    address = ""Istanbul, Turkey"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2012/pdf/1016_Paper.pdf"",
    pages = ""2001--2008"",
    abstract = ""Automatically segmenting and classifying clinical free text into sections is an important first step to automatic information retrieval, information extraction and data mining tasks, as it helps to ground the significance of the text within. In this work we describe our approach to automatic section segmentation of clinical records such as hospital discharge summaries and radiology reports, along with section classification into pre-defined section categories. We apply machine learning to the problems of section segmentation and section classification, comparing a joint (one-step) and a pipeline (two-step) approach. We demonstrate that our systems perform well when tested on three data sets, two for hospital discharge summaries and one for radiology reports. We then show the usefulness of section information by incorporating it in the task of extracting comorbidities from discharge summaries."",
}
@",discharge summar,clinical record,summar,information retriev,evalu
" ""Esculape : un syst{\`e}me de question-r{\'e}ponse dans le domaine m{\'e}dical fond{\'e} sur l{'}extraction de relations [Esculape: a question-answering system in the medical domain based on relation extraction]"",",,"{munoz-etal-2012-protermino,
    title = ""{P}ro{T}ermino: a comprehensive web-based terminological management tool based on knowledge representation"",
    author = ""Dur{\'a}n Mu{\~n}oz, Isabel  and
      Corpas Pastor, Gloria  and
      Ha, Le An"",
    booktitle = ""Proceedings of Translating and the Computer 34"",
    month = nov # "" 29-30"",
    year = ""2012"",
    address = ""London, UK"",
    publisher = ""Aslib"",
    url = ""https://aclanthology.org/2012.tc-1.17"",
}
@article{dale-yvon-2012-preface,
    title = ""Pr{\'e}face [Foreword]"",
    author = ""Dale, Robert  and
      Yvon, Fran{\c{c}}ois"",
    editor = ""Yvon, Fran{\c{c}}ois  and
      Dale, Robert"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""3"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-3.1"",
    pages = ""7--10"",
    language = ""French"",
}
@article{zesch-2012-detecting,
    title = ""Detecting Malapropisms Using Measures of Contextual Fitness"",
    author = ""Zesch, Torsten"",
    editor = ""Yvon, Fran{\c{c}}ois  and
      Dale, Robert"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""3"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-3.2"",
    pages = ""11--31"",
}
@article{gotti-etal-2012-attenuation,
    title = ""Att{\'e}nuation des surd{\'e}tections d{'}un correcteur grammatical de qualit{\'e} commerciale [Reducing overdetections in a commercial grade grammar checker]"",
    author = ""Gotti, Fabrizio  and
      Langlais, Philippe  and
      Lapalme, Guy  and
      Charest, Simon  and
      Brunelle, Eric"",
    editor = ""Yvon, Fran{\c{c}}ois  and
      Dale, Robert"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""3"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-3.3"",
    pages = ""33--60"",
    language = ""French"",
}
@article{flor-2012-four,
    title = ""Four types of context for automatic spelling correction"",
    author = ""Flor, Michael"",
    editor = ""Yvon, Fran{\c{c}}ois  and
      Dale, Robert"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""3"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-3.4"",
    pages = ""61--99"",
}
@article{juola-etal-2012-authorship,
    title = ""Authorship Attribution and Optical Character Recognition Errors"",
    author = ""Juola, Patrick  and
      Noecker, John I. Jr  and
      Ryan, Michael V."",
    editor = ""Yvon, Fran{\c{c}}ois  and
      Dale, Robert"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""3"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-3.5"",
    pages = ""101--127"",
}
@article{bonneau-etal-2012-gestion,
    title = ""Gestion d{'}erreurs pour la fiabilisation des retours automatiques en apprentissage de la prosodie d{'}une langue seconde [Handling of errors for increasing automatic feedback reliability in foreign language prosody learning]"",
    author = ""Bonneau, Anne  and
      Fohr, Dominique  and
      Illina, Irina  and
      Jouvet, Denis  and
      Mella, Odile  and
      Mesbahi, Larbi  and
      Orosanu, Luiza"",
    editor = ""Yvon, Fran{\c{c}}ois  and
      Dale, Robert"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""3"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-3.6"",
    pages = ""129--154"",
    language = ""French"",
}
@article{mani-muller-2012-preface,
    title = ""Pr{\'e}face [Introduction to the special issue]"",
    author = ""Mani, Interjeet  and
      Muller, Philippe"",
    editor = ""Mani, Interjeet  and
      Muller, Philippe"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""2"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-2.1"",
    pages = ""7--21"",
    language = ""French"",
}
@article{marsic-2012-syntactically,
    title = ""Syntactically motivated task definition for temporal relation identification"",
    author = ""Marsic, Georgiana"",
    editor = ""Mani, Interjeet  and
      Muller, Philippe"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""2"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-2.2"",
    pages = ""23--55"",
}
@article{kessler-etal-2012-extraction,
    title = ""Extraction de dates saillantes pour la construction de chronologies th{\'e}matiques [Salient date extraction for building query-based timelines]"",
    author = ""Kessler, R{\'e}my  and
      Tannier, Xavier  and
      Hag{\`e}ge, Caroline  and
      Moriceau, V{\'e}ronique  and
      Bittar, Andr{\'e}"",
    editor = ""Mani, Interjeet  and
      Muller, Philippe"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""2"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-2.3"",
    pages = ""57--86"",
    language = ""French"",
}
@article{pustejovsky-etal-2012-linguistically,
    title = ""A Linguistically Grounded Annotation Language for Spatial Information"",
    author = ""Pustejovsky, James  and
      Moszkowicz, Jessica  and
      Verhagen, Marc"",
    editor = ""Mani, Interjeet  and
      Muller, Philippe"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""2"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-2.4"",
    pages = ""87--113"",
}
@article{moot-2012-semantique,
    title = ""S{\'e}mantique {\`a} large couverture et raisonnement spatio-temporelle [Wide-coverage semantics for spatio-temporal reasoning]"",
    author = ""Moot, Richard"",
    editor = ""Mani, Interjeet  and
      Muller, Philippe"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""2"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-2.5"",
    pages = ""115--142"",
    language = ""French"",
}
@article{gaio-etal-2012-typage,
    title = ""Typage de noms toponymiques {\`a} des fins d{'}indexation g{\'e}ographique [Typing place names for geographical indexing]"",
    author = ""Gaio, Mauro  and
      Sallaberry, Christian  and
      Nguyen, Van Tien"",
    editor = ""Mani, Interjeet  and
      Muller, Philippe"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""2"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-2.6"",
    pages = ""143--176"",
    language = ""French"",
}
@article{blaylock-etal-2012-street,
    title = ""Street-level Geolocation from Natural Language Descriptions"",
    author = ""Blaylock, Nate  and
      Allen, James  and
      de Beaumont, William  and
      Galescu, Lucian  and
      Jung, Hyuckchul"",
    editor = ""Mani, Interjeet  and
      Muller, Philippe"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""2"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-2.7"",
    pages = ""177--205"",
}
@article{bouamor-etal-2012-etude,
    title = ""{\'E}tude bilingue de l{'}acquisition et de la validation automatiques de paraphrases sous-phrastiques [Automatic acquisition and validation of sub-sentential paraphrases : a bilingual study]"",
    author = ""Bouamor, Houda  and
      Max, Aur{\'e}lien  and
      Vilnat, Anne"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Minel, Jean-Luc"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""1"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-1.1"",
    pages = ""11--37"",
    language = ""French"",
}
@article{ben-abacha-zweigenbaum-2012-une,
    title = ""Une {\'e}tude comparative empirique sur la reconnaissance des entit{\'e}s m{\'e}dicales [An empirical comparative study of medical entity recognition]"",
    author = ""Ben Abacha, Asma  and
      Zweigenbaum, Pierre"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Minel, Jean-Luc"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""1"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-1.2"",
    pages = ""39--68"",
    language = ""French"",
}
@article{embarek-ferret-2012-esculape,
    title = ""Esculape : un syst{\`e}me de question-r{\'e}ponse dans le domaine m{\'e}dical fond{\'e} sur l{'}extraction de relations [Esculape: a question-answering system in the medical domain based on relation extraction]"",
    author = ""Embarek, Mehdi  and
      Ferret, Olivier"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Minel, Jean-Luc"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""53"",
    number = ""1"",
    year = ""2012"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2012.tal-1.3"",
    pages = ""69--99"",
    language = ""French"",
}
@",medical domain,natural languag,translat,relation extract,entity recognit,semant,annotat,question-answ
" ""Using Kybots for Extracting Events in Biomedical Texts"",",,"{casillas-etal-2011-using,
    title = ""Using Kybots for Extracting Events in Biomedical Texts"",
    author = ""Casillas, Arantza  and
      D{\'\i}az de Ilarraza, Arantza  and
      Gojenola, Koldo  and
      Oronoz, Maite  and
      Rigau, German"",
    editor = ""Tsujii, Jun{'}ichi  and
      Kim, Jin-Dong  and
      Pyysalo, Sampo"",
    booktitle = ""Proceedings of {B}io{NLP} Shared Task 2011 Workshop"",
    month = jun,
    year = ""2011"",
    address = ""Portland, Oregon, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W11-1819"",
    pages = ""138--142"",
}
@",medical text,nlp,shared task
" ""From Graphs to Events: A Subgraph Matching Approach for Information Extraction from Biomedical Text"",",,"{liu-etal-2011-graphs,
    title = ""From Graphs to Events: A Subgraph Matching Approach for Information Extraction from Biomedical Text"",
    author = ""Liu, Haibin  and
      Komandur, Ravikumar  and
      Verspoor, Karin"",
    editor = ""Tsujii, Jun{'}ichi  and
      Kim, Jin-Dong  and
      Pyysalo, Sampo"",
    booktitle = ""Proceedings of {B}io{NLP} Shared Task 2011 Workshop"",
    month = jun,
    year = ""2011"",
    address = ""Portland, Oregon, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W11-1826"",
    pages = ""164--172"",
}
@",medical text,nlp,shared task
" ""Proceedings of the 4th Workshop on Patent Translation"",",,"{moorkens-2011-translation,
    title = ""Translation memories guarantee consistency: truth or fiction"",
    author = ""Moorkens, Joss"",
    booktitle = ""Proceedings of Translating and the Computer 33"",
    month = nov # "" 17-18"",
    year = ""2011"",
    address = ""London, UK"",
    publisher = ""Aslib"",
    url = ""https://aclanthology.org/2011.tc-1.17"",
}
@proceedings{tal-2011-52-3,
    title = ""Traitement Automatique des Langues, Volume 52, Num{\'e}ro 3 : Ressources linguistiques libres [Free Language Resources]"",
    editor = ""Bel, Nuria  and
      Sagot, Beno{\^\i}t"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""3"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-3.0"",
}
@article{bel-sagot-2011-introduction,
    title = ""Introduction - Ressources linguistiques libres [Foreword - Free Language Resources]"",
    author = ""Bel, Nuria  and
      Sagot, Beno{\^\i}t"",
    editor = ""Bel, Nuria  and
      Sagot, Beno{\^\i}t"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""3"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-3.1"",
    pages = ""7--15"",
    language = ""French"",
}
@article{eshkol-taravella-etal-2011-un,
    title = ""Un grand corpus oral disponible : le Corpus d{'}Orl{\'e}ans 1968-2012 [A Large available oral corpus: Orleans corpus 1968-2012]"",
    author = ""Eshkol-taravella, Iris  and
      Baude, Olivier  and
      Maurel, Denis  and
      Hriba, Linda  and
      Dugua, Celine  and
      Tellier, Isabelle"",
    editor = ""Bel, Nuria  and
      Sagot, Beno{\^\i}t"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""3"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-3.2"",
    pages = ""17--46"",
    language = ""French"",
}
@article{jacobson-baude-2011-corpus,
    title = ""Corpus de la parole [Corpus de la parole]"",
    author = ""Jacobson, Michel  and
      Baude, Olivier"",
    editor = ""Bel, Nuria  and
      Sagot, Beno{\^\i}t"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""3"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-3.3"",
    pages = ""47--69"",
    language = ""French"",
}
@article{pery-woodley-etal-2011-le,
    title = ""Le corpus {ANNODIS}, un corpus enrichi d{'}annotations discursives [The {ANNODIS} corpus, a corpus enriched with discourse annotations]"",
    author = ""P{\'e}ry-Woodley, Marie-Paule  and
      Afantenos, Stergos D.  and
      Ho-Dac, Lydia-Mai  and
      Asher, Nicholas"",
    editor = ""Bel, Nuria  and
      Sagot, Beno{\^\i}t"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""3"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-3.4"",
    pages = ""71--101"",
    language = ""French"",
}
@article{falaise-etal-2011-definition,
    title = ""D{\'e}finition et conception d{'}une interface pour l{'}exploitation de corpus arbor{\'e}s pour non-informaticiens : la plateforme {S}cien{Q}uest du projet Scientext [Definition and design of an interface for treebanks exploitation by non-computer scientists: the {S}cien{Q}uest platform from Scientext project]"",
    author = ""Falaise, Achille  and
      Tutin, Agn{\`e}s  and
      Kraif, Olivier"",
    editor = ""Bel, Nuria  and
      Sagot, Beno{\^\i}t"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""3"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-3.5"",
    pages = ""103--128"",
    language = ""French"",
}
@article{balvet-etal-2011-la,
    title = ""La ressource Nomade. Confronter les attentes thoriques aux observations du comportement linguistique des nominalisations en corpus [The Nomage resource. Compare theoretical expectations with observations of linguistic behavior of nominalizations in corpus]"",
    author = ""Balvet, A.  and
      Barque, L.  and
      Condette, M.-h.  and
      Haas, P.  and
      Huyghe, R.  and
      Marin, R.  and
      Merlo, A."",
    editor = ""Bel, Nuria  and
      Sagot, Beno{\^\i}t"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""3"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-3.6"",
    pages = ""129--152"",
    language = ""French"",
}
@article{tolone-2011-maintenance,
    title = ""Maintenance du Lexique-Grammaire : Formules d{\'e}finitoires et arbre de classement [Maintenance of the Lexicon-Grammar: Definitional formulas and classification tree]"",
    author = ""Tolone, Elsa"",
    editor = ""Bel, Nuria  and
      Sagot, Beno{\^\i}t"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""3"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-3.7"",
    pages = ""153--190"",
    language = ""French"",
}
@article{vincze-bestgen-2011-une,
    title = ""Une proc{\'e}dure automatique pour {\'e}tendre des normes lexicales par l{'}analyse des cooccurrences dans des textes [An automatic procedure for extending lexical norms by means of the analysis of word co-occurrences in texts]"",
    author = ""Vincze, Nadja  and
      Bestgen, Yves"",
    editor = ""Bel, Nuria  and
      Sagot, Beno{\^\i}t"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""3"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-3.8"",
    pages = ""191--216"",
    language = ""French"",
}
@article{vetere-etal-2011-senso,
    title = ""Senso Comune, an Open Knowledge Base of {I}talian Language"",
    author = ""Vetere, Guido  and
      Oltramari, Alessandro  and
      Chiari, Isabella  and
      Jezek, Elisabetta  and
      Vieu, Laure  and
      Zanzotto, Fabio Massimo"",
    editor = ""Bel, Nuria  and
      Sagot, Beno{\^\i}t"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""3"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-3.9"",
    pages = ""217--243"",
}
@article{chiarcos-etal-2011-towards,
    title = ""Towards a Linguistic Linked Open Data cloud: The Open Linguistics Working Group"",
    author = ""Chiarcos, Christian  and
      Hellmann, Sebastian  and
      Nordhoff, Sebastian"",
    editor = ""Bel, Nuria  and
      Sagot, Beno{\^\i}t"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""3"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-3.10"",
    pages = ""245--275"",
}
@proceedings{tal-2011-52-2,
    title = ""Traitement Automatique des Langues, Volume 52, Num{\'e}ro 2 : Vers la morphologie et au-del{\`a} [Toward Morphology and beyond]"",
    editor = ""Hathout, Nabil  and
      Namer, Fiammetta"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""2"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-2.0"",
}
@article{hathout-namer-2011-preface,
    title = ""Pr{\'e}face [Foreword]"",
    author = ""Hathout, Nabil  and
      Namer, Fiammetta"",
    editor = ""Hathout, Nabil  and
      Namer, Fiammetta"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""2"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-2.1"",
    pages = ""7--15"",
    language = ""French"",
}
@article{lavallee-langlais-2011-moranapho,
    title = ""Moranapho: un syst{\`e}me multilingue d{'}analyse morphologique bas{\'e} sur l{'}analogie formelle [Moranapho: a multilingual system for morphological analysis based on formal analogy]"",
    author = ""Lavall{\'e}e, Jean-Fran{\c{c}}ois  and
      Langlais, Philippe"",
    editor = ""Hathout, Nabil  and
      Namer, Fiammetta"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""2"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-2.2"",
    pages = ""17--44"",
    language = ""French"",
}
@article{virpioja-etal-2011-empirical,
    title = ""Empirical Comparison of Evaluation Methods for Unsupervised Learning of Morphology"",
    author = ""Virpioja, Sami  and
      Turunen, Ville T.  and
      Spiegler, Sebastian  and
      Kohonen, Oskar  and
      Kurimo, Mikko"",
    editor = ""Hathout, Nabil  and
      Namer, Fiammetta"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""2"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-2.3"",
    pages = ""45--90"",
}
@article{walther-sagot-2011-modelisation,
    title = ""Mod{\'e}lisation et impl{\'e}mentation de ph{\'e}nom{\`e}nes flexionnels non canoniques [Modeling and implementing non canonical morphological phenomena]"",
    author = ""Walther, G{\'e}raldine  and
      Sagot, Beno{\^\i}t"",
    editor = ""Hathout, Nabil  and
      Namer, Fiammetta"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""2"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-2.4"",
    pages = ""91--122"",
    language = ""French"",
}
@article{celata-etal-2011-enriched,
    title = ""Enriched sublexical representations to access morphological structures. A psycho-computational account"",
    author = ""Celata, Chiara  and
      Calderone, Basilio  and
      Montermini, Fabio"",
    editor = ""Hathout, Nabil  and
      Namer, Fiammetta"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""2"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-2.5"",
    pages = ""123--149"",
}
@article{chmielik-grabar-2011-detection,
    title = ""D{\'e}tection de la sp{\'e}cialisation scientifique et technique des documents biom{\'e}dicaux gr{\^a}ce aux informations morphologiques [Spotting scientific and technical specialization in biomedical documents using morphological clues]"",
    author = ""Chmielik, Jolanta  and
      Grabar, Natalia"",
    editor = ""Hathout, Nabil  and
      Namer, Fiammetta"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""2"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-2.6"",
    pages = ""151--179"",
    language = ""French"",
}
@proceedings{tal-2011-52-1,
    title = ""Traitement Automatique des Langues, Volume 52, Num{\'e}ro 1 : Varia [Varia]"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Yvon, Fran{\c{c}}ois"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""1"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-1.0"",
}
@article{sajous-etal-2011-enrichissement,
    title = ""Enrichissement de lexiques s{\'e}mantiques approvisionn{\'e}s par les foules : le syst{\`e}me {WISIGOTH} appliqu{\'e} {\`a} {W}iktionary [Enrichment of crowdsourced semantic networks: The {WISIGOTH} system applied to {W}iktionary]"",
    author = ""Sajous, Franck  and
      Navarro, Emmanuel  and
      Gaume, Bruno"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Yvon, Fran{\c{c}}ois"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""1"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-1.1"",
    pages = ""11--35"",
    language = ""French"",
}
@article{haralambous-lavagnino-2011-la,
    title = ""La r{\'e}duction de termes complexes dans les langues de sp{\'e}cialit{\'e} [Multi-word term reduction in specialized languages]"",
    author = ""Haralambous, Yannis  and
      Lavagnino, Elisa"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Yvon, Fran{\c{c}}ois"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""1"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-1.2"",
    pages = ""37--68"",
    language = ""French"",
}
@article{maurel-etal-2011-cascades,
    title = ""Cascades de transducteurs autour de la reconnaissance des entit{\'e}s nomm{\'e}es [{C}as{EN}: a transducer cascade to recognize {F}rench Named Entities]"",
    author = ""Maurel, Denis  and
      Friburger, Nathalie  and
      Antoine, Jean-Yves  and
      Eshkol-Taravella, Iris  and
      Nouvel, Damien"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Yvon, Fran{\c{c}}ois"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""1"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-1.3"",
    pages = ""69--96"",
    language = ""French"",
}
@article{perinet-etal-2011-identification,
    title = ""Identification des assertions dans les textes m{\'e}dicaux : application {\`a} la relation patient, probl{\`e}me m{\'e}dical [Identification of assertions in the medical texts: application to the relation patient, medical problem]"",
    author = ""P{\'e}rinet, Amandine  and
      Grabar, Natalia  and
      Hamon, Thierry"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Yvon, Fran{\c{c}}ois"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""1"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-1.4"",
    pages = ""97--132"",
    language = ""French"",
}
@article{bouillon-etal-2011-pour,
    title = ""Pour une interlangue utile en traduction automatique de la parole dans des domaines limit{\'e}s [Towards an interlingua for speech translation in limited domains]"",
    author = ""Bouillon, Pierrette  and
      Rayner, Manny  and
      Estella, Paula  and
      Gerlach, Johanna  and
      Georgescul, Maria"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Yvon, Fran{\c{c}}ois"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""1"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-1.5"",
    pages = ""133--160"",
    language = ""French"",
}
@article{weissenbacher-nazarenko-2011-comprendre,
    title = ""Comprendre les effets des erreurs d{'}annotations des plateformes de {TAL}, une {\'e}tude sur la r{\'e}solution des anaphores pronominales [Understand the effects of erroneous annotations produced by {NLP} pipelines, a case study on the pronominal anaphora resolution]"",
    author = ""Weissenbacher, Davy  and
      Nazarenko, Adeline"",
    editor = ""Villemonte de La Clergerie, {\'E}ric  and
      Daille, B{\'e}atrice  and
      Lepage, Yves  and
      Yvon, Fran{\c{c}}ois"",
    journal = ""Traitement Automatique des Langues"",
    volume = ""52"",
    number = ""1"",
    year = ""2011"",
    address = ""France"",
    publisher = ""ATALA (Association pour le Traitement Automatique des Langues)"",
    url = ""https://aclanthology.org/2011.tal-1.6"",
    pages = ""161--185"",
    language = ""French"",
}
@proceedings{mtsummit-2011-patent,
    title = ""Proceedings of the 4th Workshop on Patent Translation"",
    month = sep # "" 23"",
    year = ""2011"",
    address = ""Xiamen, China"",
    url = ""https://aclanthology.org/2011.mtsummit-wpt.0"",
}
@",medical text,translat,nlp,semant,annotat,evalu
" ""Utilisation de crit{\`e}res linguistiques de surface pour l{'}extraction de relation dans les textes bio-m{\'e}dicaux (Using shallow linguistic features for relation extraction in bio-medical texts)"","," ""Dans cet article, nous proposons de mod{\'e}liser la t{\^a}che d{'}extraction de relations {\`a} partir de corpus textuels comme un probl{\`e}me de classification. Nous montrons que, dans ce cadre, des repr{\'e}sentations fond{\'e}es sur des informations linguistiques de surface sont suffisantes pour que des algorithmes d{'}apprentissage artificiel standards les exploitant rivalisent avec les meilleurs syst{\`e}mes d{'}extraction de relations reposant sur des connaissances issues d{'}analyses profondes (analyses syntaxiques ou s{\'e}mantiques). Nous montrons {\'e}galement qu{'}en prenant davantage en compte les sp{\'e}cificit{\'e}s de la t{\^a}che d{'}extraction {\`a} r{\'e}aliser et des donn{\'e}es disponibles, il est possible d{'}obtenir des m{\'e}thodes encore plus efficaces tout en exploitant ces informations simples. La technique originale {\`a} base d{'}apprentissage « paresseux » et de mod{\`e}les de langue que nous {\'e}valuons en extraction d{'}interactions g{\'e}niques sur les donn{\'e}es du challenge LLL2005 d{\'e}passe les r{\'e}sultats de l{'}{\'e}tat de l{'}art."",","{reza-ebadat-etal-2011-utilisation,
    title = ""Utilisation de crit{\`e}res linguistiques de surface pour l{'}extraction de relation dans les textes bio-m{\'e}dicaux (Using shallow linguistic features for relation extraction in bio-medical texts)"",
    author = ""Reza Ebadat, Ali  and
      Claveau, Vincent  and
      S{\'e}billot, Pascale"",
    editor = ""Lafourcade, Mathieu  and
      Prince, Violaine"",
    booktitle = ""Actes de la 18e conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts"",
    month = jun,
    year = ""2011"",
    address = ""Montpellier, France"",
    publisher = ""ATALA"",
    url = ""https://aclanthology.org/2011.jeptalnrecital-court.21"",
    pages = ""122--127"",
    abstract = ""Dans cet article, nous proposons de mod{\'e}liser la t{\^a}che d{'}extraction de relations {\`a} partir de corpus textuels comme un probl{\`e}me de classification. Nous montrons que, dans ce cadre, des repr{\'e}sentations fond{\'e}es sur des informations linguistiques de surface sont suffisantes pour que des algorithmes d{'}apprentissage artificiel standards les exploitant rivalisent avec les meilleurs syst{\`e}mes d{'}extraction de relations reposant sur des connaissances issues d{'}analyses profondes (analyses syntaxiques ou s{\'e}mantiques). Nous montrons {\'e}galement qu{'}en prenant davantage en compte les sp{\'e}cificit{\'e}s de la t{\^a}che d{'}extraction {\`a} r{\'e}aliser et des donn{\'e}es disponibles, il est possible d{'}obtenir des m{\'e}thodes encore plus efficaces tout en exploitant ces informations simples. La technique originale {\`a} base d{'}apprentissage « paresseux » et de mod{\`e}les de langue que nous {\'e}valuons en extraction d{'}interactions g{\'e}niques sur les donn{\'e}es du challenge LLL2005 d{\'e}passe les r{\'e}sultats de l{'}{\'e}tat de l{'}art."",
    language = ""French"",
}
@",medical text,relation extract,challeng
" ""Proceedings of the 4th Workshop on Syntax and Structure in Statistical Translation"",",,"{miura-etal-2010-adverse,
    title = ""Adverse-Effect Relations Extraction from Massive Clinical Records"",
    author = ""Miura, Yasuhide  and
      Aramaki, Eiji  and
      Ohkuma, Tomoko  and
      Tonoike, Masatsugu  and
      Sugihara, Daigo  and
      Masuichi, Hiroshi  and
      Ohe, Kazuhiko"",
    editor = ""Kurohashi, Sadao  and
      Utsuro, Takehito"",
    booktitle = ""Proceedings of the Second Workshop on {NLP} Challenges in the Information Explosion Era ({NLPIX} 2010)"",
    month = aug,
    year = ""2010"",
    address = ""Beijing, China"",
    publisher = ""Coling 2010 Organizing Committee"",
    url = ""https://aclanthology.org/W10-3911"",
    pages = ""75--83"",
}
@proceedings{ws-2010-syntax,
    title = ""Proceedings of the 4th Workshop on Syntax and Structure in Statistical Translation"",
    editor = ""Wu, Dekai"",
    month = aug,
    year = ""2010"",
    address = ""Beijing, China"",
    publisher = ""Coling 2010 Organizing Committee"",
    url = ""https://aclanthology.org/W10-3800"",
}
@",clinical record,translat,nlp,challeng
" ""Towards a better understanding of uncertainties and speculations in {S}wedish clinical text {--} Analysis of an initial annotation trial"",",,"{velupillai-2010-towards,
    title = ""Towards a better understanding of uncertainties and speculations in {S}wedish clinical text {--} Analysis of an initial annotation trial"",
    author = ""Velupillai, Sumithra"",
    editor = ""Morante, Roser  and
      Sporleder, Caroline"",
    booktitle = ""Proceedings of the Workshop on Negation and Speculation in Natural Language Processing"",
    month = jul,
    year = ""2010"",
    address = ""Uppsala, Sweden"",
    publisher = ""University of Antwerp"",
    url = ""https://aclanthology.org/W10-3103"",
    pages = ""14--22"",
}
@",clinical text,natural language process,natural languag,language process,annotat
" ""Automatic annotation of speculation in biomedical texts: new perspectives and large-scale evaluation"",",,"{descles-etal-2010-automatic,
    title = ""Automatic annotation of speculation in biomedical texts: new perspectives and large-scale evaluation"",
    author = ""Descl{\'e}s, Julien  and
      Makkaoui, Olfa  and
      Hac{\`e}ne, Taouise"",
    editor = ""Morante, Roser  and
      Sporleder, Caroline"",
    booktitle = ""Proceedings of the Workshop on Negation and Speculation in Natural Language Processing"",
    month = jul,
    year = ""2010"",
    address = ""Uppsala, Sweden"",
    publisher = ""University of Antwerp"",
    url = ""https://aclanthology.org/W10-3106"",
    pages = ""32--40"",
}
@",medical text,natural language process,natural languag,language process,annotat,evalu
" ""Exploiting Multi-Features to Detect Hedges and their Scope in Biomedical Texts"",",,"{zhou-etal-2010-exploiting,
    title = ""Exploiting Multi-Features to Detect Hedges and their Scope in Biomedical Texts"",
    author = ""Zhou, Huiwei  and
      Li, Xiaoyan  and
      Huang, Degen  and
      Li, Zezhong  and
      Yang, Yuansheng"",
    editor = {Farkas, Rich{\'a}rd  and
      Vincze, Veronika  and
      Szarvas, Gy{\""o}rgy  and
      M{\'o}ra, Gy{\""o}rgy  and
      Csirik, J{\'a}nos},
    booktitle = ""Proceedings of the Fourteenth Conference on Computational Natural Language Learning {--} Shared Task"",
    month = jul,
    year = ""2010"",
    address = ""Uppsala, Sweden"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W10-3015"",
    pages = ""106--113"",
}
@",medical text,natural languag,shared task
" ""A {S}wedish Scientific Medical Corpus for Terminology Management and Linguistic Exploration"","," ""This paper describes the development of a new Swedish scientific medical corpus. We provide a detailed description of the characteristics of this new collection as well results of an application of the corpus on term management tasks, including terminology validation and terminology extraction. Although the corpus is representative for the scientific medical domain it still covers in detail a lot of specialised sub-disciplines such as diabetes and osteoporosis which makes it suitable for facilitating the production of smaller but more focused sub-corpora. We address this issue by making explicit some features of the corpus in order to demonstrate the usability of the corpus particularly for the quality assessment of subsets of official terminologies such as the Systematized NOmenclature of MEDicine - Clinical Terms (SNOMED CT). Domain-dependent language resources, labelled or not, are a crucial key components for progressing R{\&}D in the human language technology field since such resources are an indispensable, integrated part for terminology management, evaluation, software prototyping and design validation and a prerequisite for the development and evaluation of a number of sublanguage dependent applications including information extraction, text mining and information retrieval."",","{kokkinakis-gerdin-2010-swedish,
    title = ""A {S}wedish Scientific Medical Corpus for Terminology Management and Linguistic Exploration"",
    author = ""Kokkinakis, Dimitrios  and
      Gerdin, Ulla"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)"",
    month = may,
    year = ""2010"",
    address = ""Valletta, Malta"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2010/pdf/60_Paper.pdf"",
    abstract = ""This paper describes the development of a new Swedish scientific medical corpus. We provide a detailed description of the characteristics of this new collection as well results of an application of the corpus on term management tasks, including terminology validation and terminology extraction. Although the corpus is representative for the scientific medical domain it still covers in detail a lot of specialised sub-disciplines such as diabetes and osteoporosis which makes it suitable for facilitating the production of smaller but more focused sub-corpora. We address this issue by making explicit some features of the corpus in order to demonstrate the usability of the corpus particularly for the quality assessment of subsets of official terminologies such as the Systematized NOmenclature of MEDicine - Clinical Terms (SNOMED CT). Domain-dependent language resources, labelled or not, are a crucial key components for progressing R{\&}D in the human language technology field since such resources are an indispensable, integrated part for terminology management, evaluation, software prototyping and design validation and a prerequisite for the development and evaluation of a number of sublanguage dependent applications including information extraction, text mining and information retrieval."",
}
@",medical domain,information retriev,evalu,assess
" ""The {G}ene{R}eg Corpus for Gene Expression Regulation Events {---} An Overview of the Corpus and its In-Domain and Out-of-Domain Interoperability"","," ""Despite the large variety of corpora in the biomedical domain their annotations differ in many respects, e.g., the coverage of different, highly specialized knowledge domains, varying degrees of granularity of targeted relations, the specificity of linguistic anchoring of relations and named entities in documents, etc. We here present GeneReg (Gene Regulation Corpus), the result of an annotation campaign led by the Jena University Language {\&} Information Engineering (JULIE) Lab. The GeneReg corpus consists of 314 abstracts dealing with the regulation of gene expression in the model organism E. coli. Our emphasis in this paper is on the compatibility of the GeneReg corpus with the alternative Genia event corpus and with several in-domain and out-of-domain lexical resources, e.g., the Specialist Lexicon, FrameNet, and WordNet. The links we established from the GeneReg corpus to these external resources will help improve the performance of the automatic relation extraction engine JREx trained and evaluated on GeneReg."",","{buyko-etal-2010-genereg,
    title = ""The {G}ene{R}eg Corpus for Gene Expression Regulation Events {---} An Overview of the Corpus and its In-Domain and Out-of-Domain Interoperability"",
    author = ""Buyko, Ekaterina  and
      Beisswanger, Elena  and
      Hahn, Udo"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)"",
    month = may,
    year = ""2010"",
    address = ""Valletta, Malta"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2010/pdf/407_Paper.pdf"",
    abstract = ""Despite the large variety of corpora in the biomedical domain their annotations differ in many respects, e.g., the coverage of different, highly specialized knowledge domains, varying degrees of granularity of targeted relations, the specificity of linguistic anchoring of relations and named entities in documents, etc. We here present GeneReg (Gene Regulation Corpus), the result of an annotation campaign led by the Jena University Language {\&} Information Engineering (JULIE) Lab. The GeneReg corpus consists of 314 abstracts dealing with the regulation of gene expression in the model organism E. coli. Our emphasis in this paper is on the compatibility of the GeneReg corpus with the alternative Genia event corpus and with several in-domain and out-of-domain lexical resources, e.g., the Specialist Lexicon, FrameNet, and WordNet. The links we established from the GeneReg corpus to these external resources will help improve the performance of the automatic relation extraction engine JREx trained and evaluated on GeneReg."",
}
@",medical domain,relation extract,annotat,evalu
" ""mwetoolkit: a Framework for Multiword Expression Identification"","," ""This paper presents the Multiword Expression Toolkit (mwetoolkit), an environment for type and language-independent MWE identification from corpora. The mwetoolkit provides a targeted list of MWE candidates, extracted and filtered according to a number of user-defined criteria and a set of standard statistical association measures. For generating corpus counts, the toolkit provides both a corpus indexation facility and a tool for integration with web search engines, while for evaluation, it provides validation and annotation facilities. The mwetoolkit also allows easy integration with a machine learning tool for the creation and application of supervised MWE extraction models if annotated data is available. In our experiment, the mwetoolkit was tested and evaluated in the context of MWE extraction in the biomedical domain. Our preliminary results show that the toolkit performs better than other approaches, especially concerning recall. Moreover, this first version can also be extended in several ways in order to improve the quality of the results."",","{ramisch-etal-2010-mwetoolkit,
    title = ""mwetoolkit: a Framework for Multiword Expression Identification"",
    author = ""Ramisch, Carlos  and
      Villavicencio, Aline  and
      Boitet, Christian"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)"",
    month = may,
    year = ""2010"",
    address = ""Valletta, Malta"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2010/pdf/803_Paper.pdf"",
    abstract = ""This paper presents the Multiword Expression Toolkit (mwetoolkit), an environment for type and language-independent MWE identification from corpora. The mwetoolkit provides a targeted list of MWE candidates, extracted and filtered according to a number of user-defined criteria and a set of standard statistical association measures. For generating corpus counts, the toolkit provides both a corpus indexation facility and a tool for integration with web search engines, while for evaluation, it provides validation and annotation facilities. The mwetoolkit also allows easy integration with a machine learning tool for the creation and application of supervised MWE extraction models if annotated data is available. In our experiment, the mwetoolkit was tested and evaluated in the context of MWE extraction in the biomedical domain. Our preliminary results show that the toolkit performs better than other approaches, especially concerning recall. Moreover, this first version can also be extended in several ways in order to improve the quality of the results."",
}
@",medical domain,generat,annotat,evalu
" ""A memory-based learning approach to event extraction in biomedical texts"",",,"{morante-etal-2009-memory,
    title = ""A memory-based learning approach to event extraction in biomedical texts"",
    author = ""Morante, Roser  and
      Van Asch, Vincent  and
      Daelemans, Walter"",
    editor = ""Tsujii, Jun{'}ichi"",
    booktitle = ""Proceedings of the {B}io{NLP} 2009 Workshop Companion Volume for Shared Task"",
    month = jun,
    year = ""2009"",
    address = ""Boulder, Colorado"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W09-1408"",
    pages = ""59--67"",
}
@",medical text,nlp,shared task
" ""Annotating and Recognising Named Entities in Clinical Notes"",",,"{wang-2009-annotating,
    title = ""Annotating and Recognising Named Entities in Clinical Notes"",
    author = ""Wang, Yefeng"",
    editor = ""Roark, Brian  and
      Ngai, Grace  and
      Dimalen, Davis Muhajereen D.  and
      Finkel, Jenny Rose  and
      Thomson, Blaise"",
    booktitle = ""Proceedings of the {ACL}-{IJCNLP} 2009 Student Research Workshop"",
    month = aug,
    year = ""2009"",
    address = ""Suntec, Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/P09-3003"",
    pages = ""18--26"",
}
@",clinical not,nlp,annotat
" ""The {B}io{S}cope corpus: annotation for negation, uncertainty and their scope in biomedical texts"",",,"{szarvas-etal-2008-bioscope,
    title = ""The {B}io{S}cope corpus: annotation for negation, uncertainty and their scope in biomedical texts"",
    author = {Szarvas, Gy{\""o}rgy  and
      Vincze, Veronika  and
      Farkas, Rich{\'a}rd  and
      Csirik, J{\'a}nos},
    editor = ""Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin Bretonnel  and
      Pestian, John  and
      Tsujii, Jun{'}ichi  and
      Webber, Bonnie"",
    booktitle = ""Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing"",
    month = jun,
    year = ""2008"",
    address = ""Columbus, Ohio"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W08-0606"",
    pages = ""38--45"",
}
@",medical text,natural language process,natural languag,language process,annotat
" ""A Pilot Annotation to Investigate Discourse Connectivity in Biomedical Text"",",,"{yu-etal-2008-pilot,
    title = ""A Pilot Annotation to Investigate Discourse Connectivity in Biomedical Text"",
    author = ""Yu, Hong  and
      Frid, Nadya  and
      McRoy, Susan  and
      Prasad, Rashmi  and
      Lee, Alan  and
      Joshi, Aravind"",
    editor = ""Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin Bretonnel  and
      Pestian, John  and
      Tsujii, Jun{'}ichi  and
      Webber, Bonnie"",
    booktitle = ""Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing"",
    month = jun,
    year = ""2008"",
    address = ""Columbus, Ohio"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W08-0614"",
    pages = ""92--93"",
}
@",medical text,natural language process,natural languag,language process,annotat
" ""Temporal Annotation of Clinical Text"",",,"{mowery-etal-2008-temporal,
    title = ""Temporal Annotation of Clinical Text"",
    author = ""Mowery, Danielle  and
      Harkema, Henk  and
      Chapman, Wendy"",
    editor = ""Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin Bretonnel  and
      Pestian, John  and
      Tsujii, Jun{'}ichi  and
      Webber, Bonnie"",
    booktitle = ""Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing"",
    month = jun,
    year = ""2008"",
    address = ""Columbus, Ohio"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W08-0621"",
    pages = ""106--107"",
}
@",clinical text,natural language process,natural languag,language process,annotat
" ""A Coreference Corpus and Resolution System for {D}utch"","," ""We present the main outcomes of the COREA project: a corpus annotated with coreferential relations and a coreference resolution system for Dutch. In the project we developed annotation guidelines for coreference resolution for Dutch and annotated a corpus of 135K tokens. We discuss these guidelines, the annotation tool, and the inter-annotator agreement. We also show a visualization of the annotated relations. The standard approach to evaluate a coreference resolution system is to compare the predictions of the system to a hand-annotated gold standard test set (cross-validation). A more practically oriented evaluation is to test the usefulness of coreference relation information in an NLP application. We run experiments with an Information Extraction module for the medical domain, and measure the performance of this module with and without the coreference relation information. We present the results of both this application-oriented evaluation of our system and of a standard cross-validation evaluation. In a separate experiment we also evaluate the effect of coreference information produced by a simple rule-based coreference module in a Question Answering application."",","{hendrickx-etal-2008-coreference,
    title = ""A Coreference Corpus and Resolution System for {D}utch"",
    author = ""Hendrickx, Iris  and
      Bouma, Gosse  and
      Coppens, Frederik  and
      Daelemans, Walter  and
      Hoste, Veronique  and
      Kloosterman, Geert  and
      Mineur, Anne-Marie  and
      Van Der Vloet, Joeri  and
      Verschelde, Jean-Luc"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)"",
    month = may,
    year = ""2008"",
    address = ""Marrakech, Morocco"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2008/pdf/49_paper.pdf"",
    abstract = ""We present the main outcomes of the COREA project: a corpus annotated with coreferential relations and a coreference resolution system for Dutch. In the project we developed annotation guidelines for coreference resolution for Dutch and annotated a corpus of 135K tokens. We discuss these guidelines, the annotation tool, and the inter-annotator agreement. We also show a visualization of the annotated relations. The standard approach to evaluate a coreference resolution system is to compare the predictions of the system to a hand-annotated gold standard test set (cross-validation). A more practically oriented evaluation is to test the usefulness of coreference relation information in an NLP application. We run experiments with an Information Extraction module for the medical domain, and measure the performance of this module with and without the coreference relation information. We present the results of both this application-oriented evaluation of our system and of a standard cross-validation evaluation. In a separate experiment we also evaluate the effect of coreference information produced by a simple rule-based coreference module in a Question Answering application."",
}
@",medical domain,nlp,annotat,evalu
" ""Combining Terminology Resources and Statistical Methods for Entity Recognition: an Evaluation"","," ""Terminologies and other knowledge resources are widely used to aid entity recognition in specialist domain texts. As well as providing lexicons of specialist terms, linkage from the text back to a resource can make additional knowledge available to applications. Use of such resources is especially pertinent in the biomedical domain, where large numbers of these resources are available, and where they are widely used in informatics applications. Terminology resources can be most readily used by simple lexical lookup of terms in the text. A major drawback with such lexical lookup, however, is poor precision caused by ambiguity between domain terms and general language words. We combine lexical lookup with simple filtering of ambiguous terms, to improve precision. We compare this lexical lookup with a statistical method of entity recognition, and to a method which combines the two approaches. We show that the combined method boosts precision with little loss of recall, and that linkage from recognised entities back to the domain knowledge resources can be maintained."",","{roberts-etal-2008-combining,
    title = ""Combining Terminology Resources and Statistical Methods for Entity Recognition: an Evaluation"",
    author = ""Roberts, Angus  and
      Gaizasukas, Robert  and
      Hepple, Mark  and
      Guo, Yikun"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)"",
    month = may,
    year = ""2008"",
    address = ""Marrakech, Morocco"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2008/pdf/758_paper.pdf"",
    abstract = ""Terminologies and other knowledge resources are widely used to aid entity recognition in specialist domain texts. As well as providing lexicons of specialist terms, linkage from the text back to a resource can make additional knowledge available to applications. Use of such resources is especially pertinent in the biomedical domain, where large numbers of these resources are available, and where they are widely used in informatics applications. Terminology resources can be most readily used by simple lexical lookup of terms in the text. A major drawback with such lexical lookup, however, is poor precision caused by ambiguity between domain terms and general language words. We combine lexical lookup with simple filtering of ambiguous terms, to improve precision. We compare this lexical lookup with a statistical method of entity recognition, and to a method which combines the two approaches. We show that the combined method boosts precision with little loss of recall, and that linkage from recognised entities back to the domain knowledge resources can be maintained."",
}
@",medical domain,entity recognit,evalu
" ""A lexicon for biology and bioinformatics: the {BOOTS}trep experience."","," ""This paper describes the design, implementation and population of a lexical resource for biology and bioinformatics (the BioLexicon) developed within an ongoing European project. The aim of this project is text-based knowledge harvesting for support to information extraction and text mining in the biomedical domain. The BioLexicon is a large-scale lexical-terminological resource encoding different information types in one single integrated resource. In the design of the resource we follow the ISO/DIS 24613 Lexical Mark-up Framework standard, which ensures reusability of the information encoded and easy exchange of both data and architecture. The design of the resource also takes into account the needs of our text mining partners who automatically extract syntactic and semantic information from texts and feed it to the lexicon. The present contribution first describes in detail the model of the BioLexicon along its three main layers: morphology, syntax and semantics; then, it briefly describes the database implementation of the model and the population strategy followed within the project, together with an example. The BioLexicon database in fact comes equipped with automatic uploading procedures based on a common exchange XML format, which guarantees that the lexicon can be properly populated with data coming from different sources."",","{quochi-etal-2008-lexicon,
    title = ""A lexicon for biology and bioinformatics: the {BOOTS}trep experience."",
    author = ""Quochi, Valeria  and
      Monachini, Monica  and
      Del Gratta, Riccardo  and
      Calzolari, Nicoletta"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)"",
    month = may,
    year = ""2008"",
    address = ""Marrakech, Morocco"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2008/pdf/576_paper.pdf"",
    abstract = ""This paper describes the design, implementation and population of a lexical resource for biology and bioinformatics (the BioLexicon) developed within an ongoing European project. The aim of this project is text-based knowledge harvesting for support to information extraction and text mining in the biomedical domain. The BioLexicon is a large-scale lexical-terminological resource encoding different information types in one single integrated resource. In the design of the resource we follow the ISO/DIS 24613 Lexical Mark-up Framework standard, which ensures reusability of the information encoded and easy exchange of both data and architecture. The design of the resource also takes into account the needs of our text mining partners who automatically extract syntactic and semantic information from texts and feed it to the lexicon. The present contribution first describes in detail the model of the BioLexicon along its three main layers: morphology, syntax and semantics; then, it briefly describes the database implementation of the model and the population strategy followed within the project, together with an example. The BioLexicon database in fact comes equipped with automatic uploading procedures based on a common exchange XML format, which guarantees that the lexicon can be properly populated with data coming from different sources."",
}
@",medical domain,semant,evalu
" ""A Semantically Annotated {S}wedish Medical Corpus"","," ""With the information overload in the life sciences there is an increasing need for annotated corpora, particularly with biological and biomedical entities, which is the driving force for data-driven language processing applications and the empirical approach to language study. Inspired by the work in the GENIA Corpus, which is one of the very few of such corpora, extensively used in the biomedical field, and in order to fulfil the needs of our research, we have collected a Swedish medical corpus, the MEDLEX Corpus. MEDLEX is a large structurally and linguistically annotated document collection, consisting of a variety of text documents related to various medical text subfields, and does not focus at a particular medical genre, due to the lack of large Swedish resources within a particular medical subdomain. Out of this collection we selected 300 documents which were manually examined by two human experts who inspected, corrected and/or accordingly modified the automatically provided annotations according to a set of provided labelling guidelines. The annotations consist of medical terminology provided by the Swedish and English MeSH{\copyright} (Medical Subject Headings) thesauri as well as named entity labels provided by an enhanced named entity recognition software."",","{kokkinakis-2008-semantically,
    title = ""A Semantically Annotated {S}wedish Medical Corpus"",
    author = ""Kokkinakis, Dimitrios"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)"",
    month = may,
    year = ""2008"",
    address = ""Marrakech, Morocco"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2008/pdf/15_paper.pdf"",
    abstract = ""With the information overload in the life sciences there is an increasing need for annotated corpora, particularly with biological and biomedical entities, which is the driving force for data-driven language processing applications and the empirical approach to language study. Inspired by the work in the GENIA Corpus, which is one of the very few of such corpora, extensively used in the biomedical field, and in order to fulfil the needs of our research, we have collected a Swedish medical corpus, the MEDLEX Corpus. MEDLEX is a large structurally and linguistically annotated document collection, consisting of a variety of text documents related to various medical text subfields, and does not focus at a particular medical genre, due to the lack of large Swedish resources within a particular medical subdomain. Out of this collection we selected 300 documents which were manually examined by two human experts who inspected, corrected and/or accordingly modified the automatically provided annotations according to a set of provided labelling guidelines. The annotations consist of medical terminology provided by the Swedish and English MeSH{\copyright} (Medical Subject Headings) thesauri as well as named entity labels provided by an enhanced named entity recognition software."",
}
@",medical text,language process,entity recognit,semant,annotat,evalu
" ""Learning Patterns for Building Resources about Semantic Relations in the Medical Domain"","," ""In this article, we present a method for extracting automatically semantic relations from texts in the medical domain using linguistic patterns. These patterns refer to three levels of information about words: inflected form, lemma and part-of-speech. The method we present consists first in identifying the entities that are part of the relations to extract, that is to say diseases, exams, treatments, drugs or symptoms. Thereafter, sentences that contain couples of entities are extracted and the presence of a semantic relation is validated by applying linguistic patterns. These patterns were previously learnt automatically from a manually annotated corpus by relying onan algorithm based on the edit distance. We first report the results of an evaluation of our medical entity tagger for the five types of entities we have mentioned above and then, more globally, the results of an evaluation of our extraction method for four relations between these entities. Both evaluations were done for French."",","{embarek-ferret-2008-learning,
    title = ""Learning Patterns for Building Resources about Semantic Relations in the Medical Domain"",
    author = ""Embarek, Mehdi  and
      Ferret, Olivier"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)"",
    month = may,
    year = ""2008"",
    address = ""Marrakech, Morocco"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2008/pdf/514_paper.pdf"",
    abstract = ""In this article, we present a method for extracting automatically semantic relations from texts in the medical domain using linguistic patterns. These patterns refer to three levels of information about words: inflected form, lemma and part-of-speech. The method we present consists first in identifying the entities that are part of the relations to extract, that is to say diseases, exams, treatments, drugs or symptoms. Thereafter, sentences that contain couples of entities are extracted and the presence of a semantic relation is validated by applying linguistic patterns. These patterns were previously learnt automatically from a manually annotated corpus by relying onan algorithm based on the edit distance. We first report the results of an evaluation of our medical entity tagger for the five types of entities we have mentioned above and then, more globally, the results of an evaluation of our extraction method for four relations between these entities. Both evaluations were done for French."",
}
@",medical domain,semant,annotat,evalu
" ""Turning a Term Extractor into a new Domain: first Experiences"","," ""Computational terminology has notably evolved since the advent of computers. Regarding the extraction of terms in particular, a large number of resources have been developed: from very general tools to other much more specific acquisition methodologies. Such acquisition methodologies range from using simple linguistic patterns or frequency counting methods to using much more evolved strategies combining morphological, syntactical, semantical and contextual information. Researchers usually develop a term extractor to be applied to a given domain and, in some cases, some testing about the tool performance is also done. Afterwards, such tools may also be applied to other domains, though frequently no additional test is made in such cases. Usually, the application of a given tool to other domain does not require any tuning. Recently, some tools using semantic resources have been developed. In such cases, either a domain-specific or a generic resource may be used. In the latter case, some tuning may be necessary in order to adapt the tool to a new domain. In this paper, we present the task started in order to adapt YATE, a term extractor that uses a generic resource as EWN and that is already developed for the medical domain, into the economic one."",","{vivaldi-etal-2008-turning,
    title = ""Turning a Term Extractor into a new Domain: first Experiences"",
    author = ""Vivaldi, Jorge  and
      Joan, Anna  and
      Lorente, Merc{\`e}"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)"",
    month = may,
    year = ""2008"",
    address = ""Marrakech, Morocco"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2008/pdf/189_paper.pdf"",
    abstract = ""Computational terminology has notably evolved since the advent of computers. Regarding the extraction of terms in particular, a large number of resources have been developed: from very general tools to other much more specific acquisition methodologies. Such acquisition methodologies range from using simple linguistic patterns or frequency counting methods to using much more evolved strategies combining morphological, syntactical, semantical and contextual information. Researchers usually develop a term extractor to be applied to a given domain and, in some cases, some testing about the tool performance is also done. Afterwards, such tools may also be applied to other domains, though frequently no additional test is made in such cases. Usually, the application of a given tool to other domain does not require any tuning. Recently, some tools using semantic resources have been developed. In such cases, either a domain-specific or a generic resource may be used. In the latter case, some tuning may be necessary in order to adapt the tool to a new domain. In this paper, we present the task started in order to adapt YATE, a term extractor that uses a generic resource as EWN and that is already developed for the medical domain, into the economic one."",
}
@",medical domain,semant,evalu
" ""System Evaluation on a Named Entity Corpus from Clinical Notes"","," ""This paper presents the evaluation of the dictionary look-up component of Mayo Clinics Information Extraction system. The component was tested on a corpus of 160 free-text clinical notes which were manually annotated with the named entity disease. This kind of clinical text presents many language challenges such as fragmented sentences and heavy use of abbreviations and acronyms. The dictionary used for this evaluation was a subset of SNOMED-CT with semantic types corresponding to diseases/disorders without any augmentation. The algorithm achieves an F-score of 0.56 for exact matches and F-scores of 0.76 and 0.62 for right and left-partial matches respectively. Machine learning techniques are currently under investigation to improve this task."",","{schuler-etal-2008-system,
    title = ""System Evaluation on a Named Entity Corpus from Clinical Notes"",
    author = ""Schuler, Karin  and
      Kaggal, Vinod  and
      Masanz, James  and
      Ogren, Philip  and
      Savova, Guergana"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)"",
    month = may,
    year = ""2008"",
    address = ""Marrakech, Morocco"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2008/pdf/764_paper.pdf"",
    abstract = ""This paper presents the evaluation of the dictionary look-up component of Mayo Clinics Information Extraction system. The component was tested on a corpus of 160 free-text clinical notes which were manually annotated with the named entity disease. This kind of clinical text presents many language challenges such as fragmented sentences and heavy use of abbreviations and acronyms. The dictionary used for this evaluation was a subset of SNOMED-CT with semantic types corresponding to diseases/disorders without any augmentation. The algorithm achieves an F-score of 0.56 for exact matches and F-scores of 0.76 and 0.62 for right and left-partial matches respectively. Machine learning techniques are currently under investigation to improve this task."",
}
@",clinical not,clinical text,semant,annotat,challeng,evalu
" ""Constructing Evaluation Corpora for Automated Clinical Named Entity Recognition"","," ""We report on the construction of a gold-standard dataset consisting of annotated clinical notes suitable for evaluating our biomedical named entity recognition system. The dataset is the result of consensus between four human annotators and contains 1,556 annotations on 160 clinical notes using 658 unique concept codes from SNOMED-CT corresponding to human disorders. Inter-annotator agreement was calculated on annotations from 100 of the documents for span (90.9{\%}), concept code (81.7{\%}), context (84.8{\%}), and status (86.0{\%}) agreement. Complete agreement for span, concept code, context, and status was 74.6{\%}. We found that creating a consensus set based on annotations from two independently-created annotation sets can reduce inter-annotator disagreement by 32.3{\%}. We found little benefit to pre-annotating the corpus with a third-party named entity recognizer."",","{ogren-etal-2008-constructing,
    title = ""Constructing Evaluation Corpora for Automated Clinical Named Entity Recognition"",
    author = ""Ogren, Philip  and
      Savova, Guergana  and
      Chute, Christopher"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)"",
    month = may,
    year = ""2008"",
    address = ""Marrakech, Morocco"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2008/pdf/796_paper.pdf"",
    abstract = ""We report on the construction of a gold-standard dataset consisting of annotated clinical notes suitable for evaluating our biomedical named entity recognition system. The dataset is the result of consensus between four human annotators and contains 1,556 annotations on 160 clinical notes using 658 unique concept codes from SNOMED-CT corresponding to human disorders. Inter-annotator agreement was calculated on annotations from 100 of the documents for span (90.9{\%}), concept code (81.7{\%}), context (84.8{\%}), and status (86.0{\%}) agreement. Complete agreement for span, concept code, context, and status was 74.6{\%}. We found that creating a consensus set based on annotations from two independently-created annotation sets can reduce inter-annotator disagreement by 32.3{\%}. We found little benefit to pre-annotating the corpus with a third-party named entity recognizer."",
}
@",clinical not,entity recognit,annotat,evalu
" ""The Role of Roles in Classifying Annotated Biomedical Text"",",,"{doan-etal-2007-role,
    title = ""The Role of Roles in Classifying Annotated Biomedical Text"",
    author = ""Doan, Son  and
      Kawazoe, Ai  and
      Collier, Nigel"",
    editor = ""Cohen, K. Bretonnel  and
      Demner-Fushman, Dina  and
      Friedman, Carol  and
      Hirschman, Lynette  and
      Pestian, John"",
    booktitle = ""Biological, translational, and clinical language processing"",
    month = jun,
    year = ""2007"",
    address = ""Prague, Czech Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W07-1003"",
    pages = ""17--24"",
}
@",medical text,language process,translat,annotat
" ""Proceedings of the Eighth Meeting of the {ACL} Special Interest Group on Computational Phonology and Morphology at {HLT}-{NAACL} 2006"",",,"{vlachos-gasperin-2006-bootstrapping,
    title = ""Bootstrapping and Evaluating Named Entity Recognition in the Biomedical Domain"",
    author = ""Vlachos, Andreas  and
      Gasperin, Caroline"",
    editor = ""Verspoor, Karin  and
      Cohen, Kevin Bretonnel  and
      Goertzel, Ben  and
      Mani, Inderjeet"",
    booktitle = ""Proceedings of the {HLT}-{NAACL} {B}io{NLP} Workshop on Linking Natural Language and Biology"",
    month = jun,
    year = ""2006"",
    address = ""New York, New York"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W06-3328"",
    pages = ""138--145"",
}
@proceedings{ws-2006-acl-special,
    title = ""Proceedings of the Eighth Meeting of the {ACL} Special Interest Group on Computational Phonology and Morphology at {HLT}-{NAACL} 2006"",
    editor = ""Wicentowski, Richard  and
      Kondrak, Greg"",
    month = jun,
    year = ""2006"",
    address = ""New York City, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W06-3200"",
}
@",medical domain,natural languag,nlp,entity recognit,evalu
" ""Annotation and Disambiguation of Semantic Types in Biomedical Text: A Cascaded Approach to Named Entity Recognition"",",,"{rebholz-schuhmann-etal-2006-annotation,
    title = ""Annotation and Disambiguation of Semantic Types in Biomedical Text: A Cascaded Approach to Named Entity Recognition"",
    author = ""Rebholz-Schuhmann, Dietrich  and
      Kirsch, Harald  and
      Gaudan, Sylvain  and
      Arregui, Miguel  and
      Nenadic, Goran"",
    booktitle = ""Proceedings of the 5th Workshop on {NLP} and {XML} ({NLPXML}-2006): Multi-Dimensional Markup in Natural Language Processing"",
    year = ""2006"",
    url = ""https://aclanthology.org/W06-2702"",
}
@",medical text,natural language process,natural languag,language process,nlp,entity recognit,semant,annotat
" ""Collection, Encoding and Linguistic Processing of a {S}wedish Medical Corpus - The {MEDLEX} Experience"","," ""Corpora annotated with structural and linguistic characteristics play a major role in nearly every area of language processing. During recent years a number of corpora and large data sets became known and available to research even in specialized fields such as medicine, but still however, targeted predominantly for the English language. This paper provides a description of the collection, encoding and linguistic processing of an ever growing Swedish medical corpus, the MEDLEX Corpus. MEDLEX consists of a variety of text-documents related to various medical text genres. The MEDLEX Corpus has been structurally annotated using the Corpus Encoding Standard for XML (XCES), lemmatized and automatically annotated with part-of-speech and semantic information (extended named entities and the Medical Subject Headings, MeSH, terminology). The results from the processing stages (part-of-speech, entities and terminology) have been merged into a single representation format and syntactically analysed using a cascaded finite state parser. Finally, the parsers results are converted into a tree structure that follows the TIGER-XML coding scheme, resulting a suitable for further exploration and fairly large Treebank of Swedish medical texts."",","{kokkinakis-2006-collection,
    title = ""Collection, Encoding and Linguistic Processing of a {S}wedish Medical Corpus - The {MEDLEX} Experience"",
    author = ""Kokkinakis, Dimitrios"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Gangemi, Aldo  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)"",
    month = may,
    year = ""2006"",
    address = ""Genoa, Italy"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2006/pdf/23_pdf.pdf"",
    abstract = ""Corpora annotated with structural and linguistic characteristics play a major role in nearly every area of language processing. During recent years a number of corpora and large data sets became known and available to research even in specialized fields such as medicine, but still however, targeted predominantly for the English language. This paper provides a description of the collection, encoding and linguistic processing of an ever growing Swedish medical corpus, the MEDLEX Corpus. MEDLEX consists of a variety of text-documents related to various medical text genres. The MEDLEX Corpus has been structurally annotated using the Corpus Encoding Standard for XML (XCES), lemmatized and automatically annotated with part-of-speech and semantic information (extended named entities and the Medical Subject Headings, MeSH, terminology). The results from the processing stages (part-of-speech, entities and terminology) have been merged into a single representation format and syntactically analysed using a cascaded finite state parser. Finally, the parsers results are converted into a tree structure that follows the TIGER-XML coding scheme, resulting a suitable for further exploration and fairly large Treebank of Swedish medical texts."",
}
@",medical text,language process,semant,annotat,evalu
" ""Clustering acronyms in biomedical text for disambiguation"","," ""Given the increasing number of neologisms in biomedicine (names of genes, diseases, molecules, etc.), the rate of acronyms used in literature also increases. Existing acronym dictionaries cannot keep up with the rate of new creations. Thus, discovering and disambiguating acronyms and their expanded forms are essential aspects of text mining and terminology management. We present a method for clustering long forms identified by an acronym recognition method. Applying the acronym recognition method to MEDLINE abstracts, we obtained a list of short/long forms. The recognized short/long forms were classified by abiologist to construct an evaluation set for clustering sets of similar long forms. We observed five types of term variation in the evaluation set and defined four similarity measures to gathers the similar longforms (i.e., orthographic, morphological, syntactic, lexico semantic variants, nested abbreviations). The complete-link clustering with the four similarity measures achieved 87.5{\%} precision and 84.9{\%} recall on the evaluation set."",","{okazaki-ananiadou-2006-clustering,
    title = ""Clustering acronyms in biomedical text for disambiguation"",
    author = ""Okazaki, Naoaki  and
      Ananiadou, Sophia"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Gangemi, Aldo  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)"",
    month = may,
    year = ""2006"",
    address = ""Genoa, Italy"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2006/pdf/351_pdf.pdf"",
    abstract = ""Given the increasing number of neologisms in biomedicine (names of genes, diseases, molecules, etc.), the rate of acronyms used in literature also increases. Existing acronym dictionaries cannot keep up with the rate of new creations. Thus, discovering and disambiguating acronyms and their expanded forms are essential aspects of text mining and terminology management. We present a method for clustering long forms identified by an acronym recognition method. Applying the acronym recognition method to MEDLINE abstracts, we obtained a list of short/long forms. The recognized short/long forms were classified by abiologist to construct an evaluation set for clustering sets of similar long forms. We observed five types of term variation in the evaluation set and defined four similarity measures to gathers the similar longforms (i.e., orthographic, morphological, syntactic, lexico semantic variants, nested abbreviations). The complete-link clustering with the four similarity measures achieved 87.5{\%} precision and 84.9{\%} recall on the evaluation set."",
}
@",medical text,semant,evalu
" ""Creating a Large-Scale {A}rabic to {F}rench Statistical {M}achine{T}ranslation System"","," ""In this work, the creation of a large-scale Arabic to French statistical machine translation system is presented. We introduce all necessary steps from corpus aquisition, preprocessing the data to training and optimizing the system and eventual evaluation. Since no corpora existed previously, we collected large amounts of data from the web. Arabic word segmentation was crucial to reduce the overall number of unknown words. We describe the phrase-based SMT system used for training and generation of the translation hypotheses. Results on the second CESTA evaluation campaign are reported. The setting was inthe medical domain. The prototype reaches a favorable BLEU score of40.8{\%}."",","{hasan-etal-2006-creating,
    title = ""Creating a Large-Scale {A}rabic to {F}rench Statistical {M}achine{T}ranslation System"",
    author = ""Hasan, Sa{\v{s}}a  and
      Isbihani, Anas El  and
      Ney, Hermann"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Gangemi, Aldo  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)"",
    month = may,
    year = ""2006"",
    address = ""Genoa, Italy"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2006/pdf/405_pdf.pdf"",
    abstract = ""In this work, the creation of a large-scale Arabic to French statistical machine translation system is presented. We introduce all necessary steps from corpus aquisition, preprocessing the data to training and optimizing the system and eventual evaluation. Since no corpora existed previously, we collected large amounts of data from the web. Arabic word segmentation was crucial to reduce the overall number of unknown words. We describe the phrase-based SMT system used for training and generation of the translation hypotheses. Results on the second CESTA evaluation campaign are reported. The setting was inthe medical domain. The prototype reaches a favorable BLEU score of40.8{\%}."",
}
@",medical domain,translat,generat,evalu
" ""Linguistic and Biological Annotations of Biological Interaction Events"","," ""This paper discusses an augmentation of a corpus ofresearch abstracts in biomedical domain (the GENIA corpus) with two kinds of annotations: tree annotation and event annotation. The tree annotation identifies the linguistic structure that encodes the relations among entities. The event annotation reveals the semantic structure of the biological interaction events encoded in the text. With these annotations we aim to provide a link between the clue and the target of biological event information extraction."",","{ohta-etal-2006-linguistic,
    title = ""Linguistic and Biological Annotations of Biological Interaction Events"",
    author = ""Ohta, Tomoko  and
      Tateisi, Yuka  and
      Kim, Jin-Dong  and
      Yakushiji, Akane  and
      Tsujii, Jun-ichi"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Gangemi, Aldo  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)"",
    month = may,
    year = ""2006"",
    address = ""Genoa, Italy"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2006/pdf/570_pdf.pdf"",
    abstract = ""This paper discusses an augmentation of a corpus ofresearch abstracts in biomedical domain (the GENIA corpus) with two kinds of annotations: tree annotation and event annotation. The tree annotation identifies the linguistic structure that encodes the relations among entities. The event annotation reveals the semantic structure of the biological interaction events encoded in the text. With these annotations we aim to provide a link between the clue and the target of biological event information extraction."",
}
@",medical domain,semant,annotat,evalu
" ""{CESTA}: First Conclusions of the Technolangue {MT} Evaluation Campaign"","," ""This article outlines the evaluation protocol and provides the main results of the French Evaluation Campaign for Machine Translation Systems, CESTA. Following the initial objectives and evaluation plans, the evaluation metrics are briefly described: along with fluency and adequacy assessed by human judges, a number of recently proposed automated metrics are used. Two evaluation campaigns were organized, the first one in the general domain, and the second one in the medical domain. Up to six systems translating from English into French, and two systems translating from Arabic into French, took part in the campaign. The numerical results illustrate the differences between classes of systems, and provide interesting indications about the reliability of the automated metrics for French as a target language, both by comparison to human judges and using correlations between metrics. The corpora that were produced, as well as the information about the reliability of metrics, constitute reusable resources for MT evaluation."",","{hamon-etal-2006-cesta,
    title = ""{CESTA}: First Conclusions of the Technolangue {MT} Evaluation Campaign"",
    author = ""Hamon, O.  and
      Popescu-Belis, A.  and
      Choukri, K.  and
      Dabbadie, M.  and
      Hartley, A.  and
      Mustafa El Hadi, W.  and
      Rajman, M.  and
      Timimi, I."",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Gangemi, Aldo  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)"",
    month = may,
    year = ""2006"",
    address = ""Genoa, Italy"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2006/pdf/586_pdf.pdf"",
    abstract = ""This article outlines the evaluation protocol and provides the main results of the French Evaluation Campaign for Machine Translation Systems, CESTA. Following the initial objectives and evaluation plans, the evaluation metrics are briefly described: along with fluency and adequacy assessed by human judges, a number of recently proposed automated metrics are used. Two evaluation campaigns were organized, the first one in the general domain, and the second one in the medical domain. Up to six systems translating from English into French, and two systems translating from Arabic into French, took part in the campaign. The numerical results illustrate the differences between classes of systems, and provide interesting indications about the reliability of the automated metrics for French as a target language, both by comparison to human judges and using correlations between metrics. The corpora that were produced, as well as the information about the reliability of metrics, constitute reusable resources for MT evaluation."",
}
@",medical domain,translat,evalu,assess
" ""Semantic Atomicity and Multilinguality in the Medical Domain: Design Considerations for the {M}orpho{S}aurus Subword Lexicon"","," ""We present the lexico-semantic foundations underlying a multilingual lexicon the entries of which are constituted by so-called subwords. These subwords reflect semantic atomicity constraints in the medical domain which diverge from canonical lexicological understanding in NLP. We focus here on criteria to identify and delimit reasonable subword units, to group them into functionally adequate synonymy classes and relate them by two types of lexical relations. The lexicon we implemented on the basis of these considerations forms the lexical backbone for MorphoSaurus, a cross-language document retrieval engine for the medical domain."",","{schulz-etal-2006-semantic,
    title = ""Semantic Atomicity and Multilinguality in the Medical Domain: Design Considerations for the {M}orpho{S}aurus Subword Lexicon"",
    author = ""Schulz, Stefan  and
      Mark{\'o}, Korn{\'e}l  and
      Daumke, Philipp  and
      Hahn, Udo  and
      Hanser, Susanne  and
      Nohama, Percy  and
      de Andrade, Roosewelt Leite  and
      Pacheco, Edson  and
      Romacker, Martin"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Gangemi, Aldo  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)"",
    month = may,
    year = ""2006"",
    address = ""Genoa, Italy"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2006/pdf/724_pdf.pdf"",
    abstract = ""We present the lexico-semantic foundations underlying a multilingual lexicon the entries of which are constituted by so-called subwords. These subwords reflect semantic atomicity constraints in the medical domain which diverge from canonical lexicological understanding in NLP. We focus here on criteria to identify and delimit reasonable subword units, to group them into functionally adequate synonymy classes and relate them by two types of lexical relations. The lexicon we implemented on the basis of these considerations forms the lexical backbone for MorphoSaurus, a cross-language document retrieval engine for the medical domain."",
}
@",medical domain,nlp,semant,evalu
" ""Subword Clusters as Light-Weight Interlingua for Multilingual Document Retrieval"","," ""We introduce a light-weight interlingua for a cross-language document retrieval system in the medical domain. It is composed of equivalence classes of semantically primitive, language-specific subwords which are clustered by interlingual and intralingual synonymy. Each subword cluster represents a basic conceptual entity of the language-independent interlingua. Documents, as well as queries, are mapped to this interlingua level on which retrieval operations are performed. Evaluation experiments reveal that this interlingua-based retrieval model outperforms a direct translation approach."",","{hahn-etal-2005-subword,
    title = ""Subword Clusters as Light-Weight Interlingua for Multilingual Document Retrieval"",
    author = ""Hahn, Udo  and
      Marko, Kornel  and
      Schulz, Stefan"",
    booktitle = ""Proceedings of Machine Translation Summit X: Papers"",
    month = sep # "" 13-15"",
    year = ""2005"",
    address = ""Phuket, Thailand"",
    url = ""https://aclanthology.org/2005.mtsummit-papers.3"",
    pages = ""17--24"",
    abstract = ""We introduce a light-weight interlingua for a cross-language document retrieval system in the medical domain. It is composed of equivalence classes of semantically primitive, language-specific subwords which are clustered by interlingual and intralingual synonymy. Each subword cluster represents a basic conceptual entity of the language-independent interlingua. Documents, as well as queries, are mapped to this interlingua level on which retrieval operations are performed. Evaluation experiments reveal that this interlingua-based retrieval model outperforms a direct translation approach."",
}
@",medical domain,translat,semant,evalu
" ""Machine Translation on the Medical Domain: The Role of {BLEU}/{NIST} and {METEOR} in a Controlled Vocabulary Setting"","," ""The main objective of our project is to extract clinical information from thoracic radiology reports in Portuguese using Machine Translation (MT) and cross language information retrieval techniques. To accomplish this task we need to evaluate the involved machine translation system. Since human MT evaluation is costly and time consuming we opted to use automated methods. We propose an evaluation methodology using NIST/BLEU and METEOR algorithms and a controlled medical vocabulary, the Unified Medical Language System (UMLS). A set of documents are generated and they are either machine translated or used as evaluation references. This methodology is used to evaluate the performance of our specialized Portuguese-English translation dictionary. A significant improvement on evaluation scores after the dictionary incorporation into a commercial MT system is demonstrated. The use of UMLS and automated MT evaluation techniques can help the development of applications on the medical domain. Our methodology can also be used on general MT research for evaluating and testing purposes."",","{castilla-etal-2005-machine,
    title = ""Machine Translation on the Medical Domain: The Role of {BLEU}/{NIST} and {METEOR} in a Controlled Vocabulary Setting"",
    author = ""Castilla, Andre  and
      Bacic, Alice  and
      Furuie, Sergio"",
    booktitle = ""Proceedings of Machine Translation Summit X: Papers"",
    month = sep # "" 13-15"",
    year = ""2005"",
    address = ""Phuket, Thailand"",
    url = ""https://aclanthology.org/2005.mtsummit-papers.7"",
    pages = ""47--54"",
    abstract = ""The main objective of our project is to extract clinical information from thoracic radiology reports in Portuguese using Machine Translation (MT) and cross language information retrieval techniques. To accomplish this task we need to evaluate the involved machine translation system. Since human MT evaluation is costly and time consuming we opted to use automated methods. We propose an evaluation methodology using NIST/BLEU and METEOR algorithms and a controlled medical vocabulary, the Unified Medical Language System (UMLS). A set of documents are generated and they are either machine translated or used as evaluation references. This methodology is used to evaluate the performance of our specialized Portuguese-English translation dictionary. A significant improvement on evaluation scores after the dictionary incorporation into a commercial MT system is demonstrated. The use of UMLS and automated MT evaluation techniques can help the development of applications on the medical domain. Our methodology can also be used on general MT research for evaluating and testing purposes."",
}
@",medical domain,translat,generat,information retriev,evalu
" ""Linguistic representation of {F}innish in the medical domain spoken language translation system"","," ""This paper describes the development of Finnish linguistic resources for use in MedSLT, an Open Source medical domain speech-to-speech translation system. The paper describes the collection of medical Finnish corpora, the creation of a Finnish grammar by adapting the original English grammar, the composition of a domain specific Finnish lexicon and the definition of interlingua to Finnish mapping rules for multilingual translation. It is shown that Finnish can be effectively introduced into the existing MedSLT framework and that despite the differences between English and Finnish, the Finnish grammar can be created by manual adaptation from the original English grammar. Regarding further development, the initial evaluation results of English-Finnish speech-to-speech translation are encouraging."",","{santaholma-2005-linguistic-representation,
    title = ""Linguistic representation of {F}innish in the medical domain spoken language translation system"",
    author = ""Santaholma, Marianne"",
    editor = ""Hernandez, Nicolas  and
      Pitel, Guillaume"",
    booktitle = ""Actes de la 12{\`e}me conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues"",
    month = jun,
    year = ""2005"",
    address = ""Dourdan, France"",
    publisher = ""ATALA"",
    url = ""https://aclanthology.org/2005.jeptalnrecital-recital.9"",
    pages = ""605--614"",
    abstract = ""This paper describes the development of Finnish linguistic resources for use in MedSLT, an Open Source medical domain speech-to-speech translation system. The paper describes the collection of medical Finnish corpora, the creation of a Finnish grammar by adapting the original English grammar, the composition of a domain specific Finnish lexicon and the definition of interlingua to Finnish mapping rules for multilingual translation. It is shown that Finnish can be effectively introduced into the existing MedSLT framework and that despite the differences between English and Finnish, the Finnish grammar can be created by manual adaptation from the original English grammar. Regarding further development, the initial evaluation results of English-Finnish speech-to-speech translation are encouraging."",
}
@",medical domain,translat,evalu
" ""Distributed Modules for Text Annotation and {IE} Applied to the Biomedical Domain"",",,"{kirsch-rebholz-schuhmann-2004-distributed,
    title = ""Distributed Modules for Text Annotation and {IE} Applied to the Biomedical Domain"",
    author = ""Kirsch, Harald  and
      Rebholz-Schuhmann, Dietrich"",
    editor = ""Collier, Nigel  and
      Ruch, Patrick  and
      Nazarenko, Adeline"",
    booktitle = ""Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications ({NLPBA}/{B}io{NLP})"",
    month = aug # "" 28th and 29th"",
    year = ""2004"",
    address = ""Geneva, Switzerland"",
    publisher = ""COLING"",
    url = ""https://aclanthology.org/W04-1208"",
    pages = ""50--56"",
}
@",medical domain,natural language process,natural languag,language process,nlp,annotat
" ""Automatically Selecting Domain Markers for Terminology Extraction"","," ""Some approaches to automatic terminology extraction from corpora imply the use of existing semantic resources for guiding the detection of terms. Most of these systems exploit specialised resources, like UMLS in the medical domain, while a few try to take profit from general-purpose semantic resources, like EuroWordNet (EWN). As the term extraction task is clearly domain depending, in the case a general-purpose resource without specific domain information is used, we need a way of attaching domain information to the units of the resource. For big resources it is desirable that this semantic enrichment could be carried out automatically. Given a specific domain, our proposal aims to detect in EWN those units that can be considered as domain markers (DM). We can define a DM as an EWN entry whose attached strings belong to the domain, as well as the variants of all its descendents through the hyponymy relation. The procedure we propose in this paper is fully automatic and, a priori, domain-independent. The only external knowledge it uses is a set of terms, which is an external vocabulary, which is considered to have at least one sense belonging to the domain."",","{vivaldi-rodriguez-2004-automatically,
    title = ""Automatically Selecting Domain Markers for Terminology Extraction"",
    author = ""Vivaldi, Jorge  and
      Rodr{\'\i}guez, Horacio"",
    editor = ""Lino, Maria Teresa  and
      Xavier, Maria Francisca  and
      Ferreira, F{\'a}tima  and
      Costa, Rute  and
      Silva, Raquel"",
    booktitle = ""Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04)"",
    month = may,
    year = ""2004"",
    address = ""Lisbon, Portugal"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2004/pdf/348.pdf"",
    abstract = ""Some approaches to automatic terminology extraction from corpora imply the use of existing semantic resources for guiding the detection of terms. Most of these systems exploit specialised resources, like UMLS in the medical domain, while a few try to take profit from general-purpose semantic resources, like EuroWordNet (EWN). As the term extraction task is clearly domain depending, in the case a general-purpose resource without specific domain information is used, we need a way of attaching domain information to the units of the resource. For big resources it is desirable that this semantic enrichment could be carried out automatically. Given a specific domain, our proposal aims to detect in EWN those units that can be considered as domain markers (DM). We can define a DM as an EWN entry whose attached strings belong to the domain, as well as the variants of all its descendents through the hyponymy relation. The procedure we propose in this paper is fully automatic and, a priori, domain-independent. The only external knowledge it uses is a set of terms, which is an external vocabulary, which is considered to have at least one sense belonging to the domain."",
}
@",medical domain,semant,evalu
" ""Annotation of Coreference Relations Among Linguistic Expressions and Images in Biological Articles"","," ""In this paper, we propose an annotation scheme which can be used not only for annotating coreference relations between linguistic expressions, but also those among linguistic expressions and images, in scientific texts such as biomedical articles. Images in biomedical domain often contain important information for analyses and diagnoses, and we consider that linking images to textual descriptions of their semantic contents in terms of coreference relations is useful for multimodal access to the information. We present our annotation scheme and the concept of a ``coreference pool,'' which plays a central role in the scheme. We also introduce a support tool for text annotation named Open Ontology Forge which we have already developed, and additional functions for the software to cover image annotations (ImageOF) which is now being developed."",","{kawazoe-etal-2004-annotation,
    title = ""Annotation of Coreference Relations Among Linguistic Expressions and Images in Biological Articles"",
    author = ""Kawazoe, Ai  and
      Kitamoto, Asanobu  and
      Collier, Nigel"",
    editor = ""Lino, Maria Teresa  and
      Xavier, Maria Francisca  and
      Ferreira, F{\'a}tima  and
      Costa, Rute  and
      Silva, Raquel"",
    booktitle = ""Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04)"",
    month = may,
    year = ""2004"",
    address = ""Lisbon, Portugal"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2004/pdf/431.pdf"",
    abstract = ""In this paper, we propose an annotation scheme which can be used not only for annotating coreference relations between linguistic expressions, but also those among linguistic expressions and images, in scientific texts such as biomedical articles. Images in biomedical domain often contain important information for analyses and diagnoses, and we consider that linking images to textual descriptions of their semantic contents in terms of coreference relations is useful for multimodal access to the information. We present our annotation scheme and the concept of a ``coreference pool,'' which plays a central role in the scheme. We also introduce a support tool for text annotation named Open Ontology Forge which we have already developed, and additional functions for the software to cover image annotations (ImageOF) which is now being developed."",
}
@",medical domain,semant,annotat,evalu
" ""Evaluation Resources for Concept-based Cross-Lingual Information Retrieval in the Medical Domain"",",,"{buitelaar-etal-2004-evaluation,
    title = ""Evaluation Resources for Concept-based Cross-Lingual Information Retrieval in the Medical Domain"",
    author = ""Buitelaar, Paul  and
      Steffen, Diana  and
      Volk, Martin  and
      Widdows, Dominic  and
      Sacaleanu, Bogdan  and
      Vintar, {\v{S}}pela  and
      Peters, Stanley  and
      Uszkoreit, Hans"",
    editor = ""Lino, Maria Teresa  and
      Xavier, Maria Francisca  and
      Ferreira, F{\'a}tima  and
      Costa, Rute  and
      Silva, Raquel"",
    booktitle = ""Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04)"",
    month = may,
    year = ""2004"",
    address = ""Lisbon, Portugal"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2004/pdf/548.pdf"",
}
@",medical domain,information retriev,evalu
" ""Proceedings of the {ACL}-02 Workshop on Natural Language Processing in the Biomedical Domain"",",,"{lin-hovy-2002-manual,
    title = ""Manual and automatic evaluation of summaries"",
    author = ""Lin, Chin-Yew  and
      Hovy, Eduard"",
    booktitle = ""Proceedings of the {ACL}-02 Workshop on Automatic Summarization"",
    month = jul,
    year = ""2002"",
    address = ""Phildadelphia, Pennsylvania, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W02-0406"",
    doi = ""10.3115/1118162.1118168"",
    pages = ""45--51"",
}
@proceedings{ws-2002-acl-02,
    title = ""Proceedings of the {ACL}-02 Workshop on Natural Language Processing in the Biomedical Domain"",
    month = jul,
    year = ""2002"",
    address = ""Phildadelphia, Pennsylvania, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W02-0300"",
}
@",medical domain,natural language process,natural languag,language process,summar,evalu
